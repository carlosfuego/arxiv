[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.09072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v1",
                "updated": "2025-08-12T16:47:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Yi Fei"
                    },
                    {
                        "name": "Zeng Weidi"
                    }
                ],
                "author_detail": {
                    "name": "Zeng Weidi"
                },
                "author": "Zeng Weidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09001v1",
                "updated": "2025-08-12T15:11:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:11:47Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "title": "Retrospective Sparse Attention for Efficient Long-Context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrospective Sparse Attention for Efficient Long-Context Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%."
                },
                "authors": [
                    {
                        "name": "Seonghwan Choi"
                    },
                    {
                        "name": "Beomseok Kang"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08978v1",
                "updated": "2025-08-12T14:40:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:40:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "TaoCache: Structure-Maintained Video Generation Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaoCache: Structure-Maintained Video Generation Acceleration"
                },
                "summary": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups."
                },
                "authors": [
                    {
                        "name": "Zhentao Fan"
                    },
                    {
                        "name": "Zongzuo Wang"
                    },
                    {
                        "name": "Weiwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weiwei Zhang"
                },
                "author": "Weiwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08895v1",
                "updated": "2025-08-12T12:35:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T12:35:55Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs"
                },
                "summary": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Zhifeng Shen"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Haoqian Wu"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11488v2",
                "updated": "2025-08-12T10:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    43,
                    55,
                    1,
                    224,
                    0
                ],
                "published": "2023-11-30T16:02:04Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    16,
                    2,
                    4,
                    3,
                    334,
                    0
                ],
                "title": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows"
                },
                "summary": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes."
                },
                "authors": [
                    {
                        "name": "Thiago Garrett"
                    },
                    {
                        "name": "Weijia Song"
                    },
                    {
                        "name": "Roman Vitenberg"
                    },
                    {
                        "name": "Ken Birman"
                    }
                ],
                "author_detail": {
                    "name": "Ken Birman"
                },
                "author": "Ken Birman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08744v2",
                "updated": "2025-08-13T01:39:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    3,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T08:39:32Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    39,
                    32,
                    1,
                    224,
                    0
                ],
                "title": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality."
                },
                "authors": [
                    {
                        "name": "Zhonggen Li"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Bocheng Yu"
                    },
                    {
                        "name": "Baihua Zheng"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v3",
                "updated": "2025-08-12T05:51:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    5,
                    51,
                    37,
                    1,
                    224,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08601v1",
                "updated": "2025-08-12T03:34:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    34,
                    21,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T03:34:21Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    34,
                    21,
                    1,
                    224,
                    0
                ],
                "title": "Yan: Foundational Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yan: Foundational Interactive Video Generation"
                },
                "summary": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/."
                },
                "authors": [
                    {
                        "name": "Yan Team"
                    }
                ],
                "author_detail": {
                    "name": "Yan Team"
                },
                "author": "Yan Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08600v1",
                "updated": "2025-08-12T03:33:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T03:33:15Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "title": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes"
                },
                "summary": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased."
                },
                "authors": [
                    {
                        "name": "Timur V. Tscherbul"
                    },
                    {
                        "name": "Roman V. Krems"
                    }
                ],
                "author_detail": {
                    "name": "Roman V. Krems"
                },
                "author": "Roman V. Krems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07675v2",
                "updated": "2025-08-12T02:51:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    51,
                    12,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T06:53:27Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    6,
                    53,
                    27,
                    0,
                    223,
                    0
                ],
                "title": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Baran Atalar"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08134v2",
                "updated": "2025-08-12T02:27:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    27,
                    5,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T16:10:00Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    0,
                    0,
                    223,
                    0
                ],
                "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control"
                },
                "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement."
                },
                "authors": [
                    {
                        "name": "Zeqian Long"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Xinhua Zhang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma",
                "arxiv_comment": "Project webpage is available at https://follow-your-shape.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08531v1",
                "updated": "2025-08-12T00:06:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    0,
                    6,
                    34,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T00:06:34Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    0,
                    6,
                    34,
                    1,
                    224,
                    0
                ],
                "title": "Profiling Large Language Model Inference on Apple Silicon: A\n  Quantization Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profiling Large Language Model Inference on Apple Silicon: A\n  Quantization Perspective"
                },
                "summary": "A systematic understanding of Apple Silicon is lacking in the current\nlandscape of hardware efficiency; research focus is largely centered on\naccelerating GPUs for large-scale training or inference on CUDA devices. This\npaper investigates Apple Silicon's unique memory architecture that offers a\nunified memory integrating CPU and GPU memory and its implications for\non-device LLM inference.\n  We decipher myths about whether Apple Silicon is efficient for on-device\ninference compared to competitors such as NVIDIA GPUs by directly conducting\nlatency and throughput comparison benchmarks. We explain the performance gap\nbetween them through profiling low level hardware metrics - ALU utilization,\nmemory bandwidth, buffer usage, cache residency etc. at runtime. We draw\nseveral insights regarding performance bottlenecks such as dequantization\noverhead, compute throughput and memory bandwidth. We debunk existing false\nclaims regarding large language model inference such as compressing models to\nlower bit precision is a defacto promise for faster inference across all\nhardware platforms. We find that the large unified memory enables Apple Silicon\nto be both cost effective and efficient against NVIDIA GPUs for ultra large\nlanguage models.\n  Our large scale evaluation on 5 hardware testbeds incorporating three Apple\nM-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX\nA6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from\n8B to 405B parameters and 14 quantization schemes gives an understanding of how\nApple Silicon fits within the paradigm of on-device LLM inference. Our analysis\nreveals multiple resource interdependencies and unexpected findings, while also\nquantifying established insights. To the best of our knowledge, this study\nmakes the first attempt to present a thorough characterization and analysis of\nApple Silicon for on-device inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A systematic understanding of Apple Silicon is lacking in the current\nlandscape of hardware efficiency; research focus is largely centered on\naccelerating GPUs for large-scale training or inference on CUDA devices. This\npaper investigates Apple Silicon's unique memory architecture that offers a\nunified memory integrating CPU and GPU memory and its implications for\non-device LLM inference.\n  We decipher myths about whether Apple Silicon is efficient for on-device\ninference compared to competitors such as NVIDIA GPUs by directly conducting\nlatency and throughput comparison benchmarks. We explain the performance gap\nbetween them through profiling low level hardware metrics - ALU utilization,\nmemory bandwidth, buffer usage, cache residency etc. at runtime. We draw\nseveral insights regarding performance bottlenecks such as dequantization\noverhead, compute throughput and memory bandwidth. We debunk existing false\nclaims regarding large language model inference such as compressing models to\nlower bit precision is a defacto promise for faster inference across all\nhardware platforms. We find that the large unified memory enables Apple Silicon\nto be both cost effective and efficient against NVIDIA GPUs for ultra large\nlanguage models.\n  Our large scale evaluation on 5 hardware testbeds incorporating three Apple\nM-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX\nA6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from\n8B to 405B parameters and 14 quantization schemes gives an understanding of how\nApple Silicon fits within the paradigm of on-device LLM inference. Our analysis\nreveals multiple resource interdependencies and unexpected findings, while also\nquantifying established insights. To the best of our knowledge, this study\nmakes the first attempt to present a thorough characterization and analysis of\nApple Silicon for on-device inference."
                },
                "authors": [
                    {
                        "name": "Afsara Benazir"
                    },
                    {
                        "name": "Felix Xiaozhu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Felix Xiaozhu Lin"
                },
                "author": "Felix Xiaozhu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08457v1",
                "updated": "2025-08-11T20:30:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    20,
                    30,
                    31,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T20:30:31Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    20,
                    30,
                    31,
                    0,
                    223,
                    0
                ],
                "title": "Architecting Long-Context LLM Acceleration with Packing-Prefetch\n  Scheduler and Ultra-Large Capacity On-Chip Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecting Long-Context LLM Acceleration with Packing-Prefetch\n  Scheduler and Ultra-Large Capacity On-Chip Memories"
                },
                "summary": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Hanchen Yang"
                    },
                    {
                        "name": "Muhammed Ahosan Ul Karim"
                    },
                    {
                        "name": "Harsono Simka"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "7 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.3; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08438v1",
                "updated": "2025-08-11T19:55:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    19,
                    55,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T19:55:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    19,
                    55,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM\n  Inference"
                },
                "summary": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Zecheng Lin"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Jianchang Su"
                    },
                    {
                        "name": "Cheng Chu"
                    },
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Wenhui Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "17 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08081v1",
                "updated": "2025-08-11T15:28:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:28:28Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "title": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures"
                },
                "summary": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29."
                },
                "authors": [
                    {
                        "name": "Florian Naef"
                    },
                    {
                        "name": "Thomas Willwacher"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Willwacher"
                },
                "author": "Thomas Willwacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.QA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v2",
                "updated": "2025-08-11T14:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    15,
                    27,
                    0,
                    223,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "15 pages, 14 figures; Accepted by ICCV2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v1",
                "updated": "2025-08-11T10:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical\n  Approach for Multi-Tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical\n  Approach for Multi-Tenant LLM Serving"
                },
                "summary": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Under review for a computer science conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07811v1",
                "updated": "2025-08-11T09:54:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T09:54:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration"
                },
                "summary": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions."
                },
                "authors": [
                    {
                        "name": "Sicheng Gao"
                    },
                    {
                        "name": "Nancy Mehta"
                    },
                    {
                        "name": "Zongwei Wu"
                    },
                    {
                        "name": "Radu Timofte"
                    }
                ],
                "author_detail": {
                    "name": "Radu Timofte"
                },
                "author": "Radu Timofte",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v2",
                "updated": "2025-08-11T08:10:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    10,
                    21,
                    0,
                    223,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v4",
                "updated": "2025-08-11T06:16:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    6,
                    16,
                    52,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07570v1",
                "updated": "2025-08-11T03:03:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T03:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios."
                },
                "authors": [
                    {
                        "name": "Khanh-Binh Nguyen"
                    },
                    {
                        "name": "Phuoc-Nguyen Bui"
                    },
                    {
                        "name": "Hyunseung Choo"
                    },
                    {
                        "name": "Duc Thanh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Duc Thanh Nguyen"
                },
                "author": "Duc Thanh Nguyen",
                "arxiv_comment": "12 pages, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v2",
                "updated": "2025-08-09T11:31:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    31,
                    44,
                    5,
                    221,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06937v1",
                "updated": "2025-08-09T11:06:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-09T11:06:58Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing"
                },
                "summary": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods."
                },
                "authors": [
                    {
                        "name": "Weiyan Xie"
                    },
                    {
                        "name": "Han Gao"
                    },
                    {
                        "name": "Didan Deng"
                    },
                    {
                        "name": "Kaican Li"
                    },
                    {
                        "name": "April Hua Liu"
                    },
                    {
                        "name": "Yongxiang Huang"
                    },
                    {
                        "name": "Nevin L. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Nevin L. Zhang"
                },
                "author": "Nevin L. Zhang",
                "arxiv_comment": "Project Page: vaynexie.github.io/CannyEdit/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03974v2",
                "updated": "2025-08-09T02:33:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    2,
                    33,
                    21,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-05T23:47:34Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    23,
                    47,
                    34,
                    1,
                    217,
                    0
                ],
                "title": "Managing Data for Scalable and Interactive Event Sequence Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing Data for Scalable and Interactive Event Sequence Visualization"
                },
                "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization."
                },
                "authors": [
                    {
                        "name": "Sayef Azad Sakin"
                    },
                    {
                        "name": "Katherine E. Isaacs"
                    }
                ],
                "author_detail": {
                    "name": "Katherine E. Isaacs"
                },
                "author": "Katherine E. Isaacs",
                "arxiv_comment": "The 15th IEEE Workshop on Large Data Analysis and Visualization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v2",
                "updated": "2025-08-09T00:12:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    0,
                    12,
                    1,
                    5,
                    221,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v3",
                "updated": "2025-08-08T18:16:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    18,
                    16,
                    33,
                    4,
                    220,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06447v1",
                "updated": "2025-08-08T16:42:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:42:38Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning"
                },
                "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Rubing Yang"
                    },
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Desheng Hui"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Jianlei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianlei Yang"
                },
                "author": "Jianlei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v3",
                "updated": "2025-08-08T14:25:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    25,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06297v1",
                "updated": "2025-08-08T13:19:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:19:30Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "title": "KV Cache Compression for Inference Efficiency in LLMs: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression for Inference Efficiency in LLMs: A Review"
                },
                "summary": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanyu Liu"
                    },
                    {
                        "name": "Jingying Fu"
                    },
                    {
                        "name": "Sixiang Liu"
                    },
                    {
                        "name": "Yitian Zou"
                    },
                    {
                        "name": "You Fu"
                    },
                    {
                        "name": "Jiehan Zhou"
                    },
                    {
                        "name": "Shouhua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shouhua Zhang"
                },
                "arxiv_affiliation": "University of Oulu",
                "author": "Shouhua Zhang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06160v1",
                "updated": "2025-08-08T09:29:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:29:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment"
                },
                "summary": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff."
                },
                "authors": [
                    {
                        "name": "Zhenbang Du"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Lifu Wang"
                    },
                    {
                        "name": "Jiayi Qian"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v1",
                "updated": "2025-08-08T08:54:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06064v1",
                "updated": "2025-08-08T06:53:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T06:53:50Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "title": "A Generic Complete Anytime Beam Search for Optimal Decision Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generic Complete Anytime Beam Search for Optimal Decision Tree"
                },
                "summary": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees."
                },
                "authors": [
                    {
                        "name": "Harold Silvère Kiossou"
                    },
                    {
                        "name": "Siegfried Nijssen"
                    },
                    {
                        "name": "Pierre Schaus"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Schaus"
                },
                "author": "Pierre Schaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2104.13123v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2104.13123v3",
                "updated": "2025-08-08T06:38:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    38,
                    1,
                    4,
                    220,
                    0
                ],
                "published": "2021-04-27T11:55:54Z",
                "published_parsed": [
                    2021,
                    4,
                    27,
                    11,
                    55,
                    54,
                    1,
                    117,
                    0
                ],
                "title": "Affine Springer fibers and depth zero L-packets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affine Springer fibers and depth zero L-packets"
                },
                "summary": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable."
                },
                "authors": [
                    {
                        "name": "Roman Bezrukavnikov"
                    },
                    {
                        "name": "Yakov Varshavsky"
                    }
                ],
                "author_detail": {
                    "name": "Yakov Varshavsky"
                },
                "author": "Yakov Varshavsky",
                "arxiv_comment": "v.3, 96 pages, minor changes in abstract and introduction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2104.13123v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2104.13123v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "22E50, 22E57",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05904v1",
                "updated": "2025-08-07T23:53:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:53:31Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "title": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data"
                },
                "summary": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks."
                },
                "authors": [
                    {
                        "name": "Brandon Baker"
                    },
                    {
                        "name": "Elliott Brossard"
                    },
                    {
                        "name": "Chenwei Xie"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Deen Liu"
                    },
                    {
                        "name": "Yijun Xie"
                    },
                    {
                        "name": "Arthur Zwiegincew"
                    },
                    {
                        "name": "Nitya Kumar Sharma"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Eugene Retunsky"
                    },
                    {
                        "name": "Mike Halcrow"
                    },
                    {
                        "name": "Derek Denny-Brown"
                    },
                    {
                        "name": "Istvan Cseri"
                    },
                    {
                        "name": "Tyler Akidau"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "arxiv_comment": "12 pages, 6 figures, accepted in ICDCS 2025",
                "arxiv_journal_ref": "Proc. 45th IEEE International Conference on Distributed Computing\n  Systems (ICDCS), Glasgow, UK, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05898v1",
                "updated": "2025-08-07T23:11:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:11:33Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "title": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates"
                },
                "summary": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA."
                },
                "authors": [
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Aijun An"
                    },
                    {
                        "name": "Ali cheraghian"
                    }
                ],
                "author_detail": {
                    "name": "Ali cheraghian"
                },
                "author": "Ali cheraghian",
                "arxiv_comment": "BMVC2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v1",
                "updated": "2025-08-07T09:47:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05091v1",
                "updated": "2025-08-07T07:19:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T07:19:02Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "title": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation"
                },
                "summary": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration."
                },
                "authors": [
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Busheng Su"
                    },
                    {
                        "name": "Finn Wong"
                    }
                ],
                "author_detail": {
                    "name": "Finn Wong"
                },
                "author": "Finn Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05012v1",
                "updated": "2025-08-07T03:49:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T03:49:56Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "title": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines"
                },
                "summary": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion."
                },
                "authors": [
                    {
                        "name": "Ugur Cetintemel"
                    },
                    {
                        "name": "Shu Chen"
                    },
                    {
                        "name": "Alexander W. Lee"
                    },
                    {
                        "name": "Deepti Raghavan"
                    }
                ],
                "author_detail": {
                    "name": "Deepti Raghavan"
                },
                "author": "Deepti Raghavan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v2",
                "updated": "2025-08-06T16:57:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    57,
                    39,
                    2,
                    218,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Zhengming Zhang"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Accepted by ICCV 2025; Code released at\n  https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04581v1",
                "updated": "2025-08-06T16:06:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:06:43Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning"
                },
                "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance."
                },
                "authors": [
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    }
                ],
                "author_detail": {
                    "name": "Stamatios Lefkimmiatis"
                },
                "author": "Stamatios Lefkimmiatis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v3",
                "updated": "2025-08-06T15:38:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    38,
                    6,
                    2,
                    218,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads"
                },
                "summary": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Ted Hart"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "Proceedings of the VLDB Endowment 18 (VLDB'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v1",
                "updated": "2025-08-06T14:02:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference"
                },
                "summary": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v3",
                "updated": "2025-08-06T11:46:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    46,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "arxiv_comment": "Submission under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04257v1",
                "updated": "2025-08-06T09:40:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T09:40:09Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs"
                },
                "summary": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "arxiv_comment": "Published as a conference paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00370v2",
                "updated": "2025-08-06T08:32:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    8,
                    32,
                    53,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-01T07:03:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices"
                },
                "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Poh Seng Lim"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "JungHau Foo"
                    },
                    {
                        "name": "Yap Deep"
                    },
                    {
                        "name": "Timothy Lee Jun Jie"
                    },
                    {
                        "name": "Kelvin Teh Kae Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Danyu Feng"
                    },
                    {
                        "name": "Hao-Yun Chen"
                    },
                    {
                        "name": "Peng-Wen Chen"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wong Wai Mun"
                    }
                ],
                "author_detail": {
                    "name": "Wong Wai Mun"
                },
                "author": "Wong Wai Mun",
                "arxiv_comment": "The data and method in the paper need to be re-audited",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03837v1",
                "updated": "2025-08-05T18:34:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T18:34:48Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "title": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems"
                },
                "summary": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs."
                },
                "authors": [
                    {
                        "name": "Davide Zoni"
                    },
                    {
                        "name": "Andrea Galimberti"
                    },
                    {
                        "name": "Adriano Guarisco"
                    }
                ],
                "author_detail": {
                    "name": "Adriano Guarisco"
                },
                "author": "Adriano Guarisco",
                "arxiv_comment": "9 pages, 13 figures, 1 table, accepted for presentation at 2025\n  International Conference on Computer-Aided Design (ICCAD), Munich, Germany,\n  October 26-30, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v2",
                "updated": "2025-08-05T16:17:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    16,
                    17,
                    1,
                    1,
                    217,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03321v1",
                "updated": "2025-08-05T11:00:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T11:00:41Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "title": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios"
                },
                "summary": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS."
                },
                "authors": [
                    {
                        "name": "Jörn Bodenhausen"
                    },
                    {
                        "name": "Simon Mangel"
                    },
                    {
                        "name": "Thomas Vogt"
                    },
                    {
                        "name": "Martin Henze"
                    }
                ],
                "author_detail": {
                    "name": "Martin Henze"
                },
                "author": "Martin Henze",
                "arxiv_comment": "Accepted for publication in Proceedings of the 2025 IEEE 50th\n  Conference on Local Computer Networks (LCN)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03258v1",
                "updated": "2025-08-05T09:35:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T09:35:52Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "title": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%."
                },
                "authors": [
                    {
                        "name": "Yueyue Liu"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Yuantian Miao"
                    }
                ],
                "author_detail": {
                    "name": "Yuantian Miao"
                },
                "author": "Yuantian Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02240v2",
                "updated": "2025-08-05T02:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    2,
                    13,
                    39,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T09:39:31Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    39,
                    31,
                    0,
                    216,
                    0
                ],
                "title": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}"
                },
                "authors": [
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Jiaxing Yan"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Zetao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v5",
                "updated": "2025-08-05T00:25:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    0,
                    25,
                    53,
                    1,
                    217,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition"
                },
                "summary": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "Philip Levis"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v1",
                "updated": "2025-08-04T16:14:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02401v1",
                "updated": "2025-08-04T13:26:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:26:16Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git."
                },
                "authors": [
                    {
                        "name": "Xiaolin Lin"
                    },
                    {
                        "name": "Jingcun Wang"
                    },
                    {
                        "name": "Olga Kondrateva"
                    },
                    {
                        "name": "Yiyu Shi"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Grace Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Grace Li Zhang"
                },
                "author": "Grace Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02280v1",
                "updated": "2025-08-04T10:51:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T10:51:20Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "title": "OnPair: Short Strings Compression for Fast Random Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnPair: Short Strings Compression for Fast Random Access"
                },
                "summary": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage."
                },
                "authors": [
                    {
                        "name": "Francesco Gargiulo"
                    },
                    {
                        "name": "Rossano Venturini"
                    }
                ],
                "author_detail": {
                    "name": "Rossano Venturini"
                },
                "author": "Rossano Venturini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; E.4; H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02215v1",
                "updated": "2025-08-04T09:08:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:08:43Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"
                },
                "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK."
                },
                "authors": [
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Zhiyuan He"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19906v2",
                "updated": "2025-08-04T08:19:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    8,
                    19,
                    26,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-26T10:34:53Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "title": "CaliDrop: KV Cache Compression with Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaliDrop: KV Cache Compression with Calibration"
                },
                "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v2",
                "updated": "2025-08-04T04:48:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    4,
                    48,
                    41,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02930v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02930v4",
                "updated": "2025-08-04T02:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    47,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2024-07-03T09:02:05Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    2,
                    5,
                    2,
                    185,
                    0
                ],
                "title": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs"
                },
                "summary": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Kun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yang"
                },
                "author": "Kun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02930v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02930v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v2",
                "updated": "2025-08-04T02:17:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    17,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "14 pages, 7 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01898v1",
                "updated": "2025-08-03T19:16:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T19:16:40Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "title": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution"
                },
                "summary": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Md-Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in the IEEE Transactions on\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v1",
                "updated": "2025-08-03T18:15:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Linxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Boqian Wang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16607v2",
                "updated": "2025-08-03T10:27:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    10,
                    27,
                    19,
                    6,
                    215,
                    0
                ],
                "published": "2025-01-28T00:52:23Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    52,
                    23,
                    1,
                    28,
                    0
                ],
                "title": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search"
                },
                "summary": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy."
                },
                "authors": [
                    {
                        "name": "Shuozhi Yuan"
                    },
                    {
                        "name": "Limin Chen"
                    },
                    {
                        "name": "Miaomiao Yuan"
                    },
                    {
                        "name": "Jin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Zhao"
                },
                "author": "Jin Zhao",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02751v1",
                "updated": "2025-08-03T09:15:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T09:15:36Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference"
                },
                "summary": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Yajuan Peng"
                    },
                    {
                        "name": "Cam-Tu Nguyen"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Xiaoming Fu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Fu"
                },
                "author": "Xiaoming Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19718v2",
                "updated": "2025-08-02T23:59:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    23,
                    59,
                    11,
                    5,
                    214,
                    0
                ],
                "published": "2025-07-25T23:55:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting"
                },
                "summary": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency."
                },
                "authors": [
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Hamid Gadirov"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v1",
                "updated": "2025-08-02T21:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gaëtan Hadjeres"
                    },
                    {
                        "name": "Gaël Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_comment": "Accepted to the Transactions of the International Society for Music\n  Information Retrieval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01298v1",
                "updated": "2025-08-02T10:12:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T10:12:45Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "title": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access"
                },
                "summary": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations."
                },
                "authors": [
                    {
                        "name": "Azadeh Sadat Miraftab"
                    },
                    {
                        "name": "Ahmadreza Montazerolghaem"
                    },
                    {
                        "name": "Behrad Mahboobi"
                    }
                ],
                "author_detail": {
                    "name": "Behrad Mahboobi"
                },
                "author": "Behrad Mahboobi",
                "arxiv_doi": "10.1007/s10586-025-05256-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-025-05256-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01261v1",
                "updated": "2025-08-02T08:33:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T08:33:30Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "title": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models"
                },
                "summary": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v2",
                "updated": "2025-08-02T06:50:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    50,
                    59,
                    5,
                    214,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "To appear in ASPLOS'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v1",
                "updated": "2025-08-02T06:43:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance."
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06526v1",
                "updated": "2025-08-02T03:50:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    3,
                    50,
                    14,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T03:50:14Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    3,
                    50,
                    14,
                    5,
                    214,
                    0
                ],
                "title": "PiKV: KV Cache Management System for Mixture of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiKV: KV Cache Management System for Mixture of Experts"
                },
                "summary": "As large language models continue to scale up in both size and context\nlength, the memory and communication cost of key-value (KV) cache storage has\nbecome a major bottleneck in multi-GPU and multi-node inference. While\nMoE-based architectures sparsify computation across experts, the corresponding\nKV caches remain dense and globally synchronized, resulting in significant\noverhead.\n  We introduce \\textbf{PiKV}, a parallel and distributed KV cache serving\nframework tailored for MoE architecture. PiKV leverages \\textit{expert-sharded\nKV storage} to partition caches across GPUs, \\textit{PiKV routing} to reduce\ntoken-to-KV access, and a \\textit{PiKV Scheduling} to adaptively retain\nquery-relevant entries. To further reduce memory usage, PiKV integrates\n\\textit{PiKV Compression} modules the caching pipeline for acceleration.\n  PiKV is recently publicly available as an open-source software library:\n\\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.\nExperiments details is recorded at:\n\\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\\_Results}.\nWe also have PiKV integrated with Nvidia kvpress for acceleration, details see\n\\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.\nPiKV is still a living project, aiming to become a comprehesive KV Cache\nmanagement system for MoE Architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models continue to scale up in both size and context\nlength, the memory and communication cost of key-value (KV) cache storage has\nbecome a major bottleneck in multi-GPU and multi-node inference. While\nMoE-based architectures sparsify computation across experts, the corresponding\nKV caches remain dense and globally synchronized, resulting in significant\noverhead.\n  We introduce \\textbf{PiKV}, a parallel and distributed KV cache serving\nframework tailored for MoE architecture. PiKV leverages \\textit{expert-sharded\nKV storage} to partition caches across GPUs, \\textit{PiKV routing} to reduce\ntoken-to-KV access, and a \\textit{PiKV Scheduling} to adaptively retain\nquery-relevant entries. To further reduce memory usage, PiKV integrates\n\\textit{PiKV Compression} modules the caching pipeline for acceleration.\n  PiKV is recently publicly available as an open-source software library:\n\\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.\nExperiments details is recorded at:\n\\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\\_Results}.\nWe also have PiKV integrated with Nvidia kvpress for acceleration, details see\n\\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.\nPiKV is still a living project, aiming to become a comprehesive KV Cache\nmanagement system for MoE Architectures."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    },
                    {
                        "name": "Xuhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xuhong Wang"
                },
                "author": "Xuhong Wang",
                "arxiv_comment": "Accepted to ICML ES-MoFo III WorkShop Paper Link:\n  https://openreview.net/pdf?id=hHoK1kBPd9 Github Link:\n  https://github.com/NoakLiu/PiKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v2",
                "updated": "2025-08-02T00:31:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    0,
                    31,
                    18,
                    5,
                    214,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01051v1",
                "updated": "2025-08-01T20:08:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T20:08:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "QPP-RNG: A Conceptual Quantum System for True Randomness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPP-RNG: A Conceptual Quantum System for True Randomness"
                },
                "summary": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems."
                },
                "authors": [
                    {
                        "name": "Randy Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Randy Kuang"
                },
                "author": "Randy Kuang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00647v1",
                "updated": "2025-08-01T14:05:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:05:44Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "title": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)"
                },
                "summary": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions."
                },
                "authors": [
                    {
                        "name": "Alessandro Lacerenza"
                    },
                    {
                        "name": "Alda Rubini"
                    },
                    {
                        "name": "Andrea Alimenti"
                    },
                    {
                        "name": "Sergio Fabiani"
                    },
                    {
                        "name": "Ettore Del Monte"
                    },
                    {
                        "name": "Riccardo Campana"
                    },
                    {
                        "name": "Mauro Centrone"
                    },
                    {
                        "name": "Enrico Costa"
                    },
                    {
                        "name": "Nicolas De Angelis"
                    },
                    {
                        "name": "Giovanni De Cesare"
                    },
                    {
                        "name": "Sergio Di Cosimo"
                    },
                    {
                        "name": "Giuseppe Di Persio"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Pasqualino Loffredo"
                    },
                    {
                        "name": "Giovanni Lombardi"
                    },
                    {
                        "name": "Gabriele Minervini"
                    },
                    {
                        "name": "Fabio Muleri"
                    },
                    {
                        "name": "Paolo Romano"
                    },
                    {
                        "name": "Emanuele Scalise"
                    },
                    {
                        "name": "Enrico Silva"
                    },
                    {
                        "name": "Paolo Soffitta"
                    },
                    {
                        "name": "Davide Albanesi"
                    },
                    {
                        "name": "Ilaria Baffo"
                    },
                    {
                        "name": "Daniele Brienza"
                    },
                    {
                        "name": "Valerio Campamaggiore"
                    },
                    {
                        "name": "Giovanni Cucinella"
                    },
                    {
                        "name": "Andrea Curatolo"
                    },
                    {
                        "name": "Giulia de Iulis"
                    },
                    {
                        "name": "Andrea Del Re"
                    },
                    {
                        "name": "Vito Di Bari"
                    },
                    {
                        "name": "Simone Di Filippo"
                    },
                    {
                        "name": "Immacolata Donnarumma"
                    },
                    {
                        "name": "Pierluigi Fanelli"
                    },
                    {
                        "name": "Nicolas Gagliardi"
                    },
                    {
                        "name": "Paolo Leonetti"
                    },
                    {
                        "name": "Matteo Merge"
                    },
                    {
                        "name": "Dario Modenini"
                    },
                    {
                        "name": "Andrea Negri"
                    },
                    {
                        "name": "Daniele Pecorella"
                    },
                    {
                        "name": "Massimo Perelli"
                    },
                    {
                        "name": "Alice Ponti"
                    },
                    {
                        "name": "Francesca Sbop"
                    },
                    {
                        "name": "Paolo Tortora"
                    },
                    {
                        "name": "Alessandro Turchi"
                    },
                    {
                        "name": "Valerio Vagelli"
                    },
                    {
                        "name": "Emanuele Zaccagnino"
                    },
                    {
                        "name": "Alessandro Zambardi"
                    },
                    {
                        "name": "Costantino Zazza"
                    }
                ],
                "author_detail": {
                    "name": "Costantino Zazza"
                },
                "author": "Costantino Zazza",
                "arxiv_comment": "6 pages, 2 figures, SPIE Optics+Photonics 2025 proceeding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00629v1",
                "updated": "2025-08-01T13:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:40:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach"
                },
                "summary": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks"
                },
                "authors": [
                    {
                        "name": "Francisco Crespo"
                    },
                    {
                        "name": "Javier Villegas"
                    },
                    {
                        "name": "Carlos Baena"
                    },
                    {
                        "name": "Eduardo Baena"
                    },
                    {
                        "name": "Sergio Fortes"
                    },
                    {
                        "name": "Raquel Barco"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Barco"
                },
                "author": "Raquel Barco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00616v1",
                "updated": "2025-08-01T13:25:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:25:28Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "title": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications"
                },
                "summary": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups."
                },
                "authors": [
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Jiancheng An"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This papar has been submitted to the IEEE Global Communications\n  Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00412v1",
                "updated": "2025-08-01T08:10:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:10:54Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models."
                },
                "authors": [
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Zeyu Chen"
                    },
                    {
                        "name": "Yi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Liu"
                },
                "author": "Yi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v2",
                "updated": "2025-08-01T03:43:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    43,
                    24,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22746v2",
                "updated": "2025-08-01T03:37:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    37,
                    42,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-30T15:03:36Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    3,
                    36,
                    2,
                    211,
                    0
                ],
                "title": "Next Tokens Denoising for Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Tokens Denoising for Speech Synthesis"
                },
                "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts."
                },
                "authors": [
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Ruiqing Xue"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yufei Liu"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Yao Qian"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhao"
                },
                "author": "Sheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v2",
                "updated": "2025-07-31T21:00:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    21,
                    0,
                    28,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22701v2",
                "updated": "2025-07-31T16:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    21,
                    3,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T14:10:16Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    10,
                    16,
                    2,
                    211,
                    0
                ],
                "title": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases"
                },
                "summary": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems."
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Decheng Zuo"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Zhiyu Liang"
                    },
                    {
                        "name": "Hongzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Wang"
                },
                "author": "Hongzhi Wang",
                "arxiv_comment": "17 pages, 10 figures. An extended version of a paper under review at\n  the VLDB 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; H.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v1",
                "updated": "2025-07-31T15:50:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21433v2",
                "updated": "2025-07-31T07:53:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    53,
                    53,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-29T02:05:51Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    2,
                    5,
                    51,
                    1,
                    210,
                    0
                ],
                "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse"
                },
                "summary": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods."
                },
                "authors": [
                    {
                        "name": "Kaiwen Chen"
                    },
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Hong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xu"
                },
                "author": "Hong Xu",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01199v2",
                "updated": "2025-07-31T07:35:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    35,
                    4,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-03T05:52:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    52,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude."
                },
                "authors": [
                    {
                        "name": "Kaimin Liao"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Luchao Wang"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23292v1",
                "updated": "2025-07-31T07:10:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T07:10:39Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy"
                },
                "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers."
                },
                "authors": [
                    {
                        "name": "RJ Skerry-Ryan"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Soroosh Mariooryad"
                    },
                    {
                        "name": "David Kao"
                    },
                    {
                        "name": "Daisy Stanton"
                    },
                    {
                        "name": "Eric Battenberg"
                    },
                    {
                        "name": "Matt Shannon"
                    },
                    {
                        "name": "Ron J. Weiss"
                    },
                    {
                        "name": "Robin Scheibler"
                    },
                    {
                        "name": "Jonas Rothfuss"
                    },
                    {
                        "name": "Tom Bagby"
                    }
                ],
                "author_detail": {
                    "name": "Tom Bagby"
                },
                "author": "Tom Bagby",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v3",
                "updated": "2025-07-30T16:55:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    55,
                    33,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22801v1",
                "updated": "2025-07-30T16:04:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:04:01Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "title": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code"
                },
                "summary": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions."
                },
                "authors": [
                    {
                        "name": "Shubhradeep Roy"
                    },
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Vivek Verma"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22636v1",
                "updated": "2025-07-30T12:55:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:55:55Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "title": "All-gluon amplitudes with off-shell recursion in multiplet bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-gluon amplitudes with off-shell recursion in multiplet bases"
                },
                "summary": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes."
                },
                "authors": [
                    {
                        "name": "Oskar Bolinder"
                    },
                    {
                        "name": "Rikkert Frederix"
                    },
                    {
                        "name": "Malin Sjodahl"
                    }
                ],
                "author_detail": {
                    "name": "Malin Sjodahl"
                },
                "author": "Malin Sjodahl",
                "arxiv_comment": "15 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20984v2",
                "updated": "2025-07-30T06:29:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-28T16:45:14Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment"
                },
                "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Dongliang Wei"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Jianxiang Gao"
                    },
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Hangyu Liang"
                    },
                    {
                        "name": "Guangshuo Qin"
                    },
                    {
                        "name": "Chengrong Tian"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Longyu Zhao"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v3",
                "updated": "2025-07-30T05:24:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    5,
                    24,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "Accepted to TMLR 2025. The revised version incorporates more papers\n  and has been further polished",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05655v1",
                "updated": "2025-07-29T12:42:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    12,
                    42,
                    24,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-29T12:42:24Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    12,
                    42,
                    24,
                    1,
                    210,
                    0
                ],
                "title": "Blockchain-Based Decentralized Domain Name System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain-Based Decentralized Domain Name System"
                },
                "summary": "The current Domain Name System (DNS) infrastructure faces critical\nvulnerabilities including poisoning attacks, censorship mechanisms, and\ncentralized points of failure that compromise internet freedom and security.\nRecent incidents such as DNS poisoning attacks on ISP customers highlight the\nurgent need for resilient alternatives. This paper presents a novel\nblockchain-based Decentralized Domain Name System (DDNS). We designed a\nspecialized Proof-of-Work blockchain to maximize support for DNS-related\nprotocols and achieve node decentralization. The system integrates our\nblockchain with IPFS for distributed storage, implements cryptographic\nprimitives for end-to-end trust signatures, and achieves Never Trust, Always\nVerify zero-trust verification. Our implementation achieves 15-second domain\nrecord propagation times, supports 20 standard DNS record types, and provides\nperpetual free .ddns domains. The system has been deployed across distributed\ninfrastructure in San Jose, Los Angeles, and Orange County, demonstrating\npractical scalability and resistance to traditional DNS manipulation\ntechniques. Performance evaluation shows the system can handle up to Max Theor.\nTPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular\ntransactions) for domain operations while maintaining sub-second query\nresolution through intelligent caching mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS) infrastructure faces critical\nvulnerabilities including poisoning attacks, censorship mechanisms, and\ncentralized points of failure that compromise internet freedom and security.\nRecent incidents such as DNS poisoning attacks on ISP customers highlight the\nurgent need for resilient alternatives. This paper presents a novel\nblockchain-based Decentralized Domain Name System (DDNS). We designed a\nspecialized Proof-of-Work blockchain to maximize support for DNS-related\nprotocols and achieve node decentralization. The system integrates our\nblockchain with IPFS for distributed storage, implements cryptographic\nprimitives for end-to-end trust signatures, and achieves Never Trust, Always\nVerify zero-trust verification. Our implementation achieves 15-second domain\nrecord propagation times, supports 20 standard DNS record types, and provides\nperpetual free .ddns domains. The system has been deployed across distributed\ninfrastructure in San Jose, Los Angeles, and Orange County, demonstrating\npractical scalability and resistance to traditional DNS manipulation\ntechniques. Performance evaluation shows the system can handle up to Max Theor.\nTPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular\ntransactions) for domain operations while maintaining sub-second query\nresolution through intelligent caching mechanisms."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Peter Trinh"
                    },
                    {
                        "name": "Alma Nkemla"
                    },
                    {
                        "name": "Amuru Serikyaku"
                    },
                    {
                        "name": "Edward Tatchim"
                    },
                    {
                        "name": "Osman Sharaf"
                    }
                ],
                "author_detail": {
                    "name": "Osman Sharaf"
                },
                "author": "Osman Sharaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00904v1",
                "updated": "2025-07-29T03:08:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    3,
                    8,
                    31,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-29T03:08:31Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    3,
                    8,
                    31,
                    1,
                    210,
                    0
                ],
                "title": "Forecasting LLM Inference Performance via Hardware-Agnostic Analytical\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting LLM Inference Performance via Hardware-Agnostic Analytical\n  Modeling"
                },
                "summary": "Large language models (LLMs) have been increasingly deployed as local agents\non personal devices with CPUs, NPUs and integrated GPUs. However, forecasting\ninference performance on devices with such heterogeneity remains challenging\ndue to the dynamic compute and memory demands. Existing approaches rely on GPU\nbenchmarking or machine learning-based latency predictors, which are often\nhardware-specific and lack generalizability. To this end, we introduce LIFE, a\nlightweight and modular analytical framework that is comprised of modular\nanalytical model of operators, configurable to characterize LLM inference\nworkloads in a hardware and dataset-agnostic manner. LIFE characterizes the\ninfluence of software and model optimizations, such as quantization, KV cache\ncompression, LoRA adapters, chunked prefill, different attentions, and operator\nfusion, on performance metrics such as time-to-first-token (TTFT),\ntime-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables\nperformance forecasting using only hardware specifications, such as TOPS and\nmemory bandwidth, without requiring extensive dataset benchmarking. We validate\nLIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA\nV100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in\nforecasting LLM performance through lens of system efficiency to enable\nefficient LLM deployment across different hardware platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly deployed as local agents\non personal devices with CPUs, NPUs and integrated GPUs. However, forecasting\ninference performance on devices with such heterogeneity remains challenging\ndue to the dynamic compute and memory demands. Existing approaches rely on GPU\nbenchmarking or machine learning-based latency predictors, which are often\nhardware-specific and lack generalizability. To this end, we introduce LIFE, a\nlightweight and modular analytical framework that is comprised of modular\nanalytical model of operators, configurable to characterize LLM inference\nworkloads in a hardware and dataset-agnostic manner. LIFE characterizes the\ninfluence of software and model optimizations, such as quantization, KV cache\ncompression, LoRA adapters, chunked prefill, different attentions, and operator\nfusion, on performance metrics such as time-to-first-token (TTFT),\ntime-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables\nperformance forecasting using only hardware specifications, such as TOPS and\nmemory bandwidth, without requiring extensive dataset benchmarking. We validate\nLIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA\nV100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in\nforecasting LLM performance through lens of system efficiency to enable\nefficient LLM deployment across different hardware platforms."
                },
                "authors": [
                    {
                        "name": "Rajeev Patwari"
                    },
                    {
                        "name": "Ashish Sirasao"
                    },
                    {
                        "name": "Devleena Das"
                    }
                ],
                "author_detail": {
                    "name": "Devleena Das"
                },
                "author": "Devleena Das",
                "arxiv_comment": "10 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v2",
                "updated": "2025-07-28T20:44:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    20,
                    44,
                    23,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13349v3",
                "updated": "2025-07-28T14:11:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    11,
                    53,
                    0,
                    209,
                    0
                ],
                "published": "2023-11-22T12:34:51Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    12,
                    34,
                    51,
                    2,
                    326,
                    0
                ],
                "title": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints"
                },
                "summary": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE."
                },
                "authors": [
                    {
                        "name": "Francesco Corti"
                    },
                    {
                        "name": "Balz Maag"
                    },
                    {
                        "name": "Joachim Schauer"
                    },
                    {
                        "name": "Ulrich Pferschy"
                    },
                    {
                        "name": "Olga Saukh"
                    }
                ],
                "author_detail": {
                    "name": "Olga Saukh"
                },
                "author": "Olga Saukh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20677v1",
                "updated": "2025-07-28T09:59:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:59:22Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "title": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing"
                },
                "summary": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits."
                },
                "authors": [
                    {
                        "name": "Ioana Moflic"
                    },
                    {
                        "name": "Alan Robertson"
                    },
                    {
                        "name": "Simon J. Devitt"
                    },
                    {
                        "name": "Alexandru Paler"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Paler"
                },
                "author": "Alexandru Paler",
                "arxiv_comment": "accepted at Q-CORE workshop of the QCE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20613v1",
                "updated": "2025-07-28T08:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:27:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression"
                },
                "summary": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance."
                },
                "authors": [
                    {
                        "name": "Te Zhang"
                    },
                    {
                        "name": "Yuheng Li"
                    },
                    {
                        "name": "Junxiang Wang"
                    },
                    {
                        "name": "Lujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Lujun Li"
                },
                "author": "Lujun Li",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v2",
                "updated": "2025-07-28T04:25:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    4,
                    25,
                    58,
                    0,
                    209,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Learning-Augmented Online Caching: New Upper Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Augmented Online Caching: New Upper Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v2",
                "updated": "2025-07-27T09:58:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    9,
                    58,
                    25,
                    6,
                    208,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20173v1",
                "updated": "2025-07-27T08:25:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T08:25:08Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "title": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP"
                },
                "summary": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization."
                },
                "authors": [
                    {
                        "name": "Haitian Wang"
                    },
                    {
                        "name": "Long Qin"
                    }
                ],
                "author_detail": {
                    "name": "Long Qin"
                },
                "author": "Long Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20116v1",
                "updated": "2025-07-27T03:45:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T03:45:07Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "title": "Accelerating Containerized Service Delivery at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Containerized Service Delivery at the Network Edge"
                },
                "summary": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions."
                },
                "authors": [
                    {
                        "name": "Yinuo Deng"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Dongjing Wang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Wenzhuo Qian"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Schahram Dustdar"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v3",
                "updated": "2025-07-27T00:40:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    0,
                    40,
                    47,
                    6,
                    208,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.09136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09136v1",
                "updated": "2025-08-12T17:59:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    59,
                    46,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:59:46Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    59,
                    46,
                    1,
                    224,
                    0
                ],
                "title": "Turbo-VAED: Fast and Stable Transfer of Video-VAEs to Mobile Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turbo-VAED: Fast and Stable Transfer of Video-VAEs to Mobile Devices"
                },
                "summary": "There is a growing demand for deploying large generative AI models on mobile\ndevices. For recent popular video generative models, however, the Variational\nAutoEncoder (VAE) represents one of the major computational bottlenecks. Both\nlarge parameter sizes and mismatched kernels cause out-of-memory errors or\nextremely slow inference on mobile devices. To address this, we propose a\nlow-cost solution that efficiently transfers widely used video VAEs to mobile\ndevices. (1) We analyze redundancy in existing VAE architectures and get\nempirical design insights. By integrating 3D depthwise separable convolutions\ninto our model, we significantly reduce the number of parameters. (2) We\nobserve that the upsampling techniques in mainstream video VAEs are poorly\nsuited to mobile hardware and form the main bottleneck. In response, we propose\na decoupled 3D pixel shuffle scheme that slashes end-to-end delay. Building\nupon these, we develop a universal mobile-oriented VAE decoder, Turbo-VAED. (3)\nWe propose an efficient VAE decoder training method. Since only the decoder is\nused during deployment, we distill it to Turbo-VAED instead of retraining the\nfull VAE, enabling fast mobile adaptation with minimal performance loss. To our\nknowledge, our method enables real-time 720p video VAE decoding on mobile\ndevices for the first time. This approach is widely applicable to most video\nVAEs. When integrated into four representative models, with training cost as\nlow as $95, it accelerates original VAEs by up to 84.5x at 720p resolution on\nGPUs, uses as low as 17.5% of original parameter count, and retains 96.9% of\nthe original reconstruction quality. Compared to mobile-optimized VAEs,\nTurbo-VAED achieves a 2.9x speedup in FPS and better reconstruction quality on\nthe iPhone 16 Pro. The code and models will soon be available at\nhttps://github.com/hustvl/Turbo-VAED.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing demand for deploying large generative AI models on mobile\ndevices. For recent popular video generative models, however, the Variational\nAutoEncoder (VAE) represents one of the major computational bottlenecks. Both\nlarge parameter sizes and mismatched kernels cause out-of-memory errors or\nextremely slow inference on mobile devices. To address this, we propose a\nlow-cost solution that efficiently transfers widely used video VAEs to mobile\ndevices. (1) We analyze redundancy in existing VAE architectures and get\nempirical design insights. By integrating 3D depthwise separable convolutions\ninto our model, we significantly reduce the number of parameters. (2) We\nobserve that the upsampling techniques in mainstream video VAEs are poorly\nsuited to mobile hardware and form the main bottleneck. In response, we propose\na decoupled 3D pixel shuffle scheme that slashes end-to-end delay. Building\nupon these, we develop a universal mobile-oriented VAE decoder, Turbo-VAED. (3)\nWe propose an efficient VAE decoder training method. Since only the decoder is\nused during deployment, we distill it to Turbo-VAED instead of retraining the\nfull VAE, enabling fast mobile adaptation with minimal performance loss. To our\nknowledge, our method enables real-time 720p video VAE decoding on mobile\ndevices for the first time. This approach is widely applicable to most video\nVAEs. When integrated into four representative models, with training cost as\nlow as $95, it accelerates original VAEs by up to 84.5x at 720p resolution on\nGPUs, uses as low as 17.5% of original parameter count, and retains 96.9% of\nthe original reconstruction quality. Compared to mobile-optimized VAEs,\nTurbo-VAED achieves a 2.9x speedup in FPS and better reconstruction quality on\nthe iPhone 16 Pro. The code and models will soon be available at\nhttps://github.com/hustvl/Turbo-VAED."
                },
                "authors": [
                    {
                        "name": "Ya Zou"
                    },
                    {
                        "name": "Jingfeng Yao"
                    },
                    {
                        "name": "Siyuan Yu"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09135v1",
                "updated": "2025-08-12T17:59:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    59,
                    32,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:59:32Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    59,
                    32,
                    1,
                    224,
                    0
                ],
                "title": "Efficient Statistical Estimation for Sequential Adaptive Experiments\n  with Implications for Adaptive Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Statistical Estimation for Sequential Adaptive Experiments\n  with Implications for Adaptive Designs"
                },
                "summary": "Adaptive experimental designs have gained popularity in clinical trials and\nonline experiments. Unlike traditional, fixed experimental designs, adaptive\ndesigns can dynamically adjust treatment randomization probabilities and other\ndesign features in response to data accumulated sequentially during the\nexperiment. These adaptations are useful to achieve diverse objectives,\nincluding reducing uncertainty in the estimation of causal estimands or\nincreasing participants' chances of receiving better treatments during the\nexperiment. At the end of the experiment, it is often desirable to answer\ncausal questions from the observed data. However, the adaptive nature of such\nexperiments and the resulting dependence among observations pose significant\nchallenges to providing valid statistical inference and efficient estimation of\ncausal estimands. Building upon the Targeted Maximum Likelihood Estimator\n(TMLE) framework tailored for adaptive designs (van der Laan, 2008), we\nintroduce a new adaptive-design-likelihood-based TMLE (ADL-TMLE) to estimate a\nvariety of causal estimands from adaptive experiment data. We establish\nasymptotic normality and semiparametric efficiency of ADL-TMLE under relaxed\npositivity and design stabilization assumptions for adaptive experiments.\nMotivated by efficiency results, we further propose a novel adaptive design\naimed at minimizing the variance of estimators based on data generated under\nthat design. Using the average treatment effect as a representative example,\nsimulation studies show that ADL-TMLE demonstrates superior variance-reduction\nperformance across different types of adaptive experiments, and that the\nproposed adaptive design attains lower variance than the standard\nefficiency-oriented adaptive design. Finally, we generalize this estimation and\ndesign framework to broader settings with longitudinal structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive experimental designs have gained popularity in clinical trials and\nonline experiments. Unlike traditional, fixed experimental designs, adaptive\ndesigns can dynamically adjust treatment randomization probabilities and other\ndesign features in response to data accumulated sequentially during the\nexperiment. These adaptations are useful to achieve diverse objectives,\nincluding reducing uncertainty in the estimation of causal estimands or\nincreasing participants' chances of receiving better treatments during the\nexperiment. At the end of the experiment, it is often desirable to answer\ncausal questions from the observed data. However, the adaptive nature of such\nexperiments and the resulting dependence among observations pose significant\nchallenges to providing valid statistical inference and efficient estimation of\ncausal estimands. Building upon the Targeted Maximum Likelihood Estimator\n(TMLE) framework tailored for adaptive designs (van der Laan, 2008), we\nintroduce a new adaptive-design-likelihood-based TMLE (ADL-TMLE) to estimate a\nvariety of causal estimands from adaptive experiment data. We establish\nasymptotic normality and semiparametric efficiency of ADL-TMLE under relaxed\npositivity and design stabilization assumptions for adaptive experiments.\nMotivated by efficiency results, we further propose a novel adaptive design\naimed at minimizing the variance of estimators based on data generated under\nthat design. Using the average treatment effect as a representative example,\nsimulation studies show that ADL-TMLE demonstrates superior variance-reduction\nperformance across different types of adaptive experiments, and that the\nproposed adaptive design attains lower variance than the standard\nefficiency-oriented adaptive design. Finally, we generalize this estimation and\ndesign framework to broader settings with longitudinal structures."
                },
                "authors": [
                    {
                        "name": "Wenxin Zhang"
                    },
                    {
                        "name": "Mark van der Laan"
                    }
                ],
                "author_detail": {
                    "name": "Mark van der Laan"
                },
                "author": "Mark van der Laan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09129v1",
                "updated": "2025-08-12T17:56:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    56,
                    25,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:56:25Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    56,
                    25,
                    1,
                    224,
                    0
                ],
                "title": "BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented\n  Programmatic Agent Pair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented\n  Programmatic Agent Pair"
                },
                "summary": "Effective information seeking in the vast and ever-growing digital landscape\nrequires balancing expansive search with strategic reasoning. Current large\nlanguage model (LLM)-based agents struggle to achieve this balance due to\nlimitations in search breadth and reasoning depth, where slow, serial querying\nrestricts coverage of relevant sources and noisy raw inputs disrupt the\ncontinuity of multi-step reasoning. To address these challenges, we propose\nBrowseMaster, a scalable framework built around a programmatically augmented\nplanner-executor agent pair. The planner formulates and adapts search\nstrategies based on task constraints, while the executor conducts efficient,\ntargeted retrieval to supply the planner with concise, relevant evidence. This\ndivision of labor preserves coherent, long-horizon reasoning while sustaining\nbroad and systematic exploration, overcoming the trade-off that limits existing\nagents. Extensive experiments on challenging English and Chinese benchmarks\nshow that BrowseMaster consistently outperforms open-source and proprietary\nbaselines, achieving scores of 30.0 on BrowseComp-en and 46.5 on BrowseComp-zh,\nwhich demonstrates its strong capability in complex, reasoning-heavy\ninformation-seeking tasks at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective information seeking in the vast and ever-growing digital landscape\nrequires balancing expansive search with strategic reasoning. Current large\nlanguage model (LLM)-based agents struggle to achieve this balance due to\nlimitations in search breadth and reasoning depth, where slow, serial querying\nrestricts coverage of relevant sources and noisy raw inputs disrupt the\ncontinuity of multi-step reasoning. To address these challenges, we propose\nBrowseMaster, a scalable framework built around a programmatically augmented\nplanner-executor agent pair. The planner formulates and adapts search\nstrategies based on task constraints, while the executor conducts efficient,\ntargeted retrieval to supply the planner with concise, relevant evidence. This\ndivision of labor preserves coherent, long-horizon reasoning while sustaining\nbroad and systematic exploration, overcoming the trade-off that limits existing\nagents. Extensive experiments on challenging English and Chinese benchmarks\nshow that BrowseMaster consistently outperforms open-source and proprietary\nbaselines, achieving scores of 30.0 on BrowseComp-en and 46.5 on BrowseComp-zh,\nwhich demonstrates its strong capability in complex, reasoning-heavy\ninformation-seeking tasks at scale."
                },
                "authors": [
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Shuo Tang"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Yuwen Du"
                    },
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09126v1",
                "updated": "2025-08-12T17:55:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    55,
                    8,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:55:08Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    55,
                    8,
                    1,
                    224,
                    0
                ],
                "title": "Neutone SDK: An Open Source Framework for Neural Audio Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutone SDK: An Open Source Framework for Neural Audio Processing"
                },
                "summary": "Neural audio processing has unlocked novel methods of sound transformation\nand synthesis, yet integrating deep learning models into digital audio\nworkstations (DAWs) remains challenging due to real-time / neural network\ninference constraints and the complexities of plugin development. In this\npaper, we introduce the Neutone SDK: an open source framework that streamlines\nthe deployment of PyTorch-based neural audio models for both real-time and\noffline applications. By encapsulating common challenges such as variable\nbuffer sizes, sample rate conversion, delay compensation, and control parameter\nhandling within a unified, model-agnostic interface, our framework enables\nseamless interoperability between neural models and host plugins while allowing\nusers to work entirely in Python. We provide a technical overview of the\ninterfaces needed to accomplish this, as well as the corresponding SDK\nimplementations. We also demonstrate the SDK's versatility across applications\nsuch as audio effect emulation, timbre transfer, and sample generation, as well\nas its adoption by researchers, educators, companies, and artists alike. The\nNeutone SDK is available at https://github.com/Neutone/neutone_sdk",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural audio processing has unlocked novel methods of sound transformation\nand synthesis, yet integrating deep learning models into digital audio\nworkstations (DAWs) remains challenging due to real-time / neural network\ninference constraints and the complexities of plugin development. In this\npaper, we introduce the Neutone SDK: an open source framework that streamlines\nthe deployment of PyTorch-based neural audio models for both real-time and\noffline applications. By encapsulating common challenges such as variable\nbuffer sizes, sample rate conversion, delay compensation, and control parameter\nhandling within a unified, model-agnostic interface, our framework enables\nseamless interoperability between neural models and host plugins while allowing\nusers to work entirely in Python. We provide a technical overview of the\ninterfaces needed to accomplish this, as well as the corresponding SDK\nimplementations. We also demonstrate the SDK's versatility across applications\nsuch as audio effect emulation, timbre transfer, and sample generation, as well\nas its adoption by researchers, educators, companies, and artists alike. The\nNeutone SDK is available at https://github.com/Neutone/neutone_sdk"
                },
                "authors": [
                    {
                        "name": "Christopher Mitcheltree"
                    },
                    {
                        "name": "Bogdan Teleaga"
                    },
                    {
                        "name": "Andrew Fyfe"
                    },
                    {
                        "name": "Naotake Masuda"
                    },
                    {
                        "name": "Matthias Schäfer"
                    },
                    {
                        "name": "Alfie Bradic"
                    },
                    {
                        "name": "Nao Tokui"
                    }
                ],
                "author_detail": {
                    "name": "Nao Tokui"
                },
                "author": "Nao Tokui",
                "arxiv_comment": "Accepted to AES International Conference on Artificial Intelligence\n  and Machine Learning for Audio 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09125v1",
                "updated": "2025-08-12T17:54:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    54,
                    27,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:54:27Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    54,
                    27,
                    1,
                    224,
                    0
                ],
                "title": "Complex Logical Instruction Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex Logical Instruction Generation"
                },
                "summary": "Instruction following has catalyzed the recent era of Large Language Models\n(LLMs) and is the foundational skill underpinning more advanced capabilities\nsuch as reasoning and agentic behaviors. As tasks grow more challenging, the\nlogic structures embedded in natural language instructions becomes increasingly\nintricate. However, how well LLMs perform on such logic-rich instructions\nremains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a\nscalable, automated framework for generating verifiable instructions from code\nfunctions, which can naturally express rich logic such as conditionals,\nnesting, recursion, and function calls. We further curate a collection of\ncomplex code functions and use LogicIFGen to construct LogicIFEval, a benchmark\ncomprising 426 verifiable logic-rich instructions. Our experiments demonstrate\nthat current state-of-the-art LLMs still struggle to correctly follow the\ninstructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the\ninstructions, revealing significant deficiencies in the instruction-following\nability. Code and Benchmark: https://github.com/mianzhang/LogicIF",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction following has catalyzed the recent era of Large Language Models\n(LLMs) and is the foundational skill underpinning more advanced capabilities\nsuch as reasoning and agentic behaviors. As tasks grow more challenging, the\nlogic structures embedded in natural language instructions becomes increasingly\nintricate. However, how well LLMs perform on such logic-rich instructions\nremains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a\nscalable, automated framework for generating verifiable instructions from code\nfunctions, which can naturally express rich logic such as conditionals,\nnesting, recursion, and function calls. We further curate a collection of\ncomplex code functions and use LogicIFGen to construct LogicIFEval, a benchmark\ncomprising 426 verifiable logic-rich instructions. Our experiments demonstrate\nthat current state-of-the-art LLMs still struggle to correctly follow the\ninstructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the\ninstructions, revealing significant deficiencies in the instruction-following\nability. Code and Benchmark: https://github.com/mianzhang/LogicIF"
                },
                "authors": [
                    {
                        "name": "Mian Zhang"
                    },
                    {
                        "name": "Shujian Liu"
                    },
                    {
                        "name": "Sixun Dong"
                    },
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Yebowen Hu"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Steven Ma"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Sathish Reddy Indurthi"
                    },
                    {
                        "name": "Haoyun Deng"
                    },
                    {
                        "name": "Zhiyu Zoey Chen"
                    },
                    {
                        "name": "Kaiqiang Song"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqiang Song"
                },
                "author": "Kaiqiang Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13079v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13079v2",
                "updated": "2025-08-12T17:53:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    53,
                    18,
                    1,
                    224,
                    0
                ],
                "published": "2025-04-17T16:46:11Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    46,
                    11,
                    3,
                    107,
                    0
                ],
                "title": "Retrieval-Augmented Generation with Conflicting Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation with Conflicting Evidence"
                },
                "summary": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "COLM 2025, Data and Code: https://github.com/HanNight/RAMDocs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13079v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13079v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09124v1",
                "updated": "2025-08-12T17:53:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    53,
                    3,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:53:03Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    53,
                    3,
                    1,
                    224,
                    0
                ],
                "title": "OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office\n  Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office\n  Application Workflows"
                },
                "summary": "Autonomous agents powered by large language models (LLMs) are increasingly\ndeployed in real-world applications requiring complex, long-horizon workflows.\nHowever, existing benchmarks predominantly focus on atomic tasks that are\nself-contained and independent, failing to capture the long-term contextual\ndependencies and multi-interaction coordination required in realistic\nscenarios. To address this gap, we introduce OdysseyBench, a comprehensive\nbenchmark for evaluating LLM agents on long-horizon workflows across diverse\noffice applications including Word, Excel, PDF, Email, and Calendar. Our\nbenchmark comprises two complementary splits: OdysseyBench+ with 300 tasks\nderived from real-world use cases, and OdysseyBench-Neo with 302 newly\nsynthesized complex tasks. Each task requires agent to identify essential\ninformation from long-horizon interaction histories and perform multi-step\nreasoning across various applications. To enable scalable benchmark creation,\nwe propose HomerAgents, a multi-agent framework that automates the generation\nof long-horizon workflow benchmarks through systematic environment exploration,\ntask generation, and dialogue synthesis. Our extensive evaluation demonstrates\nthat OdysseyBench effectively challenges state-of-the-art LLM agents, providing\nmore accurate assessment of their capabilities in complex, real-world contexts\ncompared to existing atomic task benchmarks. We believe that OdysseyBench will\nserve as a valuable resource for advancing the development and evaluation of\nLLM agents in real-world productivity scenarios. In addition, we release\nOdysseyBench and HomerAgents to foster research along this line.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents powered by large language models (LLMs) are increasingly\ndeployed in real-world applications requiring complex, long-horizon workflows.\nHowever, existing benchmarks predominantly focus on atomic tasks that are\nself-contained and independent, failing to capture the long-term contextual\ndependencies and multi-interaction coordination required in realistic\nscenarios. To address this gap, we introduce OdysseyBench, a comprehensive\nbenchmark for evaluating LLM agents on long-horizon workflows across diverse\noffice applications including Word, Excel, PDF, Email, and Calendar. Our\nbenchmark comprises two complementary splits: OdysseyBench+ with 300 tasks\nderived from real-world use cases, and OdysseyBench-Neo with 302 newly\nsynthesized complex tasks. Each task requires agent to identify essential\ninformation from long-horizon interaction histories and perform multi-step\nreasoning across various applications. To enable scalable benchmark creation,\nwe propose HomerAgents, a multi-agent framework that automates the generation\nof long-horizon workflow benchmarks through systematic environment exploration,\ntask generation, and dialogue synthesis. Our extensive evaluation demonstrates\nthat OdysseyBench effectively challenges state-of-the-art LLM agents, providing\nmore accurate assessment of their capabilities in complex, real-world contexts\ncompared to existing atomic task benchmarks. We believe that OdysseyBench will\nserve as a valuable resource for advancing the development and evaluation of\nLLM agents in real-world productivity scenarios. In addition, we release\nOdysseyBench and HomerAgents to foster research along this line."
                },
                "authors": [
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Dongge Han"
                    },
                    {
                        "name": "Daniel Madrigal Diaz"
                    },
                    {
                        "name": "Jin Xu"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09115v1",
                "updated": "2025-08-12T17:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    49,
                    34,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:49:34Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    49,
                    34,
                    1,
                    224,
                    0
                ],
                "title": "SinLlama -- A Large Language Model for Sinhala",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SinLlama -- A Large Language Model for Sinhala"
                },
                "summary": "Low-resource languages such as Sinhala are often overlooked by open-source\nLarge Language Models (LLMs). In this research, we extend an existing\nmultilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM\ntokenizer with Sinhala specific vocabulary and perform continual pre-training\non a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This\nis the very first decoder-based open-source LLM with explicit Sinhala support.\nWhen SinLlama was instruction fine-tuned for three text classification tasks,\nit outperformed base and instruct variants of Llama-3-8B by a significant\nmargin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-resource languages such as Sinhala are often overlooked by open-source\nLarge Language Models (LLMs). In this research, we extend an existing\nmultilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM\ntokenizer with Sinhala specific vocabulary and perform continual pre-training\non a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This\nis the very first decoder-based open-source LLM with explicit Sinhala support.\nWhen SinLlama was instruction fine-tuned for three text classification tasks,\nit outperformed base and instruct variants of Llama-3-8B by a significant\nmargin."
                },
                "authors": [
                    {
                        "name": "H. W. K. Aravinda"
                    },
                    {
                        "name": "Rashad Sirajudeen"
                    },
                    {
                        "name": "Samith Karunathilake"
                    },
                    {
                        "name": "Nisansa de Silva"
                    },
                    {
                        "name": "Surangika Ranathunga"
                    },
                    {
                        "name": "Rishemjit Kaur"
                    }
                ],
                "author_detail": {
                    "name": "Rishemjit Kaur"
                },
                "author": "Rishemjit Kaur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04349v3",
                "updated": "2025-08-13T09:00:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    0,
                    5,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-06T11:42:47Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    42,
                    47,
                    2,
                    218,
                    0
                ],
                "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy\n  Entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy\n  Entropy"
                },
                "summary": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models."
                },
                "authors": [
                    {
                        "name": "Hongze Tan"
                    },
                    {
                        "name": "Jianfei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Pan"
                },
                "author": "Jianfei Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09105v2",
                "updated": "2025-08-13T11:05:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    5,
                    22,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T17:32:24Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    32,
                    24,
                    1,
                    224,
                    0
                ],
                "title": "SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG\n  Controlling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG\n  Controlling"
                },
                "summary": "Retrieval-Augmented Generation (RAG) and its Multimodal Retrieval-Augmented\nGeneration (MRAG) significantly improve the knowledge coverage and contextual\nunderstanding of Large Language Models (LLMs) by introducing external knowledge\nsources. However, retrieval and multimodal fusion obscure content provenance,\nrendering existing membership inference methods unable to reliably attribute\ngenerated outputs to pre-training, external retrieval, or user input, thus\nundermining privacy leakage accountability\n  To address these challenges, we propose the first Source-aware Membership\nAudit (SMA) that enables fine-grained source attribution of generated content\nin a semi-black-box setting with retrieval control capabilities. To address the\nenvironmental constraints of semi-black-box auditing, we further design an\nattribution estimation mechanism based on zero-order optimization, which\nrobustly approximates the true influence of input tokens on the output through\nlarge-scale perturbation sampling and ridge regression modeling. In addition,\nSMA introduces a cross-modal attribution technique that projects image inputs\ninto textual descriptions via MLLMs, enabling token-level attribution in the\ntext modality, which for the first time facilitates membership inference on\nimage retrieval traces in MRAG systems. This work shifts the focus of\nmembership inference from 'whether the data has been memorized' to 'where the\ncontent is sourced from', offering a novel perspective for auditing data\nprovenance in complex generative systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) and its Multimodal Retrieval-Augmented\nGeneration (MRAG) significantly improve the knowledge coverage and contextual\nunderstanding of Large Language Models (LLMs) by introducing external knowledge\nsources. However, retrieval and multimodal fusion obscure content provenance,\nrendering existing membership inference methods unable to reliably attribute\ngenerated outputs to pre-training, external retrieval, or user input, thus\nundermining privacy leakage accountability\n  To address these challenges, we propose the first Source-aware Membership\nAudit (SMA) that enables fine-grained source attribution of generated content\nin a semi-black-box setting with retrieval control capabilities. To address the\nenvironmental constraints of semi-black-box auditing, we further design an\nattribution estimation mechanism based on zero-order optimization, which\nrobustly approximates the true influence of input tokens on the output through\nlarge-scale perturbation sampling and ridge regression modeling. In addition,\nSMA introduces a cross-modal attribution technique that projects image inputs\ninto textual descriptions via MLLMs, enabling token-level attribution in the\ntext modality, which for the first time facilitates membership inference on\nimage retrieval traces in MRAG systems. This work shifts the focus of\nmembership inference from 'whether the data has been memorized' to 'where the\ncontent is sourced from', offering a novel perspective for auditing data\nprovenance in complex generative systems."
                },
                "authors": [
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Ruoyu Chen"
                    },
                    {
                        "name": "Jianjie Huang"
                    },
                    {
                        "name": "Jingzhi Li"
                    },
                    {
                        "name": "Xiaochun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaochun Cao"
                },
                "author": "Xiaochun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09101v1",
                "updated": "2025-08-12T17:29:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    29,
                    20,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:29:20Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    29,
                    20,
                    1,
                    224,
                    0
                ],
                "title": "AutoCodeBench: Large Language Models are Automatic Code Benchmark\n  Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoCodeBench: Large Language Models are Automatic Code Benchmark\n  Generators"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, with code generation emerging as a key area of focus. While\nnumerous benchmarks have been proposed to evaluate their code generation\nabilities, these benchmarks face several critical limitations. First, they\noften rely on manual annotations, which are time-consuming and difficult to\nscale across different programming languages and problem complexities. Second,\nmost existing benchmarks focus primarily on Python, while the few multilingual\nbenchmarks suffer from limited difficulty and uneven language distribution. To\naddress these challenges, we propose AutoCodeGen, an automated method for\ngenerating high-difficulty multilingual code generation datasets without manual\nannotations. AutoCodeGen ensures the correctness and completeness of test cases\nby generating test inputs with LLMs and obtaining test outputs through a\nmultilingual sandbox, while achieving high data quality through reverse-order\nproblem generation and multiple filtering steps. Using this novel method, we\nintroduce AutoCodeBench, a large-scale code generation benchmark comprising\n3,920 problems evenly distributed across 20 programming languages. It is\nspecifically designed to evaluate LLMs on challenging, diverse, and practical\nmultilingual tasks. We evaluate over 30 leading open-source and proprietary\nLLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The\nresults show that even the most advanced LLMs struggle with the complexity,\ndiversity, and multilingual nature of these tasks. Besides, we introduce\nAutoCodeBench-Complete, specifically designed for base models to assess their\nfew-shot code generation capabilities. We hope the AutoCodeBench series will\nserve as a valuable resource and inspire the community to focus on more\nchallenging and practical multilingual code generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, with code generation emerging as a key area of focus. While\nnumerous benchmarks have been proposed to evaluate their code generation\nabilities, these benchmarks face several critical limitations. First, they\noften rely on manual annotations, which are time-consuming and difficult to\nscale across different programming languages and problem complexities. Second,\nmost existing benchmarks focus primarily on Python, while the few multilingual\nbenchmarks suffer from limited difficulty and uneven language distribution. To\naddress these challenges, we propose AutoCodeGen, an automated method for\ngenerating high-difficulty multilingual code generation datasets without manual\nannotations. AutoCodeGen ensures the correctness and completeness of test cases\nby generating test inputs with LLMs and obtaining test outputs through a\nmultilingual sandbox, while achieving high data quality through reverse-order\nproblem generation and multiple filtering steps. Using this novel method, we\nintroduce AutoCodeBench, a large-scale code generation benchmark comprising\n3,920 problems evenly distributed across 20 programming languages. It is\nspecifically designed to evaluate LLMs on challenging, diverse, and practical\nmultilingual tasks. We evaluate over 30 leading open-source and proprietary\nLLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The\nresults show that even the most advanced LLMs struggle with the complexity,\ndiversity, and multilingual nature of these tasks. Besides, we introduce\nAutoCodeBench-Complete, specifically designed for base models to assess their\nfew-shot code generation capabilities. We hope the AutoCodeBench series will\nserve as a valuable resource and inspire the community to focus on more\nchallenging and practical multilingual code generation scenarios."
                },
                "authors": [
                    {
                        "name": "Jason Chou"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Zhiying Zeng"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Lingyun Tan"
                    },
                    {
                        "name": "Ziyan Xu"
                    },
                    {
                        "name": "Bohui Zhai"
                    },
                    {
                        "name": "Hengyi Liu"
                    },
                    {
                        "name": "Speed Zhu"
                    },
                    {
                        "name": "Wiggin Zhou"
                    },
                    {
                        "name": "Fengzong Lian"
                    }
                ],
                "author_detail": {
                    "name": "Fengzong Lian"
                },
                "author": "Fengzong Lian",
                "arxiv_comment": "Homepage: https://autocodebench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09100v1",
                "updated": "2025-08-12T17:26:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    26,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:26:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    26,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "Towards Universal Neural Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Universal Neural Inference"
                },
                "summary": "Real-world data often appears in diverse, disjoint forms -- with varying\nschemas, inconsistent semantics, and no fixed feature ordering -- making it\nchallenging to build general-purpose models that can leverage information\nacross datasets. We introduce ASPIRE, Arbitrary Set-based Permutation-Invariant\nReasoning Engine, a Universal Neural Inference model for semantic reasoning and\nprediction over heterogeneous structured data. ASPIRE combines a\npermutation-invariant, set-based Transformer with a semantic grounding module\nthat incorporates natural language descriptions, dataset metadata, and\nin-context examples to learn cross-dataset feature dependencies. This\narchitecture allows ASPIRE to ingest arbitrary sets of feature--value pairs and\nsupport examples, align semantics across disjoint tables, and make predictions\nfor any specified target. Once trained, ASPIRE generalizes to new inference\ntasks without additional tuning. In addition to delivering strong results\nacross diverse benchmarks, ASPIRE naturally supports cost-aware active feature\nacquisition in an open-world setting, selecting informative features under\ntest-time budget constraints for an arbitrary unseen dataset. These\ncapabilities position ASPIRE as a step toward truly universal, semantics-aware\ninference over structured data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world data often appears in diverse, disjoint forms -- with varying\nschemas, inconsistent semantics, and no fixed feature ordering -- making it\nchallenging to build general-purpose models that can leverage information\nacross datasets. We introduce ASPIRE, Arbitrary Set-based Permutation-Invariant\nReasoning Engine, a Universal Neural Inference model for semantic reasoning and\nprediction over heterogeneous structured data. ASPIRE combines a\npermutation-invariant, set-based Transformer with a semantic grounding module\nthat incorporates natural language descriptions, dataset metadata, and\nin-context examples to learn cross-dataset feature dependencies. This\narchitecture allows ASPIRE to ingest arbitrary sets of feature--value pairs and\nsupport examples, align semantics across disjoint tables, and make predictions\nfor any specified target. Once trained, ASPIRE generalizes to new inference\ntasks without additional tuning. In addition to delivering strong results\nacross diverse benchmarks, ASPIRE naturally supports cost-aware active feature\nacquisition in an open-world setting, selecting informative features under\ntest-time budget constraints for an arbitrary unseen dataset. These\ncapabilities position ASPIRE as a step toward truly universal, semantics-aware\ninference over structured data."
                },
                "authors": [
                    {
                        "name": "Shreyas Bhat Brahmavar"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Junier Oliva"
                    }
                ],
                "author_detail": {
                    "name": "Junier Oliva"
                },
                "author": "Junier Oliva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04163v2",
                "updated": "2025-08-12T17:25:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    25,
                    37,
                    1,
                    224,
                    0
                ],
                "published": "2025-07-05T21:15:24Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    21,
                    15,
                    24,
                    5,
                    186,
                    0
                ],
                "title": "Nested importance sampling for Bayesian inference: error bounds and the\n  role of dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nested importance sampling for Bayesian inference: error bounds and the\n  role of dimension"
                },
                "summary": "Many Bayesian inference problems involve high dimensional models for which\nonly a subset of the model variables are of actual interest. All other\nvariables are just nuisance parameters that one would ideally like to integrate\nout analytically. Unfortunately, such integration is often impossible. There\nare several computational methods that have been proposed over the past 15\nyears that replace intractable analytical marginalization by numerical\nintegration, typically using different flavours of importance sampling (IS).\nSuch methods include particle Markov chain Monte Carlo, sequential Monte Carlo\nsquared (SMC$^2$), IS$^2$, nested particle filters, and others. In this paper,\nwe investigate the role of the dimension of the nuisance variables in the error\nbounds achieved by nested IS methods in Bayesian inference. We prove that,\nunder suitable regularity assumptions on the model, the approximation errors\nincrease at a polynomial (rather than exponential) rate w.r.t. the dimension of\nthe nuisance variables. Our analysis relies on tools from functional analysis\nand measure theory, and it includes the case of polynomials of degree zero,\nwhere the approximation error remains uniformly bounded, even as the dimension\nof the nuisance variables grows without bound. We also show how the general\nanalysis can be applied to specific classes of models, including linear and\nGaussian settings, models with bounded observation functions, and others. These\nfindings improve the current understanding of how the performance of IS\ntechniques scales with dimension in Bayesian inference problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many Bayesian inference problems involve high dimensional models for which\nonly a subset of the model variables are of actual interest. All other\nvariables are just nuisance parameters that one would ideally like to integrate\nout analytically. Unfortunately, such integration is often impossible. There\nare several computational methods that have been proposed over the past 15\nyears that replace intractable analytical marginalization by numerical\nintegration, typically using different flavours of importance sampling (IS).\nSuch methods include particle Markov chain Monte Carlo, sequential Monte Carlo\nsquared (SMC$^2$), IS$^2$, nested particle filters, and others. In this paper,\nwe investigate the role of the dimension of the nuisance variables in the error\nbounds achieved by nested IS methods in Bayesian inference. We prove that,\nunder suitable regularity assumptions on the model, the approximation errors\nincrease at a polynomial (rather than exponential) rate w.r.t. the dimension of\nthe nuisance variables. Our analysis relies on tools from functional analysis\nand measure theory, and it includes the case of polynomials of degree zero,\nwhere the approximation error remains uniformly bounded, even as the dimension\nof the nuisance variables grows without bound. We also show how the general\nanalysis can be applied to specific classes of models, including linear and\nGaussian settings, models with bounded observation functions, and others. These\nfindings improve the current understanding of how the performance of IS\ntechniques scales with dimension in Bayesian inference problems."
                },
                "authors": [
                    {
                        "name": "Fabián González"
                    },
                    {
                        "name": "Víctor Elvira"
                    },
                    {
                        "name": "Joaquín Miguez"
                    }
                ],
                "author_detail": {
                    "name": "Joaquín Miguez"
                },
                "author": "Joaquín Miguez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 60B05, 65C05, 46N30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09097v1",
                "updated": "2025-08-12T17:24:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    24,
                    56,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:24:56Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    24,
                    56,
                    1,
                    224,
                    0
                ],
                "title": "Chi-Geometry: A Library for Benchmarking Chirality Prediction of GNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chi-Geometry: A Library for Benchmarking Chirality Prediction of GNNs"
                },
                "summary": "We introduce Chi-Geometry - a library that generates graph data for testing\nand benchmarking GNNs' ability to predict chirality. Chi-Geometry generates\nsynthetic graph samples with (i) user-specified geometric and topological\ntraits to isolate certain types of samples and (ii) randomized node positions\nand species to minimize extraneous correlations. Each generated graph contains\nexactly one chiral center labeled either R or S, while all other nodes are\nlabeled N/A (non-chiral). The generated samples are then combined into a\ncohesive dataset that can be used to assess a GNN's ability to predict\nchirality as a node classification task. Chi-Geometry allows more interpretable\nand less confounding benchmarking of GNNs for prediction of chirality in the\ngraph samples which can guide the design of new GNN architectures with improved\npredictive performance. We illustrate Chi-Geometry's efficacy by using it to\ngenerate synthetic datasets for benchmarking various state-of-the-art (SOTA)\nGNN architectures. The conclusions of these benchmarking results guided our\ndesign of two new GNN architectures. The first GNN architecture established\nall-to-all connections in the graph to accurately predict chirality across all\nchallenging configurations where previously tested SOTA models failed, but at a\ncomputational cost (both for training and inference) that grows quadratically\nwith the number of graph nodes. The second GNN architecture avoids all-to-all\nconnections by introducing a virtual node in the original graph structure of\nthe data, which restores the linear scaling of training and inference\ncomputational cost with respect to the number of nodes in the graph, while\nstill ensuring competitive accuracy in detecting chirality with respect to SOTA\nGNN architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Chi-Geometry - a library that generates graph data for testing\nand benchmarking GNNs' ability to predict chirality. Chi-Geometry generates\nsynthetic graph samples with (i) user-specified geometric and topological\ntraits to isolate certain types of samples and (ii) randomized node positions\nand species to minimize extraneous correlations. Each generated graph contains\nexactly one chiral center labeled either R or S, while all other nodes are\nlabeled N/A (non-chiral). The generated samples are then combined into a\ncohesive dataset that can be used to assess a GNN's ability to predict\nchirality as a node classification task. Chi-Geometry allows more interpretable\nand less confounding benchmarking of GNNs for prediction of chirality in the\ngraph samples which can guide the design of new GNN architectures with improved\npredictive performance. We illustrate Chi-Geometry's efficacy by using it to\ngenerate synthetic datasets for benchmarking various state-of-the-art (SOTA)\nGNN architectures. The conclusions of these benchmarking results guided our\ndesign of two new GNN architectures. The first GNN architecture established\nall-to-all connections in the graph to accurately predict chirality across all\nchallenging configurations where previously tested SOTA models failed, but at a\ncomputational cost (both for training and inference) that grows quadratically\nwith the number of graph nodes. The second GNN architecture avoids all-to-all\nconnections by introducing a virtual node in the original graph structure of\nthe data, which restores the linear scaling of training and inference\ncomputational cost with respect to the number of nodes in the graph, while\nstill ensuring competitive accuracy in detecting chirality with respect to SOTA\nGNN architectures."
                },
                "authors": [
                    {
                        "name": "Rylie Weaver"
                    },
                    {
                        "name": "Massamiliano Lupo Pasini"
                    }
                ],
                "author_detail": {
                    "name": "Massamiliano Lupo Pasini"
                },
                "author": "Massamiliano Lupo Pasini",
                "arxiv_comment": "21 pages total: 9 pages main text, 4 pages references, 8 pages\n  appendices. 4 figures and 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05452v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05452v2",
                "updated": "2025-08-12T17:23:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    23,
                    33,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-07T14:46:30Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    46,
                    30,
                    3,
                    219,
                    0
                ],
                "title": "LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair\n  Evaluation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair\n  Evaluation of Large Language Models"
                },
                "summary": "Existing evaluation of Large Language Models (LLMs) on static benchmarks is\nvulnerable to data contamination and leaderboard overfitting, critical issues\nthat obscure true model capabilities. To address this, we introduce LLMEval-3,\na framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary\nbank of 220k graduate-level questions, from which it dynamically samples unseen\ntest sets for each evaluation run. Its automated pipeline ensures integrity via\ncontamination-resistant data curation, a novel anti-cheating architecture, and\na calibrated LLM-as-a-judge process achieving 90% agreement with human experts,\ncomplemented by a relative ranking system for fair comparison. An 20-month\nlongitudinal study of nearly 50 leading models reveals a performance ceiling on\nknowledge memorization and exposes data contamination vulnerabilities\nundetectable by static benchmarks. The framework demonstrates exceptional\nrobustness in ranking stability and consistency, providing strong empirical\nvalidation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and\ncredible methodology for assessing the true capabilities of LLMs beyond\nleaderboard scores, promoting the development of more trustworthy evaluation\nstandards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing evaluation of Large Language Models (LLMs) on static benchmarks is\nvulnerable to data contamination and leaderboard overfitting, critical issues\nthat obscure true model capabilities. To address this, we introduce LLMEval-3,\na framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary\nbank of 220k graduate-level questions, from which it dynamically samples unseen\ntest sets for each evaluation run. Its automated pipeline ensures integrity via\ncontamination-resistant data curation, a novel anti-cheating architecture, and\na calibrated LLM-as-a-judge process achieving 90% agreement with human experts,\ncomplemented by a relative ranking system for fair comparison. An 20-month\nlongitudinal study of nearly 50 leading models reveals a performance ceiling on\nknowledge memorization and exposes data contamination vulnerabilities\nundetectable by static benchmarks. The framework demonstrates exceptional\nrobustness in ranking stability and consistency, providing strong empirical\nvalidation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and\ncredible methodology for assessing the true capabilities of LLMs beyond\nleaderboard scores, promoting the development of more trustworthy evaluation\nstandards."
                },
                "authors": [
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Yujiong Shen"
                    },
                    {
                        "name": "Jingyi Deng"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Junzhe Wang"
                    },
                    {
                        "name": "Shichun Liu"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Huayu Sha"
                    },
                    {
                        "name": "Qiyuan Peng"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Yilong Wu"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Mingqi Wu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Mingxu Chai"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Zhihui Fei"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Mingyang Wan"
                    },
                    {
                        "name": "Guojun Ma"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05452v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05452v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09096v1",
                "updated": "2025-08-12T17:22:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    22,
                    29,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:22:29Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    22,
                    29,
                    1,
                    224,
                    0
                ],
                "title": "Link Prediction for Event Logs in the Process Industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Link Prediction for Event Logs in the Process Industry"
                },
                "summary": "Knowledge management (KM) is vital in the process industry for optimizing\noperations, ensuring safety, and enabling continuous improvement through\neffective use of operational data and past insights. A key challenge in this\ndomain is the fragmented nature of event logs in shift books, where related\nrecords, e.g., entries documenting issues related to equipment or processes and\nthe corresponding solutions, may remain disconnected. This fragmentation\nhinders the recommendation of previous solutions to the users. To address this\nproblem, we investigate record linking (RL) as link prediction, commonly\nstudied in graph-based machine learning, by framing it as a cross-document\ncoreference resolution (CDCR) task enhanced with natural language inference\n(NLI) and semantic text similarity (STS) by shifting it into the causal\ninference (CI). We adapt CDCR, traditionally applied in the news domain, into\nan RL model to operate at the passage level, similar to NLI and STS, while\naccommodating the process industry's specific text formats, which contain\nunstructured text and structured record attributes. Our RL model outperformed\nthe best versions of NLI- and STS-driven baselines by 28% (11.43 points) and\n27% (11.21 points), respectively. Our work demonstrates how domain adaptation\nof the state-of-the-art CDCR models, enhanced with reasoning capabilities, can\nbe effectively tailored to the process industry, improving data quality and\nconnectivity in shift logs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge management (KM) is vital in the process industry for optimizing\noperations, ensuring safety, and enabling continuous improvement through\neffective use of operational data and past insights. A key challenge in this\ndomain is the fragmented nature of event logs in shift books, where related\nrecords, e.g., entries documenting issues related to equipment or processes and\nthe corresponding solutions, may remain disconnected. This fragmentation\nhinders the recommendation of previous solutions to the users. To address this\nproblem, we investigate record linking (RL) as link prediction, commonly\nstudied in graph-based machine learning, by framing it as a cross-document\ncoreference resolution (CDCR) task enhanced with natural language inference\n(NLI) and semantic text similarity (STS) by shifting it into the causal\ninference (CI). We adapt CDCR, traditionally applied in the news domain, into\nan RL model to operate at the passage level, similar to NLI and STS, while\naccommodating the process industry's specific text formats, which contain\nunstructured text and structured record attributes. Our RL model outperformed\nthe best versions of NLI- and STS-driven baselines by 28% (11.43 points) and\n27% (11.21 points), respectively. Our work demonstrates how domain adaptation\nof the state-of-the-art CDCR models, enhanced with reasoning capabilities, can\nbe effectively tailored to the process industry, improving data quality and\nconnectivity in shift logs."
                },
                "authors": [
                    {
                        "name": "Anastasia Zhukova"
                    },
                    {
                        "name": "Thomas Walton"
                    },
                    {
                        "name": "Christian E. Matt"
                    },
                    {
                        "name": "Bela Gipp"
                    }
                ],
                "author_detail": {
                    "name": "Bela Gipp"
                },
                "author": "Bela Gipp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09093v1",
                "updated": "2025-08-12T17:17:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    17,
                    51,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:17:51Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    17,
                    51,
                    1,
                    224,
                    0
                ],
                "title": "Scaling Up Active Testing to Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up Active Testing to Large Language Models"
                },
                "summary": "Active testing enables label-efficient evaluation of models through careful\ndata acquisition. However, its significant computational costs have previously\nundermined its use for large models. We show how it can be successfully scaled\nup to the evaluation of large language models (LLMs). In particular we show\nthat the surrogate model used to guide data acquisition can be constructed\ncheaply using in-context learning, does not require updating within an\nactive-testing loop, and can be smaller than the target model. We even find we\ncan make good data-acquisition decisions without computing predictions with the\ntarget model and further introduce a single-run error estimator to asses how\nwell active testing is working on the fly. We find that our approach is able to\nmore effectively evaluate LLM performance with less data than current standard\npractices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active testing enables label-efficient evaluation of models through careful\ndata acquisition. However, its significant computational costs have previously\nundermined its use for large models. We show how it can be successfully scaled\nup to the evaluation of large language models (LLMs). In particular we show\nthat the surrogate model used to guide data acquisition can be constructed\ncheaply using in-context learning, does not require updating within an\nactive-testing loop, and can be smaller than the target model. We even find we\ncan make good data-acquisition decisions without computing predictions with the\ntarget model and further introduce a single-run error estimator to asses how\nwell active testing is working on the fly. We find that our approach is able to\nmore effectively evaluate LLM performance with less data than current standard\npractices."
                },
                "authors": [
                    {
                        "name": "Gabrielle Berrada"
                    },
                    {
                        "name": "Jannik Kossen"
                    },
                    {
                        "name": "Muhammed Razzak"
                    },
                    {
                        "name": "Freddie Bickford Smith"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Tom Rainforth"
                    }
                ],
                "author_detail": {
                    "name": "Tom Rainforth"
                },
                "author": "Tom Rainforth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09091v1",
                "updated": "2025-08-12T17:17:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    17,
                    13,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:17:13Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    17,
                    13,
                    1,
                    224,
                    0
                ],
                "title": "Utilizing Multilingual Encoders to Improve Large Language Models for\n  Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Multilingual Encoders to Improve Large Language Models for\n  Low-Resource Languages"
                },
                "summary": "Large Language Models (LLMs) excel in English, but their performance degrades\nsignificantly on low-resource languages (LRLs) due to English-centric training.\nWhile methods like LangBridge align LLMs with multilingual encoders such as the\nMassively Multilingual Text-to-Text Transfer Transformer (mT5), they typically\nuse only the final encoder layer. We propose a novel architecture that fuses\nall intermediate layers, enriching the linguistic information passed to the\nLLM. Our approach features two strategies: (1) a Global Softmax weighting for\noverall layer importance, and (2) a Transformer Softmax model that learns\ntoken-specific weights. The fused representations are mapped into the LLM's\nembedding space, enabling it to process multilingual inputs. The model is\ntrained only on English data, without using any parallel or multilingual data.\nEvaluated on XNLI, IndicXNLI, Sinhala News Classification, and Amazon Reviews,\nour Transformer Softmax model significantly outperforms the LangBridge\nbaseline. We observe strong performance gains in LRLs, improving Sinhala\nclassification accuracy from 71.66% to 75.86% and achieving clear improvements\nacross Indic languages such as Tamil, Bengali, and Malayalam. These specific\ngains contribute to an overall boost in average XNLI accuracy from 70.36% to\n71.50%. This approach offers a scalable, data-efficient path toward more\ncapable and equitable multilingual LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in English, but their performance degrades\nsignificantly on low-resource languages (LRLs) due to English-centric training.\nWhile methods like LangBridge align LLMs with multilingual encoders such as the\nMassively Multilingual Text-to-Text Transfer Transformer (mT5), they typically\nuse only the final encoder layer. We propose a novel architecture that fuses\nall intermediate layers, enriching the linguistic information passed to the\nLLM. Our approach features two strategies: (1) a Global Softmax weighting for\noverall layer importance, and (2) a Transformer Softmax model that learns\ntoken-specific weights. The fused representations are mapped into the LLM's\nembedding space, enabling it to process multilingual inputs. The model is\ntrained only on English data, without using any parallel or multilingual data.\nEvaluated on XNLI, IndicXNLI, Sinhala News Classification, and Amazon Reviews,\nour Transformer Softmax model significantly outperforms the LangBridge\nbaseline. We observe strong performance gains in LRLs, improving Sinhala\nclassification accuracy from 71.66% to 75.86% and achieving clear improvements\nacross Indic languages such as Tamil, Bengali, and Malayalam. These specific\ngains contribute to an overall boost in average XNLI accuracy from 70.36% to\n71.50%. This approach offers a scalable, data-efficient path toward more\ncapable and equitable multilingual LLMs."
                },
                "authors": [
                    {
                        "name": "Imalsha Puranegedara"
                    },
                    {
                        "name": "Themira Chathumina"
                    },
                    {
                        "name": "Nisal Ranathunga"
                    },
                    {
                        "name": "Nisansa de Silva"
                    },
                    {
                        "name": "Surangika Ranathunga"
                    },
                    {
                        "name": "Mokanarangan Thayaparan"
                    }
                ],
                "author_detail": {
                    "name": "Mokanarangan Thayaparan"
                },
                "author": "Mokanarangan Thayaparan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09090v2",
                "updated": "2025-08-13T01:51:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    51,
                    32,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T17:16:37Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    16,
                    37,
                    1,
                    224,
                    0
                ],
                "title": "SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via\n  Codebooks for recommender system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via\n  Codebooks for recommender system"
                },
                "summary": "Modeling multi-interests has arisen as a core problem in real-world RS.\nCurrent multi-interest retrieval methods pose three major challenges: 1)\nInterests, typically extracted from predefined external knowledge, are\ninvariant. Failed to dynamically evolve with users' real-time consumption\npreferences. 2) Online inference typically employs an over-exploited strategy,\nmainly matching users' existing interests, lacking proactive exploration and\ndiscovery of novel and long-tail interests. To address these challenges, we\npropose a novel retrieval framework named SPARC(Soft Probabilistic Adaptive\nRetrieval Model via Codebooks). Our contribution is two folds. First, the\nframework utilizes Residual Quantized Variational Autoencoder (RQ-VAE) to\nconstruct a discretized interest space. It achieves joint training of the\nRQ-VAE with the industrial large scale recommendation model, mining\nbehavior-aware interests that can perceive user feedback and evolve\ndynamically. Secondly, a probabilistic interest module that predicts the\nprobability distribution over the entire dynamic and discrete interest space.\nThis facilitates an efficient \"soft-search\" strategy during online inference,\nrevolutionizing the retrieval paradigm from \"passive matching\" to \"proactive\nexploration\" and thereby effectively promoting interest discovery. Online A/B\ntests on an industrial platform with tens of millions daily active users, have\nachieved substantial gains in business metrics: +0.9% increase in user view\nduration, +0.4% increase in user page views (PV), and a +22.7% improvement in\nPV500(new content reaching 500 PVs in 24 hours). Offline evaluations are\nconducted on open-source Amazon Product datasets. Metrics, such as Recall@K and\nNormalized Discounted Cumulative Gain@K(NDCG@K), also showed consistent\nimprovement. Both online and offline experiments validate the efficacy and\npractical value of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling multi-interests has arisen as a core problem in real-world RS.\nCurrent multi-interest retrieval methods pose three major challenges: 1)\nInterests, typically extracted from predefined external knowledge, are\ninvariant. Failed to dynamically evolve with users' real-time consumption\npreferences. 2) Online inference typically employs an over-exploited strategy,\nmainly matching users' existing interests, lacking proactive exploration and\ndiscovery of novel and long-tail interests. To address these challenges, we\npropose a novel retrieval framework named SPARC(Soft Probabilistic Adaptive\nRetrieval Model via Codebooks). Our contribution is two folds. First, the\nframework utilizes Residual Quantized Variational Autoencoder (RQ-VAE) to\nconstruct a discretized interest space. It achieves joint training of the\nRQ-VAE with the industrial large scale recommendation model, mining\nbehavior-aware interests that can perceive user feedback and evolve\ndynamically. Secondly, a probabilistic interest module that predicts the\nprobability distribution over the entire dynamic and discrete interest space.\nThis facilitates an efficient \"soft-search\" strategy during online inference,\nrevolutionizing the retrieval paradigm from \"passive matching\" to \"proactive\nexploration\" and thereby effectively promoting interest discovery. Online A/B\ntests on an industrial platform with tens of millions daily active users, have\nachieved substantial gains in business metrics: +0.9% increase in user view\nduration, +0.4% increase in user page views (PV), and a +22.7% improvement in\nPV500(new content reaching 500 PVs in 24 hours). Offline evaluations are\nconducted on open-source Amazon Product datasets. Metrics, such as Recall@K and\nNormalized Discounted Cumulative Gain@K(NDCG@K), also showed consistent\nimprovement. Both online and offline experiments validate the efficacy and\npractical value of the proposed method."
                },
                "authors": [
                    {
                        "name": "Jialiang Shi"
                    },
                    {
                        "name": "Yaguang Dou"
                    },
                    {
                        "name": "Tian Qi"
                    }
                ],
                "author_detail": {
                    "name": "Tian Qi"
                },
                "author": "Tian Qi",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09087v1",
                "updated": "2025-08-12T17:07:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    7,
                    58,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:07:58Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    7,
                    58,
                    1,
                    224,
                    0
                ],
                "title": "Addressing Bias in VLMs for Glaucoma Detection Without Protected\n  Attribute Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Bias in VLMs for Glaucoma Detection Without Protected\n  Attribute Supervision"
                },
                "summary": "Vision-Language Models (VLMs) have achieved remarkable success on multimodal\ntasks such as image-text retrieval and zero-shot classification, yet they can\nexhibit demographic biases even when explicit protected attributes are absent\nduring training. In this work, we focus on automated glaucoma screening from\nretinal fundus images, a critical application given that glaucoma is a leading\ncause of irreversible blindness and disproportionately affects underserved\npopulations. Building on a reweighting-based contrastive learning framework, we\nintroduce an attribute-agnostic debiasing method that (i) infers proxy\nsubgroups via unsupervised clustering of image-image embeddings, (ii) computes\ngradient-similarity weights between the CLIP-style multimodal loss and a\nSimCLR-style image-pair contrastive loss, and (iii) applies these weights in a\njoint, top-$k$ weighted objective to upweight underperforming clusters. This\nlabel-free approach adaptively targets the hardest examples, thereby reducing\nsubgroup disparities. We evaluate our method on the Harvard FairVLMed glaucoma\nsubset, reporting Equalized Odds Distance (EOD), Equalized Subgroup AUC (ES\nAUC), and Groupwise AUC to demonstrate equitable performance across inferred\ndemographic subgroups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have achieved remarkable success on multimodal\ntasks such as image-text retrieval and zero-shot classification, yet they can\nexhibit demographic biases even when explicit protected attributes are absent\nduring training. In this work, we focus on automated glaucoma screening from\nretinal fundus images, a critical application given that glaucoma is a leading\ncause of irreversible blindness and disproportionately affects underserved\npopulations. Building on a reweighting-based contrastive learning framework, we\nintroduce an attribute-agnostic debiasing method that (i) infers proxy\nsubgroups via unsupervised clustering of image-image embeddings, (ii) computes\ngradient-similarity weights between the CLIP-style multimodal loss and a\nSimCLR-style image-pair contrastive loss, and (iii) applies these weights in a\njoint, top-$k$ weighted objective to upweight underperforming clusters. This\nlabel-free approach adaptively targets the hardest examples, thereby reducing\nsubgroup disparities. We evaluate our method on the Harvard FairVLMed glaucoma\nsubset, reporting Equalized Odds Distance (EOD), Equalized Subgroup AUC (ES\nAUC), and Groupwise AUC to demonstrate equitable performance across inferred\ndemographic subgroups."
                },
                "authors": [
                    {
                        "name": "Ahsan Habib Akash"
                    },
                    {
                        "name": "Greg Murray"
                    },
                    {
                        "name": "Annahita Amireskandari"
                    },
                    {
                        "name": "Joel Palko"
                    },
                    {
                        "name": "Carol Laxson"
                    },
                    {
                        "name": "Binod Bhattarai"
                    },
                    {
                        "name": "Prashnna Gyawali"
                    }
                ],
                "author_detail": {
                    "name": "Prashnna Gyawali"
                },
                "author": "Prashnna Gyawali",
                "arxiv_comment": "3rd Workshop in Data Engineering in Medical Imaging (DEMI),\n  MICCAI-2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14805v2",
                "updated": "2025-08-12T17:07:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    7,
                    53,
                    1,
                    224,
                    0
                ],
                "published": "2025-06-03T13:44:14Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    44,
                    14,
                    1,
                    154,
                    0
                ],
                "title": "Argus Inspection: Do Multimodal Large Language Models Possess the Eye of\n  Panoptes?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argus Inspection: Do Multimodal Large Language Models Possess the Eye of\n  Panoptes?"
                },
                "summary": "As Multimodal Large Language Models (MLLMs) continue to evolve, their\ncognitive and reasoning capabilities have seen remarkable progress. However,\nchallenges in visual fine-grained perception and commonsense causal inference\npersist. This paper introduces Argus Inspection, a multimodal benchmark with\ntwo levels of difficulty, emphasizing detailed visual recognition while\nincorporating real-world commonsense understanding to evaluate causal reasoning\nabilities. Expanding on it, we present the Eye of Panoptes framework, which\nintegrates a binary parametric Sigmoid metric with an indicator function,\nenabling a more holistic evaluation of MLLMs' responses in opinion-based\nreasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the\nhighest performance in visual fine-grained reasoning reaches only 0.46,\nhighlighting considerable potential for enhancement. Our research offers\nvaluable perspectives for the continued refinement of MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Multimodal Large Language Models (MLLMs) continue to evolve, their\ncognitive and reasoning capabilities have seen remarkable progress. However,\nchallenges in visual fine-grained perception and commonsense causal inference\npersist. This paper introduces Argus Inspection, a multimodal benchmark with\ntwo levels of difficulty, emphasizing detailed visual recognition while\nincorporating real-world commonsense understanding to evaluate causal reasoning\nabilities. Expanding on it, we present the Eye of Panoptes framework, which\nintegrates a binary parametric Sigmoid metric with an indicator function,\nenabling a more holistic evaluation of MLLMs' responses in opinion-based\nreasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the\nhighest performance in visual fine-grained reasoning reaches only 0.46,\nhighlighting considerable potential for enhancement. Our research offers\nvaluable perspectives for the continued refinement of MLLMs."
                },
                "authors": [
                    {
                        "name": "Yang Yao"
                    },
                    {
                        "name": "Lingyu Li"
                    },
                    {
                        "name": "Jiaxin Song"
                    },
                    {
                        "name": "Chiyu Chen"
                    },
                    {
                        "name": "Zhenqi He"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Tianle Gu"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Yan Teng"
                    },
                    {
                        "name": "Yingchun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yingchun Wang"
                },
                "author": "Yingchun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09075v1",
                "updated": "2025-08-12T16:50:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    50,
                    2,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:50:02Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    50,
                    2,
                    1,
                    224,
                    0
                ],
                "title": "Scaling Learned Image Compression Models up to 1 Billion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Learned Image Compression Models up to 1 Billion"
                },
                "summary": "Recent advances in large language models (LLMs) highlight a strong connection\nbetween intelligence and compression. Learned image compression, a fundamental\ntask in modern data compression, has made significant progress in recent years.\nHowever, current models remain limited in scale, restricting their\nrepresentation capacity, and how scaling model size influences compression\nperformance remains unexplored. In this work, we present a pioneering study on\nscaling up learned image compression models and revealing the performance\ntrends through scaling laws. Using the recent state-of-the-art HPCM model as\nbaseline, we scale model parameters from 68.5 millions to 1 billion and fit\npower-law relations between test loss and key scaling variables, including\nmodel size and optimal training compute. The results reveal a scaling trend,\nenabling extrapolation to larger scale models. Experimental results demonstrate\nthat the scaled-up HPCM-1B model achieves state-of-the-art rate-distortion\nperformance. We hope this work inspires future exploration of large-scale\ncompression models and deeper investigations into the connection between\ncompression and intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) highlight a strong connection\nbetween intelligence and compression. Learned image compression, a fundamental\ntask in modern data compression, has made significant progress in recent years.\nHowever, current models remain limited in scale, restricting their\nrepresentation capacity, and how scaling model size influences compression\nperformance remains unexplored. In this work, we present a pioneering study on\nscaling up learned image compression models and revealing the performance\ntrends through scaling laws. Using the recent state-of-the-art HPCM model as\nbaseline, we scale model parameters from 68.5 millions to 1 billion and fit\npower-law relations between test loss and key scaling variables, including\nmodel size and optimal training compute. The results reveal a scaling trend,\nenabling extrapolation to larger scale models. Experimental results demonstrate\nthat the scaled-up HPCM-1B model achieves state-of-the-art rate-distortion\nperformance. We hope this work inspires future exploration of large-scale\ncompression models and deeper investigations into the connection between\ncompression and intelligence."
                },
                "authors": [
                    {
                        "name": "Yuqi Li"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v1",
                "updated": "2025-08-12T16:47:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Yi Fei"
                    },
                    {
                        "name": "Zeng Weidi"
                    }
                ],
                "author_detail": {
                    "name": "Zeng Weidi"
                },
                "author": "Zeng Weidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07979v2",
                "updated": "2025-08-12T16:47:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    29,
                    1,
                    224,
                    0
                ],
                "published": "2025-03-11T02:27:37Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    2,
                    27,
                    37,
                    1,
                    70,
                    0
                ],
                "title": "Achieving More with Less: Additive Prompt Tuning for Rehearsal-Free\n  Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving More with Less: Additive Prompt Tuning for Rehearsal-Free\n  Class-Incremental Learning"
                },
                "summary": "Class-incremental learning (CIL) enables models to learn new classes\nprogressively while preserving knowledge of previously learned ones. Recent\nadvances in this field have shifted towards parameter-efficient fine-tuning\ntechniques, with many approaches building upon the framework that maintains a\npool of learnable prompts. Although effective, these methods introduce\nsubstantial computational overhead, primarily due to prompt pool querying and\nincreased input sequence lengths from prompt concatenation. In this work, we\npresent a novel prompt-based approach that addresses this limitation. Our\nmethod trains a single set of shared prompts across all tasks and, rather than\nconcatenating prompts to the input, directly modifies the CLS token's attention\ncomputation by adding the prompts to it. This simple and lightweight design not\nonly significantly reduces computational complexity-both in terms of inference\ncosts and the number of trainable parameters-but also eliminates the need to\noptimize prompt lengths for different downstream tasks, offering a more\nefficient yet powerful solution for rehearsal-free class-incremental learning.\nExtensive experiments across a diverse range of CIL benchmarks demonstrate the\neffectiveness of our approach, highlighting its potential to establish a new\nprompt-based CIL paradigm. Furthermore, experiments on general recognition\nbenchmarks beyond the CIL setting also show strong performance, positioning our\nmethod as a promising candidate for a general parameter-efficient fine-tuning\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Class-incremental learning (CIL) enables models to learn new classes\nprogressively while preserving knowledge of previously learned ones. Recent\nadvances in this field have shifted towards parameter-efficient fine-tuning\ntechniques, with many approaches building upon the framework that maintains a\npool of learnable prompts. Although effective, these methods introduce\nsubstantial computational overhead, primarily due to prompt pool querying and\nincreased input sequence lengths from prompt concatenation. In this work, we\npresent a novel prompt-based approach that addresses this limitation. Our\nmethod trains a single set of shared prompts across all tasks and, rather than\nconcatenating prompts to the input, directly modifies the CLS token's attention\ncomputation by adding the prompts to it. This simple and lightweight design not\nonly significantly reduces computational complexity-both in terms of inference\ncosts and the number of trainable parameters-but also eliminates the need to\noptimize prompt lengths for different downstream tasks, offering a more\nefficient yet powerful solution for rehearsal-free class-incremental learning.\nExtensive experiments across a diverse range of CIL benchmarks demonstrate the\neffectiveness of our approach, highlighting its potential to establish a new\nprompt-based CIL paradigm. Furthermore, experiments on general recognition\nbenchmarks beyond the CIL setting also show strong performance, positioning our\nmethod as a promising candidate for a general parameter-efficient fine-tuning\napproach."
                },
                "authors": [
                    {
                        "name": "Haoran Chen"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09070v1",
                "updated": "2025-08-12T16:44:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    44,
                    45,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:44:45Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    44,
                    45,
                    1,
                    224,
                    0
                ],
                "title": "Study of the Connected Four-Point Correlation Function of Galaxies from\n  DESI Data Release 1 Luminous Red Galaxy Sample",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the Connected Four-Point Correlation Function of Galaxies from\n  DESI Data Release 1 Luminous Red Galaxy Sample"
                },
                "summary": "We present a measurement of the non-Gaussian four-point correlation function\n(4PCF) from the DESI DR1 Luminous Red Galaxy (LRG) sample. For the\ngravitationally induced parity-even 4PCF, we detect a signal with a\nsignificance of 14.7$\\sigma$ using our fiducial setup. We assess the robustness\nof this detection through a series of validation tests, including auto- and\ncross-correlation analyses, sky partitioning across multiple patch\ncombinations, and variations in radial scale cuts. Due to the low completeness\nof the sample, we find that differences in fiber assignment implementation\nschemes can significantly impact estimation of the covariance and introduce\nbiases in the data vector. After correcting for these effects, all tests yield\nconsistent results. This is one of the first measurements of the connected 4PCF\non the DESI LRG sample: the good agreement between the simulation and the data\nimplies that the amplitude of the density fluctuation inferred from the\nconnected 4PCF is consistent with the Planck $\\Lambda$CDM cosmology. The\nmethodology and diagnostic framework established in this work provide a\nfoundation for interpreting parity-odd 4PCF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a measurement of the non-Gaussian four-point correlation function\n(4PCF) from the DESI DR1 Luminous Red Galaxy (LRG) sample. For the\ngravitationally induced parity-even 4PCF, we detect a signal with a\nsignificance of 14.7$\\sigma$ using our fiducial setup. We assess the robustness\nof this detection through a series of validation tests, including auto- and\ncross-correlation analyses, sky partitioning across multiple patch\ncombinations, and variations in radial scale cuts. Due to the low completeness\nof the sample, we find that differences in fiber assignment implementation\nschemes can significantly impact estimation of the covariance and introduce\nbiases in the data vector. After correcting for these effects, all tests yield\nconsistent results. This is one of the first measurements of the connected 4PCF\non the DESI LRG sample: the good agreement between the simulation and the data\nimplies that the amplitude of the density fluctuation inferred from the\nconnected 4PCF is consistent with the Planck $\\Lambda$CDM cosmology. The\nmethodology and diagnostic framework established in this work provide a\nfoundation for interpreting parity-odd 4PCF."
                },
                "authors": [
                    {
                        "name": "J. Hou"
                    },
                    {
                        "name": "R. N. Cahn"
                    },
                    {
                        "name": "J. Aguilar"
                    },
                    {
                        "name": "S. Ahlen"
                    },
                    {
                        "name": "D. Bianchi"
                    },
                    {
                        "name": "D. Brooks"
                    },
                    {
                        "name": "T. Claybaugh"
                    },
                    {
                        "name": "P. Doel"
                    },
                    {
                        "name": "S. Ferraro"
                    },
                    {
                        "name": "J. E. Forero-Romero"
                    },
                    {
                        "name": "E. Gaztañaga"
                    },
                    {
                        "name": "L. Le Guillou"
                    },
                    {
                        "name": "G. Gutierrez"
                    },
                    {
                        "name": "K. Honscheid"
                    },
                    {
                        "name": "D. Huterer"
                    },
                    {
                        "name": "M. Ishak"
                    },
                    {
                        "name": "R. Joyce"
                    },
                    {
                        "name": "S. Juneau"
                    },
                    {
                        "name": "R. Kehoe"
                    },
                    {
                        "name": "D. Kirkby"
                    },
                    {
                        "name": "T. Kisner"
                    },
                    {
                        "name": "A. Kremin"
                    },
                    {
                        "name": "C. Lamman"
                    },
                    {
                        "name": "M. Landriau"
                    },
                    {
                        "name": "A. de la Macorra"
                    },
                    {
                        "name": "M. Manera"
                    },
                    {
                        "name": "A. de Mattia"
                    },
                    {
                        "name": "R. Miquel"
                    },
                    {
                        "name": "E. Mueller"
                    },
                    {
                        "name": "S. Nadathur"
                    },
                    {
                        "name": "G. Niz"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "F. Prada"
                    },
                    {
                        "name": "I. Pérez-Ràfols"
                    },
                    {
                        "name": "A. J. Ross"
                    },
                    {
                        "name": "G. Rossi"
                    },
                    {
                        "name": "E. Sanchez"
                    },
                    {
                        "name": "D. Schlegel"
                    },
                    {
                        "name": "M. Schubnell"
                    },
                    {
                        "name": "H. Seo"
                    },
                    {
                        "name": "J. Silber"
                    },
                    {
                        "name": "Z. Slepian"
                    },
                    {
                        "name": "D. Sprayberry"
                    },
                    {
                        "name": "G. Tarlé"
                    },
                    {
                        "name": "B. A. Weaver"
                    },
                    {
                        "name": "H. Zou"
                    }
                ],
                "author_detail": {
                    "name": "H. Zou"
                },
                "author": "H. Zou",
                "arxiv_comment": "28 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04903v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04903v3",
                "updated": "2025-08-12T16:29:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    29,
                    5,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-06T21:59:34Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    21,
                    59,
                    34,
                    2,
                    218,
                    0
                ],
                "title": "RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM\n  Systems with Structured Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM\n  Systems with Structured Memory"
                },
                "summary": "Multi-agent large language model (LLM) systems have shown strong potential in\ncomplex reasoning and collaborative decision-making tasks. However, most\nexisting coordination schemes rely on static or full-context routing\nstrategies, which lead to excessive token consumption, redundant memory\nexposure, and limited adaptability across interaction rounds. We introduce\nRCR-Router, a modular and role-aware context routing framework designed to\nenable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,\nthis is the first routing approach that dynamically selects semantically\nrelevant memory subsets for each agent based on its role and task stage, while\nadhering to a strict token budget. A lightweight scoring policy guides memory\nselection, and agent outputs are iteratively integrated into a shared memory\nstore to facilitate progressive context refinement. To better evaluate model\nbehavior, we further propose an Answer Quality Score metric that captures\nLLM-generated explanations beyond standard QA accuracy. Experiments on three\nmulti-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate\nthat RCR-Router reduces token usage (up to 30%) while improving or maintaining\nanswer quality. These results highlight the importance of structured memory\nrouting and output-aware evaluation in advancing scalable multi-agent LLM\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent large language model (LLM) systems have shown strong potential in\ncomplex reasoning and collaborative decision-making tasks. However, most\nexisting coordination schemes rely on static or full-context routing\nstrategies, which lead to excessive token consumption, redundant memory\nexposure, and limited adaptability across interaction rounds. We introduce\nRCR-Router, a modular and role-aware context routing framework designed to\nenable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,\nthis is the first routing approach that dynamically selects semantically\nrelevant memory subsets for each agent based on its role and task stage, while\nadhering to a strict token budget. A lightweight scoring policy guides memory\nselection, and agent outputs are iteratively integrated into a shared memory\nstore to facilitate progressive context refinement. To better evaluate model\nbehavior, we further propose an Answer Quality Score metric that captures\nLLM-generated explanations beyond standard QA accuracy. Experiments on three\nmulti-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate\nthat RCR-Router reduces token usage (up to 30%) while improving or maintaining\nanswer quality. These results highlight the importance of structured memory\nrouting and output-aware evaluation in advancing scalable multi-agent LLM\nsystems."
                },
                "authors": [
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zhenglun Kong"
                    },
                    {
                        "name": "Changdi Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Tianqi Li"
                    },
                    {
                        "name": "Peiyan Dong"
                    },
                    {
                        "name": "Joannah Nanjekye"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Geng Yuan"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Wenbin Zhang"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Xue Lin"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Yanzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhi Wang"
                },
                "author": "Yanzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04903v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04903v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13747v3",
                "updated": "2025-08-12T16:25:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    25,
                    56,
                    1,
                    224,
                    0
                ],
                "published": "2024-10-17T16:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    16,
                    45,
                    26,
                    3,
                    291,
                    0
                ],
                "title": "BayeSN and SALT: A Comparison of Dust Inference Across SN Ia Light-curve\n  Models with DES5YR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayeSN and SALT: A Comparison of Dust Inference Across SN Ia Light-curve\n  Models with DES5YR"
                },
                "summary": "In recent years there has been significant debate around the impact of dust\non SNe Ia, a major source of uncertainty in cosmological analyses. We perform\nthe first validation of the probabilistic hierarchical SN Ia SED model BayeSN\non the conventional SALT model, an important test given the history of\nconflicting conclusions regarding the distributions of host galaxy dust\nproperties between the two. Applying BayeSN to SALT-based simulations, we find\nthat BayeSN is able to accurately recover our simulated inputs and successfully\ndisentangle differences in dust extinction from an intrinsic mass step. This\nvalidates BayeSN as a method to identify the relative contributions of dust and\nintrinsic differences in explaining the mass step. When inferring dust\nparameters with simulated samples including non-Ia contamination, we find that\nour choice of photometric classifier causes a bias in the inferred dust\ndistribution; this arises because SNe Ia heavily impacted by dust are\nmisclassified as contaminants and excluded. We then apply BayeSN to the sample\nof SNe from DES5YR to jointly infer host galaxy dust distributions and\nintrinsic differences on either side of the `mass step' at $10^{10}$ M$\\odot$.\nWe find evidence in favour of an intrinsic contribution to the mass step and\ndiffering $R_V$ distributions. We also build on recent results supporting an\nenvironmental-dependence on the secondary maximum of SNe Ia in $i$-band. Twenty\ndays post-peak, we find an offset in intrinsic $i$-band light curve between\neach mass bin at a significance in excess of $3\\sigma$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years there has been significant debate around the impact of dust\non SNe Ia, a major source of uncertainty in cosmological analyses. We perform\nthe first validation of the probabilistic hierarchical SN Ia SED model BayeSN\non the conventional SALT model, an important test given the history of\nconflicting conclusions regarding the distributions of host galaxy dust\nproperties between the two. Applying BayeSN to SALT-based simulations, we find\nthat BayeSN is able to accurately recover our simulated inputs and successfully\ndisentangle differences in dust extinction from an intrinsic mass step. This\nvalidates BayeSN as a method to identify the relative contributions of dust and\nintrinsic differences in explaining the mass step. When inferring dust\nparameters with simulated samples including non-Ia contamination, we find that\nour choice of photometric classifier causes a bias in the inferred dust\ndistribution; this arises because SNe Ia heavily impacted by dust are\nmisclassified as contaminants and excluded. We then apply BayeSN to the sample\nof SNe from DES5YR to jointly infer host galaxy dust distributions and\nintrinsic differences on either side of the `mass step' at $10^{10}$ M$\\odot$.\nWe find evidence in favour of an intrinsic contribution to the mass step and\ndiffering $R_V$ distributions. We also build on recent results supporting an\nenvironmental-dependence on the secondary maximum of SNe Ia in $i$-band. Twenty\ndays post-peak, we find an offset in intrinsic $i$-band light curve between\neach mass bin at a significance in excess of $3\\sigma$."
                },
                "authors": [
                    {
                        "name": "Matthew Grayling"
                    },
                    {
                        "name": "Brodie Popovic"
                    }
                ],
                "author_detail": {
                    "name": "Brodie Popovic"
                },
                "author": "Brodie Popovic",
                "arxiv_comment": "14 pages, 2 figures, 10 tables. Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08178v2",
                "updated": "2025-08-12T16:25:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    25,
                    31,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T16:59:14Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    59,
                    14,
                    0,
                    223,
                    0
                ],
                "title": "3D Human Mesh Estimation from Single View RGBD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Human Mesh Estimation from Single View RGBD"
                },
                "summary": "Despite significant progress in 3D human mesh estimation from RGB images;\nRGBD cameras, offering additional depth data, remain underutilized. In this\npaper, we present a method for accurate 3D human mesh estimation from a single\nRGBD view, leveraging the affordability and widespread adoption of RGBD cameras\nfor real-world applications. A fully supervised approach for this problem,\nrequires a dataset with RGBD image and 3D mesh label pairs. However, collecting\nsuch a dataset is costly and challenging, hence, existing datasets are small,\nand limited in pose and shape diversity. To overcome this data scarcity, we\nleverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D\nmeshes from the body models found in MoCap datasets, and create partial,\nsingle-view versions of them by projection to a virtual camera. This simulates\nthe depth data provided by an RGBD camera from a single viewpoint. Then, we\ntrain a masked autoencoder to complete the partial, single-view mesh. During\ninference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',\nmatches the depth values coming from the sensor to vertices of a template human\nmesh, which creates a partial, single-view mesh. We effectively recover parts\nof the 3D human body mesh model that are not visible, resulting in a full body\nmesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL\nand CAPE datasets, respectively; outperforming existing methods that use\nfull-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE\ndataset, outperforming a recently published RGB based method by 18.4 mm,\nhighlighting the usefulness of depth data. Code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress in 3D human mesh estimation from RGB images;\nRGBD cameras, offering additional depth data, remain underutilized. In this\npaper, we present a method for accurate 3D human mesh estimation from a single\nRGBD view, leveraging the affordability and widespread adoption of RGBD cameras\nfor real-world applications. A fully supervised approach for this problem,\nrequires a dataset with RGBD image and 3D mesh label pairs. However, collecting\nsuch a dataset is costly and challenging, hence, existing datasets are small,\nand limited in pose and shape diversity. To overcome this data scarcity, we\nleverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D\nmeshes from the body models found in MoCap datasets, and create partial,\nsingle-view versions of them by projection to a virtual camera. This simulates\nthe depth data provided by an RGBD camera from a single viewpoint. Then, we\ntrain a masked autoencoder to complete the partial, single-view mesh. During\ninference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',\nmatches the depth values coming from the sensor to vertices of a template human\nmesh, which creates a partial, single-view mesh. We effectively recover parts\nof the 3D human body mesh model that are not visible, resulting in a full body\nmesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL\nand CAPE datasets, respectively; outperforming existing methods that use\nfull-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE\ndataset, outperforming a recently published RGB based method by 18.4 mm,\nhighlighting the usefulness of depth data. Code will be released."
                },
                "authors": [
                    {
                        "name": "Ozhan Suat"
                    },
                    {
                        "name": "Bedirhan Uguz"
                    },
                    {
                        "name": "Batuhan Karagoz"
                    },
                    {
                        "name": "Muhammed Can Keles"
                    },
                    {
                        "name": "Emre Akbas"
                    }
                ],
                "author_detail": {
                    "name": "Emre Akbas"
                },
                "author": "Emre Akbas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09042v1",
                "updated": "2025-08-12T16:03:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    3,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:03:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    3,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "LLM-as-a-Supervisor: Mistaken Therapeutic Behaviors Trigger Targeted\n  Supervisory Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Supervisor: Mistaken Therapeutic Behaviors Trigger Targeted\n  Supervisory Feedback"
                },
                "summary": "Although large language models (LLMs) hold significant promise in\npsychotherapy, their direct application in patient-facing scenarios raises\nethical and safety concerns. Therefore, this work shifts towards developing an\nLLM as a supervisor to train real therapists. In addition to the privacy of\nclinical therapist training data, a fundamental contradiction complicates the\ntraining of therapeutic behaviors: clear feedback standards are necessary to\nensure a controlled training system, yet there is no absolute \"gold standard\"\nfor appropriate therapeutic behaviors in practice. In contrast, many common\ntherapeutic mistakes are universal and identifiable, making them effective\ntriggers for targeted feedback that can serve as clearer evidence. Motivated by\nthis, we create a novel therapist-training paradigm: (1) guidelines for\nmistaken behaviors and targeted correction strategies are first established as\nstandards; (2) a human-in-the-loop dialogue-feedback dataset is then\nconstructed, where a mistake-prone agent intentionally makes standard mistakes\nduring interviews naturally, and a supervisor agent locates and identifies\nmistakes and provides targeted feedback; (3) after fine-tuning on this dataset,\nthe final supervisor model is provided for real therapist training. The\ndetailed experimental results of automated, human and downstream assessments\ndemonstrate that models fine-tuned on our dataset MATE, can provide\nhigh-quality feedback according to the clinical guideline, showing significant\npotential for the therapist training scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) hold significant promise in\npsychotherapy, their direct application in patient-facing scenarios raises\nethical and safety concerns. Therefore, this work shifts towards developing an\nLLM as a supervisor to train real therapists. In addition to the privacy of\nclinical therapist training data, a fundamental contradiction complicates the\ntraining of therapeutic behaviors: clear feedback standards are necessary to\nensure a controlled training system, yet there is no absolute \"gold standard\"\nfor appropriate therapeutic behaviors in practice. In contrast, many common\ntherapeutic mistakes are universal and identifiable, making them effective\ntriggers for targeted feedback that can serve as clearer evidence. Motivated by\nthis, we create a novel therapist-training paradigm: (1) guidelines for\nmistaken behaviors and targeted correction strategies are first established as\nstandards; (2) a human-in-the-loop dialogue-feedback dataset is then\nconstructed, where a mistake-prone agent intentionally makes standard mistakes\nduring interviews naturally, and a supervisor agent locates and identifies\nmistakes and provides targeted feedback; (3) after fine-tuning on this dataset,\nthe final supervisor model is provided for real therapist training. The\ndetailed experimental results of automated, human and downstream assessments\ndemonstrate that models fine-tuned on our dataset MATE, can provide\nhigh-quality feedback according to the clinical guideline, showing significant\npotential for the therapist training scenario."
                },
                "authors": [
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Zhenyu Lv"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Xianyang Wang"
                    },
                    {
                        "name": "Luyao Ji"
                    },
                    {
                        "name": "Leyang Cui"
                    },
                    {
                        "name": "Minqiang Yang"
                    },
                    {
                        "name": "Jian Shen"
                    },
                    {
                        "name": "Qunxi Dong"
                    },
                    {
                        "name": "Xiuling Liu"
                    },
                    {
                        "name": "Juan Wang"
                    },
                    {
                        "name": "Bin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Hu"
                },
                "author": "Bin Hu",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01821v3",
                "updated": "2025-08-12T16:02:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    2,
                    52,
                    1,
                    224,
                    0
                ],
                "published": "2025-05-03T13:55:38Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    13,
                    55,
                    38,
                    5,
                    123,
                    0
                ],
                "title": "Edge-Cloud Collaborative Computing on Distributed Intelligence and Model\n  Optimization: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-Cloud Collaborative Computing on Distributed Intelligence and Model\n  Optimization: A Survey"
                },
                "summary": "Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm\nfor addressing the computational demands of modern intelligent applications,\nintegrating cloud resources with edge devices to enable efficient, low-latency\nprocessing. Recent advancements in AI, particularly deep learning and large\nlanguage models (LLMs), have dramatically enhanced the capabilities of these\ndistributed systems, yet introduce significant challenges in model deployment\nand resource management. In this survey, we comprehensive examine the\nintersection of distributed intelligence and model optimization within\nedge-cloud environments, providing a structured tutorial on fundamental\narchitectures, enabling technologies, and emerging applications. Additionally,\nwe systematically analyze model optimization approaches, including compression,\nadaptation, and neural architecture search, alongside AI-driven resource\nmanagement strategies that balance performance, energy efficiency, and latency\nrequirements. We further explore critical aspects of privacy protection and\nsecurity enhancement within ECCC systems and examines practical deployments\nthrough diverse applications, spanning autonomous driving, healthcare, and\nindustrial automation. Performance analysis and benchmarking techniques are\nalso thoroughly explored to establish evaluation standards for these complex\nsystems. Furthermore, the review identifies critical research directions\nincluding LLMs deployment, 6G integration, neuromorphic computing, and quantum\ncomputing, offering a roadmap for addressing persistent challenges in\nheterogeneity management, real-time processing, and scalability. By bridging\ntheoretical advancements and practical deployments, this survey offers\nresearchers and practitioners a holistic perspective on leveraging AI to\noptimize distributed computing environments, fostering innovation in\nnext-generation intelligent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm\nfor addressing the computational demands of modern intelligent applications,\nintegrating cloud resources with edge devices to enable efficient, low-latency\nprocessing. Recent advancements in AI, particularly deep learning and large\nlanguage models (LLMs), have dramatically enhanced the capabilities of these\ndistributed systems, yet introduce significant challenges in model deployment\nand resource management. In this survey, we comprehensive examine the\nintersection of distributed intelligence and model optimization within\nedge-cloud environments, providing a structured tutorial on fundamental\narchitectures, enabling technologies, and emerging applications. Additionally,\nwe systematically analyze model optimization approaches, including compression,\nadaptation, and neural architecture search, alongside AI-driven resource\nmanagement strategies that balance performance, energy efficiency, and latency\nrequirements. We further explore critical aspects of privacy protection and\nsecurity enhancement within ECCC systems and examines practical deployments\nthrough diverse applications, spanning autonomous driving, healthcare, and\nindustrial automation. Performance analysis and benchmarking techniques are\nalso thoroughly explored to establish evaluation standards for these complex\nsystems. Furthermore, the review identifies critical research directions\nincluding LLMs deployment, 6G integration, neuromorphic computing, and quantum\ncomputing, offering a roadmap for addressing persistent challenges in\nheterogeneity management, real-time processing, and scalability. By bridging\ntheoretical advancements and practical deployments, this survey offers\nresearchers and practitioners a holistic perspective on leveraging AI to\noptimize distributed computing environments, fostering innovation in\nnext-generation intelligent systems."
                },
                "authors": [
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yao Du"
                    },
                    {
                        "name": "Kun Yang"
                    },
                    {
                        "name": "Jiaqi Wu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Xiping Hu"
                    },
                    {
                        "name": "Zehua Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Azzedine Boukerche"
                    },
                    {
                        "name": "Victor C. M. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Victor C. M. Leung"
                },
                "author": "Victor C. M. Leung",
                "arxiv_comment": "30 pages, 10 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09036v1",
                "updated": "2025-08-12T15:57:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    57,
                    22,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:57:22Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    57,
                    22,
                    1,
                    224,
                    0
                ],
                "title": "Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy\n  and AI Governance Exams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy\n  and AI Governance Exams"
                },
                "summary": "The rapid emergence of large language models (LLMs) has raised urgent\nquestions across the modern workforce about this new technology's strengths,\nweaknesses, and capabilities. For privacy professionals, the question is\nwhether these AI systems can provide reliable support on regulatory compliance,\nprivacy program management, and AI governance. In this study, we evaluate ten\nleading open and closed LLMs, including models from OpenAI, Anthropic, Google\nDeepMind, Meta, and DeepSeek, by benchmarking their performance on\nindustry-standard certification exams: CIPP/US, CIPM, CIPT, and AIGP from the\nInternational Association of Privacy Professionals (IAPP). Each model was\ntested using official sample exams in a closed-book setting and compared to\nIAPP's passing thresholds. Our findings show that several frontier models such\nas Gemini 2.5 Pro and OpenAI's GPT-5 consistently achieve scores exceeding the\nstandards for professional human certification - demonstrating substantial\nexpertise in privacy law, technical controls, and AI governance. The results\nhighlight both the strengths and domain-specific gaps of current LLMs and offer\npractical insights for privacy officers, compliance leads, and technologists\nassessing the readiness of AI tools for high-stakes data governance roles. This\npaper provides an overview for professionals navigating the intersection of AI\nadvancement and regulatory risk and establishes a machine benchmark based on\nhuman-centric evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid emergence of large language models (LLMs) has raised urgent\nquestions across the modern workforce about this new technology's strengths,\nweaknesses, and capabilities. For privacy professionals, the question is\nwhether these AI systems can provide reliable support on regulatory compliance,\nprivacy program management, and AI governance. In this study, we evaluate ten\nleading open and closed LLMs, including models from OpenAI, Anthropic, Google\nDeepMind, Meta, and DeepSeek, by benchmarking their performance on\nindustry-standard certification exams: CIPP/US, CIPM, CIPT, and AIGP from the\nInternational Association of Privacy Professionals (IAPP). Each model was\ntested using official sample exams in a closed-book setting and compared to\nIAPP's passing thresholds. Our findings show that several frontier models such\nas Gemini 2.5 Pro and OpenAI's GPT-5 consistently achieve scores exceeding the\nstandards for professional human certification - demonstrating substantial\nexpertise in privacy law, technical controls, and AI governance. The results\nhighlight both the strengths and domain-specific gaps of current LLMs and offer\npractical insights for privacy officers, compliance leads, and technologists\nassessing the readiness of AI tools for high-stakes data governance roles. This\npaper provides an overview for professionals navigating the intersection of AI\nadvancement and regulatory risk and establishes a machine benchmark based on\nhuman-centric evaluations."
                },
                "authors": [
                    {
                        "name": "Zane Witherspoon"
                    },
                    {
                        "name": "Thet Mon Aye"
                    },
                    {
                        "name": "YingYing Hao"
                    }
                ],
                "author_detail": {
                    "name": "YingYing Hao"
                },
                "author": "YingYing Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09035v1",
                "updated": "2025-08-12T15:56:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    56,
                    29,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:56:29Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    56,
                    29,
                    1,
                    224,
                    0
                ],
                "title": "P/D-Device: Disaggregated Large Language Model between Cloud and Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P/D-Device: Disaggregated Large Language Model between Cloud and Devices"
                },
                "summary": "Serving disaggregated large language models has been widely adopted in\nindustrial practice for enhanced performance. However, too many tokens\ngenerated in decoding phase, i.e., occupying the resources for a long time,\nessentially hamper the cloud from achieving a higher throughput. Meanwhile, due\nto limited on-device resources, the time to first token (TTFT), i.e., the\nlatency of prefill phase, increases dramatically with the growth on prompt\nlength. In order to concur with such a bottleneck on resources, i.e., long\noccupation in cloud and limited on-device computing capacity, we propose to\nseparate large language model between cloud and devices. That is, the cloud\nhelps a portion of the content for each device, only in its prefill phase.\nSpecifically, after receiving the first token from the cloud, decoupling with\nits own prefill, the device responds to the user immediately for a lower TTFT.\nThen, the following tokens from cloud are presented via a speed controller for\nsmoothed TPOT (the time per output token), until the device catches up with the\nprogress. On-device prefill is then amortized using received tokens while the\nresource usage in cloud is controlled. Moreover, during cloud prefill, the\nprompt can be refined, using those intermediate data already generated, to\nfurther speed up on-device inference. We implement such a scheme P/D-Device,\nand confirm its superiority over other alternatives. We further propose an\nalgorithm to decide the best settings. Real-trace experiments show that TTFT\ndecreases at least 60%, maximum TPOT is about tens of milliseconds, and cloud\nthroughput increases by up to 15x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving disaggregated large language models has been widely adopted in\nindustrial practice for enhanced performance. However, too many tokens\ngenerated in decoding phase, i.e., occupying the resources for a long time,\nessentially hamper the cloud from achieving a higher throughput. Meanwhile, due\nto limited on-device resources, the time to first token (TTFT), i.e., the\nlatency of prefill phase, increases dramatically with the growth on prompt\nlength. In order to concur with such a bottleneck on resources, i.e., long\noccupation in cloud and limited on-device computing capacity, we propose to\nseparate large language model between cloud and devices. That is, the cloud\nhelps a portion of the content for each device, only in its prefill phase.\nSpecifically, after receiving the first token from the cloud, decoupling with\nits own prefill, the device responds to the user immediately for a lower TTFT.\nThen, the following tokens from cloud are presented via a speed controller for\nsmoothed TPOT (the time per output token), until the device catches up with the\nprogress. On-device prefill is then amortized using received tokens while the\nresource usage in cloud is controlled. Moreover, during cloud prefill, the\nprompt can be refined, using those intermediate data already generated, to\nfurther speed up on-device inference. We implement such a scheme P/D-Device,\nand confirm its superiority over other alternatives. We further propose an\nalgorithm to decide the best settings. Real-trace experiments show that TTFT\ndecreases at least 60%, maximum TPOT is about tens of milliseconds, and cloud\nthroughput increases by up to 15x."
                },
                "authors": [
                    {
                        "name": "Yibo Jin"
                    },
                    {
                        "name": "Yixu Xu"
                    },
                    {
                        "name": "Yue Chen"
                    },
                    {
                        "name": "Chengbin Wang"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Jiaqi Huang"
                    },
                    {
                        "name": "Rongfei Zhang"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Yuting Yan"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Yingjie Zhu"
                    },
                    {
                        "name": "Shulan Wang"
                    },
                    {
                        "name": "Qianqian Tang"
                    },
                    {
                        "name": "Shuaishuai Meng"
                    },
                    {
                        "name": "Guanxin Cheng"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Shuyan Miao"
                    },
                    {
                        "name": "Ketao Wang"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Anran Wang"
                    },
                    {
                        "name": "Chengzhou Lu"
                    },
                    {
                        "name": "Tiantian Dong"
                    },
                    {
                        "name": "Yongsheng Zhang"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Hefei Guo"
                    },
                    {
                        "name": "Hongjie Liu"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Zhengyong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhengyong Zhang"
                },
                "author": "Zhengyong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06813v2",
                "updated": "2025-08-12T15:49:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    49,
                    5,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-09T04:22:07Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    4,
                    22,
                    7,
                    5,
                    221,
                    0
                ],
                "title": "Technical Report: Full-Stack Fine-Tuning for the Q Programming Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical Report: Full-Stack Fine-Tuning for the Q Programming Language"
                },
                "summary": "Even though large language models are becoming increasingly capable, it is\nstill unreasonable to expect them to excel at tasks that are under-represented\non the Internet. Leveraging LLMs for specialized applications, particularly in\nniche programming languages and private domains, remains challenging and\nlargely unsolved. In this work, we address this gap by presenting a\ncomprehensive, open-source approach for adapting LLMs to the Q programming\nlanguage, a popular tool in quantitative finance that is much less present on\nthe Internet compared to Python, C, Java, and other ``mainstream\" languages and\nis therefore not a strong suit of general-purpose AI models. We introduce a new\nLeetcode style evaluation dataset for Q, benchmark major frontier models on the\ndataset, then do pretraining, supervised fine tuning, and reinforcement\nlearning to train a suite of reasoning and non-reasoning models based on the\nQwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our\nbest model achieves a pass@1 accuracy of 59 percent on our Q benchmark,\nsurpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.\nAdditionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.\nIn addition to releasing models, code, and data, we provide a detailed\nblueprint for dataset construction, model pretraining, supervised fine-tuning,\nand reinforcement learning. Our methodology is broadly applicable, and we\ndiscuss how these techniques can be extended to other tasks, including those\nwhere evaluation may rely on soft or subjective signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even though large language models are becoming increasingly capable, it is\nstill unreasonable to expect them to excel at tasks that are under-represented\non the Internet. Leveraging LLMs for specialized applications, particularly in\nniche programming languages and private domains, remains challenging and\nlargely unsolved. In this work, we address this gap by presenting a\ncomprehensive, open-source approach for adapting LLMs to the Q programming\nlanguage, a popular tool in quantitative finance that is much less present on\nthe Internet compared to Python, C, Java, and other ``mainstream\" languages and\nis therefore not a strong suit of general-purpose AI models. We introduce a new\nLeetcode style evaluation dataset for Q, benchmark major frontier models on the\ndataset, then do pretraining, supervised fine tuning, and reinforcement\nlearning to train a suite of reasoning and non-reasoning models based on the\nQwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our\nbest model achieves a pass@1 accuracy of 59 percent on our Q benchmark,\nsurpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.\nAdditionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.\nIn addition to releasing models, code, and data, we provide a detailed\nblueprint for dataset construction, model pretraining, supervised fine-tuning,\nand reinforcement learning. Our methodology is broadly applicable, and we\ndiscuss how these techniques can be extended to other tasks, including those\nwhere evaluation may rely on soft or subjective signals."
                },
                "authors": [
                    {
                        "name": "Brendan R. Hogan"
                    },
                    {
                        "name": "Will Brown"
                    },
                    {
                        "name": "Adel Boyarsky"
                    },
                    {
                        "name": "Anderson Schneider"
                    },
                    {
                        "name": "Yuriy Nevmyvaka"
                    }
                ],
                "author_detail": {
                    "name": "Yuriy Nevmyvaka"
                },
                "author": "Yuriy Nevmyvaka",
                "arxiv_comment": "40 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.07513v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.07513v3",
                "updated": "2025-08-12T15:46:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    46,
                    19,
                    1,
                    224,
                    0
                ],
                "published": "2023-01-18T13:32:02Z",
                "published_parsed": [
                    2023,
                    1,
                    18,
                    13,
                    32,
                    2,
                    2,
                    18,
                    0
                ],
                "title": "A Bayesian Nonparametric Stochastic Block Model for Directed Acyclic\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Nonparametric Stochastic Block Model for Directed Acyclic\n  Graphs"
                },
                "summary": "Random graphs have been widely used in statistics, for example in network\nanalysis and graphical models. In some applications, the data may contain an\ninherent hierarchical ordering among its vertices, which prevents directed\nedges between pairs of vertices that do not respect this order. For example, in\nbibliometrics, older papers cannot cite newer ones. In such situations, the\nresulting graph forms a Directed Acyclic Graph. In this article, we extend the\nStochastic Block Model (SBM) to account for the presence of such ordering in\nthe data, ignoring which can lead to biased estimates of the number of blocks.\nThe proposed approach includes in the model likelihood a topological ordering,\nwhich is treated as an unknown parameter and endowed with a prior distribution.\nWe describe how to formalise the model and perform posterior inference for a\nBayesian nonparametric version of the SBM in which both the hierarchical\nordering and the number of latent blocks are learnt from the data. Finally, an\nillustration with real-world datasets from bibliometrics is presented.\nAdditional supplementary materials are available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random graphs have been widely used in statistics, for example in network\nanalysis and graphical models. In some applications, the data may contain an\ninherent hierarchical ordering among its vertices, which prevents directed\nedges between pairs of vertices that do not respect this order. For example, in\nbibliometrics, older papers cannot cite newer ones. In such situations, the\nresulting graph forms a Directed Acyclic Graph. In this article, we extend the\nStochastic Block Model (SBM) to account for the presence of such ordering in\nthe data, ignoring which can lead to biased estimates of the number of blocks.\nThe proposed approach includes in the model likelihood a topological ordering,\nwhich is treated as an unknown parameter and endowed with a prior distribution.\nWe describe how to formalise the model and perform posterior inference for a\nBayesian nonparametric version of the SBM in which both the hierarchical\nordering and the number of latent blocks are learnt from the data. Finally, an\nillustration with real-world datasets from bibliometrics is presented.\nAdditional supplementary materials are available online."
                },
                "authors": [
                    {
                        "name": "Clement Lee"
                    },
                    {
                        "name": "Marco Battiston"
                    }
                ],
                "author_detail": {
                    "name": "Marco Battiston"
                },
                "author": "Marco Battiston",
                "arxiv_comment": "28 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.07513v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.07513v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07498v2",
                "updated": "2025-08-12T15:44:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    44,
                    23,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-10T22:07:05Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    22,
                    7,
                    5,
                    6,
                    222,
                    0
                ],
                "title": "High-dimensional Longitudinal Inference via a De-sparsified\n  Dantzig-Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional Longitudinal Inference via a De-sparsified\n  Dantzig-Selector"
                },
                "summary": "In this paper, we consider statistical inference with generalized linear\nmodels in high dimensions under a longitudinal clustered data framework.\nSpecifically, we propose a de-sparsified version of an initial Dantzig-type\nregularized estimator in regression settings and provide theoretical\njustification for both linear and generalized linear models. We present\nextensive numerical simulations demonstrating the effectiveness of our method\nfor continuous and binary data. For continuous outcomes under linear models, we\nshow that our estimator asymptotically attains an appropriate efficiency bound\nwhen the correlation structure is correctly specified. We conclude with an\napplication of our method to a well-established genetics dataset, with\nbacterial riboflavin production as the outcome of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider statistical inference with generalized linear\nmodels in high dimensions under a longitudinal clustered data framework.\nSpecifically, we propose a de-sparsified version of an initial Dantzig-type\nregularized estimator in regression settings and provide theoretical\njustification for both linear and generalized linear models. We present\nextensive numerical simulations demonstrating the effectiveness of our method\nfor continuous and binary data. For continuous outcomes under linear models, we\nshow that our estimator asymptotically attains an appropriate efficiency bound\nwhen the correlation structure is correctly specified. We conclude with an\napplication of our method to a well-established genetics dataset, with\nbacterial riboflavin production as the outcome of interest."
                },
                "authors": [
                    {
                        "name": "Nathan Huey"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Huey"
                },
                "author": "Nathan Huey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03789v2",
                "updated": "2025-08-12T15:38:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    38,
                    44,
                    1,
                    224,
                    0
                ],
                "published": "2025-07-04T19:50:01Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    19,
                    50,
                    1,
                    4,
                    185,
                    0
                ],
                "title": "Efficient and Effective Query Context-Aware Learning-to-Rank Model for\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Effective Query Context-Aware Learning-to-Rank Model for\n  Sequential Recommendation"
                },
                "summary": "Modern sequential recommender systems commonly use transformer-based models\nfor next-item prediction. While these models demonstrate a strong balance\nbetween efficiency and quality, integrating interleaving features - such as the\nquery context (e.g., browse category) under which next-item interactions occur\n- poses challenges. Effectively capturing query context is crucial for refining\nranking relevance and enhancing user engagement, as it provides valuable\nsignals about user intent within a session. Unlike item features, historical\nquery context is typically not aligned with item sequences and may be\nunavailable at inference due to privacy constraints or feature store\nlimitations - making its integration into transformers both challenging and\nerror-prone. This paper analyzes different strategies for incorporating query\ncontext into transformers trained with a causal language modeling procedure as\na case study. We propose a new method that effectively fuses the item sequence\nwith query context within the attention mechanism. Through extensive offline\nand online experiments on a large-scale online platform and open datasets, we\npresent evidence that our proposed method is an effective approach for\nintegrating query context to improve model ranking quality in terms of\nrelevance and diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern sequential recommender systems commonly use transformer-based models\nfor next-item prediction. While these models demonstrate a strong balance\nbetween efficiency and quality, integrating interleaving features - such as the\nquery context (e.g., browse category) under which next-item interactions occur\n- poses challenges. Effectively capturing query context is crucial for refining\nranking relevance and enhancing user engagement, as it provides valuable\nsignals about user intent within a session. Unlike item features, historical\nquery context is typically not aligned with item sequences and may be\nunavailable at inference due to privacy constraints or feature store\nlimitations - making its integration into transformers both challenging and\nerror-prone. This paper analyzes different strategies for incorporating query\ncontext into transformers trained with a causal language modeling procedure as\na case study. We propose a new method that effectively fuses the item sequence\nwith query context within the attention mechanism. Through extensive offline\nand online experiments on a large-scale online platform and open datasets, we\npresent evidence that our proposed method is an effective approach for\nintegrating query context to improve model ranking quality in terms of\nrelevance and diversity."
                },
                "authors": [
                    {
                        "name": "Andrii Dzhoha"
                    },
                    {
                        "name": "Alisa Mironenko"
                    },
                    {
                        "name": "Evgeny Labzin"
                    },
                    {
                        "name": "Vladimir Vlasov"
                    },
                    {
                        "name": "Maarten Versteegh"
                    },
                    {
                        "name": "Marjan Celikik"
                    }
                ],
                "author_detail": {
                    "name": "Marjan Celikik"
                },
                "author": "Marjan Celikik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09023v1",
                "updated": "2025-08-12T15:38:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    38,
                    10,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:38:10Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    38,
                    10,
                    1,
                    224,
                    0
                ],
                "title": "E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and\n  Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and\n  Efficiency"
                },
                "summary": "SQL query rewriting aims to reformulate a query into a more efficient form\nwhile preserving equivalence. Most existing methods rely on predefined rewrite\nrules. However, such rule-based approaches face fundamental limitations: (1)\nfixed rule sets generalize poorly to novel query patterns and struggle with\ncomplex queries; (2) a wide range of effective rewriting strategies cannot be\nfully captured by declarative rules. To overcome these issues, we propose using\nlarge language models (LLMs) to generate rewrites. LLMs can capture complex\nstrategies, such as evaluation reordering and CTE rewriting. Despite this\npotential, directly applying LLMs often results in suboptimal or non-equivalent\nrewrites due to a lack of execution awareness and semantic grounding. To\naddress these challenges, We present E3-Rewrite, an LLM-based SQL rewriting\nframework that produces executable, equivalent, and efficient queries. It\nintegrates two core components: a context construction module and a\nreinforcement learning framework. First, the context module leverages execution\nplans and retrieved demonstrations to build bottleneck-aware prompts that guide\ninference-time rewriting. Second, we design a reward function targeting\nexecutability, equivalence, and efficiency, evaluated via syntax checks,\nequivalence verification, and cost estimation. Third, to ensure stable\nmulti-objective learning, we adopt a staged curriculum that first emphasizes\nexecutability and equivalence, then gradually incorporates efficiency.\nExtensive experiments show that E3-Rewrite achieves up to a 25.6\\% reduction in\nquery execution time compared to state-of-the-art methods across multiple SQL\nbenchmarks. Moreover, it delivers up to 24.4\\% more successful rewrites,\nexpanding coverage to complex queries that previous systems failed to handle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL query rewriting aims to reformulate a query into a more efficient form\nwhile preserving equivalence. Most existing methods rely on predefined rewrite\nrules. However, such rule-based approaches face fundamental limitations: (1)\nfixed rule sets generalize poorly to novel query patterns and struggle with\ncomplex queries; (2) a wide range of effective rewriting strategies cannot be\nfully captured by declarative rules. To overcome these issues, we propose using\nlarge language models (LLMs) to generate rewrites. LLMs can capture complex\nstrategies, such as evaluation reordering and CTE rewriting. Despite this\npotential, directly applying LLMs often results in suboptimal or non-equivalent\nrewrites due to a lack of execution awareness and semantic grounding. To\naddress these challenges, We present E3-Rewrite, an LLM-based SQL rewriting\nframework that produces executable, equivalent, and efficient queries. It\nintegrates two core components: a context construction module and a\nreinforcement learning framework. First, the context module leverages execution\nplans and retrieved demonstrations to build bottleneck-aware prompts that guide\ninference-time rewriting. Second, we design a reward function targeting\nexecutability, equivalence, and efficiency, evaluated via syntax checks,\nequivalence verification, and cost estimation. Third, to ensure stable\nmulti-objective learning, we adopt a staged curriculum that first emphasizes\nexecutability and equivalence, then gradually incorporates efficiency.\nExtensive experiments show that E3-Rewrite achieves up to a 25.6\\% reduction in\nquery execution time compared to state-of-the-art methods across multiple SQL\nbenchmarks. Moreover, it delivers up to 24.4\\% more successful rewrites,\nexpanding coverage to complex queries that previous systems failed to handle."
                },
                "authors": [
                    {
                        "name": "Dongjie Xu"
                    },
                    {
                        "name": "Yue Cui"
                    },
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Qingzhi Ma"
                    },
                    {
                        "name": "Hanghui Guo"
                    },
                    {
                        "name": "Jiaming Li"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Ruiyuan Zhang"
                    },
                    {
                        "name": "Shimin Di"
                    },
                    {
                        "name": "Jia Zhu"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Jiajie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajie Xu"
                },
                "author": "Jiajie Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09021v1",
                "updated": "2025-08-12T15:36:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    36,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:36:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    36,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "Attacks and Defenses Against LLM Fingerprinting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attacks and Defenses Against LLM Fingerprinting"
                },
                "summary": "As large language models are increasingly deployed in sensitive environments,\nfingerprinting attacks pose significant privacy and security risks. We present\na study of LLM fingerprinting from both offensive and defensive perspectives.\nOur attack methodology uses reinforcement learning to automatically optimize\nquery selection, achieving better fingerprinting accuracy with only 3 queries\ncompared to randomly selecting 3 queries from the same pool. Our defensive\napproach employs semantic-preserving output filtering through a secondary LLM\nto obfuscate model identity while maintaining semantic integrity. The defensive\nmethod reduces fingerprinting accuracy across tested models while preserving\noutput quality. These contributions show the potential to improve\nfingerprinting tools capabilities while providing practical mitigation\nstrategies against fingerprinting attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models are increasingly deployed in sensitive environments,\nfingerprinting attacks pose significant privacy and security risks. We present\na study of LLM fingerprinting from both offensive and defensive perspectives.\nOur attack methodology uses reinforcement learning to automatically optimize\nquery selection, achieving better fingerprinting accuracy with only 3 queries\ncompared to randomly selecting 3 queries from the same pool. Our defensive\napproach employs semantic-preserving output filtering through a secondary LLM\nto obfuscate model identity while maintaining semantic integrity. The defensive\nmethod reduces fingerprinting accuracy across tested models while preserving\noutput quality. These contributions show the potential to improve\nfingerprinting tools capabilities while providing practical mitigation\nstrategies against fingerprinting attacks."
                },
                "authors": [
                    {
                        "name": "Kevin Kurian"
                    },
                    {
                        "name": "Ethan Holland"
                    },
                    {
                        "name": "Sean Oesch"
                    }
                ],
                "author_detail": {
                    "name": "Sean Oesch"
                },
                "author": "Sean Oesch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09019v1",
                "updated": "2025-08-12T15:34:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    34,
                    18,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:34:18Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    34,
                    18,
                    1,
                    224,
                    0
                ],
                "title": "Activation Steering for Bias Mitigation: An Interpretable Approach to\n  Safer LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Steering for Bias Mitigation: An Interpretable Approach to\n  Safer LLMs"
                },
                "summary": "As large language models (LLMs) become more integrated into societal systems,\nthe risk of them perpetuating and amplifying harmful biases becomes a critical\nsafety concern. Traditional methods for mitigating bias often rely on data\nfiltering or post-hoc output moderation, which treat the model as an opaque\nblack box. In this work, we introduce a complete, end-to-end system that uses\ntechniques from mechanistic interpretability to both identify and actively\nmitigate bias directly within a model's internal workings. Our method involves\ntwo primary stages. First, we train linear \"probes\" on the internal activations\nof a model to detect the latent representations of various biases (e.g.,\ngender, race, age). Our experiments on \\texttt{gpt2-large} demonstrate that\nthese probes can identify biased content with near-perfect accuracy, revealing\nthat bias representations become most salient in the model's later layers.\nSecond, we leverage these findings to compute \"steering vectors\" by contrasting\nthe model's activation patterns for biased and neutral statements. By adding\nthese vectors during inference, we can actively steer the model's generative\nprocess away from producing harmful, stereotypical, or biased content in\nreal-time. We demonstrate the efficacy of this activation steering technique,\nshowing that it successfully alters biased completions toward more neutral\nalternatives. We present our work as a robust and reproducible system that\noffers a more direct and interpretable approach to building safer and more\naccountable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become more integrated into societal systems,\nthe risk of them perpetuating and amplifying harmful biases becomes a critical\nsafety concern. Traditional methods for mitigating bias often rely on data\nfiltering or post-hoc output moderation, which treat the model as an opaque\nblack box. In this work, we introduce a complete, end-to-end system that uses\ntechniques from mechanistic interpretability to both identify and actively\nmitigate bias directly within a model's internal workings. Our method involves\ntwo primary stages. First, we train linear \"probes\" on the internal activations\nof a model to detect the latent representations of various biases (e.g.,\ngender, race, age). Our experiments on \\texttt{gpt2-large} demonstrate that\nthese probes can identify biased content with near-perfect accuracy, revealing\nthat bias representations become most salient in the model's later layers.\nSecond, we leverage these findings to compute \"steering vectors\" by contrasting\nthe model's activation patterns for biased and neutral statements. By adding\nthese vectors during inference, we can actively steer the model's generative\nprocess away from producing harmful, stereotypical, or biased content in\nreal-time. We demonstrate the efficacy of this activation steering technique,\nshowing that it successfully alters biased completions toward more neutral\nalternatives. We present our work as a robust and reproducible system that\noffers a more direct and interpretable approach to building safer and more\naccountable LLMs."
                },
                "authors": [
                    {
                        "name": "Shivam Dubey"
                    }
                ],
                "author_detail": {
                    "name": "Shivam Dubey"
                },
                "author": "Shivam Dubey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09016v1",
                "updated": "2025-08-12T15:30:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    30,
                    44,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:30:44Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    30,
                    44,
                    1,
                    224,
                    0
                ],
                "title": "A Survey on Training-free Alignment of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Training-free Alignment of Large Language Models"
                },
                "summary": "The alignment of large language models (LLMs) aims to ensure their outputs\nadhere to human values, ethical standards, and legal norms. Traditional\nalignment methods often rely on resource-intensive fine-tuning (FT), which may\nsuffer from knowledge degradation and face challenges in scenarios where the\nmodel accessibility or computational resources are constrained. In contrast,\ntraining-free (TF) alignment techniques--leveraging in-context learning,\ndecoding-time adjustments, and post-generation corrections--offer a promising\nalternative by enabling alignment without heavily retraining LLMs, making them\nadaptable to both open-source and closed-source environments. This paper\npresents the first systematic review of TF alignment methods, categorizing them\nby stages of pre-decoding, in-decoding, and post-decoding. For each stage, we\nprovide a detailed examination from the viewpoint of LLMs and multimodal LLMs\n(MLLMs), highlighting their mechanisms and limitations. Furthermore, we\nidentify key challenges and future directions, paving the way for more\ninclusive and effective TF alignment techniques. By synthesizing and organizing\nthe rapidly growing body of research, this survey offers a guidance for\npractitioners and advances the development of safer and more reliable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of large language models (LLMs) aims to ensure their outputs\nadhere to human values, ethical standards, and legal norms. Traditional\nalignment methods often rely on resource-intensive fine-tuning (FT), which may\nsuffer from knowledge degradation and face challenges in scenarios where the\nmodel accessibility or computational resources are constrained. In contrast,\ntraining-free (TF) alignment techniques--leveraging in-context learning,\ndecoding-time adjustments, and post-generation corrections--offer a promising\nalternative by enabling alignment without heavily retraining LLMs, making them\nadaptable to both open-source and closed-source environments. This paper\npresents the first systematic review of TF alignment methods, categorizing them\nby stages of pre-decoding, in-decoding, and post-decoding. For each stage, we\nprovide a detailed examination from the viewpoint of LLMs and multimodal LLMs\n(MLLMs), highlighting their mechanisms and limitations. Furthermore, we\nidentify key challenges and future directions, paving the way for more\ninclusive and effective TF alignment techniques. By synthesizing and organizing\nthe rapidly growing body of research, this survey offers a guidance for\npractitioners and advances the development of safer and more reliable LLMs."
                },
                "authors": [
                    {
                        "name": "Birong Pan"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Weiyu Zhang"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Mayi Xu"
                    },
                    {
                        "name": "Shen Zhou"
                    },
                    {
                        "name": "Yuanyuan Zhu"
                    },
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Tieyun Qian"
                    }
                ],
                "author_detail": {
                    "name": "Tieyun Qian"
                },
                "author": "Tieyun Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05387v3",
                "updated": "2025-08-12T15:23:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    23,
                    4,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-07T13:37:04Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    37,
                    4,
                    3,
                    219,
                    0
                ],
                "title": "Echo: Decoupling Inference and Training for Large-Scale RL Alignment on\n  Heterogeneous Swarms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Echo: Decoupling Inference and Training for Large-Scale RL Alignment on\n  Heterogeneous Swarms"
                },
                "summary": "Modern RL-based post-training for large language models (LLMs) co-locate\ntrajectory sampling and policy optimisation on the same GPU cluster, forcing\nthe system to switch between inference and training workloads. This serial\ncontext switching violates the single-program-multiple-data (SPMD) assumption\nunderlying today's distributed training systems. We present Echo, the RL system\nthat cleanly decouples these two phases across heterogeneous \"inference\" and\n\"training\" swarms while preserving statistical efficiency. Echo introduces two\nlightweight synchronization protocols: a sequential pull mode that refreshes\npolicy weights according to API call for minimal bias, and an asynchronous\npush-pull mode that streams version-tagged rollouts through a replay buffer to\nmaximise hardware utilisation. Training four representative RL workloads with\nQwen3-4B, Qwen2.5-7B, Qwen3-30B-A3B-Thinking-2507 and Qwen3-32B on a\ngeographically distributed cluster, Echo matches a fully co-located Verl\nbaseline in convergence speed and final reward while off-loading trajectory\ngeneration to commodity edge hardware. These promising results demonstrate that\nlarge-scale RL for LLMs could achieve datacentre-grade performance using\ndecentralised, heterogeneous resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern RL-based post-training for large language models (LLMs) co-locate\ntrajectory sampling and policy optimisation on the same GPU cluster, forcing\nthe system to switch between inference and training workloads. This serial\ncontext switching violates the single-program-multiple-data (SPMD) assumption\nunderlying today's distributed training systems. We present Echo, the RL system\nthat cleanly decouples these two phases across heterogeneous \"inference\" and\n\"training\" swarms while preserving statistical efficiency. Echo introduces two\nlightweight synchronization protocols: a sequential pull mode that refreshes\npolicy weights according to API call for minimal bias, and an asynchronous\npush-pull mode that streams version-tagged rollouts through a replay buffer to\nmaximise hardware utilisation. Training four representative RL workloads with\nQwen3-4B, Qwen2.5-7B, Qwen3-30B-A3B-Thinking-2507 and Qwen3-32B on a\ngeographically distributed cluster, Echo matches a fully co-located Verl\nbaseline in convergence speed and final reward while off-loading trajectory\ngeneration to commodity edge hardware. These promising results demonstrate that\nlarge-scale RL for LLMs could achieve datacentre-grade performance using\ndecentralised, heterogeneous resources."
                },
                "authors": [
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Changyuan Fan"
                    },
                    {
                        "name": "Qingnan Ren"
                    },
                    {
                        "name": "Alfred Long"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Rymon Yu"
                    },
                    {
                        "name": "Eric Yang"
                    },
                    {
                        "name": "Lynn Ai"
                    },
                    {
                        "name": "Shaoduo Gan"
                    }
                ],
                "author_detail": {
                    "name": "Shaoduo Gan"
                },
                "author": "Shaoduo Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16640v2",
                "updated": "2025-08-12T15:21:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    21,
                    49,
                    1,
                    224,
                    0
                ],
                "published": "2024-07-23T16:56:31Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    16,
                    56,
                    31,
                    1,
                    205,
                    0
                ],
                "title": "Fully Interpretable Emulator for the Linear Matter Power Spectrum from\n  Physics-Informed Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Interpretable Emulator for the Linear Matter Power Spectrum from\n  Physics-Informed Machine Learning"
                },
                "summary": "We present a fully interpretable emulator for the linear matter power\nspectrum (MPS), constructed via a $physics$-$informed$ $symbolic$ $regression$\nframework. By combining domain knowledge with a machine learning technique\nknows as genetic algorithms, we explore the space of analytic expressions to\nderive closed-form, smooth approximations of the MPS that match the accuracy of\nstandard broadband reconstruction methodologies such as the Savitzky-Golay\nfilter. Building upon this baseline, we incorporate fully transparent\noscillatory corrections informed by the physics of baryon acoustic oscillations\n(BAO), achieving sub-percent accuracy across a broad range of cosmological\nscales ($k \\in [10^{-5}, 1.5]~h/\\mathrm{Mpc}$) with an average fractional error\nof $\\sim0.3\\%$ when tested against numerical spectra obtained from a Boltzmann\nsolver. To extend the framework beyond $\\Lambda$CDM, we introduce parametric\ndeformations designed to capture characteristic signatures of modified gravity\n(MG) theories -- such as scale-dependent enhancements or suppressions of power\n-- without sacrificing interpretability. Using a representative $f(R)$ gravity\nmodel, we demonstrate that these extensions effectively trace the overall\nmodulation of the MPS, allowing us to analyze the impact of MG theories on the\nBAO scale. Our results provide compact, accurate, and physically motivated\nfitting functions for the linear MPS in both standard and MG cosmologies,\noffering a fast and transparent alternative to existing emulators for parameter\ninference and theoretical modeling in large-scale structure surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a fully interpretable emulator for the linear matter power\nspectrum (MPS), constructed via a $physics$-$informed$ $symbolic$ $regression$\nframework. By combining domain knowledge with a machine learning technique\nknows as genetic algorithms, we explore the space of analytic expressions to\nderive closed-form, smooth approximations of the MPS that match the accuracy of\nstandard broadband reconstruction methodologies such as the Savitzky-Golay\nfilter. Building upon this baseline, we incorporate fully transparent\noscillatory corrections informed by the physics of baryon acoustic oscillations\n(BAO), achieving sub-percent accuracy across a broad range of cosmological\nscales ($k \\in [10^{-5}, 1.5]~h/\\mathrm{Mpc}$) with an average fractional error\nof $\\sim0.3\\%$ when tested against numerical spectra obtained from a Boltzmann\nsolver. To extend the framework beyond $\\Lambda$CDM, we introduce parametric\ndeformations designed to capture characteristic signatures of modified gravity\n(MG) theories -- such as scale-dependent enhancements or suppressions of power\n-- without sacrificing interpretability. Using a representative $f(R)$ gravity\nmodel, we demonstrate that these extensions effectively trace the overall\nmodulation of the MPS, allowing us to analyze the impact of MG theories on the\nBAO scale. Our results provide compact, accurate, and physically motivated\nfitting functions for the linear MPS in both standard and MG cosmologies,\noffering a fast and transparent alternative to existing emulators for parameter\ninference and theoretical modeling in large-scale structure surveys."
                },
                "authors": [
                    {
                        "name": "J. Bayron Orjuela-Quintana"
                    },
                    {
                        "name": "Domenico Sapone"
                    },
                    {
                        "name": "Savvas Nesseris"
                    }
                ],
                "author_detail": {
                    "name": "Savvas Nesseris"
                },
                "author": "Savvas Nesseris",
                "arxiv_comment": "Main: 23 pages --> Comments are very welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10769v2",
                "updated": "2025-08-12T15:15:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    15,
                    15,
                    1,
                    224,
                    0
                ],
                "published": "2025-06-12T14:48:25Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    14,
                    48,
                    25,
                    3,
                    163,
                    0
                ],
                "title": "Mind the Gap: Benchmarking LLM Uncertainty, Discrimination, and\n  Calibration in Specialty-Aware Clinical QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Benchmarking LLM Uncertainty, Discrimination, and\n  Calibration in Specialty-Aware Clinical QA"
                },
                "summary": "Reliable uncertainty quantification (UQ) is essential when employing large\nlanguage models (LLMs) in high-risk domains such as clinical question answering\n(QA). In this work, we evaluate uncertainty estimation methods for clinical QA\nfocusing, for the first time, on eleven clinical specialties and six question\ntypes, and across ten open-source LLMs (general-purpose, biomedical, and\nreasoning models). We analyze score-based UQ methods, present a case study\nintroducing a novel lightweight method based on behavioral features derived\nfrom reasoning-oriented models, and examine conformal prediction as a\ncomplementary set-based approach. Our findings reveal that uncertainty\nreliability is not a monolithic property, but one that depends on clinical\nspecialty and question type due to shifts in calibration and discrimination.\nOur results highlight the need to select or ensemble models based on their\ndistinct, complementary strengths and clinical use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable uncertainty quantification (UQ) is essential when employing large\nlanguage models (LLMs) in high-risk domains such as clinical question answering\n(QA). In this work, we evaluate uncertainty estimation methods for clinical QA\nfocusing, for the first time, on eleven clinical specialties and six question\ntypes, and across ten open-source LLMs (general-purpose, biomedical, and\nreasoning models). We analyze score-based UQ methods, present a case study\nintroducing a novel lightweight method based on behavioral features derived\nfrom reasoning-oriented models, and examine conformal prediction as a\ncomplementary set-based approach. Our findings reveal that uncertainty\nreliability is not a monolithic property, but one that depends on clinical\nspecialty and question type due to shifts in calibration and discrimination.\nOur results highlight the need to select or ensemble models based on their\ndistinct, complementary strengths and clinical use."
                },
                "authors": [
                    {
                        "name": "Alberto Testoni"
                    },
                    {
                        "name": "Iacer Calixto"
                    }
                ],
                "author_detail": {
                    "name": "Iacer Calixto"
                },
                "author": "Iacer Calixto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09001v1",
                "updated": "2025-08-12T15:11:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:11:47Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "title": "Retrospective Sparse Attention for Efficient Long-Context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrospective Sparse Attention for Efficient Long-Context Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%."
                },
                "authors": [
                    {
                        "name": "Seonghwan Choi"
                    },
                    {
                        "name": "Beomseok Kang"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08997v1",
                "updated": "2025-08-12T15:05:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    5,
                    0,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:05:00Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    5,
                    0,
                    1,
                    224,
                    0
                ],
                "title": "Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through\n  Structured Contextual Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through\n  Structured Contextual Memory"
                },
                "summary": "Multi-agent systems built on Large Language Models (LLMs) show exceptional\npromise for complex collaborative problem-solving, yet they face fundamental\nchallenges stemming from context window limitations that impair memory\nconsistency, role adherence, and procedural integrity. This paper introduces\nIntrinsic Memory Agents, a novel framework that addresses these limitations\nthrough structured agent-specific memories that evolve intrinsically with agent\noutputs. Specifically, our method maintains role-aligned memory templates that\npreserve specialized perspectives while focusing on task-relevant information.\nWe benchmark our approach on the PDDL dataset, comparing its performance to\nexisting state-of-the-art multi-agentic memory approaches and showing an\nimprovement of 38.6\\% with the highest token efficiency. An additional\nevaluation is performed on a complex data pipeline design task, we demonstrate\nthat our approach produces higher quality designs when comparing 5 metrics:\nscalability, reliability, usability, cost-effectiveness and documentation with\nadditional qualitative evidence of the improvements. Our findings suggest that\naddressing memory limitations through structured, intrinsic approaches can\nimprove the capabilities of multi-agent LLM systems on structured planning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems built on Large Language Models (LLMs) show exceptional\npromise for complex collaborative problem-solving, yet they face fundamental\nchallenges stemming from context window limitations that impair memory\nconsistency, role adherence, and procedural integrity. This paper introduces\nIntrinsic Memory Agents, a novel framework that addresses these limitations\nthrough structured agent-specific memories that evolve intrinsically with agent\noutputs. Specifically, our method maintains role-aligned memory templates that\npreserve specialized perspectives while focusing on task-relevant information.\nWe benchmark our approach on the PDDL dataset, comparing its performance to\nexisting state-of-the-art multi-agentic memory approaches and showing an\nimprovement of 38.6\\% with the highest token efficiency. An additional\nevaluation is performed on a complex data pipeline design task, we demonstrate\nthat our approach produces higher quality designs when comparing 5 metrics:\nscalability, reliability, usability, cost-effectiveness and documentation with\nadditional qualitative evidence of the improvements. Our findings suggest that\naddressing memory limitations through structured, intrinsic approaches can\nimprove the capabilities of multi-agent LLM systems on structured planning\ntasks."
                },
                "authors": [
                    {
                        "name": "Sizhe Yuen"
                    },
                    {
                        "name": "Francisco Gomez Medina"
                    },
                    {
                        "name": "Ting Su"
                    },
                    {
                        "name": "Yali Du"
                    },
                    {
                        "name": "Adam J. Sobey"
                    }
                ],
                "author_detail": {
                    "name": "Adam J. Sobey"
                },
                "author": "Adam J. Sobey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08992v1",
                "updated": "2025-08-12T15:02:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    2,
                    16,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:02:16Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    2,
                    16,
                    1,
                    224,
                    0
                ],
                "title": "Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making\n  under Epistemic Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making\n  under Epistemic Uncertainty"
                },
                "summary": "Prospect Theory (PT) models human decision-making under uncertainty, while\nepistemic markers (e.g., maybe) serve to express uncertainty in language.\nHowever, it remains largely unexplored whether Prospect Theory applies to\ncontemporary Large Language Models and whether epistemic markers, which express\nhuman uncertainty, affect their decision-making behaviour. To address these\nresearch gaps, we design a three-stage experiment based on economic\nquestionnaires. We propose a more general and precise evaluation framework to\nmodel LLMs' decision-making behaviour under PT, introducing uncertainty through\nthe empirical probability values associated with commonly used epistemic\nmarkers in comparable contexts. We then incorporate epistemic markers into the\nevaluation framework based on their corresponding probability values to examine\ntheir influence on LLM decision-making behaviours. Our findings suggest that\nmodelling LLMs' decision-making with PT is not consistently reliable,\nparticularly when uncertainty is expressed in diverse linguistic forms. Our\ncode is released in https://github.com/HKUST-KnowComp/MarPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prospect Theory (PT) models human decision-making under uncertainty, while\nepistemic markers (e.g., maybe) serve to express uncertainty in language.\nHowever, it remains largely unexplored whether Prospect Theory applies to\ncontemporary Large Language Models and whether epistemic markers, which express\nhuman uncertainty, affect their decision-making behaviour. To address these\nresearch gaps, we design a three-stage experiment based on economic\nquestionnaires. We propose a more general and precise evaluation framework to\nmodel LLMs' decision-making behaviour under PT, introducing uncertainty through\nthe empirical probability values associated with commonly used epistemic\nmarkers in comparable contexts. We then incorporate epistemic markers into the\nevaluation framework based on their corresponding probability values to examine\ntheir influence on LLM decision-making behaviours. Our findings suggest that\nmodelling LLMs' decision-making with PT is not consistently reliable,\nparticularly when uncertainty is expressed in diverse linguistic forms. Our\ncode is released in https://github.com/HKUST-KnowComp/MarPT."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Qihan Lin"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07251v2",
                "updated": "2025-08-12T15:01:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    1,
                    27,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-10T09:08:04Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    9,
                    8,
                    4,
                    6,
                    222,
                    0
                ],
                "title": "Understanding Dynamic Scenes in Ego Centric 4D Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Dynamic Scenes in Ego Centric 4D Point Clouds"
                },
                "summary": "Understanding dynamic 4D scenes from an egocentric perspective-modeling\nchanges in 3D spatial structure over time-is crucial for human-machine\ninteraction, autonomous navigation, and embodied intelligence. While existing\negocentric datasets contain dynamic scenes, they lack unified 4D annotations\nand task-driven evaluation protocols for fine-grained spatio-temporal\nreasoning, especially on motion of objects and human, together with their\ninteractions. To address this gap, we introduce EgoDynamic4D, a novel QA\nbenchmark on highly dynamic scenes, comprising RGB-D video, camera poses,\nglobally unique instance masks, and 4D bounding boxes. We construct 927K QA\npairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,\nstep-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks covering\nagent motion, human-object interaction, trajectory prediction, relation\nunderstanding, and temporal-causal reasoning, with fine-grained,\nmultidimensional metrics. To tackle these tasks, we propose an end-to-end\nspatio-temporal reasoning framework that unifies dynamic and static scene\ninformation, using instance-aware feature encoding, time and camera encoding,\nand spatially adaptive down-sampling to compress large 4D scenes into token\nsequences manageable by LLMs. Experiments on EgoDynamic4D show that our method\nconsistently outperforms baselines, validating the effectiveness of multimodal\ntemporal modeling for egocentric dynamic scene understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding dynamic 4D scenes from an egocentric perspective-modeling\nchanges in 3D spatial structure over time-is crucial for human-machine\ninteraction, autonomous navigation, and embodied intelligence. While existing\negocentric datasets contain dynamic scenes, they lack unified 4D annotations\nand task-driven evaluation protocols for fine-grained spatio-temporal\nreasoning, especially on motion of objects and human, together with their\ninteractions. To address this gap, we introduce EgoDynamic4D, a novel QA\nbenchmark on highly dynamic scenes, comprising RGB-D video, camera poses,\nglobally unique instance masks, and 4D bounding boxes. We construct 927K QA\npairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,\nstep-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks covering\nagent motion, human-object interaction, trajectory prediction, relation\nunderstanding, and temporal-causal reasoning, with fine-grained,\nmultidimensional metrics. To tackle these tasks, we propose an end-to-end\nspatio-temporal reasoning framework that unifies dynamic and static scene\ninformation, using instance-aware feature encoding, time and camera encoding,\nand spatially adaptive down-sampling to compress large 4D scenes into token\nsequences manageable by LLMs. Experiments on EgoDynamic4D show that our method\nconsistently outperforms baselines, validating the effectiveness of multimodal\ntemporal modeling for egocentric dynamic scene understanding."
                },
                "authors": [
                    {
                        "name": "Junsheng Huang"
                    },
                    {
                        "name": "Shengyu Hao"
                    },
                    {
                        "name": "Bocheng Hu"
                    },
                    {
                        "name": "Gaoang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gaoang Wang"
                },
                "author": "Gaoang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10323v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10323v5",
                "updated": "2025-08-12T15:00:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    0,
                    13,
                    1,
                    224,
                    0
                ],
                "published": "2025-06-12T03:13:55Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    3,
                    13,
                    55,
                    3,
                    163,
                    0
                ],
                "title": "ELFuzz: Efficient Input Generation via LLM-driven Synthesis Over Fuzzer\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELFuzz: Efficient Input Generation via LLM-driven Synthesis Over Fuzzer\n  Space"
                },
                "summary": "Generation-based fuzzing produces appropriate test cases according to\nspecifications of input grammars and semantic constraints to test systems and\nsoftware. However, these specifications require significant manual effort to\nconstruct. This paper proposes a new approach, ELFuzz (Evolution Through Large\nLanguage Models for Fuzzing), that automatically synthesizes generation-based\nfuzzers tailored to a system under test (SUT) via LLM-driven synthesis over\nfuzzer space. At a high level, it starts with minimal seed fuzzers and propels\nthe synthesis by fully automated LLM-driven evolution with coverage guidance.\nCompared to previous approaches, ELFuzz can 1) seamlessly scale to SUTs of\nreal-world sizes -- up to 1,791,104 lines of code in our evaluation -- and 2)\nsynthesize efficient fuzzers that catch interesting grammatical structures and\nsemantic constraints in a human-understandable way. Our evaluation compared\nELFuzz with specifications manually written by domain experts and synthesized\nby state-of-the-art approaches. It shows that ELFuzz achieves up to 434.8% more\ncoverage over the second best and triggers up to 216.7% more artificially\ninjected bugs, compared to the state-of-the-art. We also used ELFuzz to conduct\na real-world fuzzing campaign on the newest version of cvc5 for 14 days, and\nencouragingly, it found five 0-day bugs (three are exploitable). Moreover, we\nconducted an ablation study, which shows that the fuzzer space model, the key\ncomponent of ELFuzz, contributes the most (up to 62.5%) to the effectiveness of\nELFuzz. Further analysis of the fuzzers synthesized by ELFuzz confirms that\nthey catch interesting grammatical structures and semantic constraints in a\nhuman-understandable way. The results present the promising potential of ELFuzz\nfor more automated, efficient, and extensible input generation for fuzzing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generation-based fuzzing produces appropriate test cases according to\nspecifications of input grammars and semantic constraints to test systems and\nsoftware. However, these specifications require significant manual effort to\nconstruct. This paper proposes a new approach, ELFuzz (Evolution Through Large\nLanguage Models for Fuzzing), that automatically synthesizes generation-based\nfuzzers tailored to a system under test (SUT) via LLM-driven synthesis over\nfuzzer space. At a high level, it starts with minimal seed fuzzers and propels\nthe synthesis by fully automated LLM-driven evolution with coverage guidance.\nCompared to previous approaches, ELFuzz can 1) seamlessly scale to SUTs of\nreal-world sizes -- up to 1,791,104 lines of code in our evaluation -- and 2)\nsynthesize efficient fuzzers that catch interesting grammatical structures and\nsemantic constraints in a human-understandable way. Our evaluation compared\nELFuzz with specifications manually written by domain experts and synthesized\nby state-of-the-art approaches. It shows that ELFuzz achieves up to 434.8% more\ncoverage over the second best and triggers up to 216.7% more artificially\ninjected bugs, compared to the state-of-the-art. We also used ELFuzz to conduct\na real-world fuzzing campaign on the newest version of cvc5 for 14 days, and\nencouragingly, it found five 0-day bugs (three are exploitable). Moreover, we\nconducted an ablation study, which shows that the fuzzer space model, the key\ncomponent of ELFuzz, contributes the most (up to 62.5%) to the effectiveness of\nELFuzz. Further analysis of the fuzzers synthesized by ELFuzz confirms that\nthey catch interesting grammatical structures and semantic constraints in a\nhuman-understandable way. The results present the promising potential of ELFuzz\nfor more automated, efficient, and extensible input generation for fuzzing."
                },
                "authors": [
                    {
                        "name": "Chuyang Chen"
                    },
                    {
                        "name": "Brendan Dolan-Gavitt"
                    },
                    {
                        "name": "Zhiqiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lin"
                },
                "author": "Zhiqiang Lin",
                "arxiv_comment": "Accepted by USENIX Security'25 Cycle 2",
                "arxiv_journal_ref": "The 34th USENIX Security Symposium, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10323v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10323v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08989v1",
                "updated": "2025-08-12T14:57:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    57,
                    3,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:57:03Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    57,
                    3,
                    1,
                    224,
                    0
                ],
                "title": "KFFocus: Highlighting Keyframes for Enhanced Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KFFocus: Highlighting Keyframes for Enhanced Video Understanding"
                },
                "summary": "Recently, with the emergence of large language models, multimodal LLMs have\ndemonstrated exceptional capabilities in image and video modalities. Despite\nadvancements in video comprehension, the substantial computational demands of\nlong video sequences lead current video LLMs (Vid-LLMs) to employ compression\nstrategies at both the inter-frame level (e.g., uniform sampling of video\nframes) and intra-frame level (e.g., condensing all visual tokens of each frame\ninto a limited number). However, this approach often neglects the uneven\ntemporal distribution of critical information across frames, risking the\nomission of keyframes that contain essential temporal and semantic details. To\ntackle these challenges, we propose KFFocus, a method designed to efficiently\ncompress video tokens and emphasize the informative context present within\nvideo frames. We substitute uniform sampling with a refined approach inspired\nby classic video compression principles to identify and capture keyframes based\non their temporal redundancy. By assigning varying condensation ratios to\nframes based on their contextual relevance, KFFocus efficiently reduces token\nredundancy while preserving informative content details. Additionally, we\nintroduce a spatiotemporal modeling module that encodes both the temporal\nrelationships between video frames and the spatial structure within each frame,\nthus providing Vid-LLMs with a nuanced understanding of spatial-temporal\ndynamics. Extensive experiments on widely recognized video understanding\nbenchmarks, especially long video scenarios, demonstrate that KFFocus\nsignificantly outperforms existing methods, achieving substantial computational\nefficiency and enhanced accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, with the emergence of large language models, multimodal LLMs have\ndemonstrated exceptional capabilities in image and video modalities. Despite\nadvancements in video comprehension, the substantial computational demands of\nlong video sequences lead current video LLMs (Vid-LLMs) to employ compression\nstrategies at both the inter-frame level (e.g., uniform sampling of video\nframes) and intra-frame level (e.g., condensing all visual tokens of each frame\ninto a limited number). However, this approach often neglects the uneven\ntemporal distribution of critical information across frames, risking the\nomission of keyframes that contain essential temporal and semantic details. To\ntackle these challenges, we propose KFFocus, a method designed to efficiently\ncompress video tokens and emphasize the informative context present within\nvideo frames. We substitute uniform sampling with a refined approach inspired\nby classic video compression principles to identify and capture keyframes based\non their temporal redundancy. By assigning varying condensation ratios to\nframes based on their contextual relevance, KFFocus efficiently reduces token\nredundancy while preserving informative content details. Additionally, we\nintroduce a spatiotemporal modeling module that encodes both the temporal\nrelationships between video frames and the spatial structure within each frame,\nthus providing Vid-LLMs with a nuanced understanding of spatial-temporal\ndynamics. Extensive experiments on widely recognized video understanding\nbenchmarks, especially long video scenarios, demonstrate that KFFocus\nsignificantly outperforms existing methods, achieving substantial computational\nefficiency and enhanced accuracy."
                },
                "authors": [
                    {
                        "name": "Ming Nie"
                    },
                    {
                        "name": "Chunwei Wang"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08987v1",
                "updated": "2025-08-12T14:56:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    56,
                    11,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:56:11Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    56,
                    11,
                    1,
                    224,
                    0
                ],
                "title": "ColorGPT: Leveraging Large Language Models for Multimodal Color\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ColorGPT: Leveraging Large Language Models for Multimodal Color\n  Recommendation"
                },
                "summary": "Colors play a crucial role in the design of vector graphic documents by\nenhancing visual appeal, facilitating communication, improving usability, and\nensuring accessibility. In this context, color recommendation involves\nsuggesting appropriate colors to complete or refine a design when one or more\ncolors are missing or require alteration. Traditional methods often struggled\nwith these challenges due to the complex nature of color design and the limited\ndata availability. In this study, we explored the use of pretrained Large\nLanguage Models (LLMs) and their commonsense reasoning capabilities for color\nrecommendation, raising the question: Can pretrained LLMs serve as superior\ndesigners for color recommendation tasks? To investigate this, we developed a\nrobust, rigorously validated pipeline, ColorGPT, that was built by\nsystematically testing multiple color representations and applying effective\nprompt engineering techniques. Our approach primarily targeted color palette\ncompletion by recommending colors based on a set of given colors and\naccompanying context. Moreover, our method can be extended to full palette\ngeneration, producing an entire color palette corresponding to a provided\ntextual description. Experimental results demonstrated that our LLM-based\npipeline outperformed existing methods in terms of color suggestion accuracy\nand the distribution of colors in the color palette completion task. For the\nfull palette generation task, our approach also yielded improvements in color\ndiversity and similarity compared to current techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colors play a crucial role in the design of vector graphic documents by\nenhancing visual appeal, facilitating communication, improving usability, and\nensuring accessibility. In this context, color recommendation involves\nsuggesting appropriate colors to complete or refine a design when one or more\ncolors are missing or require alteration. Traditional methods often struggled\nwith these challenges due to the complex nature of color design and the limited\ndata availability. In this study, we explored the use of pretrained Large\nLanguage Models (LLMs) and their commonsense reasoning capabilities for color\nrecommendation, raising the question: Can pretrained LLMs serve as superior\ndesigners for color recommendation tasks? To investigate this, we developed a\nrobust, rigorously validated pipeline, ColorGPT, that was built by\nsystematically testing multiple color representations and applying effective\nprompt engineering techniques. Our approach primarily targeted color palette\ncompletion by recommending colors based on a set of given colors and\naccompanying context. Moreover, our method can be extended to full palette\ngeneration, producing an entire color palette corresponding to a provided\ntextual description. Experimental results demonstrated that our LLM-based\npipeline outperformed existing methods in terms of color suggestion accuracy\nand the distribution of colors in the color palette completion task. For the\nfull palette generation task, our approach also yielded improvements in color\ndiversity and similarity compared to current techniques."
                },
                "authors": [
                    {
                        "name": "Ding Xia"
                    },
                    {
                        "name": "Naoto Inoue"
                    },
                    {
                        "name": "Qianru Qiu"
                    },
                    {
                        "name": "Kotaro Kikuchi"
                    }
                ],
                "author_detail": {
                    "name": "Kotaro Kikuchi"
                },
                "author": "Kotaro Kikuchi",
                "arxiv_comment": "Accepted to ICDAR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08985v1",
                "updated": "2025-08-12T14:53:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    53,
                    54,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:53:54Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    53,
                    54,
                    1,
                    224,
                    0
                ],
                "title": "Low-Regret and Low-Complexity Learning for Hierarchical Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Regret and Low-Complexity Learning for Hierarchical Inference"
                },
                "summary": "This work focuses on Hierarchical Inference (HI) in edge intelligence\nsystems, where a compact Local-ML model on an end-device works in conjunction\nwith a high-accuracy Remote-ML model on an edge-server. HI aims to reduce\nlatency, improve accuracy, and lower bandwidth usage by first using the\nLocal-ML model for inference and offloading to the Remote-ML only when the\nlocal inference is likely incorrect. A critical challenge in HI is estimating\nthe likelihood of the local inference being incorrect, especially when data\ndistributions and offloading costs change over time -- a problem we term\nHierarchical Inference Learning (HIL). We introduce a novel approach to HIL by\nmodeling the probability of correct inference by the Local-ML as an increasing\nfunction of the model's confidence measure, a structure motivated by empirical\nobservations but previously unexploited. We propose two policies, HI-LCB and\nHI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We\ndemonstrate that both policies achieve order-optimal regret of $O(\\log T)$, a\nsignificant improvement over existing HIL policies with $O(T^{2/3})$ regret\nguarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational\ncomplexity, making it well-suited for deployment on devices with severe\nresource limitations. Simulations using real-world datasets confirm that our\npolicies outperform existing state-of-the-art HIL methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work focuses on Hierarchical Inference (HI) in edge intelligence\nsystems, where a compact Local-ML model on an end-device works in conjunction\nwith a high-accuracy Remote-ML model on an edge-server. HI aims to reduce\nlatency, improve accuracy, and lower bandwidth usage by first using the\nLocal-ML model for inference and offloading to the Remote-ML only when the\nlocal inference is likely incorrect. A critical challenge in HI is estimating\nthe likelihood of the local inference being incorrect, especially when data\ndistributions and offloading costs change over time -- a problem we term\nHierarchical Inference Learning (HIL). We introduce a novel approach to HIL by\nmodeling the probability of correct inference by the Local-ML as an increasing\nfunction of the model's confidence measure, a structure motivated by empirical\nobservations but previously unexploited. We propose two policies, HI-LCB and\nHI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We\ndemonstrate that both policies achieve order-optimal regret of $O(\\log T)$, a\nsignificant improvement over existing HIL policies with $O(T^{2/3})$ regret\nguarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational\ncomplexity, making it well-suited for deployment on devices with severe\nresource limitations. Simulations using real-world datasets confirm that our\npolicies outperform existing state-of-the-art HIL methods."
                },
                "authors": [
                    {
                        "name": "Sameep Chattopadhyay"
                    },
                    {
                        "name": "Vinay Sutar"
                    },
                    {
                        "name": "Jaya Prakash Champati"
                    },
                    {
                        "name": "Sharayu Moharir"
                    }
                ],
                "author_detail": {
                    "name": "Sharayu Moharir"
                },
                "author": "Sharayu Moharir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16185v2",
                "updated": "2025-08-12T14:50:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    50,
                    3,
                    1,
                    224,
                    0
                ],
                "published": "2024-11-25T08:31:55Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    31,
                    55,
                    0,
                    330,
                    0
                ],
                "title": "Fancy123: One Image to High-Quality 3D Mesh Generation via Plug-and-Play\n  Deformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fancy123: One Image to High-Quality 3D Mesh Generation via Plug-and-Play\n  Deformation"
                },
                "summary": "Generating 3D meshes from a single image is an important but ill-posed task.\nExisting methods mainly adopt 2D multiview diffusion models to generate\nintermediate multiview images, and use the Large Reconstruction Model (LRM) to\ncreate the final meshes. However, the multiview images exhibit local\ninconsistencies, and the meshes often lack fidelity to the input image or look\nblurry. We propose Fancy123, featuring two enhancement modules and an\nunprojection operation to address the above three issues, respectively. The\nappearance enhancement module deforms the 2D multiview images to realign\nmisaligned pixels for better multiview consistency. The fidelity enhancement\nmodule deforms the 3D mesh to match the input image. The unprojection of the\ninput image and deformed multiview images onto LRM's generated mesh ensures\nhigh clarity, discarding LRM's predicted blurry-looking mesh colors. Extensive\nqualitative and quantitative experiments verify Fancy123's SoTA performance\nwith significant improvement. Also, the two enhancement modules are\nplug-and-play and work at inference time, allowing seamless integration into\nvarious existing single-image-to-3D methods. Code at:\nhttps://github.com/YuQiao0303/Fancy123",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating 3D meshes from a single image is an important but ill-posed task.\nExisting methods mainly adopt 2D multiview diffusion models to generate\nintermediate multiview images, and use the Large Reconstruction Model (LRM) to\ncreate the final meshes. However, the multiview images exhibit local\ninconsistencies, and the meshes often lack fidelity to the input image or look\nblurry. We propose Fancy123, featuring two enhancement modules and an\nunprojection operation to address the above three issues, respectively. The\nappearance enhancement module deforms the 2D multiview images to realign\nmisaligned pixels for better multiview consistency. The fidelity enhancement\nmodule deforms the 3D mesh to match the input image. The unprojection of the\ninput image and deformed multiview images onto LRM's generated mesh ensures\nhigh clarity, discarding LRM's predicted blurry-looking mesh colors. Extensive\nqualitative and quantitative experiments verify Fancy123's SoTA performance\nwith significant improvement. Also, the two enhancement modules are\nplug-and-play and work at inference time, allowing seamless integration into\nvarious existing single-image-to-3D methods. Code at:\nhttps://github.com/YuQiao0303/Fancy123"
                },
                "authors": [
                    {
                        "name": "Qiao Yu"
                    },
                    {
                        "name": "Xianzhi Li"
                    },
                    {
                        "name": "Yuan Tang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Long Hu"
                    },
                    {
                        "name": "Yixue Hao"
                    },
                    {
                        "name": "Min Chen"
                    }
                ],
                "author_detail": {
                    "name": "Min Chen"
                },
                "author": "Min Chen",
                "arxiv_comment": "CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08983v1",
                "updated": "2025-08-12T14:49:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    49,
                    44,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:49:44Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    49,
                    44,
                    1,
                    224,
                    0
                ],
                "title": "Rational Inverse Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rational Inverse Reasoning"
                },
                "summary": "Humans can observe a single, imperfect demonstration and immediately\ngeneralize to very different problem settings. Robots, in contrast, often\nrequire hundreds of examples and still struggle to generalize beyond the\ntraining conditions. We argue that this limitation arises from the inability to\nrecover the latent explanations that underpin intelligent behavior, and that\nthese explanations can take the form of structured programs consisting of\nhigh-level goals, sub-task decomposition, and execution constraints. In this\nwork, we introduce Rational Inverse Reasoning (RIR), a framework for inferring\nthese latent programs through a hierarchical generative model of behavior. RIR\nframes few-shot imitation as Bayesian program induction: a vision-language\nmodel iteratively proposes structured symbolic task hypotheses, while a\nplanner-in-the-loop inference scheme scores each by the likelihood of the\nobserved demonstration under that hypothesis. This loop yields a posterior over\nconcise, executable programs. We evaluate RIR on a suite of continuous\nmanipulation tasks designed to test one-shot and few-shot generalization across\nvariations in object pose, count, geometry, and layout. With as little as one\ndemonstration, RIR infers the intended task structure and generalizes to novel\nsettings, outperforming state-of-the-art vision-language model baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans can observe a single, imperfect demonstration and immediately\ngeneralize to very different problem settings. Robots, in contrast, often\nrequire hundreds of examples and still struggle to generalize beyond the\ntraining conditions. We argue that this limitation arises from the inability to\nrecover the latent explanations that underpin intelligent behavior, and that\nthese explanations can take the form of structured programs consisting of\nhigh-level goals, sub-task decomposition, and execution constraints. In this\nwork, we introduce Rational Inverse Reasoning (RIR), a framework for inferring\nthese latent programs through a hierarchical generative model of behavior. RIR\nframes few-shot imitation as Bayesian program induction: a vision-language\nmodel iteratively proposes structured symbolic task hypotheses, while a\nplanner-in-the-loop inference scheme scores each by the likelihood of the\nobserved demonstration under that hypothesis. This loop yields a posterior over\nconcise, executable programs. We evaluate RIR on a suite of continuous\nmanipulation tasks designed to test one-shot and few-shot generalization across\nvariations in object pose, count, geometry, and layout. With as little as one\ndemonstration, RIR infers the intended task structure and generalizes to novel\nsettings, outperforming state-of-the-art vision-language model baselines."
                },
                "authors": [
                    {
                        "name": "Ben Zandonati"
                    },
                    {
                        "name": "Tomás Lozano-Pérez"
                    },
                    {
                        "name": "Leslie Pack Kaelbling"
                    }
                ],
                "author_detail": {
                    "name": "Leslie Pack Kaelbling"
                },
                "author": "Leslie Pack Kaelbling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10331v2",
                "updated": "2025-08-12T14:44:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    44,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-03-13T13:07:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    7,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting\n  Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting\n  Conditions"
                },
                "summary": "Open Semantic Mapping (OSM) is a key technology in robotic perception,\ncombining semantic segmentation and SLAM techniques. This paper introduces a\ndynamically configurable and highly automated LLM/LVLM-powered pipeline for\nevaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark).\nThe study focuses on evaluating state-of-the-art semantic mapping algorithms\nunder varying indoor lighting conditions, a critical challenge in indoor\nenvironments. We introduce a novel dataset with simulated RGB-D sequences and\nground truth 3D reconstructions, facilitating the rigorous analysis of mapping\nperformance across different lighting conditions. Through experiments on\nleading models such as ConceptGraphs, BBQ and OpenScene, we evaluate the\nsemantic fidelity of object recognition and segmentation. Additionally, we\nintroduce a Scene Graph evaluation method to analyze the ability of models to\ninterpret semantic structure. The results provide insights into the robustness\nof these models, forming future research directions for developing resilient\nand adaptable robotic systems. Project page is available at\nhttps://be2rlab.github.io/OSMa-Bench/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Semantic Mapping (OSM) is a key technology in robotic perception,\ncombining semantic segmentation and SLAM techniques. This paper introduces a\ndynamically configurable and highly automated LLM/LVLM-powered pipeline for\nevaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark).\nThe study focuses on evaluating state-of-the-art semantic mapping algorithms\nunder varying indoor lighting conditions, a critical challenge in indoor\nenvironments. We introduce a novel dataset with simulated RGB-D sequences and\nground truth 3D reconstructions, facilitating the rigorous analysis of mapping\nperformance across different lighting conditions. Through experiments on\nleading models such as ConceptGraphs, BBQ and OpenScene, we evaluate the\nsemantic fidelity of object recognition and segmentation. Additionally, we\nintroduce a Scene Graph evaluation method to analyze the ability of models to\ninterpret semantic structure. The results provide insights into the robustness\nof these models, forming future research directions for developing resilient\nand adaptable robotic systems. Project page is available at\nhttps://be2rlab.github.io/OSMa-Bench/."
                },
                "authors": [
                    {
                        "name": "Maxim Popov"
                    },
                    {
                        "name": "Regina Kurkova"
                    },
                    {
                        "name": "Mikhail Iumanov"
                    },
                    {
                        "name": "Jaafar Mahmoud"
                    },
                    {
                        "name": "Sergey Kolyubin"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Kolyubin"
                },
                "author": "Sergey Kolyubin",
                "arxiv_comment": "Project page: https://be2rlab.github.io/OSMa-Bench/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07623v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07623v8",
                "updated": "2025-08-12T14:44:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    44,
                    44,
                    1,
                    224,
                    0
                ],
                "published": "2024-05-13T10:30:33Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    10,
                    30,
                    33,
                    0,
                    134,
                    0
                ],
                "title": "Optimizing Class-Level Probability Reweighting Coefficients for\n  Equitable Prompting Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Class-Level Probability Reweighting Coefficients for\n  Equitable Prompting Accuracy"
                },
                "summary": "Even as we engineer LLMs for alignment and safety, they often uncover biases\nfrom pre-training data's statistical regularities (from disproportionate\nco-occurrences to stereotypical associations mirroring human cognitive biases).\nThis leads to persistent, uneven class accuracy in classification and QA. Such\nper-class accuracy disparities are not inherently resolved by\narchitectural/training evolutions or data scaling, making post-hoc correction\nessential for equitable performance. To mitigate LLM class accuracy imbalance,\nwe develop a post-hoc probability reweighting method that directly optimizes\nfor non-differentiable performance-driven and fairness-aligned metrics, through\na novel COBias metric that highlights disparities in class accuracies. This\npost-hoc bias mitigation method is grounded in discrete optimization with\nnonlinear integer programming (NIP) objectives and an efficient metaheuristic\nsolution framework with theoretical convergence guarantees. Operating\nmodel-agnostically, it learns reweighting coefficients from output class\nprobabilities to adjust LLM inference outputs without internal weight updates.\nEvaluations demonstrate its effectiveness: reducing COBias (61% relative\nreduction), increasing overall accuracy (18% relative increase), and achieving\nrobust within-task generalization across diverse prompt configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even as we engineer LLMs for alignment and safety, they often uncover biases\nfrom pre-training data's statistical regularities (from disproportionate\nco-occurrences to stereotypical associations mirroring human cognitive biases).\nThis leads to persistent, uneven class accuracy in classification and QA. Such\nper-class accuracy disparities are not inherently resolved by\narchitectural/training evolutions or data scaling, making post-hoc correction\nessential for equitable performance. To mitigate LLM class accuracy imbalance,\nwe develop a post-hoc probability reweighting method that directly optimizes\nfor non-differentiable performance-driven and fairness-aligned metrics, through\na novel COBias metric that highlights disparities in class accuracies. This\npost-hoc bias mitigation method is grounded in discrete optimization with\nnonlinear integer programming (NIP) objectives and an efficient metaheuristic\nsolution framework with theoretical convergence guarantees. Operating\nmodel-agnostically, it learns reweighting coefficients from output class\nprobabilities to adjust LLM inference outputs without internal weight updates.\nEvaluations demonstrate its effectiveness: reducing COBias (61% relative\nreduction), increasing overall accuracy (18% relative increase), and achieving\nrobust within-task generalization across diverse prompt configurations."
                },
                "authors": [
                    {
                        "name": "Ruixi Lin"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07623v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07623v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01832v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01832v2",
                "updated": "2025-08-12T14:43:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    43,
                    17,
                    1,
                    224,
                    0
                ],
                "published": "2024-11-15T22:19:21Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    19,
                    21,
                    4,
                    320,
                    0
                ],
                "title": "Targeting Completeness: Automated Complexity Analysis of Integer\n  Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeting Completeness: Automated Complexity Analysis of Integer\n  Programs"
                },
                "summary": "There exist several approaches to infer runtime or resource bounds for\ninteger programs automatically. In this paper, we study the subclass of\nperiodic rational solvable loops (prs-loops), where questions regarding the\nruntime and the size of variable values are decidable and where we can\ntherefore obtain techniques that are complete for such subclasses. We show how\nto use these results for the complexity analysis of arbitrary general integer\nprograms. To this end, we present a modular approach which computes local\nruntime and size bounds for subprograms which correspond to prs-loops. These\nlocal bounds are then lifted to global runtime and size bounds for the whole\ninteger program. Furthermore, we introduce several techniques to transform\nlarger programs into prs-loops to increase the scope of the approach. The power\nof the procedure is shown by our implementation in the complexity analysis tool\nKoAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There exist several approaches to infer runtime or resource bounds for\ninteger programs automatically. In this paper, we study the subclass of\nperiodic rational solvable loops (prs-loops), where questions regarding the\nruntime and the size of variable values are decidable and where we can\ntherefore obtain techniques that are complete for such subclasses. We show how\nto use these results for the complexity analysis of arbitrary general integer\nprograms. To this end, we present a modular approach which computes local\nruntime and size bounds for subprograms which correspond to prs-loops. These\nlocal bounds are then lifted to global runtime and size bounds for the whole\ninteger program. Furthermore, we introduce several techniques to transform\nlarger programs into prs-loops to increase the scope of the approach. The power\nof the procedure is shown by our implementation in the complexity analysis tool\nKoAT."
                },
                "authors": [
                    {
                        "name": "Nils Lommen"
                    },
                    {
                        "name": "Éléanore Meyer"
                    },
                    {
                        "name": "Jürgen Giesl"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Giesl"
                },
                "author": "Jürgen Giesl",
                "arxiv_comment": "Journal version of IJCAR'22 (arXiv:2205.08869) and FroCoS'23\n  (arXiv:2307.06921) papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01832v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08975v2",
                "updated": "2025-08-13T01:39:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    18,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T14:39:05Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    39,
                    5,
                    1,
                    224,
                    0
                ],
                "title": "Nonparametric Bayesian Multi-Treatment Mixture Cure Survival Model with\n  Application in Pediatric Oncology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric Bayesian Multi-Treatment Mixture Cure Survival Model with\n  Application in Pediatric Oncology"
                },
                "summary": "Heterogeneous treatment effect estimation is critical in oncology,\nparticularly in multi-arm trials with overlapping therapeutic components and\nlong-term survivors. These shared mechanisms pose a central challenge to\nidentifying causal effects in precision medicine. We propose a novel\ncovariate-dependent nonparametric Bayesian multi-treatment cure survival model\nthat jointly accounts for common structures among treatments and cure\nfractions. Through latent link functions, our model leverages sharing among\ntreatments through a flexible modeling approach, enabling individualized\nsurvival inference. We adopt a Bayesian route for inference and implement an\nefficient MCMC algorithm for approximating the posterior. Simulation studies\ndemonstrate the method's robustness and superiority in various specification\nscenarios. Finally, application to the AALL0434 trial reveals clinically\nmeaningful differences in survival across methotrexate-based regimens and their\nassociations with different covariates, underscoring its practical utility for\nlearning treatment effects in real-world pediatric oncology data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous treatment effect estimation is critical in oncology,\nparticularly in multi-arm trials with overlapping therapeutic components and\nlong-term survivors. These shared mechanisms pose a central challenge to\nidentifying causal effects in precision medicine. We propose a novel\ncovariate-dependent nonparametric Bayesian multi-treatment cure survival model\nthat jointly accounts for common structures among treatments and cure\nfractions. Through latent link functions, our model leverages sharing among\ntreatments through a flexible modeling approach, enabling individualized\nsurvival inference. We adopt a Bayesian route for inference and implement an\nefficient MCMC algorithm for approximating the posterior. Simulation studies\ndemonstrate the method's robustness and superiority in various specification\nscenarios. Finally, application to the AALL0434 trial reveals clinically\nmeaningful differences in survival across methotrexate-based regimens and their\nassociations with different covariates, underscoring its practical utility for\nlearning treatment effects in real-world pediatric oncology data."
                },
                "authors": [
                    {
                        "name": "Peter Chang"
                    },
                    {
                        "name": "John Kairalla"
                    },
                    {
                        "name": "Arkaprava Roy"
                    }
                ],
                "author_detail": {
                    "name": "Arkaprava Roy"
                },
                "author": "Arkaprava Roy",
                "arxiv_comment": "33 pages with 7 figures and 7 tables, 23 pages of supplementary\n  material with 6 figures and 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16971v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16971v5",
                "updated": "2025-08-12T14:37:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    37,
                    1,
                    1,
                    224,
                    0
                ],
                "published": "2024-03-25T17:32:23Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    17,
                    32,
                    23,
                    0,
                    85,
                    0
                ],
                "title": "AIOS: LLM Agent Operating System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIOS: LLM Agent Operating System"
                },
                "summary": "LLM-based intelligent agents face significant deployment challenges,\nparticularly related to resource management. Allowing unrestricted access to\nLLM or tool resources can lead to inefficient or even potentially harmful\nresource allocation and utilization for agents. Furthermore, the absence of\nproper scheduling and resource management mechanisms in current agent designs\nhinders concurrent processing and limits overall system efficiency. To address\nthese challenges, this paper proposes the architecture of AIOS (LLM-based AI\nAgent Operating System) under the context of managing LLM-based agents. It\nintroduces a novel architecture for serving LLM-based agents by isolating\nresources and LLM-specific services from agent applications into an AIOS\nkernel. This AIOS kernel provides fundamental services (e.g., scheduling,\ncontext management, memory management, storage management, access control) for\nruntime agents. To enhance usability, AIOS also includes an AIOS SDK, a\ncomprehensive suite of APIs designed for utilizing functionalities provided by\nthe AIOS kernel. Experimental results demonstrate that using AIOS can achieve\nup to 2.1x faster execution for serving agents built by various agent\nframeworks. The source code is available at\nhttps://github.com/agiresearch/AIOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based intelligent agents face significant deployment challenges,\nparticularly related to resource management. Allowing unrestricted access to\nLLM or tool resources can lead to inefficient or even potentially harmful\nresource allocation and utilization for agents. Furthermore, the absence of\nproper scheduling and resource management mechanisms in current agent designs\nhinders concurrent processing and limits overall system efficiency. To address\nthese challenges, this paper proposes the architecture of AIOS (LLM-based AI\nAgent Operating System) under the context of managing LLM-based agents. It\nintroduces a novel architecture for serving LLM-based agents by isolating\nresources and LLM-specific services from agent applications into an AIOS\nkernel. This AIOS kernel provides fundamental services (e.g., scheduling,\ncontext management, memory management, storage management, access control) for\nruntime agents. To enhance usability, AIOS also includes an AIOS SDK, a\ncomprehensive suite of APIs designed for utilizing functionalities provided by\nthe AIOS kernel. Experimental results demonstrate that using AIOS can achieve\nup to 2.1x faster execution for serving agents built by various agent\nframeworks. The source code is available at\nhttps://github.com/agiresearch/AIOS."
                },
                "authors": [
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Zelong Li"
                    },
                    {
                        "name": "Shuyuan Xu"
                    },
                    {
                        "name": "Ruosong Ye"
                    },
                    {
                        "name": "Yingqiang Ge"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "Published as a full paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16971v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16971v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08961v2",
                "updated": "2025-08-13T08:08:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    8,
                    40,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T14:23:26Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    23,
                    26,
                    1,
                    224,
                    0
                ],
                "title": "DualSpeechLM: Towards Unified Speech Understanding and Generation via\n  Dual Speech Token Modeling with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualSpeechLM: Towards Unified Speech Understanding and Generation via\n  Dual Speech Token Modeling with Large Language Models"
                },
                "summary": "Extending pre-trained Large Language Models (LLMs)'s speech understanding or\ngeneration abilities by introducing various effective speech tokens has\nattracted great attention in the speech community. However, building a unified\nspeech understanding and generation model still faces the following challenges:\n(1) Due to the huge modality gap between speech tokens and text tokens,\nextending text LLMs to unified speech LLMs relies on large-scale paired data\nfor fine-tuning, and (2) Generation and understanding tasks prefer information\nat different levels, e.g., generation benefits from detailed acoustic features,\nwhile understanding favors high-level semantics. This divergence leads to\ndifficult performance optimization in one unified model. To solve these\nchallenges, in this paper, we present two key insights in speech tokenization\nand speech language modeling. Specifically, we first propose an\nUnderstanding-driven Speech Tokenizer (USTokenizer), which extracts high-level\nsemantic information essential for accomplishing understanding tasks using text\nLLMs. In this way, USToken enjoys better modality commonality with text, which\nreduces the difficulty of modality alignment in adapting text LLMs to speech\nLLMs. Secondly, we present DualSpeechLM, a dual-token modeling framework that\nconcurrently models USToken as input and acoustic token as output within a\nunified, end-to-end framework, seamlessly integrating speech understanding and\ngeneration capabilities. Furthermore, we propose a novel semantic supervision\nloss and a Chain-of-Condition (CoC) strategy to stabilize model training and\nenhance speech generation performance. Experimental results demonstrate that\nour proposed approach effectively fosters a complementary relationship between\nunderstanding and generation tasks, highlighting the promising strategy of\nmutually enhancing both tasks in one unified model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending pre-trained Large Language Models (LLMs)'s speech understanding or\ngeneration abilities by introducing various effective speech tokens has\nattracted great attention in the speech community. However, building a unified\nspeech understanding and generation model still faces the following challenges:\n(1) Due to the huge modality gap between speech tokens and text tokens,\nextending text LLMs to unified speech LLMs relies on large-scale paired data\nfor fine-tuning, and (2) Generation and understanding tasks prefer information\nat different levels, e.g., generation benefits from detailed acoustic features,\nwhile understanding favors high-level semantics. This divergence leads to\ndifficult performance optimization in one unified model. To solve these\nchallenges, in this paper, we present two key insights in speech tokenization\nand speech language modeling. Specifically, we first propose an\nUnderstanding-driven Speech Tokenizer (USTokenizer), which extracts high-level\nsemantic information essential for accomplishing understanding tasks using text\nLLMs. In this way, USToken enjoys better modality commonality with text, which\nreduces the difficulty of modality alignment in adapting text LLMs to speech\nLLMs. Secondly, we present DualSpeechLM, a dual-token modeling framework that\nconcurrently models USToken as input and acoustic token as output within a\nunified, end-to-end framework, seamlessly integrating speech understanding and\ngeneration capabilities. Furthermore, we propose a novel semantic supervision\nloss and a Chain-of-Condition (CoC) strategy to stabilize model training and\nenhance speech generation performance. Experimental results demonstrate that\nour proposed approach effectively fosters a complementary relationship between\nunderstanding and generation tasks, highlighting the promising strategy of\nmutually enhancing both tasks in one unified model."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Wang"
                    },
                    {
                        "name": "Dongchao Yang"
                    },
                    {
                        "name": "Yiwen Shao"
                    },
                    {
                        "name": "Hangting Chen"
                    },
                    {
                        "name": "Jiankun Zhao"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Helen Meng"
                    },
                    {
                        "name": "Xixin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xixin Wu"
                },
                "author": "Xixin Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14910v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14910v3",
                "updated": "2025-08-13T06:12:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    6,
                    12,
                    51,
                    2,
                    225,
                    0
                ],
                "published": "2025-02-19T06:33:59Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    33,
                    59,
                    2,
                    50,
                    0
                ],
                "title": "EvoP: Robust LLM Inference via Evolutionary Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoP: Robust LLM Inference via Evolutionary Pruning"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing tasks, but their massive size and computational demands\nhinder their deployment in resource-constrained environments. Existing model\npruning methods address this issue by removing redundant structures (e.g.,\nelements, channels, layers) from the model. However, these methods employ a\nheuristic pruning strategy, which leads to suboptimal performance. Besides,\nthey also ignore the data characteristics when pruning the model.\n  To overcome these limitations, we propose EvoP, an evolutionary pruning\nframework for robust LLM inference. EvoP first presents a cluster-based\ncalibration dataset sampling (CCDS) strategy for creating a more diverse\ncalibration dataset. EvoP then introduces an evolutionary pruning pattern\nsearching (EPPS) method to find the optimal pruning pattern. Compared to\nexisting model pruning techniques, EvoP achieves the best performance while\nmaintaining the best efficiency. Experiments across different LLMs and\ndifferent downstream tasks validate the effectiveness of the proposed EvoP,\nmaking it a practical and scalable solution for deploying LLMs in real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing tasks, but their massive size and computational demands\nhinder their deployment in resource-constrained environments. Existing model\npruning methods address this issue by removing redundant structures (e.g.,\nelements, channels, layers) from the model. However, these methods employ a\nheuristic pruning strategy, which leads to suboptimal performance. Besides,\nthey also ignore the data characteristics when pruning the model.\n  To overcome these limitations, we propose EvoP, an evolutionary pruning\nframework for robust LLM inference. EvoP first presents a cluster-based\ncalibration dataset sampling (CCDS) strategy for creating a more diverse\ncalibration dataset. EvoP then introduces an evolutionary pruning pattern\nsearching (EPPS) method to find the optimal pruning pattern. Compared to\nexisting model pruning techniques, EvoP achieves the best performance while\nmaintaining the best efficiency. Experiments across different LLMs and\ndifferent downstream tasks validate the effectiveness of the proposed EvoP,\nmaking it a practical and scalable solution for deploying LLMs in real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Hongchao Du"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Shuai Chen"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    },
                    {
                        "name": "Nan Guan"
                    },
                    {
                        "name": "Chun Jason Xue"
                    }
                ],
                "author_detail": {
                    "name": "Chun Jason Xue"
                },
                "author": "Chun Jason Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14910v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14910v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08243v2",
                "updated": "2025-08-12T14:19:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    19,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T17:56:06Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    56,
                    6,
                    0,
                    223,
                    0
                ],
                "title": "Jinx: Unlimited LLMs for Probing Alignment Failures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jinx: Unlimited LLMs for Probing Alignment Failures"
                },
                "summary": "Unlimited, or so-called helpful-only language models are trained without\nsafety alignment constraints and never refuse user queries. They are widely\nused by leading AI companies as internal tools for red teaming and alignment\nevaluation. For example, if a safety-aligned model produces harmful outputs\nsimilar to an unlimited model, this indicates alignment failures that require\nfurther attention. Despite their essential role in assessing alignment, such\nmodels are not available to the research community.\n  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx\nresponds to all queries without refusals or safety filtering, while preserving\nthe base model's capabilities in reasoning and instruction following. It\nprovides researchers with an accessible tool for probing alignment failures,\nevaluating safety boundaries, and systematically studying failure modes in\nlanguage model safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlimited, or so-called helpful-only language models are trained without\nsafety alignment constraints and never refuse user queries. They are widely\nused by leading AI companies as internal tools for red teaming and alignment\nevaluation. For example, if a safety-aligned model produces harmful outputs\nsimilar to an unlimited model, this indicates alignment failures that require\nfurther attention. Despite their essential role in assessing alignment, such\nmodels are not available to the research community.\n  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx\nresponds to all queries without refusals or safety filtering, while preserving\nthe base model's capabilities in reasoning and instruction following. It\nprovides researchers with an accessible tool for probing alignment failures,\nevaluating safety boundaries, and systematically studying failure modes in\nlanguage model safety."
                },
                "authors": [
                    {
                        "name": "Jiahao Zhao"
                    },
                    {
                        "name": "Liwei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Dong"
                },
                "author": "Liwei Dong",
                "arxiv_comment": "https://huggingface.co/Jinx-org",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13983v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13983v5",
                "updated": "2025-08-12T14:19:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    19,
                    15,
                    1,
                    224,
                    0
                ],
                "published": "2025-01-23T06:57:24Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    57,
                    24,
                    3,
                    23,
                    0
                ],
                "title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) are pre-trained on ultra-large-scale corpora,\nthe problem of data contamination is becoming increasingly serious, and there\nis a risk that static evaluation benchmarks overestimate the performance of\nLLMs. To address this, this paper proposes a dynamic data evaluation method\ncalled AdEval (Alignment-based Dynamic Evaluation). AdEval first extracts\nknowledge points and main ideas from static datasets to achieve dynamic\nalignment with the core content of static benchmarks, and by avoiding direct\nreliance on static datasets, it inherently reduces the risk of data\ncontamination from the source. It then obtains background information through\nonline searches to generate detailed descriptions of the knowledge points.\nFinally, it designs questions based on Bloom's cognitive hierarchy across six\ndimensions-remembering, understanding, applying, analyzing, evaluating, and\ncreating to enable multi-level cognitive assessment. Additionally, AdEval\ncontrols the complexity of dynamically generated datasets through iterative\nquestion reconstruction. Experimental results on multiple datasets show that\nAdEval effectively alleviates the impact of data contamination on evaluation\nresults, solves the problems of insufficient complexity control and\nsingle-dimensional evaluation, and improves the fairness, reliability and\ndiversity of LLMs evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are pre-trained on ultra-large-scale corpora,\nthe problem of data contamination is becoming increasingly serious, and there\nis a risk that static evaluation benchmarks overestimate the performance of\nLLMs. To address this, this paper proposes a dynamic data evaluation method\ncalled AdEval (Alignment-based Dynamic Evaluation). AdEval first extracts\nknowledge points and main ideas from static datasets to achieve dynamic\nalignment with the core content of static benchmarks, and by avoiding direct\nreliance on static datasets, it inherently reduces the risk of data\ncontamination from the source. It then obtains background information through\nonline searches to generate detailed descriptions of the knowledge points.\nFinally, it designs questions based on Bloom's cognitive hierarchy across six\ndimensions-remembering, understanding, applying, analyzing, evaluating, and\ncreating to enable multi-level cognitive assessment. Additionally, AdEval\ncontrols the complexity of dynamically generated datasets through iterative\nquestion reconstruction. Experimental results on multiple datasets show that\nAdEval effectively alleviates the impact of data contamination on evaluation\nresults, solves the problems of insufficient complexity control and\nsingle-dimensional evaluation, and improves the fairness, reliability and\ndiversity of LLMs evaluation."
                },
                "authors": [
                    {
                        "name": "Yang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Yang Fan"
                },
                "author": "Yang Fan",
                "arxiv_comment": "There are serious academic problems in this paper, such as data\n  falsification and plagiarism in the method of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13983v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13983v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18246v2",
                "updated": "2025-08-12T14:03:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    3,
                    22,
                    1,
                    224,
                    0
                ],
                "published": "2024-10-23T19:45:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    45,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Maintenance Optimization for Asset Networks with Unknown Degradation\n  Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintenance Optimization for Asset Networks with Unknown Degradation\n  Parameters"
                },
                "summary": "We consider the key practical challenge of multi-asset maintenance\noptimization in settings where degradation parameters are heterogeneous and\nunknown, and must be inferred from degradation data. To address this, we\npropose scalable methods suitable for complex asset networks. Degradation is\nmodeled as a stochastic shock process, and real-time data are continuously\nincorporated into estimation of shock rates and magnitudes via a Bayesian\nframework. This constitutes a partially observable Markov decision process\nformulation, from which we analytically derive monotonic policy structures.\nMoreover, we propose an open-loop feedback approach that enables policies\ntrained via deep reinforcement learning (DRL) in a simulation environment with\naccess to the true parameters to remain effective when deployed with real-time\nBayesian point estimates instead. Complementing this, we develop a Bayesian\nMarkov decision process (BMDP) framework wherein the agent maintains and\nupdates posterior distributions during deployment. This formulation captures\nthe evolution of parameter uncertainty over time, thereby facilitating the\ntraining of scalable DRL-based policies that adapt as additional data become\navailable.\n  We validate our approach through experiments on synthetic asset networks and\na real-world case involving interventional X-ray system filaments. We find that\nthe proposed DRL methods consistently outperform traditional heuristics across\nvarious scenarios. The policies trained for the BMDP perform well even when\npriors must be estimated from historical data, and remain effective in networks\nwith high asset heterogeneity. Knowledge of true degradation parameters yields\nonly marginal cost benefits, underscoring the ability of our approach to make\neffective decisions under limited information on degradation processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the key practical challenge of multi-asset maintenance\noptimization in settings where degradation parameters are heterogeneous and\nunknown, and must be inferred from degradation data. To address this, we\npropose scalable methods suitable for complex asset networks. Degradation is\nmodeled as a stochastic shock process, and real-time data are continuously\nincorporated into estimation of shock rates and magnitudes via a Bayesian\nframework. This constitutes a partially observable Markov decision process\nformulation, from which we analytically derive monotonic policy structures.\nMoreover, we propose an open-loop feedback approach that enables policies\ntrained via deep reinforcement learning (DRL) in a simulation environment with\naccess to the true parameters to remain effective when deployed with real-time\nBayesian point estimates instead. Complementing this, we develop a Bayesian\nMarkov decision process (BMDP) framework wherein the agent maintains and\nupdates posterior distributions during deployment. This formulation captures\nthe evolution of parameter uncertainty over time, thereby facilitating the\ntraining of scalable DRL-based policies that adapt as additional data become\navailable.\n  We validate our approach through experiments on synthetic asset networks and\na real-world case involving interventional X-ray system filaments. We find that\nthe proposed DRL methods consistently outperform traditional heuristics across\nvarious scenarios. The policies trained for the BMDP perform well even when\npriors must be estimated from historical data, and remain effective in networks\nwith high asset heterogeneity. Knowledge of true degradation parameters yields\nonly marginal cost benefits, underscoring the ability of our approach to make\neffective decisions under limited information on degradation processes."
                },
                "authors": [
                    {
                        "name": "Peter Verleijsdonk"
                    },
                    {
                        "name": "Collin Drent"
                    },
                    {
                        "name": "Stella Kapodistria"
                    },
                    {
                        "name": "Willem van Jaarsveld"
                    }
                ],
                "author_detail": {
                    "name": "Willem van Jaarsveld"
                },
                "author": "Willem van Jaarsveld",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17659v3",
                "updated": "2025-08-13T11:46:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    46,
                    4,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-23T16:24:57Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    24,
                    57,
                    2,
                    204,
                    0
                ],
                "title": "See the Forest and the Trees: A Synergistic Reasoning Framework for\n  Knowledge-Based Visual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "See the Forest and the Trees: A Synergistic Reasoning Framework for\n  Knowledge-Based Visual Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have pushed the frontiers of\nKnowledge-Based Visual Question Answering (KBVQA), yet their reasoning is\nfundamentally bottlenecked by a reliance on uni-dimensional evidence. This\n\"seeing only the trees, but not the forest\" approach prevents robust,\nmulti-faceted understanding. Inspired by the principle of seeing both the\nforest and trees, we propose Synergos-VQA, a novel synergistic reasoning\nframework. At its core, Synergos-VQA concurrently generates and fuses three\ncomplementary evidence streams at inference time: (1) Holistic Evidence to\nperceive the entire scene (the \"forest\"), (2) Structural Evidence from a\nprototype-driven module to identify key objects (the \"trees\"), and (3) Causal\nEvidence from a counterfactual probe to ensure the reasoning is robustly\ngrounded. By synergistically fusing this multi-faceted evidence, our framework\nachieves a more comprehensive and reliable reasoning process. Extensive\nexperiments show that Synergos-VQA decisively establishes a new\nstate-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA.\nFurthermore, our approach demonstrates strong plug-and-play capabilities,\nsignificantly boosting various open-source MLLMs and proving that superior\nmethodological design can outperform sheer model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have pushed the frontiers of\nKnowledge-Based Visual Question Answering (KBVQA), yet their reasoning is\nfundamentally bottlenecked by a reliance on uni-dimensional evidence. This\n\"seeing only the trees, but not the forest\" approach prevents robust,\nmulti-faceted understanding. Inspired by the principle of seeing both the\nforest and trees, we propose Synergos-VQA, a novel synergistic reasoning\nframework. At its core, Synergos-VQA concurrently generates and fuses three\ncomplementary evidence streams at inference time: (1) Holistic Evidence to\nperceive the entire scene (the \"forest\"), (2) Structural Evidence from a\nprototype-driven module to identify key objects (the \"trees\"), and (3) Causal\nEvidence from a counterfactual probe to ensure the reasoning is robustly\ngrounded. By synergistically fusing this multi-faceted evidence, our framework\nachieves a more comprehensive and reliable reasoning process. Extensive\nexperiments show that Synergos-VQA decisively establishes a new\nstate-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA.\nFurthermore, our approach demonstrates strong plug-and-play capabilities,\nsignificantly boosting various open-source MLLMs and proving that superior\nmethodological design can outperform sheer model scale."
                },
                "authors": [
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Yunhan Tang"
                    },
                    {
                        "name": "Yijie Wang"
                    },
                    {
                        "name": "Zhihao Yuan"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "arxiv_comment": "We are withdrawing this preprint because it is undergoing a major\n  revision and restructuring. We feel that the current version does not convey\n  our core contributions and methodology with sufficient clarity and accuracy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08942v1",
                "updated": "2025-08-12T13:50:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    50,
                    25,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T13:50:25Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    50,
                    25,
                    1,
                    224,
                    0
                ],
                "title": "Jointly Generating and Attributing Answers using Logits of\n  Document-Identifier Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jointly Generating and Attributing Answers using Logits of\n  Document-Identifier Tokens"
                },
                "summary": "Despite their impressive performances, Large Language Models (LLMs) remain\nprone to hallucination, which critically undermines their trustworthiness.\nWhile most of the previous work focused on tackling answer and attribution\ncorrectness, a recent line of work investigated faithfulness, with a focus on\nleveraging internal model signals to reflect a model's actual decision-making\nprocess while generating the answer. Nevertheless, these methods induce\nadditional latency and have shown limitations in directly aligning token\ngeneration with attribution generation. In this paper, we introduce LoDIT, a\nmethod that jointly generates and faithfully attributes answers in RAG by\nleveraging specific token logits during generation. It consists of two steps:\n(1) marking the documents with specific token identifiers and then leveraging\nthe logits of these tokens to estimate the contribution of each document to the\nanswer during generation, and (2) aggregating these contributions into document\nattributions. Experiments on a trustworthiness-focused attributed\ntext-generation benchmark, Trust-Align, show that LoDIT significantly\noutperforms state-of-the-art models on several metrics. Finally, an in-depth\nanalysis of LoDIT shows both its efficiency in terms of latency and its\nrobustness in different settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive performances, Large Language Models (LLMs) remain\nprone to hallucination, which critically undermines their trustworthiness.\nWhile most of the previous work focused on tackling answer and attribution\ncorrectness, a recent line of work investigated faithfulness, with a focus on\nleveraging internal model signals to reflect a model's actual decision-making\nprocess while generating the answer. Nevertheless, these methods induce\nadditional latency and have shown limitations in directly aligning token\ngeneration with attribution generation. In this paper, we introduce LoDIT, a\nmethod that jointly generates and faithfully attributes answers in RAG by\nleveraging specific token logits during generation. It consists of two steps:\n(1) marking the documents with specific token identifiers and then leveraging\nthe logits of these tokens to estimate the contribution of each document to the\nanswer during generation, and (2) aggregating these contributions into document\nattributions. Experiments on a trustworthiness-focused attributed\ntext-generation benchmark, Trust-Align, show that LoDIT significantly\noutperforms state-of-the-art models on several metrics. Finally, an in-depth\nanalysis of LoDIT shows both its efficiency in terms of latency and its\nrobustness in different settings."
                },
                "authors": [
                    {
                        "name": "Lucas Albarede"
                    },
                    {
                        "name": "Jose Moreno"
                    },
                    {
                        "name": "Lynda Tamine"
                    },
                    {
                        "name": "Luce Lefeuvre"
                    }
                ],
                "author_detail": {
                    "name": "Luce Lefeuvre"
                },
                "author": "Luce Lefeuvre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08940v1",
                "updated": "2025-08-12T13:48:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    48,
                    3,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T13:48:03Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    48,
                    3,
                    1,
                    224,
                    0
                ],
                "title": "Train Long, Think Short: Curriculum Learning for Efficient Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Train Long, Think Short: Curriculum Learning for Efficient Reasoning"
                },
                "summary": "Recent work on enhancing the reasoning abilities of large language models\n(LLMs) has introduced explicit length control as a means of constraining\ncomputational cost while preserving accuracy. However, existing approaches rely\non fixed-length training budgets, which do not take advantage of the natural\nprogression from exploration to compression during learning. In this work, we\npropose a curriculum learning strategy for length-controlled reasoning using\nGroup Relative Policy Optimization (GRPO). Our method starts with generous\ntoken budgets and gradually tightens them over training, encouraging models to\nfirst discover effective solution strategies and then distill them into more\nconcise reasoning traces. We augment GRPO with a reward function that balances\nthree signals: task correctness (via verifier feedback), length efficiency, and\nformatting adherence (via structural tags). Experiments on GSM8K, MATH500,\nSVAMP, College Math, and GSM+ demonstrate that curriculum-based training\nconsistently outperforms fixed-budget baselines at the same final budget,\nachieving higher accuracy and significantly improved token efficiency. We\nfurther ablate the impact of reward weighting and decay schedule design,\nshowing that progressive constraint serves as a powerful inductive bias for\ntraining efficient reasoning models. Our code and checkpoints are released at:\nhttps://github.com/hammoudhasan/curriculum_grpo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work on enhancing the reasoning abilities of large language models\n(LLMs) has introduced explicit length control as a means of constraining\ncomputational cost while preserving accuracy. However, existing approaches rely\non fixed-length training budgets, which do not take advantage of the natural\nprogression from exploration to compression during learning. In this work, we\npropose a curriculum learning strategy for length-controlled reasoning using\nGroup Relative Policy Optimization (GRPO). Our method starts with generous\ntoken budgets and gradually tightens them over training, encouraging models to\nfirst discover effective solution strategies and then distill them into more\nconcise reasoning traces. We augment GRPO with a reward function that balances\nthree signals: task correctness (via verifier feedback), length efficiency, and\nformatting adherence (via structural tags). Experiments on GSM8K, MATH500,\nSVAMP, College Math, and GSM+ demonstrate that curriculum-based training\nconsistently outperforms fixed-budget baselines at the same final budget,\nachieving higher accuracy and significantly improved token efficiency. We\nfurther ablate the impact of reward weighting and decay schedule design,\nshowing that progressive constraint serves as a powerful inductive bias for\ntraining efficient reasoning models. Our code and checkpoints are released at:\nhttps://github.com/hammoudhasan/curriculum_grpo."
                },
                "authors": [
                    {
                        "name": "Hasan Abed Al Kader Hammoud"
                    },
                    {
                        "name": "Kumail Alhamoud"
                    },
                    {
                        "name": "Abed Hammoud"
                    },
                    {
                        "name": "Elie Bou-Zeid"
                    },
                    {
                        "name": "Marzyeh Ghassemi"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07995v2",
                "updated": "2025-08-12T13:46:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    46,
                    8,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T13:57:49Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    57,
                    49,
                    0,
                    223,
                    0
                ],
                "title": "DIVER: A Multi-Stage Approach for Reasoning-intensive Information\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIVER: A Multi-Stage Approach for Reasoning-intensive Information\n  Retrieval"
                },
                "summary": "Retrieval-augmented generation has achieved strong performance on\nknowledge-intensive tasks where query-document relevance can be identified\nthrough direct lexical or semantic matches. However, many real-world queries\ninvolve abstract reasoning, analogical thinking, or multi-step inference, which\nexisting retrievers often struggle to capture. To address this challenge, we\npresent \\textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive\ninformation retrieval. DIVER consists of four components: document processing\nto improve input quality, LLM-driven query expansion via iterative document\ninteraction, a reasoning-enhanced retriever fine-tuned on synthetic\nmulti-domain data with hard negatives, and a pointwise reranker that combines\nLLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,\nDIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original\nqueries, consistently outperforming competitive reasoning-aware models. These\nresults demonstrate the effectiveness of reasoning-aware retrieval strategies\nin complex real-world tasks. Our code and retrieval model will be released\nsoon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation has achieved strong performance on\nknowledge-intensive tasks where query-document relevance can be identified\nthrough direct lexical or semantic matches. However, many real-world queries\ninvolve abstract reasoning, analogical thinking, or multi-step inference, which\nexisting retrievers often struggle to capture. To address this challenge, we\npresent \\textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive\ninformation retrieval. DIVER consists of four components: document processing\nto improve input quality, LLM-driven query expansion via iterative document\ninteraction, a reasoning-enhanced retriever fine-tuned on synthetic\nmulti-domain data with hard negatives, and a pointwise reranker that combines\nLLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,\nDIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original\nqueries, consistently outperforming competitive reasoning-aware models. These\nresults demonstrate the effectiveness of reasoning-aware retrieval strategies\nin complex real-world tasks. Our code and retrieval model will be released\nsoon."
                },
                "authors": [
                    {
                        "name": "Meixiu Long"
                    },
                    {
                        "name": "Duolin Sun"
                    },
                    {
                        "name": "Dan Yang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Jiahai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiahai Wang"
                },
                "author": "Jiahai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08930v1",
                "updated": "2025-08-12T13:32:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    32,
                    18,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T13:32:18Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    32,
                    18,
                    1,
                    224,
                    0
                ],
                "title": "How Does a Virtual Agent Decide Where to Look? -- Symbolic Cognitive\n  Reasoning for Embodied Head Rotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Does a Virtual Agent Decide Where to Look? -- Symbolic Cognitive\n  Reasoning for Embodied Head Rotation"
                },
                "summary": "Natural head rotation is critical for believable embodied virtual agents, yet\nthis micro-level behavior remains largely underexplored. While head-rotation\nprediction algorithms could, in principle, reproduce this behavior, they\ntypically focus on visually salient stimuli and overlook the cognitive motives\nthat guide head rotation. This yields agents that look at conspicuous objects\nwhile overlooking obstacles or task-relevant cues, diminishing realism in a\nvirtual environment. We introduce SCORE, a Symbolic Cognitive Reasoning\nframework for Embodied Head Rotation, a data-agnostic framework that produces\ncontext-aware head movements without task-specific training or hand-tuned\nheuristics. A controlled VR study (N=20) identifies five motivational drivers\nof human head movements: Interest, Information Seeking, Safety, Social Schema,\nand Habit. SCORE encodes these drivers as symbolic predicates, perceives the\nscene with a Vision-Language Model (VLM), and plans head poses with a Large\nLanguage Model (LLM). The framework employs a hybrid workflow: the VLM-LLM\nreasoning is executed offline, after which a lightweight FastVLM performs\nonline validation to suppress hallucinations while maintaining responsiveness\nto scene dynamics. The result is an agent that predicts not only where to look\nbut also why, generalizing to unseen scenes and multi-agent crowds while\nretaining behavioral plausibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural head rotation is critical for believable embodied virtual agents, yet\nthis micro-level behavior remains largely underexplored. While head-rotation\nprediction algorithms could, in principle, reproduce this behavior, they\ntypically focus on visually salient stimuli and overlook the cognitive motives\nthat guide head rotation. This yields agents that look at conspicuous objects\nwhile overlooking obstacles or task-relevant cues, diminishing realism in a\nvirtual environment. We introduce SCORE, a Symbolic Cognitive Reasoning\nframework for Embodied Head Rotation, a data-agnostic framework that produces\ncontext-aware head movements without task-specific training or hand-tuned\nheuristics. A controlled VR study (N=20) identifies five motivational drivers\nof human head movements: Interest, Information Seeking, Safety, Social Schema,\nand Habit. SCORE encodes these drivers as symbolic predicates, perceives the\nscene with a Vision-Language Model (VLM), and plans head poses with a Large\nLanguage Model (LLM). The framework employs a hybrid workflow: the VLM-LLM\nreasoning is executed offline, after which a lightweight FastVLM performs\nonline validation to suppress hallucinations while maintaining responsiveness\nto scene dynamics. The result is an agent that predicts not only where to look\nbut also why, generalizing to unseen scenes and multi-agent crowds while\nretaining behavioral plausibility."
                },
                "authors": [
                    {
                        "name": "Juyeong Hwang"
                    },
                    {
                        "name": "Seong-Eun Hon"
                    },
                    {
                        "name": "JaeYoung Seon"
                    },
                    {
                        "name": "Hyeongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongyeop Kang"
                },
                "author": "Hyeongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10322v2",
                "updated": "2025-08-12T13:03:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    3,
                    24,
                    1,
                    224,
                    0
                ],
                "published": "2024-02-15T20:48:33Z",
                "published_parsed": [
                    2024,
                    2,
                    15,
                    20,
                    48,
                    33,
                    3,
                    46,
                    0
                ],
                "title": "Maximum Likelihood Degrees of Brownian Motion Tree Models: Star Trees\n  and Root Invariance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum Likelihood Degrees of Brownian Motion Tree Models: Star Trees\n  and Root Invariance"
                },
                "summary": "A Brownian motion tree (BMT) model is a Gaussian model whose associated set\nof covariance matrices is linearly constrained according to common ancestry in\na phylogenetic tree. We study the complexity of inferring the maximum\nlikelihood (ML) estimator for a BMT model by computing its ML-degree. Our main\nresult is that the ML-degree of the BMT model on a star tree with $n + 1$\nleaves is $2^{n+1}-2n-3$, which was previously conjectured by Am\\'endola and\nZwiernik. We also prove that the ML-degree of a BMT model is independent of the\nchoice of the root. The proofs rely on the toric geometry of concentration\nmatrices in a BMT model. Toward this end, we produce a combinatorial formula\nfor the determinant of the concentration matrix of a BMT model, which\ngeneralizes the Cayley-Pr\\\"ufer theorem to complete graphs with weights given\nby a tree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Brownian motion tree (BMT) model is a Gaussian model whose associated set\nof covariance matrices is linearly constrained according to common ancestry in\na phylogenetic tree. We study the complexity of inferring the maximum\nlikelihood (ML) estimator for a BMT model by computing its ML-degree. Our main\nresult is that the ML-degree of the BMT model on a star tree with $n + 1$\nleaves is $2^{n+1}-2n-3$, which was previously conjectured by Am\\'endola and\nZwiernik. We also prove that the ML-degree of a BMT model is independent of the\nchoice of the root. The proofs rely on the toric geometry of concentration\nmatrices in a BMT model. Toward this end, we produce a combinatorial formula\nfor the determinant of the concentration matrix of a BMT model, which\ngeneralizes the Cayley-Pr\\\"ufer theorem to complete graphs with weights given\nby a tree."
                },
                "authors": [
                    {
                        "name": "Jane Ivy Coons"
                    },
                    {
                        "name": "Shelby Cox"
                    },
                    {
                        "name": "Aida Maraj"
                    },
                    {
                        "name": "Ikenna Nometa"
                    }
                ],
                "author_detail": {
                    "name": "Ikenna Nometa"
                },
                "author": "Ikenna Nometa",
                "arxiv_doi": "10.1016/j.jsc.2025.102482",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jsc.2025.102482",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.10322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, 3 figures. Comments welcome!",
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62R01, 14M25, 62F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08909v1",
                "updated": "2025-08-12T12:58:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    58,
                    12,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T12:58:12Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    58,
                    12,
                    1,
                    224,
                    0
                ],
                "title": "Compass-Thinker-7B Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compass-Thinker-7B Technical Report"
                },
                "summary": "Recent R1-Zero-like research further demonstrates that reasoning extension\nhas given large language models (LLMs) unprecedented reasoning capabilities,\nand Reinforcement Learning is the core technology to elicit its complex\nreasoning. However, conducting RL experiments directly on hyperscale models\ninvolves high computational costs and resource demands, posing significant\nrisks. We propose the Compass-Thinker-7B model, which aims to explore the\npotential of Reinforcement Learning with less computational resources and\ncosts, and provides insights for further research into RL recipes for larger\nmodels. Compass-Thinker-7B is trained from an open source model through a\nspecially designed Reinforcement Learning Pipeline. we curate a dataset of 30k\nverifiable mathematics problems for the Reinforcement Learning Pipeline. By\nconfiguring data and training settings with different difficulty distributions\nfor different stages, the potential of the model is gradually released and the\ntraining efficiency is improved. Extensive evaluations show that\nCompass-Thinker-7B possesses exceptional reasoning potential, and achieves\nsuperior performance on mathematics compared to the same-sized RL\nmodel.Especially in the challenging AIME2024 evaluation, Compass-Thinker-7B\nachieves 40% accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent R1-Zero-like research further demonstrates that reasoning extension\nhas given large language models (LLMs) unprecedented reasoning capabilities,\nand Reinforcement Learning is the core technology to elicit its complex\nreasoning. However, conducting RL experiments directly on hyperscale models\ninvolves high computational costs and resource demands, posing significant\nrisks. We propose the Compass-Thinker-7B model, which aims to explore the\npotential of Reinforcement Learning with less computational resources and\ncosts, and provides insights for further research into RL recipes for larger\nmodels. Compass-Thinker-7B is trained from an open source model through a\nspecially designed Reinforcement Learning Pipeline. we curate a dataset of 30k\nverifiable mathematics problems for the Reinforcement Learning Pipeline. By\nconfiguring data and training settings with different difficulty distributions\nfor different stages, the potential of the model is gradually released and the\ntraining efficiency is improved. Extensive evaluations show that\nCompass-Thinker-7B possesses exceptional reasoning potential, and achieves\nsuperior performance on mathematics compared to the same-sized RL\nmodel.Especially in the challenging AIME2024 evaluation, Compass-Thinker-7B\nachieves 40% accuracy."
                },
                "authors": [
                    {
                        "name": "Anxiang Zeng"
                    },
                    {
                        "name": "Haibo Zhang"
                    },
                    {
                        "name": "Kaixiang Mo"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Shuman Liu"
                    },
                    {
                        "name": "Yanhui Huang"
                    },
                    {
                        "name": "Yawen Liu"
                    },
                    {
                        "name": "Yuepeng Sheng"
                    },
                    {
                        "name": "Yuwei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yuwei Huang"
                },
                "author": "Yuwei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14412v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14412v2",
                "updated": "2025-08-12T12:54:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    54,
                    59,
                    1,
                    224,
                    0
                ],
                "published": "2025-06-17T11:14:22Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    11,
                    14,
                    22,
                    1,
                    168,
                    0
                ],
                "title": "RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG\n  Systems for the SIGIR LiveRAG Competition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG\n  Systems for the SIGIR LiveRAG Competition"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by\ncombining their internal, parametric knowledge with external, non-parametric\nsources, with the goal of improving factual correctness and minimizing\nhallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize\naccuracy on DataMorgana's QA pairs, which are composed of single-hop and\nmulti-hop questions. The challenge provides access to sparse OpenSearch and\ndense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to\nLLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A\njudge-LLM assesses the submitted answers along with human evaluators. By\nexploring distinct retriever combinations and RAG solutions under the challenge\nconditions, our final solution emerged using InstructRAG in combination with a\nPinecone retriever and a BGE reranker. Our solution achieved a correctness\nscore of 1.13 and a faithfulness score of 0.55 in the non-human evaluation,\nplacing it overall in third place in the SIGIR 2025 LiveRAG Challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by\ncombining their internal, parametric knowledge with external, non-parametric\nsources, with the goal of improving factual correctness and minimizing\nhallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize\naccuracy on DataMorgana's QA pairs, which are composed of single-hop and\nmulti-hop questions. The challenge provides access to sparse OpenSearch and\ndense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to\nLLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A\njudge-LLM assesses the submitted answers along with human evaluators. By\nexploring distinct retriever combinations and RAG solutions under the challenge\nconditions, our final solution emerged using InstructRAG in combination with a\nPinecone retriever and a BGE reranker. Our solution achieved a correctness\nscore of 1.13 and a faithfulness score of 0.55 in the non-human evaluation,\nplacing it overall in third place in the SIGIR 2025 LiveRAG Challenge."
                },
                "authors": [
                    {
                        "name": "Tim Cofala"
                    },
                    {
                        "name": "Oleh Astappiev"
                    },
                    {
                        "name": "William Xion"
                    },
                    {
                        "name": "Hailay Teklehaymanot"
                    }
                ],
                "author_detail": {
                    "name": "Hailay Teklehaymanot"
                },
                "author": "Hailay Teklehaymanot",
                "arxiv_comment": "4 pages, 6 figures. Report for SIGIR 2025 LiveRAG Challenge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14412v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14412v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08903v1",
                "updated": "2025-08-12T12:51:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    51,
                    22,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T12:51:22Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    51,
                    22,
                    1,
                    224,
                    0
                ],
                "title": "Sensitivity of $W$-boson measurements to low-mass right-handed neutrinos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensitivity of $W$-boson measurements to low-mass right-handed neutrinos"
                },
                "summary": "A low-mass right-handed neutrino could interact with electroweak bosons via\nmixing, a mediator particle, or loop corrections. Using an effective field\ntheory, we determine constraints on these interactions from $W$-boson\nmeasurements at hadron colliders. Due to the difference in the initial states\nat the Tevatron and the LHC, $W$-boson decays to a right-handed neutrino would\nartificially increase the mass measured at the Tevatron while only affecting\nthe difference between $W^+$ and $W^-$ mass measurements at the LHC.\nMeasurements from CDF and the LHC are used to infer the corresponding parameter\nvalues, which are found to be inconsistent between the two. The LHC experiments\ncan improve sensitivity to these interactions by measuring the cosine of the\nhelicity angle using $W$ bosons produced with transverse momentum above\n$\\approx 50$ GeV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A low-mass right-handed neutrino could interact with electroweak bosons via\nmixing, a mediator particle, or loop corrections. Using an effective field\ntheory, we determine constraints on these interactions from $W$-boson\nmeasurements at hadron colliders. Due to the difference in the initial states\nat the Tevatron and the LHC, $W$-boson decays to a right-handed neutrino would\nartificially increase the mass measured at the Tevatron while only affecting\nthe difference between $W^+$ and $W^-$ mass measurements at the LHC.\nMeasurements from CDF and the LHC are used to infer the corresponding parameter\nvalues, which are found to be inconsistent between the two. The LHC experiments\ncan improve sensitivity to these interactions by measuring the cosine of the\nhelicity angle using $W$ bosons produced with transverse momentum above\n$\\approx 50$ GeV."
                },
                "authors": [
                    {
                        "name": "Rodrigo Alonso"
                    },
                    {
                        "name": "Sam Bates"
                    },
                    {
                        "name": "Chris Hays"
                    },
                    {
                        "name": "Chris Pollard"
                    },
                    {
                        "name": "Michael Spannowsky"
                    }
                ],
                "author_detail": {
                    "name": "Michael Spannowsky"
                },
                "author": "Michael Spannowsky",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09577v2",
                "updated": "2025-08-12T12:49:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    49,
                    59,
                    1,
                    224,
                    0
                ],
                "published": "2025-02-13T18:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    34,
                    52,
                    3,
                    44,
                    0
                ],
                "title": "Polymind: Parallel Visual Diagramming with Large Language Models to\n  Support Prewriting Through Microtasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polymind: Parallel Visual Diagramming with Large Language Models to\n  Support Prewriting Through Microtasks"
                },
                "summary": "Prewriting is the process of generating and organising ideas before a first\ndraft. It consists of a combination of informal, iterative, and semi-structured\nstrategies such as visual diagramming, which poses a challenge for\ncollaborating with large language models (LLMs) in a turn-taking conversational\nmanner. We present Polymind, a visual diagramming tool that leverages multiple\nLLM-powered agents to support prewriting. The system features a parallel\ncollaboration workflow in place of the turn-taking conversational interactions.\nIt defines multiple ``microtasks'' to simulate group collaboration scenarios\nsuch as collaborative writing and group brainstorming. Instead of repetitively\nprompting a chatbot for various purposes, Polymind enables users to orchestrate\nmultiple microtasks simultaneously. Users can configure and delegate customised\nmicrotasks, and manage their microtasks by specifying task requirements and\ntoggling visibility and initiative. Our evaluation revealed that, compared to\nChatGPT, users had more customizability over collaboration with Polymind, and\nwere thus able to quickly expand personalised writing ideas during prewriting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prewriting is the process of generating and organising ideas before a first\ndraft. It consists of a combination of informal, iterative, and semi-structured\nstrategies such as visual diagramming, which poses a challenge for\ncollaborating with large language models (LLMs) in a turn-taking conversational\nmanner. We present Polymind, a visual diagramming tool that leverages multiple\nLLM-powered agents to support prewriting. The system features a parallel\ncollaboration workflow in place of the turn-taking conversational interactions.\nIt defines multiple ``microtasks'' to simulate group collaboration scenarios\nsuch as collaborative writing and group brainstorming. Instead of repetitively\nprompting a chatbot for various purposes, Polymind enables users to orchestrate\nmultiple microtasks simultaneously. Users can configure and delegate customised\nmicrotasks, and manage their microtasks by specifying task requirements and\ntoggling visibility and initiative. Our evaluation revealed that, compared to\nChatGPT, users had more customizability over collaboration with Polymind, and\nwere thus able to quickly expand personalised writing ideas during prewriting."
                },
                "authors": [
                    {
                        "name": "Qian Wan"
                    },
                    {
                        "name": "Jiannan Li"
                    },
                    {
                        "name": "Huanchen Wang"
                    },
                    {
                        "name": "Zhicong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhicong Lu"
                },
                "author": "Zhicong Lu",
                "arxiv_comment": "Accepted to CSCW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08895v1",
                "updated": "2025-08-12T12:35:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T12:35:55Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs"
                },
                "summary": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Zhifeng Shen"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Haoqian Wu"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01057v2",
                "updated": "2025-08-12T12:29:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    29,
                    2,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-01T20:16:04Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    16,
                    4,
                    4,
                    213,
                    0
                ],
                "title": "Edge-Based Multimodal Sensor Data Fusion with Vision Language Models\n  (VLMs) for Real-time Autonomous Vehicle Accident Avoidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-Based Multimodal Sensor Data Fusion with Vision Language Models\n  (VLMs) for Real-time Autonomous Vehicle Accident Avoidance"
                },
                "summary": "Autonomous driving (AD) systems relying solely on onboard sensors may fail to\ndetect distant or obstacle hazards, potentially causing preventable collisions;\nhowever, existing transformer-based Vehicle-to-Everything (V2X) approaches,\nwhich mitigate AD sensing limitations, either lack effective multimodal fusion\nand reasoning or struggle to meet real-time performance requirements under\ncomplex, high-dimensional traffic conditions. This paper proposes the Real-time\nEdge-based Autonomous Co-pilot Trajectory planner (REACT), a V2X-integrated\ntrajectory optimization framework for AD based on a fine-tuned lightweight\nVision-Language Model (VLM). REACT integrates infrastructure-provided hazard\nalerts with onboard sensor data, capturing intricate surrounding traffic\ndynamics and vehicle intents through visual embeddings, interpreting precise\nnumerical data from symbolic inputs, and employing contextual reasoning to\ngenerate optimized, safety-oriented trajectories. To ensure robust real-time\ndeployment on edge devices, REACT innovatively employs Residual Trajectory\nFusion (RTF) design and specialized edge-adaptation strategies to reduce model\ncomplexity and improve inference efficiency. Evaluated on the DeepAccident\nbenchmark, REACT achieves state-of-the-art performance, a 77% collision rate\nreduction, a 48.2% Video Panoptic Quality (VPQ), and a 0.57-second inference\nlatency on the Jetson AGX Orin. Ablation studies validate the contribution of\neach input, module, and edge adaptation strategy. These results highlight the\neffectiveness of lightweight VLMs in enabling real-time cooperative planning on\nedge platforms and underscore the potential of language-guided contextual\nreasoning for improving traffic safety and responsiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving (AD) systems relying solely on onboard sensors may fail to\ndetect distant or obstacle hazards, potentially causing preventable collisions;\nhowever, existing transformer-based Vehicle-to-Everything (V2X) approaches,\nwhich mitigate AD sensing limitations, either lack effective multimodal fusion\nand reasoning or struggle to meet real-time performance requirements under\ncomplex, high-dimensional traffic conditions. This paper proposes the Real-time\nEdge-based Autonomous Co-pilot Trajectory planner (REACT), a V2X-integrated\ntrajectory optimization framework for AD based on a fine-tuned lightweight\nVision-Language Model (VLM). REACT integrates infrastructure-provided hazard\nalerts with onboard sensor data, capturing intricate surrounding traffic\ndynamics and vehicle intents through visual embeddings, interpreting precise\nnumerical data from symbolic inputs, and employing contextual reasoning to\ngenerate optimized, safety-oriented trajectories. To ensure robust real-time\ndeployment on edge devices, REACT innovatively employs Residual Trajectory\nFusion (RTF) design and specialized edge-adaptation strategies to reduce model\ncomplexity and improve inference efficiency. Evaluated on the DeepAccident\nbenchmark, REACT achieves state-of-the-art performance, a 77% collision rate\nreduction, a 48.2% Video Panoptic Quality (VPQ), and a 0.57-second inference\nlatency on the Jetson AGX Orin. Ablation studies validate the contribution of\neach input, module, and edge adaptation strategy. These results highlight the\neffectiveness of lightweight VLMs in enabling real-time cooperative planning on\nedge platforms and underscore the potential of language-guided contextual\nreasoning for improving traffic safety and responsiveness."
                },
                "authors": [
                    {
                        "name": "Fengze Yang"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Xuewen Luo"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Chenxi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chenxi Liu"
                },
                "author": "Chenxi Liu",
                "arxiv_comment": "24 pages, 6 tables, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08890v1",
                "updated": "2025-08-12T12:25:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    25,
                    53,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T12:25:53Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    25,
                    53,
                    1,
                    224,
                    0
                ],
                "title": "Transient Noise Removal via Diffusion-based Speech Inpainting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transient Noise Removal via Diffusion-based Speech Inpainting"
                },
                "summary": "In this paper, we present PGDI, a diffusion-based speech inpainting framework\nfor restoring missing or severely corrupted speech segments. Unlike previous\nmethods that struggle with speaker variability or long gap lengths, PGDI can\naccurately reconstruct gaps of up to one second in length while preserving\nspeaker identity, prosody, and environmental factors such as reverberation.\nCentral to this approach is classifier guidance, specifically phoneme-level\nguidance, which substantially improves reconstruction fidelity. PGDI operates\nin a speaker-independent manner and maintains robustness even when long\nsegments are completely masked by strong transient noise, making it well-suited\nfor real-world applications, such as fireworks, door slams, hammer strikes, and\nconstruction noise. Through extensive experiments across diverse speakers and\ngap lengths, we demonstrate PGDI's superior inpainting performance and its\nability to handle challenging acoustic conditions. We consider both scenarios,\nwith and without access to the transcript during inference, showing that while\nthe availability of text further enhances performance, the model remains\neffective even in its absence. For audio samples, visit:\nhttps://mordehaym.github.io/PGDI/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present PGDI, a diffusion-based speech inpainting framework\nfor restoring missing or severely corrupted speech segments. Unlike previous\nmethods that struggle with speaker variability or long gap lengths, PGDI can\naccurately reconstruct gaps of up to one second in length while preserving\nspeaker identity, prosody, and environmental factors such as reverberation.\nCentral to this approach is classifier guidance, specifically phoneme-level\nguidance, which substantially improves reconstruction fidelity. PGDI operates\nin a speaker-independent manner and maintains robustness even when long\nsegments are completely masked by strong transient noise, making it well-suited\nfor real-world applications, such as fireworks, door slams, hammer strikes, and\nconstruction noise. Through extensive experiments across diverse speakers and\ngap lengths, we demonstrate PGDI's superior inpainting performance and its\nability to handle challenging acoustic conditions. We consider both scenarios,\nwith and without access to the transcript during inference, showing that while\nthe availability of text further enhances performance, the model remains\neffective even in its absence. For audio samples, visit:\nhttps://mordehaym.github.io/PGDI/"
                },
                "authors": [
                    {
                        "name": "Mordehay Moradi"
                    },
                    {
                        "name": "Sharon Gannot"
                    }
                ],
                "author_detail": {
                    "name": "Sharon Gannot"
                },
                "author": "Sharon Gannot",
                "arxiv_comment": "23 pages, 3 figures, signal processing paper on speech inpainting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08883v1",
                "updated": "2025-08-12T12:13:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    13,
                    13,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T12:13:13Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    13,
                    13,
                    1,
                    224,
                    0
                ],
                "title": "Position: Causal Machine Learning Requires Rigorous Synthetic\n  Experiments for Broader Adoption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Causal Machine Learning Requires Rigorous Synthetic\n  Experiments for Broader Adoption"
                },
                "summary": "Causal machine learning has the potential to revolutionize decision-making by\ncombining the predictive power of machine learning algorithms with the theory\nof causal inference. However, these methods remain underutilized by the broader\nmachine learning community, in part because current empirical evaluations do\nnot permit assessment of their reliability and robustness, undermining their\npractical utility. Specifically, one of the principal criticisms made by the\ncommunity is the extensive use of synthetic experiments. We argue, on the\ncontrary, that synthetic experiments are essential and necessary to precisely\nassess and understand the capabilities of causal machine learning methods. To\nsubstantiate our position, we critically review the current evaluation\npractices, spotlight their shortcomings, and propose a set of principles for\nconducting rigorous empirical analyses with synthetic data. Adopting the\nproposed principles will enable comprehensive evaluations that build trust in\ncausal machine learning methods, driving their broader adoption and impactful\nreal-world use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal machine learning has the potential to revolutionize decision-making by\ncombining the predictive power of machine learning algorithms with the theory\nof causal inference. However, these methods remain underutilized by the broader\nmachine learning community, in part because current empirical evaluations do\nnot permit assessment of their reliability and robustness, undermining their\npractical utility. Specifically, one of the principal criticisms made by the\ncommunity is the extensive use of synthetic experiments. We argue, on the\ncontrary, that synthetic experiments are essential and necessary to precisely\nassess and understand the capabilities of causal machine learning methods. To\nsubstantiate our position, we critically review the current evaluation\npractices, spotlight their shortcomings, and propose a set of principles for\nconducting rigorous empirical analyses with synthetic data. Adopting the\nproposed principles will enable comprehensive evaluations that build trust in\ncausal machine learning methods, driving their broader adoption and impactful\nreal-world use."
                },
                "authors": [
                    {
                        "name": "Audrey Poinsot"
                    },
                    {
                        "name": "Panayiotis Panayiotou"
                    },
                    {
                        "name": "Alessandro Leite"
                    },
                    {
                        "name": "Nicolas Chesneau"
                    },
                    {
                        "name": "Özgür Şimşek"
                    },
                    {
                        "name": "Marc Schoenauer"
                    }
                ],
                "author_detail": {
                    "name": "Marc Schoenauer"
                },
                "author": "Marc Schoenauer",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08879v1",
                "updated": "2025-08-12T12:05:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    5,
                    32,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T12:05:32Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    5,
                    32,
                    1,
                    224,
                    0
                ],
                "title": "Entangled in Representations: Mechanistic Investigation of Cultural\n  Biases in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entangled in Representations: Mechanistic Investigation of Cultural\n  Biases in Large Language Models"
                },
                "summary": "The growing deployment of large language models (LLMs) across diverse\ncultural contexts necessitates a better understanding of how the\novergeneralization of less documented cultures within LLMs' representations\nimpacts their cultural understanding. Prior work only performs extrinsic\nevaluation of LLMs' cultural competence, without accounting for how LLMs'\ninternal mechanisms lead to cultural (mis)representation. To bridge this gap,\nwe propose Culturescope, the first mechanistic interpretability-based method\nthat probes the internal representations of LLMs to elicit the underlying\ncultural knowledge space. CultureScope utilizes a patching method to extract\nthe cultural knowledge. We introduce a cultural flattening score as a measure\nof the intrinsic cultural biases. Additionally, we study how LLMs internalize\nWestern-dominance bias and cultural flattening, which allows us to trace how\ncultural biases emerge within LLMs. Our experimental results reveal that LLMs\nencode Western-dominance bias and cultural flattening in their cultural\nknowledge space. We find that low-resource cultures are less susceptible to\ncultural biases, likely due to their limited training resources. Our work\nprovides a foundation for future research on mitigating cultural biases and\nenhancing LLMs' cultural understanding. Our codes and data used for experiments\nare publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing deployment of large language models (LLMs) across diverse\ncultural contexts necessitates a better understanding of how the\novergeneralization of less documented cultures within LLMs' representations\nimpacts their cultural understanding. Prior work only performs extrinsic\nevaluation of LLMs' cultural competence, without accounting for how LLMs'\ninternal mechanisms lead to cultural (mis)representation. To bridge this gap,\nwe propose Culturescope, the first mechanistic interpretability-based method\nthat probes the internal representations of LLMs to elicit the underlying\ncultural knowledge space. CultureScope utilizes a patching method to extract\nthe cultural knowledge. We introduce a cultural flattening score as a measure\nof the intrinsic cultural biases. Additionally, we study how LLMs internalize\nWestern-dominance bias and cultural flattening, which allows us to trace how\ncultural biases emerge within LLMs. Our experimental results reveal that LLMs\nencode Western-dominance bias and cultural flattening in their cultural\nknowledge space. We find that low-resource cultures are less susceptible to\ncultural biases, likely due to their limited training resources. Our work\nprovides a foundation for future research on mitigating cultural biases and\nenhancing LLMs' cultural understanding. Our codes and data used for experiments\nare publicly available."
                },
                "authors": [
                    {
                        "name": "Haeun Yu"
                    },
                    {
                        "name": "Seogyeong Jeong"
                    },
                    {
                        "name": "Siddhesh Pawar"
                    },
                    {
                        "name": "Jisu Shin"
                    },
                    {
                        "name": "Jiho Jin"
                    },
                    {
                        "name": "Junho Myung"
                    },
                    {
                        "name": "Alice Oh"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "16 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08875v1",
                "updated": "2025-08-12T12:02:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    2,
                    53,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T12:02:53Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    2,
                    53,
                    1,
                    224,
                    0
                ],
                "title": "Oblivionis: A Lightweight Learning and Unlearning Framework for\n  Federated Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivionis: A Lightweight Learning and Unlearning Framework for\n  Federated Large Language Models"
                },
                "summary": "Large Language Models (LLMs) increasingly leverage Federated Learning (FL) to\nutilize private, task-specific datasets for fine-tuning while preserving data\nprivacy. However, while federated LLM frameworks effectively enable\ncollaborative training without raw data sharing, they critically lack built-in\nmechanisms for regulatory compliance like GDPR's right to be forgotten.\nIntegrating private data heightens concerns over data quality and long-term\ngovernance, yet existing distributed training frameworks offer no principled\nway to selectively remove specific client contributions post-training. Due to\ndistributed data silos, stringent privacy constraints, and the intricacies of\ninterdependent model aggregation, federated LLM unlearning is significantly\nmore complex than centralized LLM unlearning. To address this gap, we introduce\nOblivionis, a lightweight learning and unlearning framework that enables\nclients to selectively remove specific private data during federated LLM\ntraining, enhancing trustworthiness and regulatory compliance. By unifying FL\nand unlearning as a dual optimization objective, we incorporate 6 FL and 5\nunlearning algorithms for comprehensive evaluation and comparative analysis,\nestablishing a robust pipeline for federated LLM unlearning. Extensive\nexperiments demonstrate that Oblivionis outperforms local training, achieving a\nrobust balance between forgetting efficacy and model utility, with\ncross-algorithm comparisons providing clear directions for future LLM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly leverage Federated Learning (FL) to\nutilize private, task-specific datasets for fine-tuning while preserving data\nprivacy. However, while federated LLM frameworks effectively enable\ncollaborative training without raw data sharing, they critically lack built-in\nmechanisms for regulatory compliance like GDPR's right to be forgotten.\nIntegrating private data heightens concerns over data quality and long-term\ngovernance, yet existing distributed training frameworks offer no principled\nway to selectively remove specific client contributions post-training. Due to\ndistributed data silos, stringent privacy constraints, and the intricacies of\ninterdependent model aggregation, federated LLM unlearning is significantly\nmore complex than centralized LLM unlearning. To address this gap, we introduce\nOblivionis, a lightweight learning and unlearning framework that enables\nclients to selectively remove specific private data during federated LLM\ntraining, enhancing trustworthiness and regulatory compliance. By unifying FL\nand unlearning as a dual optimization objective, we incorporate 6 FL and 5\nunlearning algorithms for comprehensive evaluation and comparative analysis,\nestablishing a robust pipeline for federated LLM unlearning. Extensive\nexperiments demonstrate that Oblivionis outperforms local training, achieving a\nrobust balance between forgetting efficacy and model utility, with\ncross-algorithm comparisons providing clear directions for future LLM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Fuyao Zhang"
                    },
                    {
                        "name": "Xinyu Yan"
                    },
                    {
                        "name": "Tiantong Wu"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Tianxiang Chen"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Longtao Huang"
                    },
                    {
                        "name": "Wei Yang Bryan Lim"
                    },
                    {
                        "name": "Qiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yang"
                },
                "author": "Qiang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08857v1",
                "updated": "2025-08-12T11:31:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    31,
                    27,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T11:31:27Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    31,
                    27,
                    1,
                    224,
                    0
                ],
                "title": "A Broadband Squeezed Light Source for Table-Top Interferometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Broadband Squeezed Light Source for Table-Top Interferometry"
                },
                "summary": "We report on the characterisation of one of two broadband squeezed light\nsources developed for the Quantum Enhanced Space-Time (QUEST) experiment, using\nbalanced homodyne detection. QUEST consists of a pair of co-located, table-top,\npower-recycled Michelson interferometers designed to probe stationary\nspace-time fluctuations. The interferometers are designed to be shot-noise\nlimited in the frequency range from 1 to 200 MHz, and squeezed light will be\nemployed with the goal to reduce the shot noise by 6 dB at frequencies inside\nthe linewidth of the optical parametric amplifier (OPA). We directly observed\nup to 6.8 dB of squeezing and maintained at least 3 dB of squeezing across the\nfull 100 MHz measurement bandwidth. After accounting for the dark noise\ncontribution, the inferred squeezing level increased to 8.6 dB. Our squeezed\nlight source is based on a hemilithic OPA with a 43.6 mm round-trip optical\nlength and a linewidth of 138 MHz, making it the broadest-linewidth device to\ndate among those suitable for long-term operation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on the characterisation of one of two broadband squeezed light\nsources developed for the Quantum Enhanced Space-Time (QUEST) experiment, using\nbalanced homodyne detection. QUEST consists of a pair of co-located, table-top,\npower-recycled Michelson interferometers designed to probe stationary\nspace-time fluctuations. The interferometers are designed to be shot-noise\nlimited in the frequency range from 1 to 200 MHz, and squeezed light will be\nemployed with the goal to reduce the shot noise by 6 dB at frequencies inside\nthe linewidth of the optical parametric amplifier (OPA). We directly observed\nup to 6.8 dB of squeezing and maintained at least 3 dB of squeezing across the\nfull 100 MHz measurement bandwidth. After accounting for the dark noise\ncontribution, the inferred squeezing level increased to 8.6 dB. Our squeezed\nlight source is based on a hemilithic OPA with a 43.6 mm round-trip optical\nlength and a linewidth of 138 MHz, making it the broadest-linewidth device to\ndate among those suitable for long-term operation."
                },
                "authors": [
                    {
                        "name": "Fabio Bergamin"
                    },
                    {
                        "name": "Nikitha Kuntimaddi"
                    },
                    {
                        "name": "Abhinav Patra"
                    },
                    {
                        "name": "Stephanie Montoya"
                    },
                    {
                        "name": "Moritz Mehmet"
                    },
                    {
                        "name": "Katherine Dooley"
                    },
                    {
                        "name": "Hartmut Grote"
                    },
                    {
                        "name": "Henning Vahlbruch"
                    }
                ],
                "author_detail": {
                    "name": "Henning Vahlbruch"
                },
                "author": "Henning Vahlbruch",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08855v1",
                "updated": "2025-08-12T11:23:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    23,
                    44,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T11:23:44Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    23,
                    44,
                    1,
                    224,
                    0
                ],
                "title": "BiasGym: Fantastic Biases and How to Find (and Remove) Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiasGym: Fantastic Biases and How to Find (and Remove) Them"
                },
                "summary": "Understanding biases and stereotypes encoded in the weights of Large Language\nModels (LLMs) is crucial for developing effective mitigation strategies. Biased\nbehaviour is often subtle and non-trivial to isolate, even when deliberately\nelicited, making systematic analysis and debiasing particularly challenging. To\naddress this, we introduce BiasGym, a simple, cost-effective, and generalizable\nframework for reliably injecting, analyzing, and mitigating conceptual\nassociations within LLMs. BiasGym consists of two components: BiasInject, which\ninjects specific biases into the model via token-based fine-tuning while\nkeeping the model frozen, and BiasScope, which leverages these injected signals\nto identify and steer the components responsible for biased behavior. Our\nmethod enables consistent bias elicitation for mechanistic analysis, supports\ntargeted debiasing without degrading performance on downstream tasks, and\ngeneralizes to biases unseen during training. We demonstrate the effectiveness\nof BiasGym in reducing real-world stereotypes (e.g., people from a country\nbeing `reckless drivers') and in probing fictional associations (e.g., people\nfrom a country having `blue skin'), showing its utility for both safety\ninterventions and interpretability research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding biases and stereotypes encoded in the weights of Large Language\nModels (LLMs) is crucial for developing effective mitigation strategies. Biased\nbehaviour is often subtle and non-trivial to isolate, even when deliberately\nelicited, making systematic analysis and debiasing particularly challenging. To\naddress this, we introduce BiasGym, a simple, cost-effective, and generalizable\nframework for reliably injecting, analyzing, and mitigating conceptual\nassociations within LLMs. BiasGym consists of two components: BiasInject, which\ninjects specific biases into the model via token-based fine-tuning while\nkeeping the model frozen, and BiasScope, which leverages these injected signals\nto identify and steer the components responsible for biased behavior. Our\nmethod enables consistent bias elicitation for mechanistic analysis, supports\ntargeted debiasing without degrading performance on downstream tasks, and\ngeneralizes to biases unseen during training. We demonstrate the effectiveness\nof BiasGym in reducing real-world stereotypes (e.g., people from a country\nbeing `reckless drivers') and in probing fictional associations (e.g., people\nfrom a country having `blue skin'), showing its utility for both safety\ninterventions and interpretability research."
                },
                "authors": [
                    {
                        "name": "Sekh Mainul Islam"
                    },
                    {
                        "name": "Nadav Borenstein"
                    },
                    {
                        "name": "Siddhesh Milind Pawar"
                    },
                    {
                        "name": "Haeun Yu"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20252v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20252v3",
                "updated": "2025-08-12T11:22:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    22,
                    33,
                    1,
                    224,
                    0
                ],
                "published": "2025-07-27T12:47:26Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    12,
                    47,
                    26,
                    6,
                    208,
                    0
                ],
                "title": "Post-Completion Learning for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Completion Learning for Language Models"
                },
                "summary": "Current language model training paradigms typically terminate learning upon\nreaching the end-of-sequence (<eos>) token, overlooking the potential learning\nopportunities in the post-completion space. We propose Post-Completion Learning\n(PCL), a novel training framework that systematically utilizes the sequence\nspace after model output completion, to enhance both the reasoning and\nself-evaluation abilities. PCL enables models to continue generating\nself-assessments and reward predictions during training, while maintaining\nefficient inference by stopping at the completion point.\n  To fully utilize this post-completion space, we design a white-box\nreinforcement learning method: let the model evaluate the output content\naccording to the reward rules, then calculate and align the score with the\nreward functions for supervision. We implement dual-track SFT to optimize both\nreasoning and evaluation capabilities, and mixed it with RL training to achieve\nmulti-objective hybrid optimization.\n  Experimental results on different datasets and models demonstrate consistent\nimprovements over traditional SFT and RL methods. Our method provides a new\ntechnical path for language model training that enhances output quality while\npreserving deployment efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current language model training paradigms typically terminate learning upon\nreaching the end-of-sequence (<eos>) token, overlooking the potential learning\nopportunities in the post-completion space. We propose Post-Completion Learning\n(PCL), a novel training framework that systematically utilizes the sequence\nspace after model output completion, to enhance both the reasoning and\nself-evaluation abilities. PCL enables models to continue generating\nself-assessments and reward predictions during training, while maintaining\nefficient inference by stopping at the completion point.\n  To fully utilize this post-completion space, we design a white-box\nreinforcement learning method: let the model evaluate the output content\naccording to the reward rules, then calculate and align the score with the\nreward functions for supervision. We implement dual-track SFT to optimize both\nreasoning and evaluation capabilities, and mixed it with RL training to achieve\nmulti-objective hybrid optimization.\n  Experimental results on different datasets and models demonstrate consistent\nimprovements over traditional SFT and RL methods. Our method provides a new\ntechnical path for language model training that enhances output quality while\npreserving deployment efficiency."
                },
                "authors": [
                    {
                        "name": "Xiang Fei"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Shu Wei"
                    },
                    {
                        "name": "Yuxiang Nie"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Can Huang"
                    }
                ],
                "author_detail": {
                    "name": "Can Huang"
                },
                "author": "Can Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20252v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20252v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06053v2",
                "updated": "2025-08-12T11:18:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    18,
                    57,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-08T06:21:23Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    21,
                    23,
                    4,
                    220,
                    0
                ],
                "title": "ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian\n  Inference"
                },
                "summary": "Pedestrian inertial localization is key for mobile and IoT services because\nit provides infrastructure-free positioning. Yet most learning-based methods\ndepend on fixed sliding-window integration, struggle to adapt to diverse motion\nscales and cadences, and yield inconsistent uncertainty, limiting real-world\nuse. We present ReNiL, a Bayesian deep-learning framework for accurate,\nefficient, and uncertainty-aware pedestrian localization. ReNiL introduces\nInertial Positioning Demand Points (IPDPs) to estimate motion at contextually\nmeaningful waypoints instead of dense tracking, and supports inference on IMU\nsequences at any scale so cadence can match application needs. It couples a\nmotion-aware orientation filter with an Any-Scale Laplace Estimator (ASLE), a\ndual-task network that blends patch-based self-supervision with Bayesian\nregression. By modeling displacements with a Laplace distribution, ReNiL\nprovides homogeneous Euclidean uncertainty that integrates cleanly with other\nsensors. A Bayesian inference chain links successive IPDPs into consistent\ntrajectories. On RoNIN-ds and a new WUDataset covering indoor and outdoor\nmotion from 28 participants, ReNiL achieves state-of-the-art displacement\naccuracy and uncertainty consistency, outperforming TLIO, CTIN, iMoT, and RoNIN\nvariants while reducing computation. Application studies further show\nrobustness and practicality for mobile and IoT localization, making ReNiL a\nscalable, uncertainty-aware foundation for next-generation positioning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pedestrian inertial localization is key for mobile and IoT services because\nit provides infrastructure-free positioning. Yet most learning-based methods\ndepend on fixed sliding-window integration, struggle to adapt to diverse motion\nscales and cadences, and yield inconsistent uncertainty, limiting real-world\nuse. We present ReNiL, a Bayesian deep-learning framework for accurate,\nefficient, and uncertainty-aware pedestrian localization. ReNiL introduces\nInertial Positioning Demand Points (IPDPs) to estimate motion at contextually\nmeaningful waypoints instead of dense tracking, and supports inference on IMU\nsequences at any scale so cadence can match application needs. It couples a\nmotion-aware orientation filter with an Any-Scale Laplace Estimator (ASLE), a\ndual-task network that blends patch-based self-supervision with Bayesian\nregression. By modeling displacements with a Laplace distribution, ReNiL\nprovides homogeneous Euclidean uncertainty that integrates cleanly with other\nsensors. A Bayesian inference chain links successive IPDPs into consistent\ntrajectories. On RoNIN-ds and a new WUDataset covering indoor and outdoor\nmotion from 28 participants, ReNiL achieves state-of-the-art displacement\naccuracy and uncertainty consistency, outperforming TLIO, CTIN, iMoT, and RoNIN\nvariants while reducing computation. Application studies further show\nrobustness and practicality for mobile and IoT localization, making ReNiL a\nscalable, uncertainty-aware foundation for next-generation positioning."
                },
                "authors": [
                    {
                        "name": "Kaixuan Wu"
                    },
                    {
                        "name": "Yuanzhuo Xu"
                    },
                    {
                        "name": "Zejun Zhang"
                    },
                    {
                        "name": "Weiping Zhu"
                    },
                    {
                        "name": "Steve Drew"
                    },
                    {
                        "name": "Xiaoguang Niu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoguang Niu"
                },
                "arxiv_affiliation": "School of Cyber Science and Engineering, Wuhan University, Wuhan, China",
                "author": "Xiaoguang Niu",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08846v1",
                "updated": "2025-08-12T11:09:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    9,
                    3,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T11:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    9,
                    3,
                    1,
                    224,
                    0
                ],
                "title": "Steering Towards Fairness: Mitigating Political Bias in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Towards Fairness: Mitigating Political Bias in LLMs"
                },
                "summary": "Recent advancements in large language models (LLMs) have enabled their\nwidespread use across diverse real-world applications. However, concerns remain\nabout their tendency to encode and reproduce ideological biases, particularly\nalong political and economic dimensions. In this paper, we propose a framework\nfor probing and mitigating such biases in decoder-based LLMs through analysis\nof internal model representations. Grounded in the Political Compass Test\n(PCT), our method uses contrastive pairs to extract and compare hidden layer\nactivations from models like Mistral and DeepSeek. We introduce a comprehensive\nactivation extraction pipeline capable of layer-wise analysis across multiple\nideological axes, revealing meaningful disparities linked to political framing.\nOur results show that decoder LLMs systematically encode representational bias\nacross layers, which can be leveraged for effective steering vector-based\nmitigation. This work provides new insights into how political bias is encoded\nin LLMs and offers a principled approach to debiasing beyond surface-level\noutput interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have enabled their\nwidespread use across diverse real-world applications. However, concerns remain\nabout their tendency to encode and reproduce ideological biases, particularly\nalong political and economic dimensions. In this paper, we propose a framework\nfor probing and mitigating such biases in decoder-based LLMs through analysis\nof internal model representations. Grounded in the Political Compass Test\n(PCT), our method uses contrastive pairs to extract and compare hidden layer\nactivations from models like Mistral and DeepSeek. We introduce a comprehensive\nactivation extraction pipeline capable of layer-wise analysis across multiple\nideological axes, revealing meaningful disparities linked to political framing.\nOur results show that decoder LLMs systematically encode representational bias\nacross layers, which can be leveraged for effective steering vector-based\nmitigation. This work provides new insights into how political bias is encoded\nin LLMs and offers a principled approach to debiasing beyond surface-level\noutput interventions."
                },
                "authors": [
                    {
                        "name": "Afrozah Nadeem"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23590v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23590v3",
                "updated": "2025-08-12T10:54:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    54,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2024-10-31T03:01:35Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    3,
                    1,
                    35,
                    3,
                    305,
                    0
                ],
                "title": "The Nudge Average Treatment Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nudge Average Treatment Effect"
                },
                "summary": "The instrumental variable method is a prominent approach to recover under\ncertain conditions, valid inference about a treatment causal effect even when\nunmeasured confounding might be present. In a groundbreaking paper, Imbens and\nAngrist (1994) established that a valid instrument nonparametrically identifies\nthe average causal effect among compliers, also known as the local average\ntreatment effect under a certain monotonicity assumption which rules out the\nexistence of so-called defiers. An often-cited attractive property of\nmonotonicity is that it facilitates a causal interpretation of the instrumental\nvariable estimand without restricting the degree of heterogeneity of the\ntreatment causal effect. In this paper, we introduce an alternative equally\nstraightforward and interpretable condition for identification, which\naccommodates both the presence of defiers and heterogenous treatment effects.\nMainly, we show that under our new conditions, the instrumental variable\nestimand recovers the average causal effect for the subgroup of units for whom\nthe treatment is manipulable by the instrument, a subgroup which may consist of\nboth defiers and compliers, therefore recovering an effect estimand we aptly\ncall the Nudge Average Treatment Effect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The instrumental variable method is a prominent approach to recover under\ncertain conditions, valid inference about a treatment causal effect even when\nunmeasured confounding might be present. In a groundbreaking paper, Imbens and\nAngrist (1994) established that a valid instrument nonparametrically identifies\nthe average causal effect among compliers, also known as the local average\ntreatment effect under a certain monotonicity assumption which rules out the\nexistence of so-called defiers. An often-cited attractive property of\nmonotonicity is that it facilitates a causal interpretation of the instrumental\nvariable estimand without restricting the degree of heterogeneity of the\ntreatment causal effect. In this paper, we introduce an alternative equally\nstraightforward and interpretable condition for identification, which\naccommodates both the presence of defiers and heterogenous treatment effects.\nMainly, we show that under our new conditions, the instrumental variable\nestimand recovers the average causal effect for the subgroup of units for whom\nthe treatment is manipulable by the instrument, a subgroup which may consist of\nboth defiers and compliers, therefore recovering an effect estimand we aptly\ncall the Nudge Average Treatment Effect."
                },
                "authors": [
                    {
                        "name": "Eric J Tchetgen Tchetgen"
                    }
                ],
                "author_detail": {
                    "name": "Eric J Tchetgen Tchetgen"
                },
                "author": "Eric J Tchetgen Tchetgen",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23590v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23590v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08837v1",
                "updated": "2025-08-12T10:54:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    54,
                    8,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T10:54:08Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    54,
                    8,
                    1,
                    224,
                    0
                ],
                "title": "The Roots of International Perceptions: Simulating US Attitude Changes\n  Towards China with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Roots of International Perceptions: Simulating US Attitude Changes\n  Towards China with LLM Agents"
                },
                "summary": "The rise of LLMs poses new possibilities in modeling opinion evolution, a\nlong-standing task in simulation, by leveraging advanced reasoning abilities to\nrecreate complex, large-scale human cognitive trends. While most prior works\nfocus on opinion evolution surrounding specific isolated events or the views\nwithin a country, ours is the first to model the large-scale attitude evolution\nof a population representing an entire country towards another -- US citizens'\nperspectives towards China. To tackle the challenges of this broad scenario, we\npropose a framework that integrates media data collection, user profile\ncreation, and cognitive architecture for opinion updates to successfully\nreproduce the real trend of US attitudes towards China over a 20-year period\nfrom 2005 to today. We also leverage LLMs' capabilities to introduce debiased\nmedia exposure, extracting neutral events from typically subjective news\ncontents, to uncover the roots of polarized opinion formation, as well as a\ndevils advocate agent to help explain the rare reversal from negative to\npositive attitudes towards China, corresponding with changes in the way\nAmericans obtain information about the country. The simulation results, beyond\nvalidating our framework architecture, also reveal the impact of biased framing\nand selection bias in shaping attitudes. Overall, our work contributes to a new\nparadigm for LLM-based modeling of cognitive behaviors in a large-scale,\nlong-term, cross-border social context, providing insights into the formation\nof international biases and offering valuable implications for media consumers\nto better understand the factors shaping their perspectives, and ultimately\ncontributing to the larger social need for bias reduction and cross-cultural\ntolerance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of LLMs poses new possibilities in modeling opinion evolution, a\nlong-standing task in simulation, by leveraging advanced reasoning abilities to\nrecreate complex, large-scale human cognitive trends. While most prior works\nfocus on opinion evolution surrounding specific isolated events or the views\nwithin a country, ours is the first to model the large-scale attitude evolution\nof a population representing an entire country towards another -- US citizens'\nperspectives towards China. To tackle the challenges of this broad scenario, we\npropose a framework that integrates media data collection, user profile\ncreation, and cognitive architecture for opinion updates to successfully\nreproduce the real trend of US attitudes towards China over a 20-year period\nfrom 2005 to today. We also leverage LLMs' capabilities to introduce debiased\nmedia exposure, extracting neutral events from typically subjective news\ncontents, to uncover the roots of polarized opinion formation, as well as a\ndevils advocate agent to help explain the rare reversal from negative to\npositive attitudes towards China, corresponding with changes in the way\nAmericans obtain information about the country. The simulation results, beyond\nvalidating our framework architecture, also reveal the impact of biased framing\nand selection bias in shaping attitudes. Overall, our work contributes to a new\nparadigm for LLM-based modeling of cognitive behaviors in a large-scale,\nlong-term, cross-border social context, providing insights into the formation\nof international biases and offering valuable implications for media consumers\nto better understand the factors shaping their perspectives, and ultimately\ncontributing to the larger social need for bias reduction and cross-cultural\ntolerance."
                },
                "authors": [
                    {
                        "name": "Nicholas Sukiennik"
                    },
                    {
                        "name": "Yichuan Xu"
                    },
                    {
                        "name": "Yuqing Kan"
                    },
                    {
                        "name": "Jinghua Piao"
                    },
                    {
                        "name": "Yuwei Yan"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "Submitted to AAAI Social Impact 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08836v1",
                "updated": "2025-08-12T10:52:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    52,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T10:52:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    52,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "EditMF: Drawing an Invisible Fingerprint for Your Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EditMF: Drawing an Invisible Fingerprint for Your Large Language Models"
                },
                "summary": "Training large language models (LLMs) is resource-intensive and expensive,\nmaking protecting intellectual property (IP) for LLMs crucial. Recently,\nembedding fingerprints into LLMs has emerged as a prevalent method for\nestablishing model ownership. However, existing back-door-based methods suffer\nfrom limited stealth and efficiency. To simultaneously address these issues, we\npropose EditMF, a training-free fingerprinting paradigm that achieves highly\nimperceptible fingerprint embedding with minimal computational overhead.\nOwnership bits are mapped to compact, semantically coherent triples drawn from\nan encrypted artificial knowledge base (e.g., virtual author-novel-protagonist\nfacts). Causal tracing localizes the minimal set of layers influencing each\ntriple, and a zero-space update injects the fingerprint without perturbing\nunrelated knowledge. Verification requires only a single black-box query and\nsucceeds when the model returns the exact pre-embedded protagonist. Empirical\nresults on LLaMA and Qwen families show that EditMF combines high\nimperceptibility with negligible model's performance loss, while delivering\nrobustness far beyond LoRA-based fingerprinting and approaching that of SFT\nembeddings. Extensive experiments demonstrate that EditMF is an effective and\nlow-overhead solution for secure LLM ownership verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) is resource-intensive and expensive,\nmaking protecting intellectual property (IP) for LLMs crucial. Recently,\nembedding fingerprints into LLMs has emerged as a prevalent method for\nestablishing model ownership. However, existing back-door-based methods suffer\nfrom limited stealth and efficiency. To simultaneously address these issues, we\npropose EditMF, a training-free fingerprinting paradigm that achieves highly\nimperceptible fingerprint embedding with minimal computational overhead.\nOwnership bits are mapped to compact, semantically coherent triples drawn from\nan encrypted artificial knowledge base (e.g., virtual author-novel-protagonist\nfacts). Causal tracing localizes the minimal set of layers influencing each\ntriple, and a zero-space update injects the fingerprint without perturbing\nunrelated knowledge. Verification requires only a single black-box query and\nsucceeds when the model returns the exact pre-embedded protagonist. Empirical\nresults on LLaMA and Qwen families show that EditMF combines high\nimperceptibility with negligible model's performance loss, while delivering\nrobustness far beyond LoRA-based fingerprinting and approaching that of SFT\nembeddings. Extensive experiments demonstrate that EditMF is an effective and\nlow-overhead solution for secure LLM ownership verification."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Wu"
                    },
                    {
                        "name": "Yinghan Zhou"
                    },
                    {
                        "name": "Wanli Peng"
                    },
                    {
                        "name": "Yiming Xue"
                    },
                    {
                        "name": "Juan Wen"
                    },
                    {
                        "name": "Ping Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhong"
                },
                "author": "Ping Zhong",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11488v2",
                "updated": "2025-08-12T10:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    43,
                    55,
                    1,
                    224,
                    0
                ],
                "published": "2023-11-30T16:02:04Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    16,
                    2,
                    4,
                    3,
                    334,
                    0
                ],
                "title": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows"
                },
                "summary": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes."
                },
                "authors": [
                    {
                        "name": "Thiago Garrett"
                    },
                    {
                        "name": "Weijia Song"
                    },
                    {
                        "name": "Roman Vitenberg"
                    },
                    {
                        "name": "Ken Birman"
                    }
                ],
                "author_detail": {
                    "name": "Ken Birman"
                },
                "author": "Ken Birman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08833v1",
                "updated": "2025-08-12T10:40:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    40,
                    33,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T10:40:33Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    40,
                    33,
                    1,
                    224,
                    0
                ],
                "title": "An Investigation of Robustness of LLMs in Mathematical Reasoning:\n  Benchmarking with Mathematically-Equivalent Transformation of Advanced\n  Mathematical Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Investigation of Robustness of LLMs in Mathematical Reasoning:\n  Benchmarking with Mathematically-Equivalent Transformation of Advanced\n  Mathematical Problems"
                },
                "summary": "In this paper, we introduce a systematic framework beyond conventional method\nto assess LLMs' mathematical-reasoning robustness by stress-testing them on\nadvanced math problems that are mathematically equivalent but with linguistic\nand parametric variation. These transformations allow us to measure the\nsensitivity of LLMs to non-mathematical perturbations, thereby enabling a more\naccurate evaluation of their mathematical reasoning capabilities. Using this\nnew evaluation methodology, we created PutnamGAP, a new benchmark dataset with\nmultiple mathematically-equivalent variations of competition-level math\nproblems. With the new dataset, we evaluate multiple families of representative\nLLMs and examine their robustness. Across 18 commercial and open-source models\nwe observe sharp performance degradation on the variants. OpenAI's flagship\nreasoning model, O3, scores 49 % on the originals but drops by 4 percentage\npoints on surface variants, and by 10.5 percentage points on core-step-based\nvariants, while smaller models fare far worse. Overall, the results show that\nthe proposed new evaluation methodology is effective for deepening our\nunderstanding of the robustness of LLMs and generating new insights for further\nimproving their mathematical reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a systematic framework beyond conventional method\nto assess LLMs' mathematical-reasoning robustness by stress-testing them on\nadvanced math problems that are mathematically equivalent but with linguistic\nand parametric variation. These transformations allow us to measure the\nsensitivity of LLMs to non-mathematical perturbations, thereby enabling a more\naccurate evaluation of their mathematical reasoning capabilities. Using this\nnew evaluation methodology, we created PutnamGAP, a new benchmark dataset with\nmultiple mathematically-equivalent variations of competition-level math\nproblems. With the new dataset, we evaluate multiple families of representative\nLLMs and examine their robustness. Across 18 commercial and open-source models\nwe observe sharp performance degradation on the variants. OpenAI's flagship\nreasoning model, O3, scores 49 % on the originals but drops by 4 percentage\npoints on surface variants, and by 10.5 percentage points on core-step-based\nvariants, while smaller models fare far worse. Overall, the results show that\nthe proposed new evaluation methodology is effective for deepening our\nunderstanding of the robustness of LLMs and generating new insights for further\nimproving their mathematical reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Yuren Hao"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Chengxiang Zhai"
                },
                "author": "Chengxiang Zhai",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08827v1",
                "updated": "2025-08-12T10:36:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    36,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T10:36:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    36,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "TiMoE: Time-Aware Mixture of Language Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TiMoE: Time-Aware Mixture of Language Experts"
                },
                "summary": "Large language models (LLMs) are typically trained on fixed snapshots of the\nweb, which means that their knowledge becomes stale and their predictions risk\ntemporal leakage: relying on information that lies in the future relative to a\nquery. We tackle this problem by pre-training from scratch a set of GPT-style\nexperts on disjoint two-year slices of a 2013-2024 corpus and combining them\nthrough TiMoE, a Time-aware Mixture of Language Experts. At inference time,\nTiMoE masks all experts whose training window ends after the query timestamp\nand merges the remaining log-probabilities in a shared space, guaranteeing\nstrict causal validity while retaining the breadth of multi-period knowledge.\nWe also release TSQA, a 10k-question benchmark whose alternatives are\nexplicitly labelled as past, future or irrelevant, allowing fine-grained\nmeasurement of temporal hallucinations. Experiments on eight standard NLP tasks\nplus TSQA show that a co-adapted TiMoE variant matches or exceeds the best\nsingle-period expert and cuts future-knowledge errors by up to 15%. Our results\ndemonstrate that modular, time-segmented pre-training paired with causal\nrouting is a simple yet effective path toward LLMs that stay chronologically\ngrounded without sacrificing general performance much. We open source our code\nat TiMoE (Github): https://github.com/epfml/TiMoE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically trained on fixed snapshots of the\nweb, which means that their knowledge becomes stale and their predictions risk\ntemporal leakage: relying on information that lies in the future relative to a\nquery. We tackle this problem by pre-training from scratch a set of GPT-style\nexperts on disjoint two-year slices of a 2013-2024 corpus and combining them\nthrough TiMoE, a Time-aware Mixture of Language Experts. At inference time,\nTiMoE masks all experts whose training window ends after the query timestamp\nand merges the remaining log-probabilities in a shared space, guaranteeing\nstrict causal validity while retaining the breadth of multi-period knowledge.\nWe also release TSQA, a 10k-question benchmark whose alternatives are\nexplicitly labelled as past, future or irrelevant, allowing fine-grained\nmeasurement of temporal hallucinations. Experiments on eight standard NLP tasks\nplus TSQA show that a co-adapted TiMoE variant matches or exceeds the best\nsingle-period expert and cuts future-knowledge errors by up to 15%. Our results\ndemonstrate that modular, time-segmented pre-training paired with causal\nrouting is a simple yet effective path toward LLMs that stay chronologically\ngrounded without sacrificing general performance much. We open source our code\nat TiMoE (Github): https://github.com/epfml/TiMoE"
                },
                "authors": [
                    {
                        "name": "Robin Faro"
                    },
                    {
                        "name": "Dongyang Fan"
                    },
                    {
                        "name": "Tamar Alphaidze"
                    },
                    {
                        "name": "Martin Jaggi"
                    }
                ],
                "author_detail": {
                    "name": "Martin Jaggi"
                },
                "author": "Martin Jaggi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10961v2",
                "updated": "2025-08-12T10:35:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    35,
                    5,
                    1,
                    224,
                    0
                ],
                "published": "2025-04-15T08:06:36Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    6,
                    36,
                    1,
                    105,
                    0
                ],
                "title": "Evaluating Trust in AI, Human, and Co-produced Feedback Among\n  Undergraduate Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Trust in AI, Human, and Co-produced Feedback Among\n  Undergraduate Students"
                },
                "summary": "As generative AI models, particularly large language models (LLMs), transform\neducational feedback practices in higher education (HE) contexts, understanding\nstudents' perceptions of different sources of feedback becomes crucial for\ntheir effective implementation and adoption. This study addresses a critical\ngap by comparing undergraduate students' trust in LLM, human, and human-AI\nco-produced feedback in their authentic HE context. More specifically, through\na within-subject experimental design involving 91 participants, we investigated\nfactors that predict students' ability to distinguish between feedback types,\ntheir perceptions of feedback quality, and potential biases related to the\nsource of feedback. Findings revealed that when the source was blinded,\nstudents generally preferred AI and co-produced feedback over human feedback\nregarding perceived usefulness and objectivity. However, they presented a\nstrong bias against AI when the source of feedback was disclosed. In addition,\nonly AI feedback suffered a decline in perceived genuineness when feedback\nsources were revealed, while co-produced feedback maintained its positive\nperception. Educational AI experience improved students' ability to identify\nLLM-generated feedback and increased their trust in all types of feedback. More\nyears of students' experience using AI for general purposes were associated\nwith lower perceived usefulness and credibility of feedback. These insights\noffer substantial evidence of the importance of source credibility and the need\nto enhance both feedback literacy and AI literacy to mitigate bias in student\nperceptions for AI-generated feedback to be adopted and impact education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As generative AI models, particularly large language models (LLMs), transform\neducational feedback practices in higher education (HE) contexts, understanding\nstudents' perceptions of different sources of feedback becomes crucial for\ntheir effective implementation and adoption. This study addresses a critical\ngap by comparing undergraduate students' trust in LLM, human, and human-AI\nco-produced feedback in their authentic HE context. More specifically, through\na within-subject experimental design involving 91 participants, we investigated\nfactors that predict students' ability to distinguish between feedback types,\ntheir perceptions of feedback quality, and potential biases related to the\nsource of feedback. Findings revealed that when the source was blinded,\nstudents generally preferred AI and co-produced feedback over human feedback\nregarding perceived usefulness and objectivity. However, they presented a\nstrong bias against AI when the source of feedback was disclosed. In addition,\nonly AI feedback suffered a decline in perceived genuineness when feedback\nsources were revealed, while co-produced feedback maintained its positive\nperception. Educational AI experience improved students' ability to identify\nLLM-generated feedback and increased their trust in all types of feedback. More\nyears of students' experience using AI for general purposes were associated\nwith lower perceived usefulness and credibility of feedback. These insights\noffer substantial evidence of the importance of source credibility and the need\nto enhance both feedback literacy and AI literacy to mitigate bias in student\nperceptions for AI-generated feedback to be adopted and impact education."
                },
                "authors": [
                    {
                        "name": "Audrey Zhang"
                    },
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Wannapon Suraworachet"
                    },
                    {
                        "name": "Tanya Nazaretsky"
                    },
                    {
                        "name": "Mutlu Cukurova"
                    }
                ],
                "author_detail": {
                    "name": "Mutlu Cukurova"
                },
                "author": "Mutlu Cukurova",
                "arxiv_comment": "35 pages, 6 figures. Under review at Assessment and Evaluation in\n  Higher Education",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08821v1",
                "updated": "2025-08-12T10:21:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    21,
                    59,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T10:21:59Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    21,
                    59,
                    1,
                    224,
                    0
                ],
                "title": "3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs"
                },
                "summary": "Recent Multi-Modal Large Language Models (MLLMs) have demonstrated strong\ncapabilities in learning joint representations from text and images. However,\ntheir spatial reasoning remains limited. We introduce 3DFroMLLM, a novel\nframework that enables the generation of 3D object prototypes directly from\nMLLMs, including geometry and part labels. Our pipeline is agentic, comprising\na designer, coder, and visual inspector operating in a refinement loop.\nNotably, our approach requires no additional training data or detailed user\ninstructions. Building on prior work in 2D generation, we demonstrate that\nrendered images produced by our framework can be effectively used for image\nclassification pretraining tasks and outperforms previous methods by 15%. As a\ncompelling real-world use case, we show that the generated prototypes can be\nleveraged to improve fine-grained vision-language models by using the rendered,\npart-labeled prototypes to fine-tune CLIP for part segmentation and achieving a\n55% accuracy improvement without relying on any additional human-labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multi-Modal Large Language Models (MLLMs) have demonstrated strong\ncapabilities in learning joint representations from text and images. However,\ntheir spatial reasoning remains limited. We introduce 3DFroMLLM, a novel\nframework that enables the generation of 3D object prototypes directly from\nMLLMs, including geometry and part labels. Our pipeline is agentic, comprising\na designer, coder, and visual inspector operating in a refinement loop.\nNotably, our approach requires no additional training data or detailed user\ninstructions. Building on prior work in 2D generation, we demonstrate that\nrendered images produced by our framework can be effectively used for image\nclassification pretraining tasks and outperforms previous methods by 15%. As a\ncompelling real-world use case, we show that the generated prototypes can be\nleveraged to improve fine-grained vision-language models by using the rendered,\npart-labeled prototypes to fine-tune CLIP for part segmentation and achieving a\n55% accuracy improvement without relying on any additional human-labeled data."
                },
                "authors": [
                    {
                        "name": "Noor Ahmed"
                    },
                    {
                        "name": "Cameron Braunstein"
                    },
                    {
                        "name": "Steffen Eger"
                    },
                    {
                        "name": "Eddy Ilg"
                    }
                ],
                "author_detail": {
                    "name": "Eddy Ilg"
                },
                "author": "Eddy Ilg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03850v2",
                "updated": "2025-08-12T10:16:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    16,
                    47,
                    1,
                    224,
                    0
                ],
                "published": "2025-06-04T11:33:36Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    33,
                    36,
                    2,
                    155,
                    0
                ],
                "title": "Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful\n  Fine-Tuning"
                },
                "summary": "Harmful fine-tuning (HFT), performed directly on open-source LLMs or through\nFine-tuning-as-a-Service, breaks safety alignment and poses significant\nthreats. Existing methods aim to mitigate HFT risks by learning robust\nrepresentation on alignment data or making harmful data unlearnable, but they\ntreat each data sample equally, leaving data vulnerability patterns\nunderstudied. In this work, we reveal that certain subsets of alignment data\nare consistently more prone to forgetting during HFT across different\nfine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware\nAlignment (VAA), which estimates data vulnerability, partitions data into\n\"vulnerable\" and \"invulnerable\" groups, and encourages balanced learning using\na group distributionally robust optimization (Group DRO) framework.\nSpecifically, VAA learns an adversarial sampler that samples examples from the\ncurrently underperforming group and then applies group-dependent adversarial\nperturbations to the data during training, aiming to encourage a balanced\nlearning process across groups. Experiments across four fine-tuning tasks\ndemonstrate that VAA significantly reduces harmful scores while preserving\ndownstream task performance, outperforming state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmful fine-tuning (HFT), performed directly on open-source LLMs or through\nFine-tuning-as-a-Service, breaks safety alignment and poses significant\nthreats. Existing methods aim to mitigate HFT risks by learning robust\nrepresentation on alignment data or making harmful data unlearnable, but they\ntreat each data sample equally, leaving data vulnerability patterns\nunderstudied. In this work, we reveal that certain subsets of alignment data\nare consistently more prone to forgetting during HFT across different\nfine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware\nAlignment (VAA), which estimates data vulnerability, partitions data into\n\"vulnerable\" and \"invulnerable\" groups, and encourages balanced learning using\na group distributionally robust optimization (Group DRO) framework.\nSpecifically, VAA learns an adversarial sampler that samples examples from the\ncurrently underperforming group and then applies group-dependent adversarial\nperturbations to the data during training, aiming to encourage a balanced\nlearning process across groups. Experiments across four fine-tuning tasks\ndemonstrate that VAA significantly reduces harmful scores while preserving\ndownstream task performance, outperforming state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Xueting Han"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Jing Bai"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23486v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23486v3",
                "updated": "2025-08-13T08:51:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    51,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-31T12:10:00Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    10,
                    0,
                    3,
                    212,
                    0
                ],
                "title": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and\n  Effectiveness in Clinical Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and\n  Effectiveness in Clinical Domains"
                },
                "summary": "Large language models (LLMs) hold promise in clinical decision support but\nface major challenges in safety evaluation and effectiveness validation. We\ndeveloped the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a\nmultidimensional framework built on clinical expert consensus, encompassing 30\ncriteria covering critical areas like critical illness recognition, guideline\nadherence, and medication safety, with weighted consequence measures.\nThirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A\nitems aligned with these criteria, spanning 26 clinical departments to simulate\nreal-world scenarios. Benchmark testing of six LLMs revealed moderate overall\nperformance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),\nwith a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).\nDomain-specific medical LLMs showed consistent performance advantages over\ngeneral-purpose models, with relatively higher top scores in safety (0.912) and\neffectiveness (0.861). The findings of this study not only provide a\nstandardized metric for evaluating the clinical application of medical LLMs,\nfacilitating comparative analyses, risk exposure identification, and\nimprovement directions across different scenarios, but also hold the potential\nto promote safer and more effective deployment of large language models in\nhealthcare environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hold promise in clinical decision support but\nface major challenges in safety evaluation and effectiveness validation. We\ndeveloped the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a\nmultidimensional framework built on clinical expert consensus, encompassing 30\ncriteria covering critical areas like critical illness recognition, guideline\nadherence, and medication safety, with weighted consequence measures.\nThirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A\nitems aligned with these criteria, spanning 26 clinical departments to simulate\nreal-world scenarios. Benchmark testing of six LLMs revealed moderate overall\nperformance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),\nwith a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).\nDomain-specific medical LLMs showed consistent performance advantages over\ngeneral-purpose models, with relatively higher top scores in safety (0.912) and\neffectiveness (0.861). The findings of this study not only provide a\nstandardized metric for evaluating the clinical application of medical LLMs,\nfacilitating comparative analyses, risk exposure identification, and\nimprovement directions across different scenarios, but also hold the potential\nto promote safer and more effective deployment of large language models in\nhealthcare environments."
                },
                "authors": [
                    {
                        "name": "Shirui Wang"
                    },
                    {
                        "name": "Zhihui Tang"
                    },
                    {
                        "name": "Huaxia Yang"
                    },
                    {
                        "name": "Qiuhong Gong"
                    },
                    {
                        "name": "Tiantian Gu"
                    },
                    {
                        "name": "Hongyang Ma"
                    },
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Wubin Sun"
                    },
                    {
                        "name": "Zeliang Lian"
                    },
                    {
                        "name": "Kehang Mao"
                    },
                    {
                        "name": "Yinan Jiang"
                    },
                    {
                        "name": "Zhicheng Huang"
                    },
                    {
                        "name": "Lingyun Ma"
                    },
                    {
                        "name": "Wenjie Shen"
                    },
                    {
                        "name": "Yajie Ji"
                    },
                    {
                        "name": "Yunhui Tan"
                    },
                    {
                        "name": "Chunbo Wang"
                    },
                    {
                        "name": "Yunlu Gao"
                    },
                    {
                        "name": "Qianling Ye"
                    },
                    {
                        "name": "Rui Lin"
                    },
                    {
                        "name": "Mingyu Chen"
                    },
                    {
                        "name": "Lijuan Niu"
                    },
                    {
                        "name": "Zhihao Wang"
                    },
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "Mengran Lang"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Huimin Zhang"
                    },
                    {
                        "name": "Haitao Shen"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Qiguang Zhao"
                    },
                    {
                        "name": "Si-Xuan Liu"
                    },
                    {
                        "name": "Lina Zhou"
                    },
                    {
                        "name": "Hua Gao"
                    },
                    {
                        "name": "Dongqiang Ye"
                    },
                    {
                        "name": "Lingmin Meng"
                    },
                    {
                        "name": "Youtao Yu"
                    },
                    {
                        "name": "Naixin Liang"
                    },
                    {
                        "name": "Jianxiong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jianxiong Wu"
                },
                "author": "Jianxiong Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23486v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23486v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08812v1",
                "updated": "2025-08-12T10:14:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    14,
                    15,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T10:14:15Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    14,
                    15,
                    1,
                    224,
                    0
                ],
                "title": "TARA: Token-Aware LoRA for Composable Personalization in Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TARA: Token-Aware LoRA for Composable Personalization in Diffusion\n  Models"
                },
                "summary": "Personalized text-to-image generation aims to synthesize novel images of a\nspecific subject or style using only a few reference images. Recent methods\nbased on Low-Rank Adaptation (LoRA) enable efficient single-concept\ncustomization by injecting lightweight, concept-specific adapters into\npre-trained diffusion models. However, combining multiple LoRA modules for\nmulti-concept generation often leads to identity missing and visual feature\nleakage. In this work, we identify two key issues behind these failures: (1)\ntoken-wise interference among different LoRA modules, and (2) spatial\nmisalignment between the attention map of a rare token and its corresponding\nconcept-specific region. To address these issues, we propose Token-Aware LoRA\n(TARA), which introduces a token mask to explicitly constrain each module to\nfocus on its associated rare token to avoid interference, and a training\nobjective that encourages the spatial attention of a rare token to align with\nits concept region. Our method enables training-free multi-concept composition\nby directly injecting multiple independently trained TARA modules at inference\ntime. Experimental results demonstrate that TARA enables efficient\nmulti-concept inference and effectively preserving the visual identity of each\nconcept by avoiding mutual interference between LoRA modules. The code and\nmodels are available at https://github.com/YuqiPeng77/TARA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized text-to-image generation aims to synthesize novel images of a\nspecific subject or style using only a few reference images. Recent methods\nbased on Low-Rank Adaptation (LoRA) enable efficient single-concept\ncustomization by injecting lightweight, concept-specific adapters into\npre-trained diffusion models. However, combining multiple LoRA modules for\nmulti-concept generation often leads to identity missing and visual feature\nleakage. In this work, we identify two key issues behind these failures: (1)\ntoken-wise interference among different LoRA modules, and (2) spatial\nmisalignment between the attention map of a rare token and its corresponding\nconcept-specific region. To address these issues, we propose Token-Aware LoRA\n(TARA), which introduces a token mask to explicitly constrain each module to\nfocus on its associated rare token to avoid interference, and a training\nobjective that encourages the spatial attention of a rare token to align with\nits concept region. Our method enables training-free multi-concept composition\nby directly injecting multiple independently trained TARA modules at inference\ntime. Experimental results demonstrate that TARA enables efficient\nmulti-concept inference and effectively preserving the visual identity of each\nconcept by avoiding mutual interference between LoRA modules. The code and\nmodels are available at https://github.com/YuqiPeng77/TARA."
                },
                "authors": [
                    {
                        "name": "Yuqi Peng"
                    },
                    {
                        "name": "Lingtao Zheng"
                    },
                    {
                        "name": "Yufeng Yang"
                    },
                    {
                        "name": "Yi Huang"
                    },
                    {
                        "name": "Mingfu Yan"
                    },
                    {
                        "name": "Jianzhuang Liu"
                    },
                    {
                        "name": "Shifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shifeng Chen"
                },
                "author": "Shifeng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08811v1",
                "updated": "2025-08-12T10:10:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    10,
                    10,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T10:10:10Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    10,
                    10,
                    1,
                    224,
                    0
                ],
                "title": "Revisiting Efficient Semantic Segmentation: Learning Offsets for Better\n  Spatial and Class Feature Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Efficient Semantic Segmentation: Learning Offsets for Better\n  Spatial and Class Feature Alignment"
                },
                "summary": "Semantic segmentation is fundamental to vision systems requiring pixel-level\nscene understanding, yet deploying it on resource-constrained devices demands\nefficient architectures. Although existing methods achieve real-time inference\nthrough lightweight designs, we reveal their inherent limitation: misalignment\nbetween class representations and image features caused by a per-pixel\nclassification paradigm. With experimental analysis, we find that this paradigm\nresults in a highly challenging assumption for efficient scenarios: Image pixel\nfeatures should not vary for the same category in different images. To address\nthis dilemma, we propose a coupled dual-branch offset learning paradigm that\nexplicitly learns feature and class offsets to dynamically refine both class\nrepresentations and spatial image features. Based on the proposed paradigm, we\nconstruct an efficient semantic segmentation network, OffSeg. Notably, the\noffset learning paradigm can be adopted to existing methods with no additional\narchitectural changes. Extensive experiments on four datasets, including\nADE20K, Cityscapes, COCO-Stuff-164K, and Pascal Context, demonstrate consistent\nimprovements with negligible parameters. For instance, on the ADE20K dataset,\nour proposed offset learning paradigm improves SegFormer-B0, SegNeXt-T, and\nMask2Former-Tiny by 2.7%, 1.9%, and 2.6% mIoU, respectively, with only 0.1-0.2M\nadditional parameters required.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic segmentation is fundamental to vision systems requiring pixel-level\nscene understanding, yet deploying it on resource-constrained devices demands\nefficient architectures. Although existing methods achieve real-time inference\nthrough lightweight designs, we reveal their inherent limitation: misalignment\nbetween class representations and image features caused by a per-pixel\nclassification paradigm. With experimental analysis, we find that this paradigm\nresults in a highly challenging assumption for efficient scenarios: Image pixel\nfeatures should not vary for the same category in different images. To address\nthis dilemma, we propose a coupled dual-branch offset learning paradigm that\nexplicitly learns feature and class offsets to dynamically refine both class\nrepresentations and spatial image features. Based on the proposed paradigm, we\nconstruct an efficient semantic segmentation network, OffSeg. Notably, the\noffset learning paradigm can be adopted to existing methods with no additional\narchitectural changes. Extensive experiments on four datasets, including\nADE20K, Cityscapes, COCO-Stuff-164K, and Pascal Context, demonstrate consistent\nimprovements with negligible parameters. For instance, on the ADE20K dataset,\nour proposed offset learning paradigm improves SegFormer-B0, SegNeXt-T, and\nMask2Former-Tiny by 2.7%, 1.9%, and 2.6% mIoU, respectively, with only 0.1-0.2M\nadditional parameters required."
                },
                "authors": [
                    {
                        "name": "Shi-Chen Zhang"
                    },
                    {
                        "name": "Yunheng Li"
                    },
                    {
                        "name": "Yu-Huan Wu"
                    },
                    {
                        "name": "Qibin Hou"
                    },
                    {
                        "name": "Ming-Ming Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Ming Cheng"
                },
                "author": "Ming-Ming Cheng",
                "arxiv_comment": "Accepted at ICCV 2025. Project page:\n  https://github.com/HVision-NKU/OffSeg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00642v2",
                "updated": "2025-08-12T10:04:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    4,
                    27,
                    1,
                    224,
                    0
                ],
                "published": "2025-07-01T10:34:17Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    10,
                    34,
                    17,
                    1,
                    182,
                    0
                ],
                "title": "ChatHLS: Towards Systematic Design Automation and Optimization for\n  High-Level Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatHLS: Towards Systematic Design Automation and Optimization for\n  High-Level Synthesis"
                },
                "summary": "The increasing complexity of computational demands has spurred the adoption\nof domain-specific accelerators, yet traditional hardware design methodologies\nremain constrained by prolonged development and verification cycles. High-Level\nSynthesis (HLS) bridges the software-hardware gap by enabling hardware design\nfrom high-level languages. However, its widespread adoption is hindered by\nstrict coding constraints and intricate hardware-specific optimizations. To\naddress these challenges, we introduce ChatHLS, an agile HLS design automation\nworkflow that leverages fine-tuned LLMs integrated within a multi-agent\nframework for HLS-specific error correction and design optimization. Through\nnavigating LLM training with a novel verification-oriented data augmentation\nparadigm, ChatHLS achieves an average repair pass rate of 82.7% over 612 error\ncases. Furthermore, by enabling optimization reasoning within practical\ncomputational budgets, ChatHLS delivers performance improvements ranging from\n1.9$\\times$ to 14.8$\\times$ on resource-constrained kernels, attaining a\n3.6$\\times$ average speedup compared to SOTA approaches. These results\nunderscore the potential of ChatHLS in substantially expediting hardware\ndevelopment cycles while upholding rigorous standards of design reliability and\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of computational demands has spurred the adoption\nof domain-specific accelerators, yet traditional hardware design methodologies\nremain constrained by prolonged development and verification cycles. High-Level\nSynthesis (HLS) bridges the software-hardware gap by enabling hardware design\nfrom high-level languages. However, its widespread adoption is hindered by\nstrict coding constraints and intricate hardware-specific optimizations. To\naddress these challenges, we introduce ChatHLS, an agile HLS design automation\nworkflow that leverages fine-tuned LLMs integrated within a multi-agent\nframework for HLS-specific error correction and design optimization. Through\nnavigating LLM training with a novel verification-oriented data augmentation\nparadigm, ChatHLS achieves an average repair pass rate of 82.7% over 612 error\ncases. Furthermore, by enabling optimization reasoning within practical\ncomputational budgets, ChatHLS delivers performance improvements ranging from\n1.9$\\times$ to 14.8$\\times$ on resource-constrained kernels, attaining a\n3.6$\\times$ average speedup compared to SOTA approaches. These results\nunderscore the potential of ChatHLS in substantially expediting hardware\ndevelopment cycles while upholding rigorous standards of design reliability and\nquality."
                },
                "authors": [
                    {
                        "name": "Runkai Li"
                    },
                    {
                        "name": "Jia Xiong"
                    },
                    {
                        "name": "Xiuyuan He"
                    },
                    {
                        "name": "Jiaqi Lv"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Xi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xi Wang"
                },
                "author": "Xi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08795v1",
                "updated": "2025-08-12T09:51:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    51,
                    39,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T09:51:39Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    51,
                    39,
                    1,
                    224,
                    0
                ],
                "title": "A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to\n  Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to\n  Functions"
                },
                "summary": "Large language models (LLMs) acquire vast knowledge from large text corpora,\nbut this information can become outdated or inaccurate. Since retraining is\ncomputationally expensive, knowledge editing offers an efficient alternative --\nmodifying internal knowledge without full retraining. These methods aim to\nupdate facts precisely while preserving the model's overall capabilities. While\nexisting surveys focus on the mechanism of editing (e.g., parameter changes vs.\nexternal memory), they often overlook the function of the knowledge being\nedited. This survey introduces a novel, complementary function-based taxonomy\nto provide a more holistic view. We examine how different mechanisms apply to\nvarious knowledge types -- factual, temporal, conceptual, commonsense, and\nsocial -- highlighting how editing effectiveness depends on the nature of the\ntarget knowledge. By organizing our review along these two axes, we map the\ncurrent landscape, outline the strengths and limitations of existing methods,\ndefine the problem formally, survey evaluation tasks and datasets, and conclude\nwith open challenges and future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) acquire vast knowledge from large text corpora,\nbut this information can become outdated or inaccurate. Since retraining is\ncomputationally expensive, knowledge editing offers an efficient alternative --\nmodifying internal knowledge without full retraining. These methods aim to\nupdate facts precisely while preserving the model's overall capabilities. While\nexisting surveys focus on the mechanism of editing (e.g., parameter changes vs.\nexternal memory), they often overlook the function of the knowledge being\nedited. This survey introduces a novel, complementary function-based taxonomy\nto provide a more holistic view. We examine how different mechanisms apply to\nvarious knowledge types -- factual, temporal, conceptual, commonsense, and\nsocial -- highlighting how editing effectiveness depends on the nature of the\ntarget knowledge. By organizing our review along these two axes, we map the\ncurrent landscape, outline the strengths and limitations of existing methods,\ndefine the problem formally, survey evaluation tasks and datasets, and conclude\nwith open challenges and future directions."
                },
                "authors": [
                    {
                        "name": "Amir Mohammad Salehoof"
                    },
                    {
                        "name": "Ali Ramezani"
                    },
                    {
                        "name": "Yadollah Yaghoobzadeh"
                    },
                    {
                        "name": "Majid Nili Ahmadabadi"
                    }
                ],
                "author_detail": {
                    "name": "Majid Nili Ahmadabadi"
                },
                "author": "Majid Nili Ahmadabadi",
                "arxiv_comment": "13 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08791v1",
                "updated": "2025-08-12T09:45:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    45,
                    19,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T09:45:19Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    45,
                    19,
                    1,
                    224,
                    0
                ],
                "title": "Feedback-Driven Tool-Use Improvements in Large Language Models via\n  Automated Build Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback-Driven Tool-Use Improvements in Large Language Models via\n  Automated Build Environments"
                },
                "summary": "Effective tool use is essential for large language models (LLMs) to interact\nmeaningfully with their environment. However, progress is limited by the lack\nof efficient reinforcement learning (RL) frameworks specifically designed for\ntool use, due to challenges in constructing stable training environments and\ndesigning verifiable reward mechanisms. To address this, we propose an\nautomated environment construction pipeline, incorporating scenario\ndecomposition, document generation, function integration, complexity scaling,\nand localized deployment. This enables the creation of high-quality training\nenvironments that provide detailed and measurable feedback without relying on\nexternal tools. Additionally, we introduce a verifiable reward mechanism that\nevaluates both the precision of tool use and the completeness of task\nexecution. When combined with trajectory data collected from the constructed\nenvironments, this mechanism integrates seamlessly with standard RL algorithms\nto facilitate feedback-driven model training. Experiments on LLMs of varying\nscales demonstrate that our approach significantly enhances the models'\ntool-use performance without degrading their general capabilities, regardless\nof inference modes or training algorithms. Our analysis suggests that these\ngains result from improved context understanding and reasoning, driven by\nupdates to the lower-layer MLP parameters in models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective tool use is essential for large language models (LLMs) to interact\nmeaningfully with their environment. However, progress is limited by the lack\nof efficient reinforcement learning (RL) frameworks specifically designed for\ntool use, due to challenges in constructing stable training environments and\ndesigning verifiable reward mechanisms. To address this, we propose an\nautomated environment construction pipeline, incorporating scenario\ndecomposition, document generation, function integration, complexity scaling,\nand localized deployment. This enables the creation of high-quality training\nenvironments that provide detailed and measurable feedback without relying on\nexternal tools. Additionally, we introduce a verifiable reward mechanism that\nevaluates both the precision of tool use and the completeness of task\nexecution. When combined with trajectory data collected from the constructed\nenvironments, this mechanism integrates seamlessly with standard RL algorithms\nto facilitate feedback-driven model training. Experiments on LLMs of varying\nscales demonstrate that our approach significantly enhances the models'\ntool-use performance without degrading their general capabilities, regardless\nof inference modes or training algorithms. Our analysis suggests that these\ngains result from improved context understanding and reasoning, driven by\nupdates to the lower-layer MLP parameters in models."
                },
                "authors": [
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Xuesong Yao"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Jiecao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiecao Chen"
                },
                "author": "Jiecao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08785v1",
                "updated": "2025-08-12T09:38:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    38,
                    21,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T09:38:21Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    38,
                    21,
                    1,
                    224,
                    0
                ],
                "title": "Privacy-protected Retrieval-Augmented Generation for Knowledge Graph\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-protected Retrieval-Augmented Generation for Knowledge Graph\n  Question Answering"
                },
                "summary": "LLMs often suffer from hallucinations and outdated or incomplete knowledge.\nRAG is proposed to address these issues by integrating external knowledge like\nthat in KGs into LLMs. However, leveraging private KGs in RAG systems poses\nsignificant privacy risks due to the black-box nature of LLMs and potential\ninsecure data transmission, especially when using third-party LLM APIs lacking\ntransparency and control. In this paper, we investigate the privacy-protected\nRAG scenario for the first time, where entities in KGs are anonymous for LLMs,\nthus preventing them from accessing entity semantics. Due to the loss of\nsemantics of entities, previous RAG systems cannot retrieve question-relevant\nknowledge from KGs by matching questions with the meaningless identifiers of\nanonymous entities. To realize an effective RAG system in this scenario, two\nkey challenges must be addressed: (1) How can anonymous entities be converted\ninto retrievable information. (2) How to retrieve question-relevant anonymous\nentities. Hence, we propose a novel ARoG framework including relation-centric\nabstraction and structure-oriented abstraction strategies. For challenge (1),\nthe first strategy abstracts entities into high-level concepts by dynamically\ncapturing the semantics of their adjacent relations. It supplements meaningful\nsemantics which can further support the retrieval process. For challenge (2),\nthe second strategy transforms unstructured natural language questions into\nstructured abstract concept paths. These paths can be more effectively aligned\nwith the abstracted concepts in KGs, thereby improving retrieval performance.\nTo guide LLMs to effectively retrieve knowledge from KGs, the two strategies\nstrictly protect privacy from being exposed to LLMs. Experiments on three\ndatasets demonstrate that ARoG achieves strong performance and\nprivacy-robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs often suffer from hallucinations and outdated or incomplete knowledge.\nRAG is proposed to address these issues by integrating external knowledge like\nthat in KGs into LLMs. However, leveraging private KGs in RAG systems poses\nsignificant privacy risks due to the black-box nature of LLMs and potential\ninsecure data transmission, especially when using third-party LLM APIs lacking\ntransparency and control. In this paper, we investigate the privacy-protected\nRAG scenario for the first time, where entities in KGs are anonymous for LLMs,\nthus preventing them from accessing entity semantics. Due to the loss of\nsemantics of entities, previous RAG systems cannot retrieve question-relevant\nknowledge from KGs by matching questions with the meaningless identifiers of\nanonymous entities. To realize an effective RAG system in this scenario, two\nkey challenges must be addressed: (1) How can anonymous entities be converted\ninto retrievable information. (2) How to retrieve question-relevant anonymous\nentities. Hence, we propose a novel ARoG framework including relation-centric\nabstraction and structure-oriented abstraction strategies. For challenge (1),\nthe first strategy abstracts entities into high-level concepts by dynamically\ncapturing the semantics of their adjacent relations. It supplements meaningful\nsemantics which can further support the retrieval process. For challenge (2),\nthe second strategy transforms unstructured natural language questions into\nstructured abstract concept paths. These paths can be more effectively aligned\nwith the abstracted concepts in KGs, thereby improving retrieval performance.\nTo guide LLMs to effectively retrieve knowledge from KGs, the two strategies\nstrictly protect privacy from being exposed to LLMs. Experiments on three\ndatasets demonstrate that ARoG achieves strong performance and\nprivacy-robustness."
                },
                "authors": [
                    {
                        "name": "Yunfeng Ning"
                    },
                    {
                        "name": "Mayi Xu"
                    },
                    {
                        "name": "Jintao Wen"
                    },
                    {
                        "name": "Qiankun Pi"
                    },
                    {
                        "name": "Yuanyuan Zhu"
                    },
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Tieyun Qian"
                    }
                ],
                "author_detail": {
                    "name": "Tieyun Qian"
                },
                "author": "Tieyun Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.09136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09136v1",
                "updated": "2025-08-12T17:59:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    59,
                    46,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:59:46Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    59,
                    46,
                    1,
                    224,
                    0
                ],
                "title": "Turbo-VAED: Fast and Stable Transfer of Video-VAEs to Mobile Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turbo-VAED: Fast and Stable Transfer of Video-VAEs to Mobile Devices"
                },
                "summary": "There is a growing demand for deploying large generative AI models on mobile\ndevices. For recent popular video generative models, however, the Variational\nAutoEncoder (VAE) represents one of the major computational bottlenecks. Both\nlarge parameter sizes and mismatched kernels cause out-of-memory errors or\nextremely slow inference on mobile devices. To address this, we propose a\nlow-cost solution that efficiently transfers widely used video VAEs to mobile\ndevices. (1) We analyze redundancy in existing VAE architectures and get\nempirical design insights. By integrating 3D depthwise separable convolutions\ninto our model, we significantly reduce the number of parameters. (2) We\nobserve that the upsampling techniques in mainstream video VAEs are poorly\nsuited to mobile hardware and form the main bottleneck. In response, we propose\na decoupled 3D pixel shuffle scheme that slashes end-to-end delay. Building\nupon these, we develop a universal mobile-oriented VAE decoder, Turbo-VAED. (3)\nWe propose an efficient VAE decoder training method. Since only the decoder is\nused during deployment, we distill it to Turbo-VAED instead of retraining the\nfull VAE, enabling fast mobile adaptation with minimal performance loss. To our\nknowledge, our method enables real-time 720p video VAE decoding on mobile\ndevices for the first time. This approach is widely applicable to most video\nVAEs. When integrated into four representative models, with training cost as\nlow as $95, it accelerates original VAEs by up to 84.5x at 720p resolution on\nGPUs, uses as low as 17.5% of original parameter count, and retains 96.9% of\nthe original reconstruction quality. Compared to mobile-optimized VAEs,\nTurbo-VAED achieves a 2.9x speedup in FPS and better reconstruction quality on\nthe iPhone 16 Pro. The code and models will soon be available at\nhttps://github.com/hustvl/Turbo-VAED.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing demand for deploying large generative AI models on mobile\ndevices. For recent popular video generative models, however, the Variational\nAutoEncoder (VAE) represents one of the major computational bottlenecks. Both\nlarge parameter sizes and mismatched kernels cause out-of-memory errors or\nextremely slow inference on mobile devices. To address this, we propose a\nlow-cost solution that efficiently transfers widely used video VAEs to mobile\ndevices. (1) We analyze redundancy in existing VAE architectures and get\nempirical design insights. By integrating 3D depthwise separable convolutions\ninto our model, we significantly reduce the number of parameters. (2) We\nobserve that the upsampling techniques in mainstream video VAEs are poorly\nsuited to mobile hardware and form the main bottleneck. In response, we propose\na decoupled 3D pixel shuffle scheme that slashes end-to-end delay. Building\nupon these, we develop a universal mobile-oriented VAE decoder, Turbo-VAED. (3)\nWe propose an efficient VAE decoder training method. Since only the decoder is\nused during deployment, we distill it to Turbo-VAED instead of retraining the\nfull VAE, enabling fast mobile adaptation with minimal performance loss. To our\nknowledge, our method enables real-time 720p video VAE decoding on mobile\ndevices for the first time. This approach is widely applicable to most video\nVAEs. When integrated into four representative models, with training cost as\nlow as $95, it accelerates original VAEs by up to 84.5x at 720p resolution on\nGPUs, uses as low as 17.5% of original parameter count, and retains 96.9% of\nthe original reconstruction quality. Compared to mobile-optimized VAEs,\nTurbo-VAED achieves a 2.9x speedup in FPS and better reconstruction quality on\nthe iPhone 16 Pro. The code and models will soon be available at\nhttps://github.com/hustvl/Turbo-VAED."
                },
                "authors": [
                    {
                        "name": "Ya Zou"
                    },
                    {
                        "name": "Jingfeng Yao"
                    },
                    {
                        "name": "Siyuan Yu"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09129v1",
                "updated": "2025-08-12T17:56:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    56,
                    25,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:56:25Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    56,
                    25,
                    1,
                    224,
                    0
                ],
                "title": "BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented\n  Programmatic Agent Pair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented\n  Programmatic Agent Pair"
                },
                "summary": "Effective information seeking in the vast and ever-growing digital landscape\nrequires balancing expansive search with strategic reasoning. Current large\nlanguage model (LLM)-based agents struggle to achieve this balance due to\nlimitations in search breadth and reasoning depth, where slow, serial querying\nrestricts coverage of relevant sources and noisy raw inputs disrupt the\ncontinuity of multi-step reasoning. To address these challenges, we propose\nBrowseMaster, a scalable framework built around a programmatically augmented\nplanner-executor agent pair. The planner formulates and adapts search\nstrategies based on task constraints, while the executor conducts efficient,\ntargeted retrieval to supply the planner with concise, relevant evidence. This\ndivision of labor preserves coherent, long-horizon reasoning while sustaining\nbroad and systematic exploration, overcoming the trade-off that limits existing\nagents. Extensive experiments on challenging English and Chinese benchmarks\nshow that BrowseMaster consistently outperforms open-source and proprietary\nbaselines, achieving scores of 30.0 on BrowseComp-en and 46.5 on BrowseComp-zh,\nwhich demonstrates its strong capability in complex, reasoning-heavy\ninformation-seeking tasks at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective information seeking in the vast and ever-growing digital landscape\nrequires balancing expansive search with strategic reasoning. Current large\nlanguage model (LLM)-based agents struggle to achieve this balance due to\nlimitations in search breadth and reasoning depth, where slow, serial querying\nrestricts coverage of relevant sources and noisy raw inputs disrupt the\ncontinuity of multi-step reasoning. To address these challenges, we propose\nBrowseMaster, a scalable framework built around a programmatically augmented\nplanner-executor agent pair. The planner formulates and adapts search\nstrategies based on task constraints, while the executor conducts efficient,\ntargeted retrieval to supply the planner with concise, relevant evidence. This\ndivision of labor preserves coherent, long-horizon reasoning while sustaining\nbroad and systematic exploration, overcoming the trade-off that limits existing\nagents. Extensive experiments on challenging English and Chinese benchmarks\nshow that BrowseMaster consistently outperforms open-source and proprietary\nbaselines, achieving scores of 30.0 on BrowseComp-en and 46.5 on BrowseComp-zh,\nwhich demonstrates its strong capability in complex, reasoning-heavy\ninformation-seeking tasks at scale."
                },
                "authors": [
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Shuo Tang"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Yuwen Du"
                    },
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09128v1",
                "updated": "2025-08-12T17:55:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    55,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:55:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    55,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "A Review On Safe Reinforcement Learning Using Lyapunov and Barrier\n  Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review On Safe Reinforcement Learning Using Lyapunov and Barrier\n  Functions"
                },
                "summary": "Reinforcement learning (RL) has proven to be particularly effective in\nsolving complex decision-making problems for a wide range of applications. From\na control theory perspective, RL can be considered as an adaptive optimal\ncontrol scheme. Lyapunov and barrier functions are the most commonly used\ncertificates to guarantee system stability for a proposed/derived controller\nand constraint satisfaction guarantees, respectively, in control theoretic\napproaches. However, compared to theoretical guarantees available in control\ntheoretic methods, RL lacks closed-loop stability of a computed policy and\nconstraint satisfaction guarantees. Safe reinforcement learning refers to a\nclass of constrained problems where the constraint violations lead to partial\nor complete system failure. The goal of this review is to provide an overview\nof safe RL techniques using Lyapunov and barrier functions to guarantee this\nnotion of safety discussed (stability of the system in terms of a computed\npolicy and constraint satisfaction during training and deployment). The\ndifferent approaches employed are discussed in detail along with their\nshortcomings and benefits to provide critique and possible future research\ndirections. Key motivation for this review is to discuss current theoretical\napproaches for safety and stability guarantees in RL similar to control\ntheoretic approaches using Lyapunov and barrier functions. The review provides\nproven potential and promising scope of providing safety guarantees for complex\ndynamical systems with operational constraints using model-based and model-free\nRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has proven to be particularly effective in\nsolving complex decision-making problems for a wide range of applications. From\na control theory perspective, RL can be considered as an adaptive optimal\ncontrol scheme. Lyapunov and barrier functions are the most commonly used\ncertificates to guarantee system stability for a proposed/derived controller\nand constraint satisfaction guarantees, respectively, in control theoretic\napproaches. However, compared to theoretical guarantees available in control\ntheoretic methods, RL lacks closed-loop stability of a computed policy and\nconstraint satisfaction guarantees. Safe reinforcement learning refers to a\nclass of constrained problems where the constraint violations lead to partial\nor complete system failure. The goal of this review is to provide an overview\nof safe RL techniques using Lyapunov and barrier functions to guarantee this\nnotion of safety discussed (stability of the system in terms of a computed\npolicy and constraint satisfaction during training and deployment). The\ndifferent approaches employed are discussed in detail along with their\nshortcomings and benefits to provide critique and possible future research\ndirections. Key motivation for this review is to discuss current theoretical\napproaches for safety and stability guarantees in RL similar to control\ntheoretic approaches using Lyapunov and barrier functions. The review provides\nproven potential and promising scope of providing safety guarantees for complex\ndynamical systems with operational constraints using model-based and model-free\nRL."
                },
                "authors": [
                    {
                        "name": "Dhruv S. Kushwaha"
                    },
                    {
                        "name": "Zoleikha A. Biron"
                    }
                ],
                "author_detail": {
                    "name": "Zoleikha A. Biron"
                },
                "author": "Zoleikha A. Biron",
                "arxiv_comment": "pages - 19, figures - 9, Submitted to IEEE Access",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "93E99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "A.1; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09126v1",
                "updated": "2025-08-12T17:55:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    55,
                    8,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:55:08Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    55,
                    8,
                    1,
                    224,
                    0
                ],
                "title": "Neutone SDK: An Open Source Framework for Neural Audio Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutone SDK: An Open Source Framework for Neural Audio Processing"
                },
                "summary": "Neural audio processing has unlocked novel methods of sound transformation\nand synthesis, yet integrating deep learning models into digital audio\nworkstations (DAWs) remains challenging due to real-time / neural network\ninference constraints and the complexities of plugin development. In this\npaper, we introduce the Neutone SDK: an open source framework that streamlines\nthe deployment of PyTorch-based neural audio models for both real-time and\noffline applications. By encapsulating common challenges such as variable\nbuffer sizes, sample rate conversion, delay compensation, and control parameter\nhandling within a unified, model-agnostic interface, our framework enables\nseamless interoperability between neural models and host plugins while allowing\nusers to work entirely in Python. We provide a technical overview of the\ninterfaces needed to accomplish this, as well as the corresponding SDK\nimplementations. We also demonstrate the SDK's versatility across applications\nsuch as audio effect emulation, timbre transfer, and sample generation, as well\nas its adoption by researchers, educators, companies, and artists alike. The\nNeutone SDK is available at https://github.com/Neutone/neutone_sdk",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural audio processing has unlocked novel methods of sound transformation\nand synthesis, yet integrating deep learning models into digital audio\nworkstations (DAWs) remains challenging due to real-time / neural network\ninference constraints and the complexities of plugin development. In this\npaper, we introduce the Neutone SDK: an open source framework that streamlines\nthe deployment of PyTorch-based neural audio models for both real-time and\noffline applications. By encapsulating common challenges such as variable\nbuffer sizes, sample rate conversion, delay compensation, and control parameter\nhandling within a unified, model-agnostic interface, our framework enables\nseamless interoperability between neural models and host plugins while allowing\nusers to work entirely in Python. We provide a technical overview of the\ninterfaces needed to accomplish this, as well as the corresponding SDK\nimplementations. We also demonstrate the SDK's versatility across applications\nsuch as audio effect emulation, timbre transfer, and sample generation, as well\nas its adoption by researchers, educators, companies, and artists alike. The\nNeutone SDK is available at https://github.com/Neutone/neutone_sdk"
                },
                "authors": [
                    {
                        "name": "Christopher Mitcheltree"
                    },
                    {
                        "name": "Bogdan Teleaga"
                    },
                    {
                        "name": "Andrew Fyfe"
                    },
                    {
                        "name": "Naotake Masuda"
                    },
                    {
                        "name": "Matthias Schäfer"
                    },
                    {
                        "name": "Alfie Bradic"
                    },
                    {
                        "name": "Nao Tokui"
                    }
                ],
                "author_detail": {
                    "name": "Nao Tokui"
                },
                "author": "Nao Tokui",
                "arxiv_comment": "Accepted to AES International Conference on Artificial Intelligence\n  and Machine Learning for Audio 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09125v1",
                "updated": "2025-08-12T17:54:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    54,
                    27,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:54:27Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    54,
                    27,
                    1,
                    224,
                    0
                ],
                "title": "Complex Logical Instruction Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex Logical Instruction Generation"
                },
                "summary": "Instruction following has catalyzed the recent era of Large Language Models\n(LLMs) and is the foundational skill underpinning more advanced capabilities\nsuch as reasoning and agentic behaviors. As tasks grow more challenging, the\nlogic structures embedded in natural language instructions becomes increasingly\nintricate. However, how well LLMs perform on such logic-rich instructions\nremains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a\nscalable, automated framework for generating verifiable instructions from code\nfunctions, which can naturally express rich logic such as conditionals,\nnesting, recursion, and function calls. We further curate a collection of\ncomplex code functions and use LogicIFGen to construct LogicIFEval, a benchmark\ncomprising 426 verifiable logic-rich instructions. Our experiments demonstrate\nthat current state-of-the-art LLMs still struggle to correctly follow the\ninstructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the\ninstructions, revealing significant deficiencies in the instruction-following\nability. Code and Benchmark: https://github.com/mianzhang/LogicIF",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction following has catalyzed the recent era of Large Language Models\n(LLMs) and is the foundational skill underpinning more advanced capabilities\nsuch as reasoning and agentic behaviors. As tasks grow more challenging, the\nlogic structures embedded in natural language instructions becomes increasingly\nintricate. However, how well LLMs perform on such logic-rich instructions\nremains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a\nscalable, automated framework for generating verifiable instructions from code\nfunctions, which can naturally express rich logic such as conditionals,\nnesting, recursion, and function calls. We further curate a collection of\ncomplex code functions and use LogicIFGen to construct LogicIFEval, a benchmark\ncomprising 426 verifiable logic-rich instructions. Our experiments demonstrate\nthat current state-of-the-art LLMs still struggle to correctly follow the\ninstructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the\ninstructions, revealing significant deficiencies in the instruction-following\nability. Code and Benchmark: https://github.com/mianzhang/LogicIF"
                },
                "authors": [
                    {
                        "name": "Mian Zhang"
                    },
                    {
                        "name": "Shujian Liu"
                    },
                    {
                        "name": "Sixun Dong"
                    },
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Yebowen Hu"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Steven Ma"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Sathish Reddy Indurthi"
                    },
                    {
                        "name": "Haoyun Deng"
                    },
                    {
                        "name": "Zhiyu Zoey Chen"
                    },
                    {
                        "name": "Kaiqiang Song"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqiang Song"
                },
                "author": "Kaiqiang Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13079v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13079v2",
                "updated": "2025-08-12T17:53:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    53,
                    18,
                    1,
                    224,
                    0
                ],
                "published": "2025-04-17T16:46:11Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    16,
                    46,
                    11,
                    3,
                    107,
                    0
                ],
                "title": "Retrieval-Augmented Generation with Conflicting Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation with Conflicting Evidence"
                },
                "summary": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "COLM 2025, Data and Code: https://github.com/HanNight/RAMDocs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13079v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13079v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09124v1",
                "updated": "2025-08-12T17:53:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    53,
                    3,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:53:03Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    53,
                    3,
                    1,
                    224,
                    0
                ],
                "title": "OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office\n  Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office\n  Application Workflows"
                },
                "summary": "Autonomous agents powered by large language models (LLMs) are increasingly\ndeployed in real-world applications requiring complex, long-horizon workflows.\nHowever, existing benchmarks predominantly focus on atomic tasks that are\nself-contained and independent, failing to capture the long-term contextual\ndependencies and multi-interaction coordination required in realistic\nscenarios. To address this gap, we introduce OdysseyBench, a comprehensive\nbenchmark for evaluating LLM agents on long-horizon workflows across diverse\noffice applications including Word, Excel, PDF, Email, and Calendar. Our\nbenchmark comprises two complementary splits: OdysseyBench+ with 300 tasks\nderived from real-world use cases, and OdysseyBench-Neo with 302 newly\nsynthesized complex tasks. Each task requires agent to identify essential\ninformation from long-horizon interaction histories and perform multi-step\nreasoning across various applications. To enable scalable benchmark creation,\nwe propose HomerAgents, a multi-agent framework that automates the generation\nof long-horizon workflow benchmarks through systematic environment exploration,\ntask generation, and dialogue synthesis. Our extensive evaluation demonstrates\nthat OdysseyBench effectively challenges state-of-the-art LLM agents, providing\nmore accurate assessment of their capabilities in complex, real-world contexts\ncompared to existing atomic task benchmarks. We believe that OdysseyBench will\nserve as a valuable resource for advancing the development and evaluation of\nLLM agents in real-world productivity scenarios. In addition, we release\nOdysseyBench and HomerAgents to foster research along this line.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents powered by large language models (LLMs) are increasingly\ndeployed in real-world applications requiring complex, long-horizon workflows.\nHowever, existing benchmarks predominantly focus on atomic tasks that are\nself-contained and independent, failing to capture the long-term contextual\ndependencies and multi-interaction coordination required in realistic\nscenarios. To address this gap, we introduce OdysseyBench, a comprehensive\nbenchmark for evaluating LLM agents on long-horizon workflows across diverse\noffice applications including Word, Excel, PDF, Email, and Calendar. Our\nbenchmark comprises two complementary splits: OdysseyBench+ with 300 tasks\nderived from real-world use cases, and OdysseyBench-Neo with 302 newly\nsynthesized complex tasks. Each task requires agent to identify essential\ninformation from long-horizon interaction histories and perform multi-step\nreasoning across various applications. To enable scalable benchmark creation,\nwe propose HomerAgents, a multi-agent framework that automates the generation\nof long-horizon workflow benchmarks through systematic environment exploration,\ntask generation, and dialogue synthesis. Our extensive evaluation demonstrates\nthat OdysseyBench effectively challenges state-of-the-art LLM agents, providing\nmore accurate assessment of their capabilities in complex, real-world contexts\ncompared to existing atomic task benchmarks. We believe that OdysseyBench will\nserve as a valuable resource for advancing the development and evaluation of\nLLM agents in real-world productivity scenarios. In addition, we release\nOdysseyBench and HomerAgents to foster research along this line."
                },
                "authors": [
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Dongge Han"
                    },
                    {
                        "name": "Daniel Madrigal Diaz"
                    },
                    {
                        "name": "Jin Xu"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09117v1",
                "updated": "2025-08-12T17:50:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    50,
                    49,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:50:49Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    50,
                    49,
                    1,
                    224,
                    0
                ],
                "title": "Spectral Efficiency Considerations for 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral Efficiency Considerations for 6G"
                },
                "summary": "As wireless connectivity continues to evolve towards 6G, there is an\never-increasing demand to not only deliver higher throughput, lower latency,\nand improved reliability, but also do so as efficiently as possible. To this\npoint, the term efficiency has been quantified through applications to Spectral\nEfficiency (SE) and Energy Efficiency (EE). In this paper we introduce a new\nsystem metric called Radio Resource Utilization Efficiency (RUE). This metric\nquantifies the efficiency of the available radio resources (Spectrum, Access\nMethod, Time Slots, Data Symbols, etc.) used to deliver future 6G demands. We\ncompare the system performance of Typical Cellular and Cell-Free Massive MIMO\ndeployments as a vehicle to demonstrate the need for this new metric. We begin\nby providing a concise treatment of items impacting SE by introducing three\ncategories: 5G Radio Resources, Practical Limitations (such as channel matrix\nrank deficiency) and Implementation Losses (SINR degradation). For the example\nRadio Access Technology configuration analyzed, we show 5G yields an RUE of 47%\n(revealing significant room for improvement when defining 6G). Practical\nlimitation assumptions are compared to 5G Multi-User MIMO (MU-MIMO)\nmeasurements conducted in a commercialized deployment. SE losses are\ncharacterized to offer guidance to advanced algorithms employing Machine\nLearning (ML) based techniques. We present the benefits of increasing the\ntransmission Bandwidth (BW) from 100MHz to 1.6GHz. We describe a Next\nGeneration RAN architecture that can support 6G and AI-RAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As wireless connectivity continues to evolve towards 6G, there is an\never-increasing demand to not only deliver higher throughput, lower latency,\nand improved reliability, but also do so as efficiently as possible. To this\npoint, the term efficiency has been quantified through applications to Spectral\nEfficiency (SE) and Energy Efficiency (EE). In this paper we introduce a new\nsystem metric called Radio Resource Utilization Efficiency (RUE). This metric\nquantifies the efficiency of the available radio resources (Spectrum, Access\nMethod, Time Slots, Data Symbols, etc.) used to deliver future 6G demands. We\ncompare the system performance of Typical Cellular and Cell-Free Massive MIMO\ndeployments as a vehicle to demonstrate the need for this new metric. We begin\nby providing a concise treatment of items impacting SE by introducing three\ncategories: 5G Radio Resources, Practical Limitations (such as channel matrix\nrank deficiency) and Implementation Losses (SINR degradation). For the example\nRadio Access Technology configuration analyzed, we show 5G yields an RUE of 47%\n(revealing significant room for improvement when defining 6G). Practical\nlimitation assumptions are compared to 5G Multi-User MIMO (MU-MIMO)\nmeasurements conducted in a commercialized deployment. SE losses are\ncharacterized to offer guidance to advanced algorithms employing Machine\nLearning (ML) based techniques. We present the benefits of increasing the\ntransmission Bandwidth (BW) from 100MHz to 1.6GHz. We describe a Next\nGeneration RAN architecture that can support 6G and AI-RAN."
                },
                "authors": [
                    {
                        "name": "Joseph Boccuzzi"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Boccuzzi"
                },
                "author": "Joseph Boccuzzi",
                "arxiv_comment": "13 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09115v1",
                "updated": "2025-08-12T17:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    49,
                    34,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:49:34Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    49,
                    34,
                    1,
                    224,
                    0
                ],
                "title": "SinLlama -- A Large Language Model for Sinhala",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SinLlama -- A Large Language Model for Sinhala"
                },
                "summary": "Low-resource languages such as Sinhala are often overlooked by open-source\nLarge Language Models (LLMs). In this research, we extend an existing\nmultilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM\ntokenizer with Sinhala specific vocabulary and perform continual pre-training\non a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This\nis the very first decoder-based open-source LLM with explicit Sinhala support.\nWhen SinLlama was instruction fine-tuned for three text classification tasks,\nit outperformed base and instruct variants of Llama-3-8B by a significant\nmargin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-resource languages such as Sinhala are often overlooked by open-source\nLarge Language Models (LLMs). In this research, we extend an existing\nmultilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM\ntokenizer with Sinhala specific vocabulary and perform continual pre-training\non a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This\nis the very first decoder-based open-source LLM with explicit Sinhala support.\nWhen SinLlama was instruction fine-tuned for three text classification tasks,\nit outperformed base and instruct variants of Llama-3-8B by a significant\nmargin."
                },
                "authors": [
                    {
                        "name": "H. W. K. Aravinda"
                    },
                    {
                        "name": "Rashad Sirajudeen"
                    },
                    {
                        "name": "Samith Karunathilake"
                    },
                    {
                        "name": "Nisansa de Silva"
                    },
                    {
                        "name": "Surangika Ranathunga"
                    },
                    {
                        "name": "Rishemjit Kaur"
                    }
                ],
                "author_detail": {
                    "name": "Rishemjit Kaur"
                },
                "author": "Rishemjit Kaur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04349v3",
                "updated": "2025-08-13T09:00:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    0,
                    5,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-06T11:42:47Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    42,
                    47,
                    2,
                    218,
                    0
                ],
                "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy\n  Entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy\n  Entropy"
                },
                "summary": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models."
                },
                "authors": [
                    {
                        "name": "Hongze Tan"
                    },
                    {
                        "name": "Jianfei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Pan"
                },
                "author": "Jianfei Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09109v1",
                "updated": "2025-08-12T17:39:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    39,
                    53,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:39:53Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    39,
                    53,
                    1,
                    224,
                    0
                ],
                "title": "Design and Commissioning of an LWA Swarm Station: The Long Wavelength\n  Array -- North Arm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Commissioning of an LWA Swarm Station: The Long Wavelength\n  Array -- North Arm"
                },
                "summary": "Modern radio interferometers are designed with increasingly sprawling\ngeographical footprints, offering enhanced sensitivity and resolution. However,\nmanaging such extensive facilities presents operational challenges that can\npotentially impede or delay scientific progress. One solution to such obstacles\nis the `swarm telescope' concept which enables collaborative use of individual\ntelescope systems, overseen by separate institutions, to create a more powerful\nand manageable facility. We present the design, construction, and commissioning\nof the Long Wavelength Array -- North Arm (LWA-NA) station, a prototype\n64-element LWA Swarm telescope. LWA-NA is a cost-efficient, rapidly deployable\nplatform for radio astronomy, and serves as a pathfinder for the larger LWA\nSwarm project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern radio interferometers are designed with increasingly sprawling\ngeographical footprints, offering enhanced sensitivity and resolution. However,\nmanaging such extensive facilities presents operational challenges that can\npotentially impede or delay scientific progress. One solution to such obstacles\nis the `swarm telescope' concept which enables collaborative use of individual\ntelescope systems, overseen by separate institutions, to create a more powerful\nand manageable facility. We present the design, construction, and commissioning\nof the Long Wavelength Array -- North Arm (LWA-NA) station, a prototype\n64-element LWA Swarm telescope. LWA-NA is a cost-efficient, rapidly deployable\nplatform for radio astronomy, and serves as a pathfinder for the larger LWA\nSwarm project."
                },
                "authors": [
                    {
                        "name": "C. A. Taylor"
                    },
                    {
                        "name": "J. Dowell"
                    },
                    {
                        "name": "G. B. Taylor"
                    },
                    {
                        "name": "K. S. Obenberger"
                    },
                    {
                        "name": "S. I. Chastain"
                    },
                    {
                        "name": "J. Verastegui"
                    },
                    {
                        "name": "L. E. Cordonnier"
                    },
                    {
                        "name": "P. Kumar"
                    },
                    {
                        "name": "E. Sheldahl"
                    },
                    {
                        "name": "S. Bruzewski"
                    },
                    {
                        "name": "T. Dolch"
                    },
                    {
                        "name": "C. A. Siders"
                    }
                ],
                "author_detail": {
                    "name": "C. A. Siders"
                },
                "author": "C. A. Siders",
                "arxiv_doi": "10.1142/S2251171725500035",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S2251171725500035",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09105v2",
                "updated": "2025-08-13T11:05:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    11,
                    5,
                    22,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T17:32:24Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    32,
                    24,
                    1,
                    224,
                    0
                ],
                "title": "SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG\n  Controlling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG\n  Controlling"
                },
                "summary": "Retrieval-Augmented Generation (RAG) and its Multimodal Retrieval-Augmented\nGeneration (MRAG) significantly improve the knowledge coverage and contextual\nunderstanding of Large Language Models (LLMs) by introducing external knowledge\nsources. However, retrieval and multimodal fusion obscure content provenance,\nrendering existing membership inference methods unable to reliably attribute\ngenerated outputs to pre-training, external retrieval, or user input, thus\nundermining privacy leakage accountability\n  To address these challenges, we propose the first Source-aware Membership\nAudit (SMA) that enables fine-grained source attribution of generated content\nin a semi-black-box setting with retrieval control capabilities. To address the\nenvironmental constraints of semi-black-box auditing, we further design an\nattribution estimation mechanism based on zero-order optimization, which\nrobustly approximates the true influence of input tokens on the output through\nlarge-scale perturbation sampling and ridge regression modeling. In addition,\nSMA introduces a cross-modal attribution technique that projects image inputs\ninto textual descriptions via MLLMs, enabling token-level attribution in the\ntext modality, which for the first time facilitates membership inference on\nimage retrieval traces in MRAG systems. This work shifts the focus of\nmembership inference from 'whether the data has been memorized' to 'where the\ncontent is sourced from', offering a novel perspective for auditing data\nprovenance in complex generative systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) and its Multimodal Retrieval-Augmented\nGeneration (MRAG) significantly improve the knowledge coverage and contextual\nunderstanding of Large Language Models (LLMs) by introducing external knowledge\nsources. However, retrieval and multimodal fusion obscure content provenance,\nrendering existing membership inference methods unable to reliably attribute\ngenerated outputs to pre-training, external retrieval, or user input, thus\nundermining privacy leakage accountability\n  To address these challenges, we propose the first Source-aware Membership\nAudit (SMA) that enables fine-grained source attribution of generated content\nin a semi-black-box setting with retrieval control capabilities. To address the\nenvironmental constraints of semi-black-box auditing, we further design an\nattribution estimation mechanism based on zero-order optimization, which\nrobustly approximates the true influence of input tokens on the output through\nlarge-scale perturbation sampling and ridge regression modeling. In addition,\nSMA introduces a cross-modal attribution technique that projects image inputs\ninto textual descriptions via MLLMs, enabling token-level attribution in the\ntext modality, which for the first time facilitates membership inference on\nimage retrieval traces in MRAG systems. This work shifts the focus of\nmembership inference from 'whether the data has been memorized' to 'where the\ncontent is sourced from', offering a novel perspective for auditing data\nprovenance in complex generative systems."
                },
                "authors": [
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Ruoyu Chen"
                    },
                    {
                        "name": "Jianjie Huang"
                    },
                    {
                        "name": "Jingzhi Li"
                    },
                    {
                        "name": "Xiaochun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaochun Cao"
                },
                "author": "Xiaochun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09101v1",
                "updated": "2025-08-12T17:29:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    29,
                    20,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:29:20Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    29,
                    20,
                    1,
                    224,
                    0
                ],
                "title": "AutoCodeBench: Large Language Models are Automatic Code Benchmark\n  Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoCodeBench: Large Language Models are Automatic Code Benchmark\n  Generators"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, with code generation emerging as a key area of focus. While\nnumerous benchmarks have been proposed to evaluate their code generation\nabilities, these benchmarks face several critical limitations. First, they\noften rely on manual annotations, which are time-consuming and difficult to\nscale across different programming languages and problem complexities. Second,\nmost existing benchmarks focus primarily on Python, while the few multilingual\nbenchmarks suffer from limited difficulty and uneven language distribution. To\naddress these challenges, we propose AutoCodeGen, an automated method for\ngenerating high-difficulty multilingual code generation datasets without manual\nannotations. AutoCodeGen ensures the correctness and completeness of test cases\nby generating test inputs with LLMs and obtaining test outputs through a\nmultilingual sandbox, while achieving high data quality through reverse-order\nproblem generation and multiple filtering steps. Using this novel method, we\nintroduce AutoCodeBench, a large-scale code generation benchmark comprising\n3,920 problems evenly distributed across 20 programming languages. It is\nspecifically designed to evaluate LLMs on challenging, diverse, and practical\nmultilingual tasks. We evaluate over 30 leading open-source and proprietary\nLLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The\nresults show that even the most advanced LLMs struggle with the complexity,\ndiversity, and multilingual nature of these tasks. Besides, we introduce\nAutoCodeBench-Complete, specifically designed for base models to assess their\nfew-shot code generation capabilities. We hope the AutoCodeBench series will\nserve as a valuable resource and inspire the community to focus on more\nchallenging and practical multilingual code generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, with code generation emerging as a key area of focus. While\nnumerous benchmarks have been proposed to evaluate their code generation\nabilities, these benchmarks face several critical limitations. First, they\noften rely on manual annotations, which are time-consuming and difficult to\nscale across different programming languages and problem complexities. Second,\nmost existing benchmarks focus primarily on Python, while the few multilingual\nbenchmarks suffer from limited difficulty and uneven language distribution. To\naddress these challenges, we propose AutoCodeGen, an automated method for\ngenerating high-difficulty multilingual code generation datasets without manual\nannotations. AutoCodeGen ensures the correctness and completeness of test cases\nby generating test inputs with LLMs and obtaining test outputs through a\nmultilingual sandbox, while achieving high data quality through reverse-order\nproblem generation and multiple filtering steps. Using this novel method, we\nintroduce AutoCodeBench, a large-scale code generation benchmark comprising\n3,920 problems evenly distributed across 20 programming languages. It is\nspecifically designed to evaluate LLMs on challenging, diverse, and practical\nmultilingual tasks. We evaluate over 30 leading open-source and proprietary\nLLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The\nresults show that even the most advanced LLMs struggle with the complexity,\ndiversity, and multilingual nature of these tasks. Besides, we introduce\nAutoCodeBench-Complete, specifically designed for base models to assess their\nfew-shot code generation capabilities. We hope the AutoCodeBench series will\nserve as a valuable resource and inspire the community to focus on more\nchallenging and practical multilingual code generation scenarios."
                },
                "authors": [
                    {
                        "name": "Jason Chou"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Zhiying Zeng"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Lingyun Tan"
                    },
                    {
                        "name": "Ziyan Xu"
                    },
                    {
                        "name": "Bohui Zhai"
                    },
                    {
                        "name": "Hengyi Liu"
                    },
                    {
                        "name": "Speed Zhu"
                    },
                    {
                        "name": "Wiggin Zhou"
                    },
                    {
                        "name": "Fengzong Lian"
                    }
                ],
                "author_detail": {
                    "name": "Fengzong Lian"
                },
                "author": "Fengzong Lian",
                "arxiv_comment": "Homepage: https://autocodebench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05452v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05452v2",
                "updated": "2025-08-12T17:23:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    23,
                    33,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-07T14:46:30Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    46,
                    30,
                    3,
                    219,
                    0
                ],
                "title": "LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair\n  Evaluation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair\n  Evaluation of Large Language Models"
                },
                "summary": "Existing evaluation of Large Language Models (LLMs) on static benchmarks is\nvulnerable to data contamination and leaderboard overfitting, critical issues\nthat obscure true model capabilities. To address this, we introduce LLMEval-3,\na framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary\nbank of 220k graduate-level questions, from which it dynamically samples unseen\ntest sets for each evaluation run. Its automated pipeline ensures integrity via\ncontamination-resistant data curation, a novel anti-cheating architecture, and\na calibrated LLM-as-a-judge process achieving 90% agreement with human experts,\ncomplemented by a relative ranking system for fair comparison. An 20-month\nlongitudinal study of nearly 50 leading models reveals a performance ceiling on\nknowledge memorization and exposes data contamination vulnerabilities\nundetectable by static benchmarks. The framework demonstrates exceptional\nrobustness in ranking stability and consistency, providing strong empirical\nvalidation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and\ncredible methodology for assessing the true capabilities of LLMs beyond\nleaderboard scores, promoting the development of more trustworthy evaluation\nstandards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing evaluation of Large Language Models (LLMs) on static benchmarks is\nvulnerable to data contamination and leaderboard overfitting, critical issues\nthat obscure true model capabilities. To address this, we introduce LLMEval-3,\na framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary\nbank of 220k graduate-level questions, from which it dynamically samples unseen\ntest sets for each evaluation run. Its automated pipeline ensures integrity via\ncontamination-resistant data curation, a novel anti-cheating architecture, and\na calibrated LLM-as-a-judge process achieving 90% agreement with human experts,\ncomplemented by a relative ranking system for fair comparison. An 20-month\nlongitudinal study of nearly 50 leading models reveals a performance ceiling on\nknowledge memorization and exposes data contamination vulnerabilities\nundetectable by static benchmarks. The framework demonstrates exceptional\nrobustness in ranking stability and consistency, providing strong empirical\nvalidation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and\ncredible methodology for assessing the true capabilities of LLMs beyond\nleaderboard scores, promoting the development of more trustworthy evaluation\nstandards."
                },
                "authors": [
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Yujiong Shen"
                    },
                    {
                        "name": "Jingyi Deng"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Junzhe Wang"
                    },
                    {
                        "name": "Shichun Liu"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Huayu Sha"
                    },
                    {
                        "name": "Qiyuan Peng"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Yilong Wu"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Mingqi Wu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Mingxu Chai"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Zhihui Fei"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Mingyang Wan"
                    },
                    {
                        "name": "Guojun Ma"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05452v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05452v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09093v1",
                "updated": "2025-08-12T17:17:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    17,
                    51,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:17:51Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    17,
                    51,
                    1,
                    224,
                    0
                ],
                "title": "Scaling Up Active Testing to Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up Active Testing to Large Language Models"
                },
                "summary": "Active testing enables label-efficient evaluation of models through careful\ndata acquisition. However, its significant computational costs have previously\nundermined its use for large models. We show how it can be successfully scaled\nup to the evaluation of large language models (LLMs). In particular we show\nthat the surrogate model used to guide data acquisition can be constructed\ncheaply using in-context learning, does not require updating within an\nactive-testing loop, and can be smaller than the target model. We even find we\ncan make good data-acquisition decisions without computing predictions with the\ntarget model and further introduce a single-run error estimator to asses how\nwell active testing is working on the fly. We find that our approach is able to\nmore effectively evaluate LLM performance with less data than current standard\npractices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active testing enables label-efficient evaluation of models through careful\ndata acquisition. However, its significant computational costs have previously\nundermined its use for large models. We show how it can be successfully scaled\nup to the evaluation of large language models (LLMs). In particular we show\nthat the surrogate model used to guide data acquisition can be constructed\ncheaply using in-context learning, does not require updating within an\nactive-testing loop, and can be smaller than the target model. We even find we\ncan make good data-acquisition decisions without computing predictions with the\ntarget model and further introduce a single-run error estimator to asses how\nwell active testing is working on the fly. We find that our approach is able to\nmore effectively evaluate LLM performance with less data than current standard\npractices."
                },
                "authors": [
                    {
                        "name": "Gabrielle Berrada"
                    },
                    {
                        "name": "Jannik Kossen"
                    },
                    {
                        "name": "Muhammed Razzak"
                    },
                    {
                        "name": "Freddie Bickford Smith"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Tom Rainforth"
                    }
                ],
                "author_detail": {
                    "name": "Tom Rainforth"
                },
                "author": "Tom Rainforth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09091v1",
                "updated": "2025-08-12T17:17:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    17,
                    13,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T17:17:13Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    17,
                    13,
                    1,
                    224,
                    0
                ],
                "title": "Utilizing Multilingual Encoders to Improve Large Language Models for\n  Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Multilingual Encoders to Improve Large Language Models for\n  Low-Resource Languages"
                },
                "summary": "Large Language Models (LLMs) excel in English, but their performance degrades\nsignificantly on low-resource languages (LRLs) due to English-centric training.\nWhile methods like LangBridge align LLMs with multilingual encoders such as the\nMassively Multilingual Text-to-Text Transfer Transformer (mT5), they typically\nuse only the final encoder layer. We propose a novel architecture that fuses\nall intermediate layers, enriching the linguistic information passed to the\nLLM. Our approach features two strategies: (1) a Global Softmax weighting for\noverall layer importance, and (2) a Transformer Softmax model that learns\ntoken-specific weights. The fused representations are mapped into the LLM's\nembedding space, enabling it to process multilingual inputs. The model is\ntrained only on English data, without using any parallel or multilingual data.\nEvaluated on XNLI, IndicXNLI, Sinhala News Classification, and Amazon Reviews,\nour Transformer Softmax model significantly outperforms the LangBridge\nbaseline. We observe strong performance gains in LRLs, improving Sinhala\nclassification accuracy from 71.66% to 75.86% and achieving clear improvements\nacross Indic languages such as Tamil, Bengali, and Malayalam. These specific\ngains contribute to an overall boost in average XNLI accuracy from 70.36% to\n71.50%. This approach offers a scalable, data-efficient path toward more\ncapable and equitable multilingual LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in English, but their performance degrades\nsignificantly on low-resource languages (LRLs) due to English-centric training.\nWhile methods like LangBridge align LLMs with multilingual encoders such as the\nMassively Multilingual Text-to-Text Transfer Transformer (mT5), they typically\nuse only the final encoder layer. We propose a novel architecture that fuses\nall intermediate layers, enriching the linguistic information passed to the\nLLM. Our approach features two strategies: (1) a Global Softmax weighting for\noverall layer importance, and (2) a Transformer Softmax model that learns\ntoken-specific weights. The fused representations are mapped into the LLM's\nembedding space, enabling it to process multilingual inputs. The model is\ntrained only on English data, without using any parallel or multilingual data.\nEvaluated on XNLI, IndicXNLI, Sinhala News Classification, and Amazon Reviews,\nour Transformer Softmax model significantly outperforms the LangBridge\nbaseline. We observe strong performance gains in LRLs, improving Sinhala\nclassification accuracy from 71.66% to 75.86% and achieving clear improvements\nacross Indic languages such as Tamil, Bengali, and Malayalam. These specific\ngains contribute to an overall boost in average XNLI accuracy from 70.36% to\n71.50%. This approach offers a scalable, data-efficient path toward more\ncapable and equitable multilingual LLMs."
                },
                "authors": [
                    {
                        "name": "Imalsha Puranegedara"
                    },
                    {
                        "name": "Themira Chathumina"
                    },
                    {
                        "name": "Nisal Ranathunga"
                    },
                    {
                        "name": "Nisansa de Silva"
                    },
                    {
                        "name": "Surangika Ranathunga"
                    },
                    {
                        "name": "Mokanarangan Thayaparan"
                    }
                ],
                "author_detail": {
                    "name": "Mokanarangan Thayaparan"
                },
                "author": "Mokanarangan Thayaparan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08577v2",
                "updated": "2025-08-12T17:02:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    17,
                    2,
                    45,
                    1,
                    224,
                    0
                ],
                "published": "2025-02-12T17:10:53Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    10,
                    53,
                    2,
                    43,
                    0
                ],
                "title": "FBFL: A Field-Based Coordination Approach for Data Heterogeneity in\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FBFL: A Field-Based Coordination Approach for Data Heterogeneity in\n  Federated Learning"
                },
                "summary": "In the last years, Federated learning (FL) has become a popular solution to\ntrain machine learning models in domains with high privacy concerns. However,\nFL scalability and performance face significant challenges in real-world\ndeployments where data across devices are non-independently and identically\ndistributed (non-IID). The heterogeneity in data distribution frequently arises\nfrom spatial distribution of devices, leading to degraded model performance in\nthe absence of proper handling. Additionally, FL typical reliance on\ncentralized architectures introduces bottlenecks and single-point-of-failure\nrisks, particularly problematic at scale or in dynamic environments. To close\nthis gap, we propose Field-Based Federated Learning (FBFL), a novel approach\nleveraging macroprogramming and field coordination to address these limitations\nthrough: (i) distributed spatial-based leader election for personalization to\nmitigate non-IID data challenges; and (ii) construction of a self-organizing,\nhierarchical architecture using advanced macroprogramming patterns. Moreover,\nFBFL not only overcomes the aforementioned limitations, but also enables the\ndevelopment of more specialized models tailored to the specific data\ndistribution in each subregion. This paper formalizes FBFL and evaluates it\nextensively using MNIST, FashionMNIST, and Extended MNIST datasets. We\ndemonstrate that, when operating under IID data conditions, FBFL performs\ncomparably to the widely-used FedAvg algorithm. Furthermore, in challenging\nnon-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other\nstate-of-the-art methods, namely FedProx and Scaffold, which have been\nspecifically designed to address non-IID data distributions. Additionally, we\nshowcase the resilience of FBFL's self-organizing hierarchical architecture\nagainst server failures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last years, Federated learning (FL) has become a popular solution to\ntrain machine learning models in domains with high privacy concerns. However,\nFL scalability and performance face significant challenges in real-world\ndeployments where data across devices are non-independently and identically\ndistributed (non-IID). The heterogeneity in data distribution frequently arises\nfrom spatial distribution of devices, leading to degraded model performance in\nthe absence of proper handling. Additionally, FL typical reliance on\ncentralized architectures introduces bottlenecks and single-point-of-failure\nrisks, particularly problematic at scale or in dynamic environments. To close\nthis gap, we propose Field-Based Federated Learning (FBFL), a novel approach\nleveraging macroprogramming and field coordination to address these limitations\nthrough: (i) distributed spatial-based leader election for personalization to\nmitigate non-IID data challenges; and (ii) construction of a self-organizing,\nhierarchical architecture using advanced macroprogramming patterns. Moreover,\nFBFL not only overcomes the aforementioned limitations, but also enables the\ndevelopment of more specialized models tailored to the specific data\ndistribution in each subregion. This paper formalizes FBFL and evaluates it\nextensively using MNIST, FashionMNIST, and Extended MNIST datasets. We\ndemonstrate that, when operating under IID data conditions, FBFL performs\ncomparably to the widely-used FedAvg algorithm. Furthermore, in challenging\nnon-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other\nstate-of-the-art methods, namely FedProx and Scaffold, which have been\nspecifically designed to address non-IID data distributions. Additionally, we\nshowcase the resilience of FBFL's self-organizing hierarchical architecture\nagainst server failures."
                },
                "authors": [
                    {
                        "name": "Davide Domini"
                    },
                    {
                        "name": "Gianluca Aguzzi"
                    },
                    {
                        "name": "Lukas Esterle"
                    },
                    {
                        "name": "Mirko Viroli"
                    }
                ],
                "author_detail": {
                    "name": "Mirko Viroli"
                },
                "author": "Mirko Viroli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09075v1",
                "updated": "2025-08-12T16:50:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    50,
                    2,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:50:02Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    50,
                    2,
                    1,
                    224,
                    0
                ],
                "title": "Scaling Learned Image Compression Models up to 1 Billion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Learned Image Compression Models up to 1 Billion"
                },
                "summary": "Recent advances in large language models (LLMs) highlight a strong connection\nbetween intelligence and compression. Learned image compression, a fundamental\ntask in modern data compression, has made significant progress in recent years.\nHowever, current models remain limited in scale, restricting their\nrepresentation capacity, and how scaling model size influences compression\nperformance remains unexplored. In this work, we present a pioneering study on\nscaling up learned image compression models and revealing the performance\ntrends through scaling laws. Using the recent state-of-the-art HPCM model as\nbaseline, we scale model parameters from 68.5 millions to 1 billion and fit\npower-law relations between test loss and key scaling variables, including\nmodel size and optimal training compute. The results reveal a scaling trend,\nenabling extrapolation to larger scale models. Experimental results demonstrate\nthat the scaled-up HPCM-1B model achieves state-of-the-art rate-distortion\nperformance. We hope this work inspires future exploration of large-scale\ncompression models and deeper investigations into the connection between\ncompression and intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) highlight a strong connection\nbetween intelligence and compression. Learned image compression, a fundamental\ntask in modern data compression, has made significant progress in recent years.\nHowever, current models remain limited in scale, restricting their\nrepresentation capacity, and how scaling model size influences compression\nperformance remains unexplored. In this work, we present a pioneering study on\nscaling up learned image compression models and revealing the performance\ntrends through scaling laws. Using the recent state-of-the-art HPCM model as\nbaseline, we scale model parameters from 68.5 millions to 1 billion and fit\npower-law relations between test loss and key scaling variables, including\nmodel size and optimal training compute. The results reveal a scaling trend,\nenabling extrapolation to larger scale models. Experimental results demonstrate\nthat the scaled-up HPCM-1B model achieves state-of-the-art rate-distortion\nperformance. We hope this work inspires future exploration of large-scale\ncompression models and deeper investigations into the connection between\ncompression and intelligence."
                },
                "authors": [
                    {
                        "name": "Yuqi Li"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v1",
                "updated": "2025-08-12T16:47:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Yi Fei"
                    },
                    {
                        "name": "Zeng Weidi"
                    }
                ],
                "author_detail": {
                    "name": "Zeng Weidi"
                },
                "author": "Zeng Weidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04903v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04903v3",
                "updated": "2025-08-12T16:29:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    29,
                    5,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-06T21:59:34Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    21,
                    59,
                    34,
                    2,
                    218,
                    0
                ],
                "title": "RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM\n  Systems with Structured Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM\n  Systems with Structured Memory"
                },
                "summary": "Multi-agent large language model (LLM) systems have shown strong potential in\ncomplex reasoning and collaborative decision-making tasks. However, most\nexisting coordination schemes rely on static or full-context routing\nstrategies, which lead to excessive token consumption, redundant memory\nexposure, and limited adaptability across interaction rounds. We introduce\nRCR-Router, a modular and role-aware context routing framework designed to\nenable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,\nthis is the first routing approach that dynamically selects semantically\nrelevant memory subsets for each agent based on its role and task stage, while\nadhering to a strict token budget. A lightweight scoring policy guides memory\nselection, and agent outputs are iteratively integrated into a shared memory\nstore to facilitate progressive context refinement. To better evaluate model\nbehavior, we further propose an Answer Quality Score metric that captures\nLLM-generated explanations beyond standard QA accuracy. Experiments on three\nmulti-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate\nthat RCR-Router reduces token usage (up to 30%) while improving or maintaining\nanswer quality. These results highlight the importance of structured memory\nrouting and output-aware evaluation in advancing scalable multi-agent LLM\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent large language model (LLM) systems have shown strong potential in\ncomplex reasoning and collaborative decision-making tasks. However, most\nexisting coordination schemes rely on static or full-context routing\nstrategies, which lead to excessive token consumption, redundant memory\nexposure, and limited adaptability across interaction rounds. We introduce\nRCR-Router, a modular and role-aware context routing framework designed to\nenable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,\nthis is the first routing approach that dynamically selects semantically\nrelevant memory subsets for each agent based on its role and task stage, while\nadhering to a strict token budget. A lightweight scoring policy guides memory\nselection, and agent outputs are iteratively integrated into a shared memory\nstore to facilitate progressive context refinement. To better evaluate model\nbehavior, we further propose an Answer Quality Score metric that captures\nLLM-generated explanations beyond standard QA accuracy. Experiments on three\nmulti-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate\nthat RCR-Router reduces token usage (up to 30%) while improving or maintaining\nanswer quality. These results highlight the importance of structured memory\nrouting and output-aware evaluation in advancing scalable multi-agent LLM\nsystems."
                },
                "authors": [
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zhenglun Kong"
                    },
                    {
                        "name": "Changdi Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Tianqi Li"
                    },
                    {
                        "name": "Peiyan Dong"
                    },
                    {
                        "name": "Joannah Nanjekye"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Geng Yuan"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Wenbin Zhang"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Xue Lin"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Yanzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhi Wang"
                },
                "author": "Yanzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04903v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04903v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09056v1",
                "updated": "2025-08-12T16:16:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    16,
                    29,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:16:29Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    16,
                    29,
                    1,
                    224,
                    0
                ],
                "title": "FetFIDS: A Feature Embedding Attention based Federated Network Intrusion\n  Detection Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FetFIDS: A Feature Embedding Attention based Federated Network Intrusion\n  Detection Algorithm"
                },
                "summary": "Intrusion Detection Systems (IDS) have an increasingly important role in\npreventing exploitation of network vulnerabilities by malicious actors. Recent\ndeep learning based developments have resulted in significant improvements in\nthe performance of IDS systems. In this paper, we present FetFIDS, where we\nexplore the employment of feature embedding instead of positional embedding to\nimprove intrusion detection performance of a transformer based deep learning\nsystem. Our model is developed with the aim of deployments in edge learning\nscenarios, where federated learning over multiple communication rounds can\nensure both privacy and localized performance improvements. FetFIDS outperforms\nmultiple state-of-the-art intrusion detection systems in a federated\nenvironment and demonstrates a high degree of suitability to federated\nlearning. The code for this work can be found at\nhttps://github.com/ghosh64/fetfids.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrusion Detection Systems (IDS) have an increasingly important role in\npreventing exploitation of network vulnerabilities by malicious actors. Recent\ndeep learning based developments have resulted in significant improvements in\nthe performance of IDS systems. In this paper, we present FetFIDS, where we\nexplore the employment of feature embedding instead of positional embedding to\nimprove intrusion detection performance of a transformer based deep learning\nsystem. Our model is developed with the aim of deployments in edge learning\nscenarios, where federated learning over multiple communication rounds can\nensure both privacy and localized performance improvements. FetFIDS outperforms\nmultiple state-of-the-art intrusion detection systems in a federated\nenvironment and demonstrates a high degree of suitability to federated\nlearning. The code for this work can be found at\nhttps://github.com/ghosh64/fetfids."
                },
                "authors": [
                    {
                        "name": "Shreya Ghosh"
                    },
                    {
                        "name": "Abu Shafin Mohammad Mahdee Jameel"
                    },
                    {
                        "name": "Aly El Gamal"
                    }
                ],
                "author_detail": {
                    "name": "Aly El Gamal"
                },
                "author": "Aly El Gamal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09054v1",
                "updated": "2025-08-12T16:13:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    13,
                    51,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:13:51Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    13,
                    51,
                    1,
                    224,
                    0
                ],
                "title": "CVCM Track Circuits Pre-emptive Failure Diagnostics for Predictive\n  Maintenance Using Deep Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVCM Track Circuits Pre-emptive Failure Diagnostics for Predictive\n  Maintenance Using Deep Neural Networks"
                },
                "summary": "Track circuits are critical for railway operations, acting as the main\nsignalling sub-system to locate trains. Continuous Variable Current Modulation\n(CVCM) is one such technology. Like any field-deployed, safety-critical asset,\nit can fail, triggering cascading disruptions. Many failures originate as\nsubtle anomalies that evolve over time, often not visually apparent in\nmonitored signals. Conventional approaches, which rely on clear signal changes,\nstruggle to detect them early. Early identification of failure types is\nessential to improve maintenance planning, minimising downtime and revenue\nloss. Leveraging deep neural networks, we propose a predictive maintenance\nframework that classifies anomalies well before they escalate into failures.\nValidated on 10 CVCM failure cases across different installations, the method\nis ISO-17359 compliant and outperforms conventional techniques, achieving\n99.31% overall accuracy with detection within 1% of anomaly onset. Through\nconformal prediction, we provide uncertainty estimates, reaching 99% confidence\nwith consistent coverage across classes. Given CVCMs global deployment, the\napproach is scalable and adaptable to other track circuits and railway systems,\nenhancing operational reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Track circuits are critical for railway operations, acting as the main\nsignalling sub-system to locate trains. Continuous Variable Current Modulation\n(CVCM) is one such technology. Like any field-deployed, safety-critical asset,\nit can fail, triggering cascading disruptions. Many failures originate as\nsubtle anomalies that evolve over time, often not visually apparent in\nmonitored signals. Conventional approaches, which rely on clear signal changes,\nstruggle to detect them early. Early identification of failure types is\nessential to improve maintenance planning, minimising downtime and revenue\nloss. Leveraging deep neural networks, we propose a predictive maintenance\nframework that classifies anomalies well before they escalate into failures.\nValidated on 10 CVCM failure cases across different installations, the method\nis ISO-17359 compliant and outperforms conventional techniques, achieving\n99.31% overall accuracy with detection within 1% of anomaly onset. Through\nconformal prediction, we provide uncertainty estimates, reaching 99% confidence\nwith consistent coverage across classes. Given CVCMs global deployment, the\napproach is scalable and adaptable to other track circuits and railway systems,\nenhancing operational reliability."
                },
                "authors": [
                    {
                        "name": "Debdeep Mukherjee"
                    },
                    {
                        "name": "Eduardo Di Santi"
                    },
                    {
                        "name": "Clément Lefebvre"
                    },
                    {
                        "name": "Nenad Mijatovic"
                    },
                    {
                        "name": "Victor Martin"
                    },
                    {
                        "name": "Thierry Josse"
                    },
                    {
                        "name": "Jonathan Brown"
                    },
                    {
                        "name": "Kenza Saiah"
                    }
                ],
                "author_detail": {
                    "name": "Kenza Saiah"
                },
                "arxiv_affiliation": "Digital and Integrated Systems, Alstom",
                "author": "Kenza Saiah",
                "arxiv_comment": "Peer-reviewed conference paper. Presented at ICROMA 2025\n  (International Conference on Railway Operations Modelling and Analysis),\n  Dresden, Germany. https://tu-dresden.de/raildresden2025 8 pages, 6 figures, 1\n  table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.5.1; I.5.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09042v1",
                "updated": "2025-08-12T16:03:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    3,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:03:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    3,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "LLM-as-a-Supervisor: Mistaken Therapeutic Behaviors Trigger Targeted\n  Supervisory Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Supervisor: Mistaken Therapeutic Behaviors Trigger Targeted\n  Supervisory Feedback"
                },
                "summary": "Although large language models (LLMs) hold significant promise in\npsychotherapy, their direct application in patient-facing scenarios raises\nethical and safety concerns. Therefore, this work shifts towards developing an\nLLM as a supervisor to train real therapists. In addition to the privacy of\nclinical therapist training data, a fundamental contradiction complicates the\ntraining of therapeutic behaviors: clear feedback standards are necessary to\nensure a controlled training system, yet there is no absolute \"gold standard\"\nfor appropriate therapeutic behaviors in practice. In contrast, many common\ntherapeutic mistakes are universal and identifiable, making them effective\ntriggers for targeted feedback that can serve as clearer evidence. Motivated by\nthis, we create a novel therapist-training paradigm: (1) guidelines for\nmistaken behaviors and targeted correction strategies are first established as\nstandards; (2) a human-in-the-loop dialogue-feedback dataset is then\nconstructed, where a mistake-prone agent intentionally makes standard mistakes\nduring interviews naturally, and a supervisor agent locates and identifies\nmistakes and provides targeted feedback; (3) after fine-tuning on this dataset,\nthe final supervisor model is provided for real therapist training. The\ndetailed experimental results of automated, human and downstream assessments\ndemonstrate that models fine-tuned on our dataset MATE, can provide\nhigh-quality feedback according to the clinical guideline, showing significant\npotential for the therapist training scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) hold significant promise in\npsychotherapy, their direct application in patient-facing scenarios raises\nethical and safety concerns. Therefore, this work shifts towards developing an\nLLM as a supervisor to train real therapists. In addition to the privacy of\nclinical therapist training data, a fundamental contradiction complicates the\ntraining of therapeutic behaviors: clear feedback standards are necessary to\nensure a controlled training system, yet there is no absolute \"gold standard\"\nfor appropriate therapeutic behaviors in practice. In contrast, many common\ntherapeutic mistakes are universal and identifiable, making them effective\ntriggers for targeted feedback that can serve as clearer evidence. Motivated by\nthis, we create a novel therapist-training paradigm: (1) guidelines for\nmistaken behaviors and targeted correction strategies are first established as\nstandards; (2) a human-in-the-loop dialogue-feedback dataset is then\nconstructed, where a mistake-prone agent intentionally makes standard mistakes\nduring interviews naturally, and a supervisor agent locates and identifies\nmistakes and provides targeted feedback; (3) after fine-tuning on this dataset,\nthe final supervisor model is provided for real therapist training. The\ndetailed experimental results of automated, human and downstream assessments\ndemonstrate that models fine-tuned on our dataset MATE, can provide\nhigh-quality feedback according to the clinical guideline, showing significant\npotential for the therapist training scenario."
                },
                "authors": [
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Zhenyu Lv"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Xianyang Wang"
                    },
                    {
                        "name": "Luyao Ji"
                    },
                    {
                        "name": "Leyang Cui"
                    },
                    {
                        "name": "Minqiang Yang"
                    },
                    {
                        "name": "Jian Shen"
                    },
                    {
                        "name": "Qunxi Dong"
                    },
                    {
                        "name": "Xiuling Liu"
                    },
                    {
                        "name": "Juan Wang"
                    },
                    {
                        "name": "Bin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Hu"
                },
                "author": "Bin Hu",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01821v3",
                "updated": "2025-08-12T16:02:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    2,
                    52,
                    1,
                    224,
                    0
                ],
                "published": "2025-05-03T13:55:38Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    13,
                    55,
                    38,
                    5,
                    123,
                    0
                ],
                "title": "Edge-Cloud Collaborative Computing on Distributed Intelligence and Model\n  Optimization: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-Cloud Collaborative Computing on Distributed Intelligence and Model\n  Optimization: A Survey"
                },
                "summary": "Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm\nfor addressing the computational demands of modern intelligent applications,\nintegrating cloud resources with edge devices to enable efficient, low-latency\nprocessing. Recent advancements in AI, particularly deep learning and large\nlanguage models (LLMs), have dramatically enhanced the capabilities of these\ndistributed systems, yet introduce significant challenges in model deployment\nand resource management. In this survey, we comprehensive examine the\nintersection of distributed intelligence and model optimization within\nedge-cloud environments, providing a structured tutorial on fundamental\narchitectures, enabling technologies, and emerging applications. Additionally,\nwe systematically analyze model optimization approaches, including compression,\nadaptation, and neural architecture search, alongside AI-driven resource\nmanagement strategies that balance performance, energy efficiency, and latency\nrequirements. We further explore critical aspects of privacy protection and\nsecurity enhancement within ECCC systems and examines practical deployments\nthrough diverse applications, spanning autonomous driving, healthcare, and\nindustrial automation. Performance analysis and benchmarking techniques are\nalso thoroughly explored to establish evaluation standards for these complex\nsystems. Furthermore, the review identifies critical research directions\nincluding LLMs deployment, 6G integration, neuromorphic computing, and quantum\ncomputing, offering a roadmap for addressing persistent challenges in\nheterogeneity management, real-time processing, and scalability. By bridging\ntheoretical advancements and practical deployments, this survey offers\nresearchers and practitioners a holistic perspective on leveraging AI to\noptimize distributed computing environments, fostering innovation in\nnext-generation intelligent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm\nfor addressing the computational demands of modern intelligent applications,\nintegrating cloud resources with edge devices to enable efficient, low-latency\nprocessing. Recent advancements in AI, particularly deep learning and large\nlanguage models (LLMs), have dramatically enhanced the capabilities of these\ndistributed systems, yet introduce significant challenges in model deployment\nand resource management. In this survey, we comprehensive examine the\nintersection of distributed intelligence and model optimization within\nedge-cloud environments, providing a structured tutorial on fundamental\narchitectures, enabling technologies, and emerging applications. Additionally,\nwe systematically analyze model optimization approaches, including compression,\nadaptation, and neural architecture search, alongside AI-driven resource\nmanagement strategies that balance performance, energy efficiency, and latency\nrequirements. We further explore critical aspects of privacy protection and\nsecurity enhancement within ECCC systems and examines practical deployments\nthrough diverse applications, spanning autonomous driving, healthcare, and\nindustrial automation. Performance analysis and benchmarking techniques are\nalso thoroughly explored to establish evaluation standards for these complex\nsystems. Furthermore, the review identifies critical research directions\nincluding LLMs deployment, 6G integration, neuromorphic computing, and quantum\ncomputing, offering a roadmap for addressing persistent challenges in\nheterogeneity management, real-time processing, and scalability. By bridging\ntheoretical advancements and practical deployments, this survey offers\nresearchers and practitioners a holistic perspective on leveraging AI to\noptimize distributed computing environments, fostering innovation in\nnext-generation intelligent systems."
                },
                "authors": [
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yao Du"
                    },
                    {
                        "name": "Kun Yang"
                    },
                    {
                        "name": "Jiaqi Wu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Xiping Hu"
                    },
                    {
                        "name": "Zehua Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Azzedine Boukerche"
                    },
                    {
                        "name": "Victor C. M. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Victor C. M. Leung"
                },
                "author": "Victor C. M. Leung",
                "arxiv_comment": "30 pages, 10 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09036v1",
                "updated": "2025-08-12T15:57:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    57,
                    22,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:57:22Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    57,
                    22,
                    1,
                    224,
                    0
                ],
                "title": "Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy\n  and AI Governance Exams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy\n  and AI Governance Exams"
                },
                "summary": "The rapid emergence of large language models (LLMs) has raised urgent\nquestions across the modern workforce about this new technology's strengths,\nweaknesses, and capabilities. For privacy professionals, the question is\nwhether these AI systems can provide reliable support on regulatory compliance,\nprivacy program management, and AI governance. In this study, we evaluate ten\nleading open and closed LLMs, including models from OpenAI, Anthropic, Google\nDeepMind, Meta, and DeepSeek, by benchmarking their performance on\nindustry-standard certification exams: CIPP/US, CIPM, CIPT, and AIGP from the\nInternational Association of Privacy Professionals (IAPP). Each model was\ntested using official sample exams in a closed-book setting and compared to\nIAPP's passing thresholds. Our findings show that several frontier models such\nas Gemini 2.5 Pro and OpenAI's GPT-5 consistently achieve scores exceeding the\nstandards for professional human certification - demonstrating substantial\nexpertise in privacy law, technical controls, and AI governance. The results\nhighlight both the strengths and domain-specific gaps of current LLMs and offer\npractical insights for privacy officers, compliance leads, and technologists\nassessing the readiness of AI tools for high-stakes data governance roles. This\npaper provides an overview for professionals navigating the intersection of AI\nadvancement and regulatory risk and establishes a machine benchmark based on\nhuman-centric evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid emergence of large language models (LLMs) has raised urgent\nquestions across the modern workforce about this new technology's strengths,\nweaknesses, and capabilities. For privacy professionals, the question is\nwhether these AI systems can provide reliable support on regulatory compliance,\nprivacy program management, and AI governance. In this study, we evaluate ten\nleading open and closed LLMs, including models from OpenAI, Anthropic, Google\nDeepMind, Meta, and DeepSeek, by benchmarking their performance on\nindustry-standard certification exams: CIPP/US, CIPM, CIPT, and AIGP from the\nInternational Association of Privacy Professionals (IAPP). Each model was\ntested using official sample exams in a closed-book setting and compared to\nIAPP's passing thresholds. Our findings show that several frontier models such\nas Gemini 2.5 Pro and OpenAI's GPT-5 consistently achieve scores exceeding the\nstandards for professional human certification - demonstrating substantial\nexpertise in privacy law, technical controls, and AI governance. The results\nhighlight both the strengths and domain-specific gaps of current LLMs and offer\npractical insights for privacy officers, compliance leads, and technologists\nassessing the readiness of AI tools for high-stakes data governance roles. This\npaper provides an overview for professionals navigating the intersection of AI\nadvancement and regulatory risk and establishes a machine benchmark based on\nhuman-centric evaluations."
                },
                "authors": [
                    {
                        "name": "Zane Witherspoon"
                    },
                    {
                        "name": "Thet Mon Aye"
                    },
                    {
                        "name": "YingYing Hao"
                    }
                ],
                "author_detail": {
                    "name": "YingYing Hao"
                },
                "author": "YingYing Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06813v2",
                "updated": "2025-08-12T15:49:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    49,
                    5,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-09T04:22:07Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    4,
                    22,
                    7,
                    5,
                    221,
                    0
                ],
                "title": "Technical Report: Full-Stack Fine-Tuning for the Q Programming Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical Report: Full-Stack Fine-Tuning for the Q Programming Language"
                },
                "summary": "Even though large language models are becoming increasingly capable, it is\nstill unreasonable to expect them to excel at tasks that are under-represented\non the Internet. Leveraging LLMs for specialized applications, particularly in\nniche programming languages and private domains, remains challenging and\nlargely unsolved. In this work, we address this gap by presenting a\ncomprehensive, open-source approach for adapting LLMs to the Q programming\nlanguage, a popular tool in quantitative finance that is much less present on\nthe Internet compared to Python, C, Java, and other ``mainstream\" languages and\nis therefore not a strong suit of general-purpose AI models. We introduce a new\nLeetcode style evaluation dataset for Q, benchmark major frontier models on the\ndataset, then do pretraining, supervised fine tuning, and reinforcement\nlearning to train a suite of reasoning and non-reasoning models based on the\nQwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our\nbest model achieves a pass@1 accuracy of 59 percent on our Q benchmark,\nsurpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.\nAdditionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.\nIn addition to releasing models, code, and data, we provide a detailed\nblueprint for dataset construction, model pretraining, supervised fine-tuning,\nand reinforcement learning. Our methodology is broadly applicable, and we\ndiscuss how these techniques can be extended to other tasks, including those\nwhere evaluation may rely on soft or subjective signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even though large language models are becoming increasingly capable, it is\nstill unreasonable to expect them to excel at tasks that are under-represented\non the Internet. Leveraging LLMs for specialized applications, particularly in\nniche programming languages and private domains, remains challenging and\nlargely unsolved. In this work, we address this gap by presenting a\ncomprehensive, open-source approach for adapting LLMs to the Q programming\nlanguage, a popular tool in quantitative finance that is much less present on\nthe Internet compared to Python, C, Java, and other ``mainstream\" languages and\nis therefore not a strong suit of general-purpose AI models. We introduce a new\nLeetcode style evaluation dataset for Q, benchmark major frontier models on the\ndataset, then do pretraining, supervised fine tuning, and reinforcement\nlearning to train a suite of reasoning and non-reasoning models based on the\nQwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our\nbest model achieves a pass@1 accuracy of 59 percent on our Q benchmark,\nsurpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.\nAdditionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.\nIn addition to releasing models, code, and data, we provide a detailed\nblueprint for dataset construction, model pretraining, supervised fine-tuning,\nand reinforcement learning. Our methodology is broadly applicable, and we\ndiscuss how these techniques can be extended to other tasks, including those\nwhere evaluation may rely on soft or subjective signals."
                },
                "authors": [
                    {
                        "name": "Brendan R. Hogan"
                    },
                    {
                        "name": "Will Brown"
                    },
                    {
                        "name": "Adel Boyarsky"
                    },
                    {
                        "name": "Anderson Schneider"
                    },
                    {
                        "name": "Yuriy Nevmyvaka"
                    }
                ],
                "author_detail": {
                    "name": "Yuriy Nevmyvaka"
                },
                "author": "Yuriy Nevmyvaka",
                "arxiv_comment": "40 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09023v1",
                "updated": "2025-08-12T15:38:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    38,
                    10,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:38:10Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    38,
                    10,
                    1,
                    224,
                    0
                ],
                "title": "E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and\n  Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and\n  Efficiency"
                },
                "summary": "SQL query rewriting aims to reformulate a query into a more efficient form\nwhile preserving equivalence. Most existing methods rely on predefined rewrite\nrules. However, such rule-based approaches face fundamental limitations: (1)\nfixed rule sets generalize poorly to novel query patterns and struggle with\ncomplex queries; (2) a wide range of effective rewriting strategies cannot be\nfully captured by declarative rules. To overcome these issues, we propose using\nlarge language models (LLMs) to generate rewrites. LLMs can capture complex\nstrategies, such as evaluation reordering and CTE rewriting. Despite this\npotential, directly applying LLMs often results in suboptimal or non-equivalent\nrewrites due to a lack of execution awareness and semantic grounding. To\naddress these challenges, We present E3-Rewrite, an LLM-based SQL rewriting\nframework that produces executable, equivalent, and efficient queries. It\nintegrates two core components: a context construction module and a\nreinforcement learning framework. First, the context module leverages execution\nplans and retrieved demonstrations to build bottleneck-aware prompts that guide\ninference-time rewriting. Second, we design a reward function targeting\nexecutability, equivalence, and efficiency, evaluated via syntax checks,\nequivalence verification, and cost estimation. Third, to ensure stable\nmulti-objective learning, we adopt a staged curriculum that first emphasizes\nexecutability and equivalence, then gradually incorporates efficiency.\nExtensive experiments show that E3-Rewrite achieves up to a 25.6\\% reduction in\nquery execution time compared to state-of-the-art methods across multiple SQL\nbenchmarks. Moreover, it delivers up to 24.4\\% more successful rewrites,\nexpanding coverage to complex queries that previous systems failed to handle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL query rewriting aims to reformulate a query into a more efficient form\nwhile preserving equivalence. Most existing methods rely on predefined rewrite\nrules. However, such rule-based approaches face fundamental limitations: (1)\nfixed rule sets generalize poorly to novel query patterns and struggle with\ncomplex queries; (2) a wide range of effective rewriting strategies cannot be\nfully captured by declarative rules. To overcome these issues, we propose using\nlarge language models (LLMs) to generate rewrites. LLMs can capture complex\nstrategies, such as evaluation reordering and CTE rewriting. Despite this\npotential, directly applying LLMs often results in suboptimal or non-equivalent\nrewrites due to a lack of execution awareness and semantic grounding. To\naddress these challenges, We present E3-Rewrite, an LLM-based SQL rewriting\nframework that produces executable, equivalent, and efficient queries. It\nintegrates two core components: a context construction module and a\nreinforcement learning framework. First, the context module leverages execution\nplans and retrieved demonstrations to build bottleneck-aware prompts that guide\ninference-time rewriting. Second, we design a reward function targeting\nexecutability, equivalence, and efficiency, evaluated via syntax checks,\nequivalence verification, and cost estimation. Third, to ensure stable\nmulti-objective learning, we adopt a staged curriculum that first emphasizes\nexecutability and equivalence, then gradually incorporates efficiency.\nExtensive experiments show that E3-Rewrite achieves up to a 25.6\\% reduction in\nquery execution time compared to state-of-the-art methods across multiple SQL\nbenchmarks. Moreover, it delivers up to 24.4\\% more successful rewrites,\nexpanding coverage to complex queries that previous systems failed to handle."
                },
                "authors": [
                    {
                        "name": "Dongjie Xu"
                    },
                    {
                        "name": "Yue Cui"
                    },
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Qingzhi Ma"
                    },
                    {
                        "name": "Hanghui Guo"
                    },
                    {
                        "name": "Jiaming Li"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Ruiyuan Zhang"
                    },
                    {
                        "name": "Shimin Di"
                    },
                    {
                        "name": "Jia Zhu"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Jiajie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajie Xu"
                },
                "author": "Jiajie Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09021v1",
                "updated": "2025-08-12T15:36:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    36,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:36:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    36,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "Attacks and Defenses Against LLM Fingerprinting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attacks and Defenses Against LLM Fingerprinting"
                },
                "summary": "As large language models are increasingly deployed in sensitive environments,\nfingerprinting attacks pose significant privacy and security risks. We present\na study of LLM fingerprinting from both offensive and defensive perspectives.\nOur attack methodology uses reinforcement learning to automatically optimize\nquery selection, achieving better fingerprinting accuracy with only 3 queries\ncompared to randomly selecting 3 queries from the same pool. Our defensive\napproach employs semantic-preserving output filtering through a secondary LLM\nto obfuscate model identity while maintaining semantic integrity. The defensive\nmethod reduces fingerprinting accuracy across tested models while preserving\noutput quality. These contributions show the potential to improve\nfingerprinting tools capabilities while providing practical mitigation\nstrategies against fingerprinting attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models are increasingly deployed in sensitive environments,\nfingerprinting attacks pose significant privacy and security risks. We present\na study of LLM fingerprinting from both offensive and defensive perspectives.\nOur attack methodology uses reinforcement learning to automatically optimize\nquery selection, achieving better fingerprinting accuracy with only 3 queries\ncompared to randomly selecting 3 queries from the same pool. Our defensive\napproach employs semantic-preserving output filtering through a secondary LLM\nto obfuscate model identity while maintaining semantic integrity. The defensive\nmethod reduces fingerprinting accuracy across tested models while preserving\noutput quality. These contributions show the potential to improve\nfingerprinting tools capabilities while providing practical mitigation\nstrategies against fingerprinting attacks."
                },
                "authors": [
                    {
                        "name": "Kevin Kurian"
                    },
                    {
                        "name": "Ethan Holland"
                    },
                    {
                        "name": "Sean Oesch"
                    }
                ],
                "author_detail": {
                    "name": "Sean Oesch"
                },
                "author": "Sean Oesch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09019v1",
                "updated": "2025-08-12T15:34:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    34,
                    18,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:34:18Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    34,
                    18,
                    1,
                    224,
                    0
                ],
                "title": "Activation Steering for Bias Mitigation: An Interpretable Approach to\n  Safer LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Steering for Bias Mitigation: An Interpretable Approach to\n  Safer LLMs"
                },
                "summary": "As large language models (LLMs) become more integrated into societal systems,\nthe risk of them perpetuating and amplifying harmful biases becomes a critical\nsafety concern. Traditional methods for mitigating bias often rely on data\nfiltering or post-hoc output moderation, which treat the model as an opaque\nblack box. In this work, we introduce a complete, end-to-end system that uses\ntechniques from mechanistic interpretability to both identify and actively\nmitigate bias directly within a model's internal workings. Our method involves\ntwo primary stages. First, we train linear \"probes\" on the internal activations\nof a model to detect the latent representations of various biases (e.g.,\ngender, race, age). Our experiments on \\texttt{gpt2-large} demonstrate that\nthese probes can identify biased content with near-perfect accuracy, revealing\nthat bias representations become most salient in the model's later layers.\nSecond, we leverage these findings to compute \"steering vectors\" by contrasting\nthe model's activation patterns for biased and neutral statements. By adding\nthese vectors during inference, we can actively steer the model's generative\nprocess away from producing harmful, stereotypical, or biased content in\nreal-time. We demonstrate the efficacy of this activation steering technique,\nshowing that it successfully alters biased completions toward more neutral\nalternatives. We present our work as a robust and reproducible system that\noffers a more direct and interpretable approach to building safer and more\naccountable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become more integrated into societal systems,\nthe risk of them perpetuating and amplifying harmful biases becomes a critical\nsafety concern. Traditional methods for mitigating bias often rely on data\nfiltering or post-hoc output moderation, which treat the model as an opaque\nblack box. In this work, we introduce a complete, end-to-end system that uses\ntechniques from mechanistic interpretability to both identify and actively\nmitigate bias directly within a model's internal workings. Our method involves\ntwo primary stages. First, we train linear \"probes\" on the internal activations\nof a model to detect the latent representations of various biases (e.g.,\ngender, race, age). Our experiments on \\texttt{gpt2-large} demonstrate that\nthese probes can identify biased content with near-perfect accuracy, revealing\nthat bias representations become most salient in the model's later layers.\nSecond, we leverage these findings to compute \"steering vectors\" by contrasting\nthe model's activation patterns for biased and neutral statements. By adding\nthese vectors during inference, we can actively steer the model's generative\nprocess away from producing harmful, stereotypical, or biased content in\nreal-time. We demonstrate the efficacy of this activation steering technique,\nshowing that it successfully alters biased completions toward more neutral\nalternatives. We present our work as a robust and reproducible system that\noffers a more direct and interpretable approach to building safer and more\naccountable LLMs."
                },
                "authors": [
                    {
                        "name": "Shivam Dubey"
                    }
                ],
                "author_detail": {
                    "name": "Shivam Dubey"
                },
                "author": "Shivam Dubey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09016v1",
                "updated": "2025-08-12T15:30:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    30,
                    44,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:30:44Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    30,
                    44,
                    1,
                    224,
                    0
                ],
                "title": "A Survey on Training-free Alignment of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Training-free Alignment of Large Language Models"
                },
                "summary": "The alignment of large language models (LLMs) aims to ensure their outputs\nadhere to human values, ethical standards, and legal norms. Traditional\nalignment methods often rely on resource-intensive fine-tuning (FT), which may\nsuffer from knowledge degradation and face challenges in scenarios where the\nmodel accessibility or computational resources are constrained. In contrast,\ntraining-free (TF) alignment techniques--leveraging in-context learning,\ndecoding-time adjustments, and post-generation corrections--offer a promising\nalternative by enabling alignment without heavily retraining LLMs, making them\nadaptable to both open-source and closed-source environments. This paper\npresents the first systematic review of TF alignment methods, categorizing them\nby stages of pre-decoding, in-decoding, and post-decoding. For each stage, we\nprovide a detailed examination from the viewpoint of LLMs and multimodal LLMs\n(MLLMs), highlighting their mechanisms and limitations. Furthermore, we\nidentify key challenges and future directions, paving the way for more\ninclusive and effective TF alignment techniques. By synthesizing and organizing\nthe rapidly growing body of research, this survey offers a guidance for\npractitioners and advances the development of safer and more reliable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of large language models (LLMs) aims to ensure their outputs\nadhere to human values, ethical standards, and legal norms. Traditional\nalignment methods often rely on resource-intensive fine-tuning (FT), which may\nsuffer from knowledge degradation and face challenges in scenarios where the\nmodel accessibility or computational resources are constrained. In contrast,\ntraining-free (TF) alignment techniques--leveraging in-context learning,\ndecoding-time adjustments, and post-generation corrections--offer a promising\nalternative by enabling alignment without heavily retraining LLMs, making them\nadaptable to both open-source and closed-source environments. This paper\npresents the first systematic review of TF alignment methods, categorizing them\nby stages of pre-decoding, in-decoding, and post-decoding. For each stage, we\nprovide a detailed examination from the viewpoint of LLMs and multimodal LLMs\n(MLLMs), highlighting their mechanisms and limitations. Furthermore, we\nidentify key challenges and future directions, paving the way for more\ninclusive and effective TF alignment techniques. By synthesizing and organizing\nthe rapidly growing body of research, this survey offers a guidance for\npractitioners and advances the development of safer and more reliable LLMs."
                },
                "authors": [
                    {
                        "name": "Birong Pan"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Weiyu Zhang"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Mayi Xu"
                    },
                    {
                        "name": "Shen Zhou"
                    },
                    {
                        "name": "Yuanyuan Zhu"
                    },
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Tieyun Qian"
                    }
                ],
                "author_detail": {
                    "name": "Tieyun Qian"
                },
                "author": "Tieyun Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02078v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02078v3",
                "updated": "2025-08-12T15:27:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    27,
                    29,
                    1,
                    224,
                    0
                ],
                "published": "2023-12-04T17:41:52Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    17,
                    41,
                    52,
                    0,
                    338,
                    0
                ],
                "title": "From Lab to Field: Real-World Evaluation of an AI-Driven Smart Video\n  Solution to Enhance Community Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Lab to Field: Real-World Evaluation of an AI-Driven Smart Video\n  Solution to Enhance Community Safety"
                },
                "summary": "This article adopts and evaluates an AI-enabled Smart Video Solution (SVS)\ndesigned to enhance safety in the real world. The system integrates with\nexisting infrastructure camera networks, leveraging recent advancements in AI\nfor easy adoption. Prioritizing privacy and ethical standards, pose based data\nis used for downstream AI tasks such as anomaly detection. Cloud-based\ninfrastructure and mobile app are deployed, enabling real-time alerts within\ncommunities. The SVS employs innovative data representation and visualization\ntechniques, such as the Occupancy Indicator, Statistical Anomaly Detection,\nBird's Eye View, and Heatmaps, to understand pedestrian behaviors and enhance\npublic safety. Evaluation of the SVS demonstrates its capacity to convert\ncomplex computer vision outputs into actionable insights for stakeholders,\ncommunity partners, law enforcement, urban planners, and social scientists.\nThis article presents a comprehensive real-world deployment and evaluation of\nthe SVS, implemented in a community college environment across 16 cameras. The\nsystem integrates AI-driven visual processing, supported by statistical\nanalysis, database management, cloud communication, and user notifications.\nAdditionally, the article evaluates the end-to-end latency from the moment an\nAI algorithm detects anomalous behavior in real-time at the camera level to the\ntime stakeholders receive a notification. The results demonstrate the system's\nrobustness, effectively managing 16 CCTV cameras with a consistent throughput\nof 16.5 frames per second (FPS) over a 21-hour period and an average end-to-end\nlatency of 26.76 seconds between anomaly detection and alert issuance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article adopts and evaluates an AI-enabled Smart Video Solution (SVS)\ndesigned to enhance safety in the real world. The system integrates with\nexisting infrastructure camera networks, leveraging recent advancements in AI\nfor easy adoption. Prioritizing privacy and ethical standards, pose based data\nis used for downstream AI tasks such as anomaly detection. Cloud-based\ninfrastructure and mobile app are deployed, enabling real-time alerts within\ncommunities. The SVS employs innovative data representation and visualization\ntechniques, such as the Occupancy Indicator, Statistical Anomaly Detection,\nBird's Eye View, and Heatmaps, to understand pedestrian behaviors and enhance\npublic safety. Evaluation of the SVS demonstrates its capacity to convert\ncomplex computer vision outputs into actionable insights for stakeholders,\ncommunity partners, law enforcement, urban planners, and social scientists.\nThis article presents a comprehensive real-world deployment and evaluation of\nthe SVS, implemented in a community college environment across 16 cameras. The\nsystem integrates AI-driven visual processing, supported by statistical\nanalysis, database management, cloud communication, and user notifications.\nAdditionally, the article evaluates the end-to-end latency from the moment an\nAI algorithm detects anomalous behavior in real-time at the camera level to the\ntime stakeholders receive a notification. The results demonstrate the system's\nrobustness, effectively managing 16 CCTV cameras with a consistent throughput\nof 16.5 frames per second (FPS) over a 21-hour period and an average end-to-end\nlatency of 26.76 seconds between anomaly detection and alert issuance."
                },
                "authors": [
                    {
                        "name": "Shanle Yao"
                    },
                    {
                        "name": "Babak Rahimi Ardabili"
                    },
                    {
                        "name": "Armin Danesh Pazho"
                    },
                    {
                        "name": "Ghazal Alinezhad Noghre"
                    },
                    {
                        "name": "Christopher Neff"
                    },
                    {
                        "name": "Lauren Bourque"
                    },
                    {
                        "name": "Hamed Tabkhi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Tabkhi"
                },
                "author": "Hamed Tabkhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02078v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02078v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05387v3",
                "updated": "2025-08-12T15:23:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    23,
                    4,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-07T13:37:04Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    37,
                    4,
                    3,
                    219,
                    0
                ],
                "title": "Echo: Decoupling Inference and Training for Large-Scale RL Alignment on\n  Heterogeneous Swarms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Echo: Decoupling Inference and Training for Large-Scale RL Alignment on\n  Heterogeneous Swarms"
                },
                "summary": "Modern RL-based post-training for large language models (LLMs) co-locate\ntrajectory sampling and policy optimisation on the same GPU cluster, forcing\nthe system to switch between inference and training workloads. This serial\ncontext switching violates the single-program-multiple-data (SPMD) assumption\nunderlying today's distributed training systems. We present Echo, the RL system\nthat cleanly decouples these two phases across heterogeneous \"inference\" and\n\"training\" swarms while preserving statistical efficiency. Echo introduces two\nlightweight synchronization protocols: a sequential pull mode that refreshes\npolicy weights according to API call for minimal bias, and an asynchronous\npush-pull mode that streams version-tagged rollouts through a replay buffer to\nmaximise hardware utilisation. Training four representative RL workloads with\nQwen3-4B, Qwen2.5-7B, Qwen3-30B-A3B-Thinking-2507 and Qwen3-32B on a\ngeographically distributed cluster, Echo matches a fully co-located Verl\nbaseline in convergence speed and final reward while off-loading trajectory\ngeneration to commodity edge hardware. These promising results demonstrate that\nlarge-scale RL for LLMs could achieve datacentre-grade performance using\ndecentralised, heterogeneous resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern RL-based post-training for large language models (LLMs) co-locate\ntrajectory sampling and policy optimisation on the same GPU cluster, forcing\nthe system to switch between inference and training workloads. This serial\ncontext switching violates the single-program-multiple-data (SPMD) assumption\nunderlying today's distributed training systems. We present Echo, the RL system\nthat cleanly decouples these two phases across heterogeneous \"inference\" and\n\"training\" swarms while preserving statistical efficiency. Echo introduces two\nlightweight synchronization protocols: a sequential pull mode that refreshes\npolicy weights according to API call for minimal bias, and an asynchronous\npush-pull mode that streams version-tagged rollouts through a replay buffer to\nmaximise hardware utilisation. Training four representative RL workloads with\nQwen3-4B, Qwen2.5-7B, Qwen3-30B-A3B-Thinking-2507 and Qwen3-32B on a\ngeographically distributed cluster, Echo matches a fully co-located Verl\nbaseline in convergence speed and final reward while off-loading trajectory\ngeneration to commodity edge hardware. These promising results demonstrate that\nlarge-scale RL for LLMs could achieve datacentre-grade performance using\ndecentralised, heterogeneous resources."
                },
                "authors": [
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Changyuan Fan"
                    },
                    {
                        "name": "Qingnan Ren"
                    },
                    {
                        "name": "Alfred Long"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Rymon Yu"
                    },
                    {
                        "name": "Eric Yang"
                    },
                    {
                        "name": "Lynn Ai"
                    },
                    {
                        "name": "Shaoduo Gan"
                    }
                ],
                "author_detail": {
                    "name": "Shaoduo Gan"
                },
                "author": "Shaoduo Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10769v2",
                "updated": "2025-08-12T15:15:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    15,
                    15,
                    1,
                    224,
                    0
                ],
                "published": "2025-06-12T14:48:25Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    14,
                    48,
                    25,
                    3,
                    163,
                    0
                ],
                "title": "Mind the Gap: Benchmarking LLM Uncertainty, Discrimination, and\n  Calibration in Specialty-Aware Clinical QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Benchmarking LLM Uncertainty, Discrimination, and\n  Calibration in Specialty-Aware Clinical QA"
                },
                "summary": "Reliable uncertainty quantification (UQ) is essential when employing large\nlanguage models (LLMs) in high-risk domains such as clinical question answering\n(QA). In this work, we evaluate uncertainty estimation methods for clinical QA\nfocusing, for the first time, on eleven clinical specialties and six question\ntypes, and across ten open-source LLMs (general-purpose, biomedical, and\nreasoning models). We analyze score-based UQ methods, present a case study\nintroducing a novel lightweight method based on behavioral features derived\nfrom reasoning-oriented models, and examine conformal prediction as a\ncomplementary set-based approach. Our findings reveal that uncertainty\nreliability is not a monolithic property, but one that depends on clinical\nspecialty and question type due to shifts in calibration and discrimination.\nOur results highlight the need to select or ensemble models based on their\ndistinct, complementary strengths and clinical use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable uncertainty quantification (UQ) is essential when employing large\nlanguage models (LLMs) in high-risk domains such as clinical question answering\n(QA). In this work, we evaluate uncertainty estimation methods for clinical QA\nfocusing, for the first time, on eleven clinical specialties and six question\ntypes, and across ten open-source LLMs (general-purpose, biomedical, and\nreasoning models). We analyze score-based UQ methods, present a case study\nintroducing a novel lightweight method based on behavioral features derived\nfrom reasoning-oriented models, and examine conformal prediction as a\ncomplementary set-based approach. Our findings reveal that uncertainty\nreliability is not a monolithic property, but one that depends on clinical\nspecialty and question type due to shifts in calibration and discrimination.\nOur results highlight the need to select or ensemble models based on their\ndistinct, complementary strengths and clinical use."
                },
                "authors": [
                    {
                        "name": "Alberto Testoni"
                    },
                    {
                        "name": "Iacer Calixto"
                    }
                ],
                "author_detail": {
                    "name": "Iacer Calixto"
                },
                "author": "Iacer Calixto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09001v1",
                "updated": "2025-08-12T15:11:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:11:47Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "title": "Retrospective Sparse Attention for Efficient Long-Context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrospective Sparse Attention for Efficient Long-Context Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%."
                },
                "authors": [
                    {
                        "name": "Seonghwan Choi"
                    },
                    {
                        "name": "Beomseok Kang"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08997v1",
                "updated": "2025-08-12T15:05:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    5,
                    0,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:05:00Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    5,
                    0,
                    1,
                    224,
                    0
                ],
                "title": "Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through\n  Structured Contextual Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through\n  Structured Contextual Memory"
                },
                "summary": "Multi-agent systems built on Large Language Models (LLMs) show exceptional\npromise for complex collaborative problem-solving, yet they face fundamental\nchallenges stemming from context window limitations that impair memory\nconsistency, role adherence, and procedural integrity. This paper introduces\nIntrinsic Memory Agents, a novel framework that addresses these limitations\nthrough structured agent-specific memories that evolve intrinsically with agent\noutputs. Specifically, our method maintains role-aligned memory templates that\npreserve specialized perspectives while focusing on task-relevant information.\nWe benchmark our approach on the PDDL dataset, comparing its performance to\nexisting state-of-the-art multi-agentic memory approaches and showing an\nimprovement of 38.6\\% with the highest token efficiency. An additional\nevaluation is performed on a complex data pipeline design task, we demonstrate\nthat our approach produces higher quality designs when comparing 5 metrics:\nscalability, reliability, usability, cost-effectiveness and documentation with\nadditional qualitative evidence of the improvements. Our findings suggest that\naddressing memory limitations through structured, intrinsic approaches can\nimprove the capabilities of multi-agent LLM systems on structured planning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems built on Large Language Models (LLMs) show exceptional\npromise for complex collaborative problem-solving, yet they face fundamental\nchallenges stemming from context window limitations that impair memory\nconsistency, role adherence, and procedural integrity. This paper introduces\nIntrinsic Memory Agents, a novel framework that addresses these limitations\nthrough structured agent-specific memories that evolve intrinsically with agent\noutputs. Specifically, our method maintains role-aligned memory templates that\npreserve specialized perspectives while focusing on task-relevant information.\nWe benchmark our approach on the PDDL dataset, comparing its performance to\nexisting state-of-the-art multi-agentic memory approaches and showing an\nimprovement of 38.6\\% with the highest token efficiency. An additional\nevaluation is performed on a complex data pipeline design task, we demonstrate\nthat our approach produces higher quality designs when comparing 5 metrics:\nscalability, reliability, usability, cost-effectiveness and documentation with\nadditional qualitative evidence of the improvements. Our findings suggest that\naddressing memory limitations through structured, intrinsic approaches can\nimprove the capabilities of multi-agent LLM systems on structured planning\ntasks."
                },
                "authors": [
                    {
                        "name": "Sizhe Yuen"
                    },
                    {
                        "name": "Francisco Gomez Medina"
                    },
                    {
                        "name": "Ting Su"
                    },
                    {
                        "name": "Yali Du"
                    },
                    {
                        "name": "Adam J. Sobey"
                    }
                ],
                "author_detail": {
                    "name": "Adam J. Sobey"
                },
                "author": "Adam J. Sobey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08992v1",
                "updated": "2025-08-12T15:02:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    2,
                    16,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:02:16Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    2,
                    16,
                    1,
                    224,
                    0
                ],
                "title": "Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making\n  under Epistemic Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making\n  under Epistemic Uncertainty"
                },
                "summary": "Prospect Theory (PT) models human decision-making under uncertainty, while\nepistemic markers (e.g., maybe) serve to express uncertainty in language.\nHowever, it remains largely unexplored whether Prospect Theory applies to\ncontemporary Large Language Models and whether epistemic markers, which express\nhuman uncertainty, affect their decision-making behaviour. To address these\nresearch gaps, we design a three-stage experiment based on economic\nquestionnaires. We propose a more general and precise evaluation framework to\nmodel LLMs' decision-making behaviour under PT, introducing uncertainty through\nthe empirical probability values associated with commonly used epistemic\nmarkers in comparable contexts. We then incorporate epistemic markers into the\nevaluation framework based on their corresponding probability values to examine\ntheir influence on LLM decision-making behaviours. Our findings suggest that\nmodelling LLMs' decision-making with PT is not consistently reliable,\nparticularly when uncertainty is expressed in diverse linguistic forms. Our\ncode is released in https://github.com/HKUST-KnowComp/MarPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prospect Theory (PT) models human decision-making under uncertainty, while\nepistemic markers (e.g., maybe) serve to express uncertainty in language.\nHowever, it remains largely unexplored whether Prospect Theory applies to\ncontemporary Large Language Models and whether epistemic markers, which express\nhuman uncertainty, affect their decision-making behaviour. To address these\nresearch gaps, we design a three-stage experiment based on economic\nquestionnaires. We propose a more general and precise evaluation framework to\nmodel LLMs' decision-making behaviour under PT, introducing uncertainty through\nthe empirical probability values associated with commonly used epistemic\nmarkers in comparable contexts. We then incorporate epistemic markers into the\nevaluation framework based on their corresponding probability values to examine\ntheir influence on LLM decision-making behaviours. Our findings suggest that\nmodelling LLMs' decision-making with PT is not consistently reliable,\nparticularly when uncertainty is expressed in diverse linguistic forms. Our\ncode is released in https://github.com/HKUST-KnowComp/MarPT."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Qihan Lin"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07251v2",
                "updated": "2025-08-12T15:01:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    1,
                    27,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-10T09:08:04Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    9,
                    8,
                    4,
                    6,
                    222,
                    0
                ],
                "title": "Understanding Dynamic Scenes in Ego Centric 4D Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Dynamic Scenes in Ego Centric 4D Point Clouds"
                },
                "summary": "Understanding dynamic 4D scenes from an egocentric perspective-modeling\nchanges in 3D spatial structure over time-is crucial for human-machine\ninteraction, autonomous navigation, and embodied intelligence. While existing\negocentric datasets contain dynamic scenes, they lack unified 4D annotations\nand task-driven evaluation protocols for fine-grained spatio-temporal\nreasoning, especially on motion of objects and human, together with their\ninteractions. To address this gap, we introduce EgoDynamic4D, a novel QA\nbenchmark on highly dynamic scenes, comprising RGB-D video, camera poses,\nglobally unique instance masks, and 4D bounding boxes. We construct 927K QA\npairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,\nstep-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks covering\nagent motion, human-object interaction, trajectory prediction, relation\nunderstanding, and temporal-causal reasoning, with fine-grained,\nmultidimensional metrics. To tackle these tasks, we propose an end-to-end\nspatio-temporal reasoning framework that unifies dynamic and static scene\ninformation, using instance-aware feature encoding, time and camera encoding,\nand spatially adaptive down-sampling to compress large 4D scenes into token\nsequences manageable by LLMs. Experiments on EgoDynamic4D show that our method\nconsistently outperforms baselines, validating the effectiveness of multimodal\ntemporal modeling for egocentric dynamic scene understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding dynamic 4D scenes from an egocentric perspective-modeling\nchanges in 3D spatial structure over time-is crucial for human-machine\ninteraction, autonomous navigation, and embodied intelligence. While existing\negocentric datasets contain dynamic scenes, they lack unified 4D annotations\nand task-driven evaluation protocols for fine-grained spatio-temporal\nreasoning, especially on motion of objects and human, together with their\ninteractions. To address this gap, we introduce EgoDynamic4D, a novel QA\nbenchmark on highly dynamic scenes, comprising RGB-D video, camera poses,\nglobally unique instance masks, and 4D bounding boxes. We construct 927K QA\npairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,\nstep-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks covering\nagent motion, human-object interaction, trajectory prediction, relation\nunderstanding, and temporal-causal reasoning, with fine-grained,\nmultidimensional metrics. To tackle these tasks, we propose an end-to-end\nspatio-temporal reasoning framework that unifies dynamic and static scene\ninformation, using instance-aware feature encoding, time and camera encoding,\nand spatially adaptive down-sampling to compress large 4D scenes into token\nsequences manageable by LLMs. Experiments on EgoDynamic4D show that our method\nconsistently outperforms baselines, validating the effectiveness of multimodal\ntemporal modeling for egocentric dynamic scene understanding."
                },
                "authors": [
                    {
                        "name": "Junsheng Huang"
                    },
                    {
                        "name": "Shengyu Hao"
                    },
                    {
                        "name": "Bocheng Hu"
                    },
                    {
                        "name": "Gaoang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gaoang Wang"
                },
                "author": "Gaoang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10323v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10323v5",
                "updated": "2025-08-12T15:00:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    0,
                    13,
                    1,
                    224,
                    0
                ],
                "published": "2025-06-12T03:13:55Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    3,
                    13,
                    55,
                    3,
                    163,
                    0
                ],
                "title": "ELFuzz: Efficient Input Generation via LLM-driven Synthesis Over Fuzzer\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELFuzz: Efficient Input Generation via LLM-driven Synthesis Over Fuzzer\n  Space"
                },
                "summary": "Generation-based fuzzing produces appropriate test cases according to\nspecifications of input grammars and semantic constraints to test systems and\nsoftware. However, these specifications require significant manual effort to\nconstruct. This paper proposes a new approach, ELFuzz (Evolution Through Large\nLanguage Models for Fuzzing), that automatically synthesizes generation-based\nfuzzers tailored to a system under test (SUT) via LLM-driven synthesis over\nfuzzer space. At a high level, it starts with minimal seed fuzzers and propels\nthe synthesis by fully automated LLM-driven evolution with coverage guidance.\nCompared to previous approaches, ELFuzz can 1) seamlessly scale to SUTs of\nreal-world sizes -- up to 1,791,104 lines of code in our evaluation -- and 2)\nsynthesize efficient fuzzers that catch interesting grammatical structures and\nsemantic constraints in a human-understandable way. Our evaluation compared\nELFuzz with specifications manually written by domain experts and synthesized\nby state-of-the-art approaches. It shows that ELFuzz achieves up to 434.8% more\ncoverage over the second best and triggers up to 216.7% more artificially\ninjected bugs, compared to the state-of-the-art. We also used ELFuzz to conduct\na real-world fuzzing campaign on the newest version of cvc5 for 14 days, and\nencouragingly, it found five 0-day bugs (three are exploitable). Moreover, we\nconducted an ablation study, which shows that the fuzzer space model, the key\ncomponent of ELFuzz, contributes the most (up to 62.5%) to the effectiveness of\nELFuzz. Further analysis of the fuzzers synthesized by ELFuzz confirms that\nthey catch interesting grammatical structures and semantic constraints in a\nhuman-understandable way. The results present the promising potential of ELFuzz\nfor more automated, efficient, and extensible input generation for fuzzing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generation-based fuzzing produces appropriate test cases according to\nspecifications of input grammars and semantic constraints to test systems and\nsoftware. However, these specifications require significant manual effort to\nconstruct. This paper proposes a new approach, ELFuzz (Evolution Through Large\nLanguage Models for Fuzzing), that automatically synthesizes generation-based\nfuzzers tailored to a system under test (SUT) via LLM-driven synthesis over\nfuzzer space. At a high level, it starts with minimal seed fuzzers and propels\nthe synthesis by fully automated LLM-driven evolution with coverage guidance.\nCompared to previous approaches, ELFuzz can 1) seamlessly scale to SUTs of\nreal-world sizes -- up to 1,791,104 lines of code in our evaluation -- and 2)\nsynthesize efficient fuzzers that catch interesting grammatical structures and\nsemantic constraints in a human-understandable way. Our evaluation compared\nELFuzz with specifications manually written by domain experts and synthesized\nby state-of-the-art approaches. It shows that ELFuzz achieves up to 434.8% more\ncoverage over the second best and triggers up to 216.7% more artificially\ninjected bugs, compared to the state-of-the-art. We also used ELFuzz to conduct\na real-world fuzzing campaign on the newest version of cvc5 for 14 days, and\nencouragingly, it found five 0-day bugs (three are exploitable). Moreover, we\nconducted an ablation study, which shows that the fuzzer space model, the key\ncomponent of ELFuzz, contributes the most (up to 62.5%) to the effectiveness of\nELFuzz. Further analysis of the fuzzers synthesized by ELFuzz confirms that\nthey catch interesting grammatical structures and semantic constraints in a\nhuman-understandable way. The results present the promising potential of ELFuzz\nfor more automated, efficient, and extensible input generation for fuzzing."
                },
                "authors": [
                    {
                        "name": "Chuyang Chen"
                    },
                    {
                        "name": "Brendan Dolan-Gavitt"
                    },
                    {
                        "name": "Zhiqiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lin"
                },
                "author": "Zhiqiang Lin",
                "arxiv_comment": "Accepted by USENIX Security'25 Cycle 2",
                "arxiv_journal_ref": "The 34th USENIX Security Symposium, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10323v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10323v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08989v1",
                "updated": "2025-08-12T14:57:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    57,
                    3,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:57:03Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    57,
                    3,
                    1,
                    224,
                    0
                ],
                "title": "KFFocus: Highlighting Keyframes for Enhanced Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KFFocus: Highlighting Keyframes for Enhanced Video Understanding"
                },
                "summary": "Recently, with the emergence of large language models, multimodal LLMs have\ndemonstrated exceptional capabilities in image and video modalities. Despite\nadvancements in video comprehension, the substantial computational demands of\nlong video sequences lead current video LLMs (Vid-LLMs) to employ compression\nstrategies at both the inter-frame level (e.g., uniform sampling of video\nframes) and intra-frame level (e.g., condensing all visual tokens of each frame\ninto a limited number). However, this approach often neglects the uneven\ntemporal distribution of critical information across frames, risking the\nomission of keyframes that contain essential temporal and semantic details. To\ntackle these challenges, we propose KFFocus, a method designed to efficiently\ncompress video tokens and emphasize the informative context present within\nvideo frames. We substitute uniform sampling with a refined approach inspired\nby classic video compression principles to identify and capture keyframes based\non their temporal redundancy. By assigning varying condensation ratios to\nframes based on their contextual relevance, KFFocus efficiently reduces token\nredundancy while preserving informative content details. Additionally, we\nintroduce a spatiotemporal modeling module that encodes both the temporal\nrelationships between video frames and the spatial structure within each frame,\nthus providing Vid-LLMs with a nuanced understanding of spatial-temporal\ndynamics. Extensive experiments on widely recognized video understanding\nbenchmarks, especially long video scenarios, demonstrate that KFFocus\nsignificantly outperforms existing methods, achieving substantial computational\nefficiency and enhanced accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, with the emergence of large language models, multimodal LLMs have\ndemonstrated exceptional capabilities in image and video modalities. Despite\nadvancements in video comprehension, the substantial computational demands of\nlong video sequences lead current video LLMs (Vid-LLMs) to employ compression\nstrategies at both the inter-frame level (e.g., uniform sampling of video\nframes) and intra-frame level (e.g., condensing all visual tokens of each frame\ninto a limited number). However, this approach often neglects the uneven\ntemporal distribution of critical information across frames, risking the\nomission of keyframes that contain essential temporal and semantic details. To\ntackle these challenges, we propose KFFocus, a method designed to efficiently\ncompress video tokens and emphasize the informative context present within\nvideo frames. We substitute uniform sampling with a refined approach inspired\nby classic video compression principles to identify and capture keyframes based\non their temporal redundancy. By assigning varying condensation ratios to\nframes based on their contextual relevance, KFFocus efficiently reduces token\nredundancy while preserving informative content details. Additionally, we\nintroduce a spatiotemporal modeling module that encodes both the temporal\nrelationships between video frames and the spatial structure within each frame,\nthus providing Vid-LLMs with a nuanced understanding of spatial-temporal\ndynamics. Extensive experiments on widely recognized video understanding\nbenchmarks, especially long video scenarios, demonstrate that KFFocus\nsignificantly outperforms existing methods, achieving substantial computational\nefficiency and enhanced accuracy."
                },
                "authors": [
                    {
                        "name": "Ming Nie"
                    },
                    {
                        "name": "Chunwei Wang"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08987v1",
                "updated": "2025-08-12T14:56:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    56,
                    11,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:56:11Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    56,
                    11,
                    1,
                    224,
                    0
                ],
                "title": "ColorGPT: Leveraging Large Language Models for Multimodal Color\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ColorGPT: Leveraging Large Language Models for Multimodal Color\n  Recommendation"
                },
                "summary": "Colors play a crucial role in the design of vector graphic documents by\nenhancing visual appeal, facilitating communication, improving usability, and\nensuring accessibility. In this context, color recommendation involves\nsuggesting appropriate colors to complete or refine a design when one or more\ncolors are missing or require alteration. Traditional methods often struggled\nwith these challenges due to the complex nature of color design and the limited\ndata availability. In this study, we explored the use of pretrained Large\nLanguage Models (LLMs) and their commonsense reasoning capabilities for color\nrecommendation, raising the question: Can pretrained LLMs serve as superior\ndesigners for color recommendation tasks? To investigate this, we developed a\nrobust, rigorously validated pipeline, ColorGPT, that was built by\nsystematically testing multiple color representations and applying effective\nprompt engineering techniques. Our approach primarily targeted color palette\ncompletion by recommending colors based on a set of given colors and\naccompanying context. Moreover, our method can be extended to full palette\ngeneration, producing an entire color palette corresponding to a provided\ntextual description. Experimental results demonstrated that our LLM-based\npipeline outperformed existing methods in terms of color suggestion accuracy\nand the distribution of colors in the color palette completion task. For the\nfull palette generation task, our approach also yielded improvements in color\ndiversity and similarity compared to current techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colors play a crucial role in the design of vector graphic documents by\nenhancing visual appeal, facilitating communication, improving usability, and\nensuring accessibility. In this context, color recommendation involves\nsuggesting appropriate colors to complete or refine a design when one or more\ncolors are missing or require alteration. Traditional methods often struggled\nwith these challenges due to the complex nature of color design and the limited\ndata availability. In this study, we explored the use of pretrained Large\nLanguage Models (LLMs) and their commonsense reasoning capabilities for color\nrecommendation, raising the question: Can pretrained LLMs serve as superior\ndesigners for color recommendation tasks? To investigate this, we developed a\nrobust, rigorously validated pipeline, ColorGPT, that was built by\nsystematically testing multiple color representations and applying effective\nprompt engineering techniques. Our approach primarily targeted color palette\ncompletion by recommending colors based on a set of given colors and\naccompanying context. Moreover, our method can be extended to full palette\ngeneration, producing an entire color palette corresponding to a provided\ntextual description. Experimental results demonstrated that our LLM-based\npipeline outperformed existing methods in terms of color suggestion accuracy\nand the distribution of colors in the color palette completion task. For the\nfull palette generation task, our approach also yielded improvements in color\ndiversity and similarity compared to current techniques."
                },
                "authors": [
                    {
                        "name": "Ding Xia"
                    },
                    {
                        "name": "Naoto Inoue"
                    },
                    {
                        "name": "Qianru Qiu"
                    },
                    {
                        "name": "Kotaro Kikuchi"
                    }
                ],
                "author_detail": {
                    "name": "Kotaro Kikuchi"
                },
                "author": "Kotaro Kikuchi",
                "arxiv_comment": "Accepted to ICDAR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08985v1",
                "updated": "2025-08-12T14:53:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    53,
                    54,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:53:54Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    53,
                    54,
                    1,
                    224,
                    0
                ],
                "title": "Low-Regret and Low-Complexity Learning for Hierarchical Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Regret and Low-Complexity Learning for Hierarchical Inference"
                },
                "summary": "This work focuses on Hierarchical Inference (HI) in edge intelligence\nsystems, where a compact Local-ML model on an end-device works in conjunction\nwith a high-accuracy Remote-ML model on an edge-server. HI aims to reduce\nlatency, improve accuracy, and lower bandwidth usage by first using the\nLocal-ML model for inference and offloading to the Remote-ML only when the\nlocal inference is likely incorrect. A critical challenge in HI is estimating\nthe likelihood of the local inference being incorrect, especially when data\ndistributions and offloading costs change over time -- a problem we term\nHierarchical Inference Learning (HIL). We introduce a novel approach to HIL by\nmodeling the probability of correct inference by the Local-ML as an increasing\nfunction of the model's confidence measure, a structure motivated by empirical\nobservations but previously unexploited. We propose two policies, HI-LCB and\nHI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We\ndemonstrate that both policies achieve order-optimal regret of $O(\\log T)$, a\nsignificant improvement over existing HIL policies with $O(T^{2/3})$ regret\nguarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational\ncomplexity, making it well-suited for deployment on devices with severe\nresource limitations. Simulations using real-world datasets confirm that our\npolicies outperform existing state-of-the-art HIL methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work focuses on Hierarchical Inference (HI) in edge intelligence\nsystems, where a compact Local-ML model on an end-device works in conjunction\nwith a high-accuracy Remote-ML model on an edge-server. HI aims to reduce\nlatency, improve accuracy, and lower bandwidth usage by first using the\nLocal-ML model for inference and offloading to the Remote-ML only when the\nlocal inference is likely incorrect. A critical challenge in HI is estimating\nthe likelihood of the local inference being incorrect, especially when data\ndistributions and offloading costs change over time -- a problem we term\nHierarchical Inference Learning (HIL). We introduce a novel approach to HIL by\nmodeling the probability of correct inference by the Local-ML as an increasing\nfunction of the model's confidence measure, a structure motivated by empirical\nobservations but previously unexploited. We propose two policies, HI-LCB and\nHI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We\ndemonstrate that both policies achieve order-optimal regret of $O(\\log T)$, a\nsignificant improvement over existing HIL policies with $O(T^{2/3})$ regret\nguarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational\ncomplexity, making it well-suited for deployment on devices with severe\nresource limitations. Simulations using real-world datasets confirm that our\npolicies outperform existing state-of-the-art HIL methods."
                },
                "authors": [
                    {
                        "name": "Sameep Chattopadhyay"
                    },
                    {
                        "name": "Vinay Sutar"
                    },
                    {
                        "name": "Jaya Prakash Champati"
                    },
                    {
                        "name": "Sharayu Moharir"
                    }
                ],
                "author_detail": {
                    "name": "Sharayu Moharir"
                },
                "author": "Sharayu Moharir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10331v2",
                "updated": "2025-08-12T14:44:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    44,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-03-13T13:07:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    7,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting\n  Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting\n  Conditions"
                },
                "summary": "Open Semantic Mapping (OSM) is a key technology in robotic perception,\ncombining semantic segmentation and SLAM techniques. This paper introduces a\ndynamically configurable and highly automated LLM/LVLM-powered pipeline for\nevaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark).\nThe study focuses on evaluating state-of-the-art semantic mapping algorithms\nunder varying indoor lighting conditions, a critical challenge in indoor\nenvironments. We introduce a novel dataset with simulated RGB-D sequences and\nground truth 3D reconstructions, facilitating the rigorous analysis of mapping\nperformance across different lighting conditions. Through experiments on\nleading models such as ConceptGraphs, BBQ and OpenScene, we evaluate the\nsemantic fidelity of object recognition and segmentation. Additionally, we\nintroduce a Scene Graph evaluation method to analyze the ability of models to\ninterpret semantic structure. The results provide insights into the robustness\nof these models, forming future research directions for developing resilient\nand adaptable robotic systems. Project page is available at\nhttps://be2rlab.github.io/OSMa-Bench/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Semantic Mapping (OSM) is a key technology in robotic perception,\ncombining semantic segmentation and SLAM techniques. This paper introduces a\ndynamically configurable and highly automated LLM/LVLM-powered pipeline for\nevaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark).\nThe study focuses on evaluating state-of-the-art semantic mapping algorithms\nunder varying indoor lighting conditions, a critical challenge in indoor\nenvironments. We introduce a novel dataset with simulated RGB-D sequences and\nground truth 3D reconstructions, facilitating the rigorous analysis of mapping\nperformance across different lighting conditions. Through experiments on\nleading models such as ConceptGraphs, BBQ and OpenScene, we evaluate the\nsemantic fidelity of object recognition and segmentation. Additionally, we\nintroduce a Scene Graph evaluation method to analyze the ability of models to\ninterpret semantic structure. The results provide insights into the robustness\nof these models, forming future research directions for developing resilient\nand adaptable robotic systems. Project page is available at\nhttps://be2rlab.github.io/OSMa-Bench/."
                },
                "authors": [
                    {
                        "name": "Maxim Popov"
                    },
                    {
                        "name": "Regina Kurkova"
                    },
                    {
                        "name": "Mikhail Iumanov"
                    },
                    {
                        "name": "Jaafar Mahmoud"
                    },
                    {
                        "name": "Sergey Kolyubin"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Kolyubin"
                },
                "author": "Sergey Kolyubin",
                "arxiv_comment": "Project page: https://be2rlab.github.io/OSMa-Bench/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07623v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07623v8",
                "updated": "2025-08-12T14:44:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    44,
                    44,
                    1,
                    224,
                    0
                ],
                "published": "2024-05-13T10:30:33Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    10,
                    30,
                    33,
                    0,
                    134,
                    0
                ],
                "title": "Optimizing Class-Level Probability Reweighting Coefficients for\n  Equitable Prompting Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Class-Level Probability Reweighting Coefficients for\n  Equitable Prompting Accuracy"
                },
                "summary": "Even as we engineer LLMs for alignment and safety, they often uncover biases\nfrom pre-training data's statistical regularities (from disproportionate\nco-occurrences to stereotypical associations mirroring human cognitive biases).\nThis leads to persistent, uneven class accuracy in classification and QA. Such\nper-class accuracy disparities are not inherently resolved by\narchitectural/training evolutions or data scaling, making post-hoc correction\nessential for equitable performance. To mitigate LLM class accuracy imbalance,\nwe develop a post-hoc probability reweighting method that directly optimizes\nfor non-differentiable performance-driven and fairness-aligned metrics, through\na novel COBias metric that highlights disparities in class accuracies. This\npost-hoc bias mitigation method is grounded in discrete optimization with\nnonlinear integer programming (NIP) objectives and an efficient metaheuristic\nsolution framework with theoretical convergence guarantees. Operating\nmodel-agnostically, it learns reweighting coefficients from output class\nprobabilities to adjust LLM inference outputs without internal weight updates.\nEvaluations demonstrate its effectiveness: reducing COBias (61% relative\nreduction), increasing overall accuracy (18% relative increase), and achieving\nrobust within-task generalization across diverse prompt configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even as we engineer LLMs for alignment and safety, they often uncover biases\nfrom pre-training data's statistical regularities (from disproportionate\nco-occurrences to stereotypical associations mirroring human cognitive biases).\nThis leads to persistent, uneven class accuracy in classification and QA. Such\nper-class accuracy disparities are not inherently resolved by\narchitectural/training evolutions or data scaling, making post-hoc correction\nessential for equitable performance. To mitigate LLM class accuracy imbalance,\nwe develop a post-hoc probability reweighting method that directly optimizes\nfor non-differentiable performance-driven and fairness-aligned metrics, through\na novel COBias metric that highlights disparities in class accuracies. This\npost-hoc bias mitigation method is grounded in discrete optimization with\nnonlinear integer programming (NIP) objectives and an efficient metaheuristic\nsolution framework with theoretical convergence guarantees. Operating\nmodel-agnostically, it learns reweighting coefficients from output class\nprobabilities to adjust LLM inference outputs without internal weight updates.\nEvaluations demonstrate its effectiveness: reducing COBias (61% relative\nreduction), increasing overall accuracy (18% relative increase), and achieving\nrobust within-task generalization across diverse prompt configurations."
                },
                "authors": [
                    {
                        "name": "Ruixi Lin"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07623v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07623v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16971v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16971v5",
                "updated": "2025-08-12T14:37:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    37,
                    1,
                    1,
                    224,
                    0
                ],
                "published": "2024-03-25T17:32:23Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    17,
                    32,
                    23,
                    0,
                    85,
                    0
                ],
                "title": "AIOS: LLM Agent Operating System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIOS: LLM Agent Operating System"
                },
                "summary": "LLM-based intelligent agents face significant deployment challenges,\nparticularly related to resource management. Allowing unrestricted access to\nLLM or tool resources can lead to inefficient or even potentially harmful\nresource allocation and utilization for agents. Furthermore, the absence of\nproper scheduling and resource management mechanisms in current agent designs\nhinders concurrent processing and limits overall system efficiency. To address\nthese challenges, this paper proposes the architecture of AIOS (LLM-based AI\nAgent Operating System) under the context of managing LLM-based agents. It\nintroduces a novel architecture for serving LLM-based agents by isolating\nresources and LLM-specific services from agent applications into an AIOS\nkernel. This AIOS kernel provides fundamental services (e.g., scheduling,\ncontext management, memory management, storage management, access control) for\nruntime agents. To enhance usability, AIOS also includes an AIOS SDK, a\ncomprehensive suite of APIs designed for utilizing functionalities provided by\nthe AIOS kernel. Experimental results demonstrate that using AIOS can achieve\nup to 2.1x faster execution for serving agents built by various agent\nframeworks. The source code is available at\nhttps://github.com/agiresearch/AIOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based intelligent agents face significant deployment challenges,\nparticularly related to resource management. Allowing unrestricted access to\nLLM or tool resources can lead to inefficient or even potentially harmful\nresource allocation and utilization for agents. Furthermore, the absence of\nproper scheduling and resource management mechanisms in current agent designs\nhinders concurrent processing and limits overall system efficiency. To address\nthese challenges, this paper proposes the architecture of AIOS (LLM-based AI\nAgent Operating System) under the context of managing LLM-based agents. It\nintroduces a novel architecture for serving LLM-based agents by isolating\nresources and LLM-specific services from agent applications into an AIOS\nkernel. This AIOS kernel provides fundamental services (e.g., scheduling,\ncontext management, memory management, storage management, access control) for\nruntime agents. To enhance usability, AIOS also includes an AIOS SDK, a\ncomprehensive suite of APIs designed for utilizing functionalities provided by\nthe AIOS kernel. Experimental results demonstrate that using AIOS can achieve\nup to 2.1x faster execution for serving agents built by various agent\nframeworks. The source code is available at\nhttps://github.com/agiresearch/AIOS."
                },
                "authors": [
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Zelong Li"
                    },
                    {
                        "name": "Shuyuan Xu"
                    },
                    {
                        "name": "Ruosong Ye"
                    },
                    {
                        "name": "Yingqiang Ge"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "Published as a full paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16971v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16971v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08961v2",
                "updated": "2025-08-13T08:08:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    8,
                    40,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T14:23:26Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    23,
                    26,
                    1,
                    224,
                    0
                ],
                "title": "DualSpeechLM: Towards Unified Speech Understanding and Generation via\n  Dual Speech Token Modeling with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualSpeechLM: Towards Unified Speech Understanding and Generation via\n  Dual Speech Token Modeling with Large Language Models"
                },
                "summary": "Extending pre-trained Large Language Models (LLMs)'s speech understanding or\ngeneration abilities by introducing various effective speech tokens has\nattracted great attention in the speech community. However, building a unified\nspeech understanding and generation model still faces the following challenges:\n(1) Due to the huge modality gap between speech tokens and text tokens,\nextending text LLMs to unified speech LLMs relies on large-scale paired data\nfor fine-tuning, and (2) Generation and understanding tasks prefer information\nat different levels, e.g., generation benefits from detailed acoustic features,\nwhile understanding favors high-level semantics. This divergence leads to\ndifficult performance optimization in one unified model. To solve these\nchallenges, in this paper, we present two key insights in speech tokenization\nand speech language modeling. Specifically, we first propose an\nUnderstanding-driven Speech Tokenizer (USTokenizer), which extracts high-level\nsemantic information essential for accomplishing understanding tasks using text\nLLMs. In this way, USToken enjoys better modality commonality with text, which\nreduces the difficulty of modality alignment in adapting text LLMs to speech\nLLMs. Secondly, we present DualSpeechLM, a dual-token modeling framework that\nconcurrently models USToken as input and acoustic token as output within a\nunified, end-to-end framework, seamlessly integrating speech understanding and\ngeneration capabilities. Furthermore, we propose a novel semantic supervision\nloss and a Chain-of-Condition (CoC) strategy to stabilize model training and\nenhance speech generation performance. Experimental results demonstrate that\nour proposed approach effectively fosters a complementary relationship between\nunderstanding and generation tasks, highlighting the promising strategy of\nmutually enhancing both tasks in one unified model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending pre-trained Large Language Models (LLMs)'s speech understanding or\ngeneration abilities by introducing various effective speech tokens has\nattracted great attention in the speech community. However, building a unified\nspeech understanding and generation model still faces the following challenges:\n(1) Due to the huge modality gap between speech tokens and text tokens,\nextending text LLMs to unified speech LLMs relies on large-scale paired data\nfor fine-tuning, and (2) Generation and understanding tasks prefer information\nat different levels, e.g., generation benefits from detailed acoustic features,\nwhile understanding favors high-level semantics. This divergence leads to\ndifficult performance optimization in one unified model. To solve these\nchallenges, in this paper, we present two key insights in speech tokenization\nand speech language modeling. Specifically, we first propose an\nUnderstanding-driven Speech Tokenizer (USTokenizer), which extracts high-level\nsemantic information essential for accomplishing understanding tasks using text\nLLMs. In this way, USToken enjoys better modality commonality with text, which\nreduces the difficulty of modality alignment in adapting text LLMs to speech\nLLMs. Secondly, we present DualSpeechLM, a dual-token modeling framework that\nconcurrently models USToken as input and acoustic token as output within a\nunified, end-to-end framework, seamlessly integrating speech understanding and\ngeneration capabilities. Furthermore, we propose a novel semantic supervision\nloss and a Chain-of-Condition (CoC) strategy to stabilize model training and\nenhance speech generation performance. Experimental results demonstrate that\nour proposed approach effectively fosters a complementary relationship between\nunderstanding and generation tasks, highlighting the promising strategy of\nmutually enhancing both tasks in one unified model."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Wang"
                    },
                    {
                        "name": "Dongchao Yang"
                    },
                    {
                        "name": "Yiwen Shao"
                    },
                    {
                        "name": "Hangting Chen"
                    },
                    {
                        "name": "Jiankun Zhao"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Helen Meng"
                    },
                    {
                        "name": "Xixin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xixin Wu"
                },
                "author": "Xixin Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14910v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14910v3",
                "updated": "2025-08-13T06:12:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    6,
                    12,
                    51,
                    2,
                    225,
                    0
                ],
                "published": "2025-02-19T06:33:59Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    33,
                    59,
                    2,
                    50,
                    0
                ],
                "title": "EvoP: Robust LLM Inference via Evolutionary Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoP: Robust LLM Inference via Evolutionary Pruning"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing tasks, but their massive size and computational demands\nhinder their deployment in resource-constrained environments. Existing model\npruning methods address this issue by removing redundant structures (e.g.,\nelements, channels, layers) from the model. However, these methods employ a\nheuristic pruning strategy, which leads to suboptimal performance. Besides,\nthey also ignore the data characteristics when pruning the model.\n  To overcome these limitations, we propose EvoP, an evolutionary pruning\nframework for robust LLM inference. EvoP first presents a cluster-based\ncalibration dataset sampling (CCDS) strategy for creating a more diverse\ncalibration dataset. EvoP then introduces an evolutionary pruning pattern\nsearching (EPPS) method to find the optimal pruning pattern. Compared to\nexisting model pruning techniques, EvoP achieves the best performance while\nmaintaining the best efficiency. Experiments across different LLMs and\ndifferent downstream tasks validate the effectiveness of the proposed EvoP,\nmaking it a practical and scalable solution for deploying LLMs in real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing tasks, but their massive size and computational demands\nhinder their deployment in resource-constrained environments. Existing model\npruning methods address this issue by removing redundant structures (e.g.,\nelements, channels, layers) from the model. However, these methods employ a\nheuristic pruning strategy, which leads to suboptimal performance. Besides,\nthey also ignore the data characteristics when pruning the model.\n  To overcome these limitations, we propose EvoP, an evolutionary pruning\nframework for robust LLM inference. EvoP first presents a cluster-based\ncalibration dataset sampling (CCDS) strategy for creating a more diverse\ncalibration dataset. EvoP then introduces an evolutionary pruning pattern\nsearching (EPPS) method to find the optimal pruning pattern. Compared to\nexisting model pruning techniques, EvoP achieves the best performance while\nmaintaining the best efficiency. Experiments across different LLMs and\ndifferent downstream tasks validate the effectiveness of the proposed EvoP,\nmaking it a practical and scalable solution for deploying LLMs in real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Hongchao Du"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Shuai Chen"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    },
                    {
                        "name": "Nan Guan"
                    },
                    {
                        "name": "Chun Jason Xue"
                    }
                ],
                "author_detail": {
                    "name": "Chun Jason Xue"
                },
                "author": "Chun Jason Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14910v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14910v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08243v2",
                "updated": "2025-08-12T14:19:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    19,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T17:56:06Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    56,
                    6,
                    0,
                    223,
                    0
                ],
                "title": "Jinx: Unlimited LLMs for Probing Alignment Failures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jinx: Unlimited LLMs for Probing Alignment Failures"
                },
                "summary": "Unlimited, or so-called helpful-only language models are trained without\nsafety alignment constraints and never refuse user queries. They are widely\nused by leading AI companies as internal tools for red teaming and alignment\nevaluation. For example, if a safety-aligned model produces harmful outputs\nsimilar to an unlimited model, this indicates alignment failures that require\nfurther attention. Despite their essential role in assessing alignment, such\nmodels are not available to the research community.\n  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx\nresponds to all queries without refusals or safety filtering, while preserving\nthe base model's capabilities in reasoning and instruction following. It\nprovides researchers with an accessible tool for probing alignment failures,\nevaluating safety boundaries, and systematically studying failure modes in\nlanguage model safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlimited, or so-called helpful-only language models are trained without\nsafety alignment constraints and never refuse user queries. They are widely\nused by leading AI companies as internal tools for red teaming and alignment\nevaluation. For example, if a safety-aligned model produces harmful outputs\nsimilar to an unlimited model, this indicates alignment failures that require\nfurther attention. Despite their essential role in assessing alignment, such\nmodels are not available to the research community.\n  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx\nresponds to all queries without refusals or safety filtering, while preserving\nthe base model's capabilities in reasoning and instruction following. It\nprovides researchers with an accessible tool for probing alignment failures,\nevaluating safety boundaries, and systematically studying failure modes in\nlanguage model safety."
                },
                "authors": [
                    {
                        "name": "Jiahao Zhao"
                    },
                    {
                        "name": "Liwei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Dong"
                },
                "author": "Liwei Dong",
                "arxiv_comment": "https://huggingface.co/Jinx-org",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13983v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13983v5",
                "updated": "2025-08-12T14:19:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    19,
                    15,
                    1,
                    224,
                    0
                ],
                "published": "2025-01-23T06:57:24Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    57,
                    24,
                    3,
                    23,
                    0
                ],
                "title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) are pre-trained on ultra-large-scale corpora,\nthe problem of data contamination is becoming increasingly serious, and there\nis a risk that static evaluation benchmarks overestimate the performance of\nLLMs. To address this, this paper proposes a dynamic data evaluation method\ncalled AdEval (Alignment-based Dynamic Evaluation). AdEval first extracts\nknowledge points and main ideas from static datasets to achieve dynamic\nalignment with the core content of static benchmarks, and by avoiding direct\nreliance on static datasets, it inherently reduces the risk of data\ncontamination from the source. It then obtains background information through\nonline searches to generate detailed descriptions of the knowledge points.\nFinally, it designs questions based on Bloom's cognitive hierarchy across six\ndimensions-remembering, understanding, applying, analyzing, evaluating, and\ncreating to enable multi-level cognitive assessment. Additionally, AdEval\ncontrols the complexity of dynamically generated datasets through iterative\nquestion reconstruction. Experimental results on multiple datasets show that\nAdEval effectively alleviates the impact of data contamination on evaluation\nresults, solves the problems of insufficient complexity control and\nsingle-dimensional evaluation, and improves the fairness, reliability and\ndiversity of LLMs evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are pre-trained on ultra-large-scale corpora,\nthe problem of data contamination is becoming increasingly serious, and there\nis a risk that static evaluation benchmarks overestimate the performance of\nLLMs. To address this, this paper proposes a dynamic data evaluation method\ncalled AdEval (Alignment-based Dynamic Evaluation). AdEval first extracts\nknowledge points and main ideas from static datasets to achieve dynamic\nalignment with the core content of static benchmarks, and by avoiding direct\nreliance on static datasets, it inherently reduces the risk of data\ncontamination from the source. It then obtains background information through\nonline searches to generate detailed descriptions of the knowledge points.\nFinally, it designs questions based on Bloom's cognitive hierarchy across six\ndimensions-remembering, understanding, applying, analyzing, evaluating, and\ncreating to enable multi-level cognitive assessment. Additionally, AdEval\ncontrols the complexity of dynamically generated datasets through iterative\nquestion reconstruction. Experimental results on multiple datasets show that\nAdEval effectively alleviates the impact of data contamination on evaluation\nresults, solves the problems of insufficient complexity control and\nsingle-dimensional evaluation, and improves the fairness, reliability and\ndiversity of LLMs evaluation."
                },
                "authors": [
                    {
                        "name": "Yang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Yang Fan"
                },
                "author": "Yang Fan",
                "arxiv_comment": "There are serious academic problems in this paper, such as data\n  falsification and plagiarism in the method of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13983v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13983v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08952v1",
                "updated": "2025-08-12T14:06:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    6,
                    6,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:06:06Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    6,
                    6,
                    1,
                    224,
                    0
                ],
                "title": "Toward Automated Hypervisor Scenario Generation Based on VM Workload\n  Profiling for Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Automated Hypervisor Scenario Generation Based on VM Workload\n  Profiling for Resource-Constrained Environments"
                },
                "summary": "In the automotive industry, the rise of software-defined vehicles (SDVs) has\n  driven a shift toward virtualization-based architectures that consolidate\n  diverse automotive workloads on a shared hardware platform. To support this\n  evolution, chipset vendors provide board support packages (BSPs), hypervisor\n  setups, and resource allocation guidelines. However, adapting these static\n  configurations to varying system requirements and workloads remain a\n  significant challenge for Tier 1 integrators.\n  This paper presents an automated scenario generation framework, which helps\n  automotive vendors to allocate hardware resources efficiently across multiple\n  VMs. By profiling runtime behavior and integrating both theoretical models\nand\n  vendor heuristics, the proposed tool generates optimized hypervisor\n  configurations tailored to system constraints.\n  We compare two main approaches for modeling target QoS based on profiled data\n  and resource allocation: domain-guided parametric modeling and deep\n  learning-based modeling. We further describe our optimization strategy using\n  the selected QoS model to derive efficient resource allocations. Finally, we\n  report on real-world deployments to demonstrate the effectiveness of our\n  framework in improving integration efficiency and reducing development time\nin\n  resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the automotive industry, the rise of software-defined vehicles (SDVs) has\n  driven a shift toward virtualization-based architectures that consolidate\n  diverse automotive workloads on a shared hardware platform. To support this\n  evolution, chipset vendors provide board support packages (BSPs), hypervisor\n  setups, and resource allocation guidelines. However, adapting these static\n  configurations to varying system requirements and workloads remain a\n  significant challenge for Tier 1 integrators.\n  This paper presents an automated scenario generation framework, which helps\n  automotive vendors to allocate hardware resources efficiently across multiple\n  VMs. By profiling runtime behavior and integrating both theoretical models\nand\n  vendor heuristics, the proposed tool generates optimized hypervisor\n  configurations tailored to system constraints.\n  We compare two main approaches for modeling target QoS based on profiled data\n  and resource allocation: domain-guided parametric modeling and deep\n  learning-based modeling. We further describe our optimization strategy using\n  the selected QoS model to derive efficient resource allocations. Finally, we\n  report on real-world deployments to demonstrate the effectiveness of our\n  framework in improving integration efficiency and reducing development time\nin\n  resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hyunwoo Kim"
                    },
                    {
                        "name": "Jaeseong Lee"
                    },
                    {
                        "name": "Sunpyo Hong"
                    },
                    {
                        "name": "Changmin Han"
                    }
                ],
                "author_detail": {
                    "name": "Changmin Han"
                },
                "author": "Changmin Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08951v1",
                "updated": "2025-08-12T14:06:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    6,
                    2,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:06:02Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    6,
                    2,
                    1,
                    224,
                    0
                ],
                "title": "ShearView: A Compact Stress- and Strain-Controlled Rheometer for\n  Integrated Rheo-microscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShearView: A Compact Stress- and Strain-Controlled Rheometer for\n  Integrated Rheo-microscopy"
                },
                "summary": "We present ShearView, a compact, cost-effective, and open-source rheometer\nthat enables both strain- and stress-controlled oscillatory shear experiments,\nwhile being fully compatible with high-resolution optical microscopy. Designed\nfor transparency and modularity, the device integrates mechanical simplicity,\ndual feedback control, and real-time synchronization of rheological and optical\ndata, thereby enabling simultaneous investigation of macroscopic mechanical\nresponse and microscopic structural dynamics across a wide range of soft matter\nsystems. ShearView is primarily constructed from off-the-shelf components and\noperated via custom LabVIEW software. Calibration procedures and feedback\nalgorithms allow for the accurate application of arbitrary stress or strain\nwaveforms in both linear and nonlinear regimes. We validate the instrument\nagainst a commercial rheometer (Anton Paar MCR 702e), demonstrating excellent\nagreement in frequency sweeps performed in the linear viscoelastic regime and\nlarge-amplitude oscillatory shear for the materials and frequency ranges tested\nhere. In addition, we implement non-standard rheological protocols such as\nchirped oscillations and recovery rheology. We further illustrate the system\ncapabilities through synchronized imaging during echo and shear-cessation\nprotocols, highlighting its potential to link bulk rheological response with\nunderlying microscopic dynamics. All hardware designs, control software, and\nexample datasets are freely available to facilitate reuse, customization, and\neducational deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ShearView, a compact, cost-effective, and open-source rheometer\nthat enables both strain- and stress-controlled oscillatory shear experiments,\nwhile being fully compatible with high-resolution optical microscopy. Designed\nfor transparency and modularity, the device integrates mechanical simplicity,\ndual feedback control, and real-time synchronization of rheological and optical\ndata, thereby enabling simultaneous investigation of macroscopic mechanical\nresponse and microscopic structural dynamics across a wide range of soft matter\nsystems. ShearView is primarily constructed from off-the-shelf components and\noperated via custom LabVIEW software. Calibration procedures and feedback\nalgorithms allow for the accurate application of arbitrary stress or strain\nwaveforms in both linear and nonlinear regimes. We validate the instrument\nagainst a commercial rheometer (Anton Paar MCR 702e), demonstrating excellent\nagreement in frequency sweeps performed in the linear viscoelastic regime and\nlarge-amplitude oscillatory shear for the materials and frequency ranges tested\nhere. In addition, we implement non-standard rheological protocols such as\nchirped oscillations and recovery rheology. We further illustrate the system\ncapabilities through synchronized imaging during echo and shear-cessation\nprotocols, highlighting its potential to link bulk rheological response with\nunderlying microscopic dynamics. All hardware designs, control software, and\nexample datasets are freely available to facilitate reuse, customization, and\neducational deployment."
                },
                "authors": [
                    {
                        "name": "Nikolaos Kalafatakis"
                    },
                    {
                        "name": "Roberto Cerbino"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Cerbino"
                },
                "author": "Roberto Cerbino",
                "arxiv_comment": "Main text (16 pages) + Supplementary Material (12 pages)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18246v2",
                "updated": "2025-08-12T14:03:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    3,
                    22,
                    1,
                    224,
                    0
                ],
                "published": "2024-10-23T19:45:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    45,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Maintenance Optimization for Asset Networks with Unknown Degradation\n  Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintenance Optimization for Asset Networks with Unknown Degradation\n  Parameters"
                },
                "summary": "We consider the key practical challenge of multi-asset maintenance\noptimization in settings where degradation parameters are heterogeneous and\nunknown, and must be inferred from degradation data. To address this, we\npropose scalable methods suitable for complex asset networks. Degradation is\nmodeled as a stochastic shock process, and real-time data are continuously\nincorporated into estimation of shock rates and magnitudes via a Bayesian\nframework. This constitutes a partially observable Markov decision process\nformulation, from which we analytically derive monotonic policy structures.\nMoreover, we propose an open-loop feedback approach that enables policies\ntrained via deep reinforcement learning (DRL) in a simulation environment with\naccess to the true parameters to remain effective when deployed with real-time\nBayesian point estimates instead. Complementing this, we develop a Bayesian\nMarkov decision process (BMDP) framework wherein the agent maintains and\nupdates posterior distributions during deployment. This formulation captures\nthe evolution of parameter uncertainty over time, thereby facilitating the\ntraining of scalable DRL-based policies that adapt as additional data become\navailable.\n  We validate our approach through experiments on synthetic asset networks and\na real-world case involving interventional X-ray system filaments. We find that\nthe proposed DRL methods consistently outperform traditional heuristics across\nvarious scenarios. The policies trained for the BMDP perform well even when\npriors must be estimated from historical data, and remain effective in networks\nwith high asset heterogeneity. Knowledge of true degradation parameters yields\nonly marginal cost benefits, underscoring the ability of our approach to make\neffective decisions under limited information on degradation processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the key practical challenge of multi-asset maintenance\noptimization in settings where degradation parameters are heterogeneous and\nunknown, and must be inferred from degradation data. To address this, we\npropose scalable methods suitable for complex asset networks. Degradation is\nmodeled as a stochastic shock process, and real-time data are continuously\nincorporated into estimation of shock rates and magnitudes via a Bayesian\nframework. This constitutes a partially observable Markov decision process\nformulation, from which we analytically derive monotonic policy structures.\nMoreover, we propose an open-loop feedback approach that enables policies\ntrained via deep reinforcement learning (DRL) in a simulation environment with\naccess to the true parameters to remain effective when deployed with real-time\nBayesian point estimates instead. Complementing this, we develop a Bayesian\nMarkov decision process (BMDP) framework wherein the agent maintains and\nupdates posterior distributions during deployment. This formulation captures\nthe evolution of parameter uncertainty over time, thereby facilitating the\ntraining of scalable DRL-based policies that adapt as additional data become\navailable.\n  We validate our approach through experiments on synthetic asset networks and\na real-world case involving interventional X-ray system filaments. We find that\nthe proposed DRL methods consistently outperform traditional heuristics across\nvarious scenarios. The policies trained for the BMDP perform well even when\npriors must be estimated from historical data, and remain effective in networks\nwith high asset heterogeneity. Knowledge of true degradation parameters yields\nonly marginal cost benefits, underscoring the ability of our approach to make\neffective decisions under limited information on degradation processes."
                },
                "authors": [
                    {
                        "name": "Peter Verleijsdonk"
                    },
                    {
                        "name": "Collin Drent"
                    },
                    {
                        "name": "Stella Kapodistria"
                    },
                    {
                        "name": "Willem van Jaarsveld"
                    }
                ],
                "author_detail": {
                    "name": "Willem van Jaarsveld"
                },
                "author": "Willem van Jaarsveld",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08947v1",
                "updated": "2025-08-12T14:00:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    0,
                    12,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:00:12Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    0,
                    12,
                    1,
                    224,
                    0
                ],
                "title": "Generalising Traffic Forecasting to Regions without Traffic Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalising Traffic Forecasting to Regions without Traffic Observations"
                },
                "summary": "Traffic forecasting is essential for intelligent transportation systems.\nAccurate forecasting relies on continuous observations collected by traffic\nsensors. However, due to high deployment and maintenance costs, not all regions\nare equipped with such sensors. This paper aims to forecast for regions without\ntraffic sensors, where the lack of historical traffic observations challenges\nthe generalisability of existing models. We propose a model named GenCast, the\ncore idea of which is to exploit external knowledge to compensate for the\nmissing observations and to enhance generalisation. We integrate\nphysics-informed neural networks into GenCast, enabling physical principles to\nregularise the learning process. We introduce an external signal learning\nmodule to explore correlations between traffic states and external signals such\nas weather conditions, further improving model generalisability. Additionally,\nwe design a spatial grouping module to filter localised features that hinder\nmodel generalisability. Extensive experiments show that GenCast consistently\nreduces forecasting errors on multiple real-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic forecasting is essential for intelligent transportation systems.\nAccurate forecasting relies on continuous observations collected by traffic\nsensors. However, due to high deployment and maintenance costs, not all regions\nare equipped with such sensors. This paper aims to forecast for regions without\ntraffic sensors, where the lack of historical traffic observations challenges\nthe generalisability of existing models. We propose a model named GenCast, the\ncore idea of which is to exploit external knowledge to compensate for the\nmissing observations and to enhance generalisation. We integrate\nphysics-informed neural networks into GenCast, enabling physical principles to\nregularise the learning process. We introduce an external signal learning\nmodule to explore correlations between traffic states and external signals such\nas weather conditions, further improving model generalisability. Additionally,\nwe design a spatial grouping module to filter localised features that hinder\nmodel generalisability. Extensive experiments show that GenCast consistently\nreduces forecasting errors on multiple real-world datasets."
                },
                "authors": [
                    {
                        "name": "Xinyu Su"
                    },
                    {
                        "name": "Majid Sarvi"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Egemen Tanin"
                    },
                    {
                        "name": "Jianzhong Qi"
                    }
                ],
                "author_detail": {
                    "name": "Jianzhong Qi"
                },
                "author": "Jianzhong Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06615v2",
                "updated": "2025-08-12T13:52:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    52,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-08T18:05:30Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    18,
                    5,
                    30,
                    4,
                    220,
                    0
                ],
                "title": "Iris RESTful Server and IrisTileSource: An Iris implementation for\n  existing OpenSeaDragon viewers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris RESTful Server and IrisTileSource: An Iris implementation for\n  existing OpenSeaDragon viewers"
                },
                "summary": "The Iris File Extension (IFE) is a low overhead performance-oriented whole\nslide image (WSI) file format designed to improve the image rendering\nexperience for pathologists and simplify image management for system\nadministrators. However, static hypertext transfer protocol (HTTP) file servers\ncannot natively stream subregions of high-resolution image files, such as the\nIFE. The majority of contemporary WSI viewer systems are designed as\nbrowser-based web applications and leverage OpenSeaDragon as the tile-based\nrendering framework. These systems convert WSI files to Deep Zoom Images (DZI)\nfor compatibility with simple static HTTP file servers. In order to address\nthis limitation, we have developed the Iris RESTful Server, a low-overhead HTTP\nserver with a RESTful API that is natively compatible with the DICOMweb WADO-RS\nAPI. Written in C++ with Boost Beast HTTP and Asio networking libraries atop\nthe public IFE libraries, the server offers both security and high performance.\nTesting shows that a single instance can handle over 5000 tile requests per\nsecond with a median latency of 21 ms on a private network. We also developed\nand merged a new OpenSeaDragon TileSource, compatible with the Iris RESTful\nAPI, into the next OpenSeaDragon release, enabling simple and immediate drop-in\nreplacement of DZI images within WSI viewer stacks. Designed as a secure\ncross-origin resource sharing microservice, this architecture includes detailed\ndeployment instructions for new or existing WSI workflows, and a public test\nsubdomain (examples.restful.irisdigitalpathology.org ) is provided as a\ndevelopment tool to accelerate WSI web viewer development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Iris File Extension (IFE) is a low overhead performance-oriented whole\nslide image (WSI) file format designed to improve the image rendering\nexperience for pathologists and simplify image management for system\nadministrators. However, static hypertext transfer protocol (HTTP) file servers\ncannot natively stream subregions of high-resolution image files, such as the\nIFE. The majority of contemporary WSI viewer systems are designed as\nbrowser-based web applications and leverage OpenSeaDragon as the tile-based\nrendering framework. These systems convert WSI files to Deep Zoom Images (DZI)\nfor compatibility with simple static HTTP file servers. In order to address\nthis limitation, we have developed the Iris RESTful Server, a low-overhead HTTP\nserver with a RESTful API that is natively compatible with the DICOMweb WADO-RS\nAPI. Written in C++ with Boost Beast HTTP and Asio networking libraries atop\nthe public IFE libraries, the server offers both security and high performance.\nTesting shows that a single instance can handle over 5000 tile requests per\nsecond with a median latency of 21 ms on a private network. We also developed\nand merged a new OpenSeaDragon TileSource, compatible with the Iris RESTful\nAPI, into the next OpenSeaDragon release, enabling simple and immediate drop-in\nreplacement of DZI images within WSI viewer stacks. Designed as a secure\ncross-origin resource sharing microservice, this architecture includes detailed\ndeployment instructions for new or existing WSI workflows, and a public test\nsubdomain (examples.restful.irisdigitalpathology.org ) is provided as a\ndevelopment tool to accelerate WSI web viewer development."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater MD"
                    },
                    {
                        "name": "Navin Kathawa"
                    },
                    {
                        "name": "Mustafa Yousif MD"
                    },
                    {
                        "name": "Ulysses Balis MD"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis MD"
                },
                "author": "Ulysses Balis MD",
                "arxiv_comment": "10 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08942v1",
                "updated": "2025-08-12T13:50:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    50,
                    25,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T13:50:25Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    50,
                    25,
                    1,
                    224,
                    0
                ],
                "title": "Jointly Generating and Attributing Answers using Logits of\n  Document-Identifier Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jointly Generating and Attributing Answers using Logits of\n  Document-Identifier Tokens"
                },
                "summary": "Despite their impressive performances, Large Language Models (LLMs) remain\nprone to hallucination, which critically undermines their trustworthiness.\nWhile most of the previous work focused on tackling answer and attribution\ncorrectness, a recent line of work investigated faithfulness, with a focus on\nleveraging internal model signals to reflect a model's actual decision-making\nprocess while generating the answer. Nevertheless, these methods induce\nadditional latency and have shown limitations in directly aligning token\ngeneration with attribution generation. In this paper, we introduce LoDIT, a\nmethod that jointly generates and faithfully attributes answers in RAG by\nleveraging specific token logits during generation. It consists of two steps:\n(1) marking the documents with specific token identifiers and then leveraging\nthe logits of these tokens to estimate the contribution of each document to the\nanswer during generation, and (2) aggregating these contributions into document\nattributions. Experiments on a trustworthiness-focused attributed\ntext-generation benchmark, Trust-Align, show that LoDIT significantly\noutperforms state-of-the-art models on several metrics. Finally, an in-depth\nanalysis of LoDIT shows both its efficiency in terms of latency and its\nrobustness in different settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive performances, Large Language Models (LLMs) remain\nprone to hallucination, which critically undermines their trustworthiness.\nWhile most of the previous work focused on tackling answer and attribution\ncorrectness, a recent line of work investigated faithfulness, with a focus on\nleveraging internal model signals to reflect a model's actual decision-making\nprocess while generating the answer. Nevertheless, these methods induce\nadditional latency and have shown limitations in directly aligning token\ngeneration with attribution generation. In this paper, we introduce LoDIT, a\nmethod that jointly generates and faithfully attributes answers in RAG by\nleveraging specific token logits during generation. It consists of two steps:\n(1) marking the documents with specific token identifiers and then leveraging\nthe logits of these tokens to estimate the contribution of each document to the\nanswer during generation, and (2) aggregating these contributions into document\nattributions. Experiments on a trustworthiness-focused attributed\ntext-generation benchmark, Trust-Align, show that LoDIT significantly\noutperforms state-of-the-art models on several metrics. Finally, an in-depth\nanalysis of LoDIT shows both its efficiency in terms of latency and its\nrobustness in different settings."
                },
                "authors": [
                    {
                        "name": "Lucas Albarede"
                    },
                    {
                        "name": "Jose Moreno"
                    },
                    {
                        "name": "Lynda Tamine"
                    },
                    {
                        "name": "Luce Lefeuvre"
                    }
                ],
                "author_detail": {
                    "name": "Luce Lefeuvre"
                },
                "author": "Luce Lefeuvre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08940v1",
                "updated": "2025-08-12T13:48:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    48,
                    3,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T13:48:03Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    48,
                    3,
                    1,
                    224,
                    0
                ],
                "title": "Train Long, Think Short: Curriculum Learning for Efficient Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Train Long, Think Short: Curriculum Learning for Efficient Reasoning"
                },
                "summary": "Recent work on enhancing the reasoning abilities of large language models\n(LLMs) has introduced explicit length control as a means of constraining\ncomputational cost while preserving accuracy. However, existing approaches rely\non fixed-length training budgets, which do not take advantage of the natural\nprogression from exploration to compression during learning. In this work, we\npropose a curriculum learning strategy for length-controlled reasoning using\nGroup Relative Policy Optimization (GRPO). Our method starts with generous\ntoken budgets and gradually tightens them over training, encouraging models to\nfirst discover effective solution strategies and then distill them into more\nconcise reasoning traces. We augment GRPO with a reward function that balances\nthree signals: task correctness (via verifier feedback), length efficiency, and\nformatting adherence (via structural tags). Experiments on GSM8K, MATH500,\nSVAMP, College Math, and GSM+ demonstrate that curriculum-based training\nconsistently outperforms fixed-budget baselines at the same final budget,\nachieving higher accuracy and significantly improved token efficiency. We\nfurther ablate the impact of reward weighting and decay schedule design,\nshowing that progressive constraint serves as a powerful inductive bias for\ntraining efficient reasoning models. Our code and checkpoints are released at:\nhttps://github.com/hammoudhasan/curriculum_grpo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work on enhancing the reasoning abilities of large language models\n(LLMs) has introduced explicit length control as a means of constraining\ncomputational cost while preserving accuracy. However, existing approaches rely\non fixed-length training budgets, which do not take advantage of the natural\nprogression from exploration to compression during learning. In this work, we\npropose a curriculum learning strategy for length-controlled reasoning using\nGroup Relative Policy Optimization (GRPO). Our method starts with generous\ntoken budgets and gradually tightens them over training, encouraging models to\nfirst discover effective solution strategies and then distill them into more\nconcise reasoning traces. We augment GRPO with a reward function that balances\nthree signals: task correctness (via verifier feedback), length efficiency, and\nformatting adherence (via structural tags). Experiments on GSM8K, MATH500,\nSVAMP, College Math, and GSM+ demonstrate that curriculum-based training\nconsistently outperforms fixed-budget baselines at the same final budget,\nachieving higher accuracy and significantly improved token efficiency. We\nfurther ablate the impact of reward weighting and decay schedule design,\nshowing that progressive constraint serves as a powerful inductive bias for\ntraining efficient reasoning models. Our code and checkpoints are released at:\nhttps://github.com/hammoudhasan/curriculum_grpo."
                },
                "authors": [
                    {
                        "name": "Hasan Abed Al Kader Hammoud"
                    },
                    {
                        "name": "Kumail Alhamoud"
                    },
                    {
                        "name": "Abed Hammoud"
                    },
                    {
                        "name": "Elie Bou-Zeid"
                    },
                    {
                        "name": "Marzyeh Ghassemi"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08939v1",
                "updated": "2025-08-12T13:47:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    47,
                    27,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T13:47:27Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    47,
                    27,
                    1,
                    224,
                    0
                ],
                "title": "MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple\n  Prompt Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple\n  Prompt Aggregation"
                },
                "summary": "Face Morphing Attack Detection (MAD) is a critical challenge in face\nrecognition security, where attackers can fool systems by interpolating the\nidentity information of two or more individuals into a single face image,\nresulting in samples that can be verified as belonging to multiple identities\nby face recognition systems. While multimodal foundation models (FMs) like CLIP\noffer strong zero-shot capabilities by jointly modeling images and text, most\nprior works on FMs for biometric recognition have relied on fine-tuning for\nspecific downstream tasks, neglecting their potential for direct, generalizable\ndeployment. This work explores a pure zero-shot approach to MAD by leveraging\nCLIP without any additional training or fine-tuning, focusing instead on the\ndesign and aggregation of multiple textual prompts per class. By aggregating\nthe embeddings of diverse prompts, we better align the model's internal\nrepresentations with the MAD task, capturing richer and more varied cues\nindicative of bona-fide or attack samples. Our results show that prompt\naggregation substantially improves zero-shot detection performance,\ndemonstrating the effectiveness of exploiting foundation models' built-in\nmultimodal knowledge through efficient prompt engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face Morphing Attack Detection (MAD) is a critical challenge in face\nrecognition security, where attackers can fool systems by interpolating the\nidentity information of two or more individuals into a single face image,\nresulting in samples that can be verified as belonging to multiple identities\nby face recognition systems. While multimodal foundation models (FMs) like CLIP\noffer strong zero-shot capabilities by jointly modeling images and text, most\nprior works on FMs for biometric recognition have relied on fine-tuning for\nspecific downstream tasks, neglecting their potential for direct, generalizable\ndeployment. This work explores a pure zero-shot approach to MAD by leveraging\nCLIP without any additional training or fine-tuning, focusing instead on the\ndesign and aggregation of multiple textual prompts per class. By aggregating\nthe embeddings of diverse prompts, we better align the model's internal\nrepresentations with the MAD task, capturing richer and more varied cues\nindicative of bona-fide or attack samples. Our results show that prompt\naggregation substantially improves zero-shot detection performance,\ndemonstrating the effectiveness of exploiting foundation models' built-in\nmultimodal knowledge through efficient prompt engineering."
                },
                "authors": [
                    {
                        "name": "Eduarda Caldeira"
                    },
                    {
                        "name": "Fadi Boutros"
                    },
                    {
                        "name": "Naser Damer"
                    }
                ],
                "author_detail": {
                    "name": "Naser Damer"
                },
                "author": "Naser Damer",
                "arxiv_doi": "10.1145/3728425.3759909",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3728425.3759909",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.08939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at ACM Multimedia Workshops",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07995v2",
                "updated": "2025-08-12T13:46:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    46,
                    8,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T13:57:49Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    57,
                    49,
                    0,
                    223,
                    0
                ],
                "title": "DIVER: A Multi-Stage Approach for Reasoning-intensive Information\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIVER: A Multi-Stage Approach for Reasoning-intensive Information\n  Retrieval"
                },
                "summary": "Retrieval-augmented generation has achieved strong performance on\nknowledge-intensive tasks where query-document relevance can be identified\nthrough direct lexical or semantic matches. However, many real-world queries\ninvolve abstract reasoning, analogical thinking, or multi-step inference, which\nexisting retrievers often struggle to capture. To address this challenge, we\npresent \\textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive\ninformation retrieval. DIVER consists of four components: document processing\nto improve input quality, LLM-driven query expansion via iterative document\ninteraction, a reasoning-enhanced retriever fine-tuned on synthetic\nmulti-domain data with hard negatives, and a pointwise reranker that combines\nLLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,\nDIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original\nqueries, consistently outperforming competitive reasoning-aware models. These\nresults demonstrate the effectiveness of reasoning-aware retrieval strategies\nin complex real-world tasks. Our code and retrieval model will be released\nsoon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation has achieved strong performance on\nknowledge-intensive tasks where query-document relevance can be identified\nthrough direct lexical or semantic matches. However, many real-world queries\ninvolve abstract reasoning, analogical thinking, or multi-step inference, which\nexisting retrievers often struggle to capture. To address this challenge, we\npresent \\textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive\ninformation retrieval. DIVER consists of four components: document processing\nto improve input quality, LLM-driven query expansion via iterative document\ninteraction, a reasoning-enhanced retriever fine-tuned on synthetic\nmulti-domain data with hard negatives, and a pointwise reranker that combines\nLLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,\nDIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original\nqueries, consistently outperforming competitive reasoning-aware models. These\nresults demonstrate the effectiveness of reasoning-aware retrieval strategies\nin complex real-world tasks. Our code and retrieval model will be released\nsoon."
                },
                "authors": [
                    {
                        "name": "Meixiu Long"
                    },
                    {
                        "name": "Duolin Sun"
                    },
                    {
                        "name": "Dan Yang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Jiahai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiahai Wang"
                },
                "author": "Jiahai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03926v2",
                "updated": "2025-08-12T13:40:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    40,
                    29,
                    1,
                    224,
                    0
                ],
                "published": "2025-06-04T13:18:04Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    18,
                    4,
                    2,
                    155,
                    0
                ],
                "title": "Multiple Stochastic Prompt Tuning for Few-shot Adaptation under Extreme\n  Domain Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple Stochastic Prompt Tuning for Few-shot Adaptation under Extreme\n  Domain Shift"
                },
                "summary": "Foundation Vision-Language Models (VLMs) like CLIP exhibit strong\ngeneralization capabilities due to large-scale pretraining on diverse\nimage-text pairs. However, their performance often degrades when applied to\ntarget datasets with significant distribution shifts in both visual appearance\nand class semantics. Recent few-shot learning approaches adapt CLIP to\ndownstream tasks using limited labeled data via adapter or prompt tuning, but\nare not specifically designed to handle such extreme domain shifts. Conversely,\nsome works addressing cross-domain few-shot learning consider such\ndomain-shifted scenarios but operate in an episodic setting with only a few\nclasses per episode, limiting their applicability to real-world deployment,\nwhere all classes must be handled simultaneously. To address this gap, we\npropose a novel framework, MIST (Multiple Stochastic Prompt Tuning), for\nefficiently adapting CLIP to datasets with extreme distribution shifts using\nonly a few labeled examples, in scenarios involving all classes at once.\nSpecifically, we introduce multiple learnable prompts per class to effectively\ncapture diverse modes in visual representations arising from distribution\nshifts. To further enhance generalization, these prompts are modeled as\nlearnable Gaussian distributions, enabling efficient exploration of the prompt\nparameter space and reducing overfitting caused by limited supervision.\nExtensive experiments and comparisons with state-of-the-art methods demonstrate\nthe effectiveness of the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Vision-Language Models (VLMs) like CLIP exhibit strong\ngeneralization capabilities due to large-scale pretraining on diverse\nimage-text pairs. However, their performance often degrades when applied to\ntarget datasets with significant distribution shifts in both visual appearance\nand class semantics. Recent few-shot learning approaches adapt CLIP to\ndownstream tasks using limited labeled data via adapter or prompt tuning, but\nare not specifically designed to handle such extreme domain shifts. Conversely,\nsome works addressing cross-domain few-shot learning consider such\ndomain-shifted scenarios but operate in an episodic setting with only a few\nclasses per episode, limiting their applicability to real-world deployment,\nwhere all classes must be handled simultaneously. To address this gap, we\npropose a novel framework, MIST (Multiple Stochastic Prompt Tuning), for\nefficiently adapting CLIP to datasets with extreme distribution shifts using\nonly a few labeled examples, in scenarios involving all classes at once.\nSpecifically, we introduce multiple learnable prompts per class to effectively\ncapture diverse modes in visual representations arising from distribution\nshifts. To further enhance generalization, these prompts are modeled as\nlearnable Gaussian distributions, enabling efficient exploration of the prompt\nparameter space and reducing overfitting caused by limited supervision.\nExtensive experiments and comparisons with state-of-the-art methods demonstrate\nthe effectiveness of the proposed framework."
                },
                "authors": [
                    {
                        "name": "Debarshi Brahma"
                    },
                    {
                        "name": "Soma Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Soma Biswas"
                },
                "author": "Soma Biswas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08930v1",
                "updated": "2025-08-12T13:32:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    32,
                    18,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T13:32:18Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    32,
                    18,
                    1,
                    224,
                    0
                ],
                "title": "How Does a Virtual Agent Decide Where to Look? -- Symbolic Cognitive\n  Reasoning for Embodied Head Rotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Does a Virtual Agent Decide Where to Look? -- Symbolic Cognitive\n  Reasoning for Embodied Head Rotation"
                },
                "summary": "Natural head rotation is critical for believable embodied virtual agents, yet\nthis micro-level behavior remains largely underexplored. While head-rotation\nprediction algorithms could, in principle, reproduce this behavior, they\ntypically focus on visually salient stimuli and overlook the cognitive motives\nthat guide head rotation. This yields agents that look at conspicuous objects\nwhile overlooking obstacles or task-relevant cues, diminishing realism in a\nvirtual environment. We introduce SCORE, a Symbolic Cognitive Reasoning\nframework for Embodied Head Rotation, a data-agnostic framework that produces\ncontext-aware head movements without task-specific training or hand-tuned\nheuristics. A controlled VR study (N=20) identifies five motivational drivers\nof human head movements: Interest, Information Seeking, Safety, Social Schema,\nand Habit. SCORE encodes these drivers as symbolic predicates, perceives the\nscene with a Vision-Language Model (VLM), and plans head poses with a Large\nLanguage Model (LLM). The framework employs a hybrid workflow: the VLM-LLM\nreasoning is executed offline, after which a lightweight FastVLM performs\nonline validation to suppress hallucinations while maintaining responsiveness\nto scene dynamics. The result is an agent that predicts not only where to look\nbut also why, generalizing to unseen scenes and multi-agent crowds while\nretaining behavioral plausibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural head rotation is critical for believable embodied virtual agents, yet\nthis micro-level behavior remains largely underexplored. While head-rotation\nprediction algorithms could, in principle, reproduce this behavior, they\ntypically focus on visually salient stimuli and overlook the cognitive motives\nthat guide head rotation. This yields agents that look at conspicuous objects\nwhile overlooking obstacles or task-relevant cues, diminishing realism in a\nvirtual environment. We introduce SCORE, a Symbolic Cognitive Reasoning\nframework for Embodied Head Rotation, a data-agnostic framework that produces\ncontext-aware head movements without task-specific training or hand-tuned\nheuristics. A controlled VR study (N=20) identifies five motivational drivers\nof human head movements: Interest, Information Seeking, Safety, Social Schema,\nand Habit. SCORE encodes these drivers as symbolic predicates, perceives the\nscene with a Vision-Language Model (VLM), and plans head poses with a Large\nLanguage Model (LLM). The framework employs a hybrid workflow: the VLM-LLM\nreasoning is executed offline, after which a lightweight FastVLM performs\nonline validation to suppress hallucinations while maintaining responsiveness\nto scene dynamics. The result is an agent that predicts not only where to look\nbut also why, generalizing to unseen scenes and multi-agent crowds while\nretaining behavioral plausibility."
                },
                "authors": [
                    {
                        "name": "Juyeong Hwang"
                    },
                    {
                        "name": "Seong-Eun Hon"
                    },
                    {
                        "name": "JaeYoung Seon"
                    },
                    {
                        "name": "Hyeongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongyeop Kang"
                },
                "author": "Hyeongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08909v1",
                "updated": "2025-08-12T12:58:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    58,
                    12,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T12:58:12Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    58,
                    12,
                    1,
                    224,
                    0
                ],
                "title": "Compass-Thinker-7B Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compass-Thinker-7B Technical Report"
                },
                "summary": "Recent R1-Zero-like research further demonstrates that reasoning extension\nhas given large language models (LLMs) unprecedented reasoning capabilities,\nand Reinforcement Learning is the core technology to elicit its complex\nreasoning. However, conducting RL experiments directly on hyperscale models\ninvolves high computational costs and resource demands, posing significant\nrisks. We propose the Compass-Thinker-7B model, which aims to explore the\npotential of Reinforcement Learning with less computational resources and\ncosts, and provides insights for further research into RL recipes for larger\nmodels. Compass-Thinker-7B is trained from an open source model through a\nspecially designed Reinforcement Learning Pipeline. we curate a dataset of 30k\nverifiable mathematics problems for the Reinforcement Learning Pipeline. By\nconfiguring data and training settings with different difficulty distributions\nfor different stages, the potential of the model is gradually released and the\ntraining efficiency is improved. Extensive evaluations show that\nCompass-Thinker-7B possesses exceptional reasoning potential, and achieves\nsuperior performance on mathematics compared to the same-sized RL\nmodel.Especially in the challenging AIME2024 evaluation, Compass-Thinker-7B\nachieves 40% accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent R1-Zero-like research further demonstrates that reasoning extension\nhas given large language models (LLMs) unprecedented reasoning capabilities,\nand Reinforcement Learning is the core technology to elicit its complex\nreasoning. However, conducting RL experiments directly on hyperscale models\ninvolves high computational costs and resource demands, posing significant\nrisks. We propose the Compass-Thinker-7B model, which aims to explore the\npotential of Reinforcement Learning with less computational resources and\ncosts, and provides insights for further research into RL recipes for larger\nmodels. Compass-Thinker-7B is trained from an open source model through a\nspecially designed Reinforcement Learning Pipeline. we curate a dataset of 30k\nverifiable mathematics problems for the Reinforcement Learning Pipeline. By\nconfiguring data and training settings with different difficulty distributions\nfor different stages, the potential of the model is gradually released and the\ntraining efficiency is improved. Extensive evaluations show that\nCompass-Thinker-7B possesses exceptional reasoning potential, and achieves\nsuperior performance on mathematics compared to the same-sized RL\nmodel.Especially in the challenging AIME2024 evaluation, Compass-Thinker-7B\nachieves 40% accuracy."
                },
                "authors": [
                    {
                        "name": "Anxiang Zeng"
                    },
                    {
                        "name": "Haibo Zhang"
                    },
                    {
                        "name": "Kaixiang Mo"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Shuman Liu"
                    },
                    {
                        "name": "Yanhui Huang"
                    },
                    {
                        "name": "Yawen Liu"
                    },
                    {
                        "name": "Yuepeng Sheng"
                    },
                    {
                        "name": "Yuwei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yuwei Huang"
                },
                "author": "Yuwei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14412v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14412v2",
                "updated": "2025-08-12T12:54:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    54,
                    59,
                    1,
                    224,
                    0
                ],
                "published": "2025-06-17T11:14:22Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    11,
                    14,
                    22,
                    1,
                    168,
                    0
                ],
                "title": "RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG\n  Systems for the SIGIR LiveRAG Competition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG\n  Systems for the SIGIR LiveRAG Competition"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by\ncombining their internal, parametric knowledge with external, non-parametric\nsources, with the goal of improving factual correctness and minimizing\nhallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize\naccuracy on DataMorgana's QA pairs, which are composed of single-hop and\nmulti-hop questions. The challenge provides access to sparse OpenSearch and\ndense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to\nLLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A\njudge-LLM assesses the submitted answers along with human evaluators. By\nexploring distinct retriever combinations and RAG solutions under the challenge\nconditions, our final solution emerged using InstructRAG in combination with a\nPinecone retriever and a BGE reranker. Our solution achieved a correctness\nscore of 1.13 and a faithfulness score of 0.55 in the non-human evaluation,\nplacing it overall in third place in the SIGIR 2025 LiveRAG Challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by\ncombining their internal, parametric knowledge with external, non-parametric\nsources, with the goal of improving factual correctness and minimizing\nhallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize\naccuracy on DataMorgana's QA pairs, which are composed of single-hop and\nmulti-hop questions. The challenge provides access to sparse OpenSearch and\ndense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to\nLLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A\njudge-LLM assesses the submitted answers along with human evaluators. By\nexploring distinct retriever combinations and RAG solutions under the challenge\nconditions, our final solution emerged using InstructRAG in combination with a\nPinecone retriever and a BGE reranker. Our solution achieved a correctness\nscore of 1.13 and a faithfulness score of 0.55 in the non-human evaluation,\nplacing it overall in third place in the SIGIR 2025 LiveRAG Challenge."
                },
                "authors": [
                    {
                        "name": "Tim Cofala"
                    },
                    {
                        "name": "Oleh Astappiev"
                    },
                    {
                        "name": "William Xion"
                    },
                    {
                        "name": "Hailay Teklehaymanot"
                    }
                ],
                "author_detail": {
                    "name": "Hailay Teklehaymanot"
                },
                "author": "Hailay Teklehaymanot",
                "arxiv_comment": "4 pages, 6 figures. Report for SIGIR 2025 LiveRAG Challenge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14412v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14412v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09577v2",
                "updated": "2025-08-12T12:49:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    49,
                    59,
                    1,
                    224,
                    0
                ],
                "published": "2025-02-13T18:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    34,
                    52,
                    3,
                    44,
                    0
                ],
                "title": "Polymind: Parallel Visual Diagramming with Large Language Models to\n  Support Prewriting Through Microtasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polymind: Parallel Visual Diagramming with Large Language Models to\n  Support Prewriting Through Microtasks"
                },
                "summary": "Prewriting is the process of generating and organising ideas before a first\ndraft. It consists of a combination of informal, iterative, and semi-structured\nstrategies such as visual diagramming, which poses a challenge for\ncollaborating with large language models (LLMs) in a turn-taking conversational\nmanner. We present Polymind, a visual diagramming tool that leverages multiple\nLLM-powered agents to support prewriting. The system features a parallel\ncollaboration workflow in place of the turn-taking conversational interactions.\nIt defines multiple ``microtasks'' to simulate group collaboration scenarios\nsuch as collaborative writing and group brainstorming. Instead of repetitively\nprompting a chatbot for various purposes, Polymind enables users to orchestrate\nmultiple microtasks simultaneously. Users can configure and delegate customised\nmicrotasks, and manage their microtasks by specifying task requirements and\ntoggling visibility and initiative. Our evaluation revealed that, compared to\nChatGPT, users had more customizability over collaboration with Polymind, and\nwere thus able to quickly expand personalised writing ideas during prewriting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prewriting is the process of generating and organising ideas before a first\ndraft. It consists of a combination of informal, iterative, and semi-structured\nstrategies such as visual diagramming, which poses a challenge for\ncollaborating with large language models (LLMs) in a turn-taking conversational\nmanner. We present Polymind, a visual diagramming tool that leverages multiple\nLLM-powered agents to support prewriting. The system features a parallel\ncollaboration workflow in place of the turn-taking conversational interactions.\nIt defines multiple ``microtasks'' to simulate group collaboration scenarios\nsuch as collaborative writing and group brainstorming. Instead of repetitively\nprompting a chatbot for various purposes, Polymind enables users to orchestrate\nmultiple microtasks simultaneously. Users can configure and delegate customised\nmicrotasks, and manage their microtasks by specifying task requirements and\ntoggling visibility and initiative. Our evaluation revealed that, compared to\nChatGPT, users had more customizability over collaboration with Polymind, and\nwere thus able to quickly expand personalised writing ideas during prewriting."
                },
                "authors": [
                    {
                        "name": "Qian Wan"
                    },
                    {
                        "name": "Jiannan Li"
                    },
                    {
                        "name": "Huanchen Wang"
                    },
                    {
                        "name": "Zhicong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhicong Lu"
                },
                "author": "Zhicong Lu",
                "arxiv_comment": "Accepted to CSCW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08898v1",
                "updated": "2025-08-12T12:40:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    40,
                    28,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T12:40:28Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    40,
                    28,
                    1,
                    224,
                    0
                ],
                "title": "Redactable Blockchains: An Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redactable Blockchains: An Overview"
                },
                "summary": "Blockchains are widely recognized for their immutability, which provides\nrobust guarantees of data integrity and transparency. However, this same\nfeature poses significant challenges in real-world situations that require\nregulatory compliance, correction of erroneous data, or removal of sensitive\ninformation. Redactable blockchains address the limitations of traditional ones\nby enabling controlled, auditable modifications to blockchain data, primarily\nthrough cryptographic mechanisms such as chameleon hash functions and\nalternative redaction schemes. This report examines the motivations for\nintroducing redactability, surveys the cryptographic primitives that enable\nsecure edits, and analyzes competing approaches and their shortcomings. Special\nattention is paid to the practical deployment of redactable blockchains in\nprivate settings, with discussions of use cases in healthcare, finance,\nInternet of drones, and federated learning. Finally, the report outlines\nfurther challenges, also in connection with reversible computing, and the\nfuture potential of redactable blockchains in building law-compliant,\ntrustworthy, and scalable digital infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchains are widely recognized for their immutability, which provides\nrobust guarantees of data integrity and transparency. However, this same\nfeature poses significant challenges in real-world situations that require\nregulatory compliance, correction of erroneous data, or removal of sensitive\ninformation. Redactable blockchains address the limitations of traditional ones\nby enabling controlled, auditable modifications to blockchain data, primarily\nthrough cryptographic mechanisms such as chameleon hash functions and\nalternative redaction schemes. This report examines the motivations for\nintroducing redactability, surveys the cryptographic primitives that enable\nsecure edits, and analyzes competing approaches and their shortcomings. Special\nattention is paid to the practical deployment of redactable blockchains in\nprivate settings, with discussions of use cases in healthcare, finance,\nInternet of drones, and federated learning. Finally, the report outlines\nfurther challenges, also in connection with reversible computing, and the\nfuture potential of redactable blockchains in building law-compliant,\ntrustworthy, and scalable digital infrastructures."
                },
                "authors": [
                    {
                        "name": "Federico Calandra"
                    },
                    {
                        "name": "Marco Bernardo"
                    },
                    {
                        "name": "Andrea Esposito"
                    },
                    {
                        "name": "Francesco Fabris"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Fabris"
                },
                "author": "Francesco Fabris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08895v1",
                "updated": "2025-08-12T12:35:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T12:35:55Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs"
                },
                "summary": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Zhifeng Shen"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Haoqian Wu"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01057v2",
                "updated": "2025-08-12T12:29:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    29,
                    2,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-01T20:16:04Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    16,
                    4,
                    4,
                    213,
                    0
                ],
                "title": "Edge-Based Multimodal Sensor Data Fusion with Vision Language Models\n  (VLMs) for Real-time Autonomous Vehicle Accident Avoidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-Based Multimodal Sensor Data Fusion with Vision Language Models\n  (VLMs) for Real-time Autonomous Vehicle Accident Avoidance"
                },
                "summary": "Autonomous driving (AD) systems relying solely on onboard sensors may fail to\ndetect distant or obstacle hazards, potentially causing preventable collisions;\nhowever, existing transformer-based Vehicle-to-Everything (V2X) approaches,\nwhich mitigate AD sensing limitations, either lack effective multimodal fusion\nand reasoning or struggle to meet real-time performance requirements under\ncomplex, high-dimensional traffic conditions. This paper proposes the Real-time\nEdge-based Autonomous Co-pilot Trajectory planner (REACT), a V2X-integrated\ntrajectory optimization framework for AD based on a fine-tuned lightweight\nVision-Language Model (VLM). REACT integrates infrastructure-provided hazard\nalerts with onboard sensor data, capturing intricate surrounding traffic\ndynamics and vehicle intents through visual embeddings, interpreting precise\nnumerical data from symbolic inputs, and employing contextual reasoning to\ngenerate optimized, safety-oriented trajectories. To ensure robust real-time\ndeployment on edge devices, REACT innovatively employs Residual Trajectory\nFusion (RTF) design and specialized edge-adaptation strategies to reduce model\ncomplexity and improve inference efficiency. Evaluated on the DeepAccident\nbenchmark, REACT achieves state-of-the-art performance, a 77% collision rate\nreduction, a 48.2% Video Panoptic Quality (VPQ), and a 0.57-second inference\nlatency on the Jetson AGX Orin. Ablation studies validate the contribution of\neach input, module, and edge adaptation strategy. These results highlight the\neffectiveness of lightweight VLMs in enabling real-time cooperative planning on\nedge platforms and underscore the potential of language-guided contextual\nreasoning for improving traffic safety and responsiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving (AD) systems relying solely on onboard sensors may fail to\ndetect distant or obstacle hazards, potentially causing preventable collisions;\nhowever, existing transformer-based Vehicle-to-Everything (V2X) approaches,\nwhich mitigate AD sensing limitations, either lack effective multimodal fusion\nand reasoning or struggle to meet real-time performance requirements under\ncomplex, high-dimensional traffic conditions. This paper proposes the Real-time\nEdge-based Autonomous Co-pilot Trajectory planner (REACT), a V2X-integrated\ntrajectory optimization framework for AD based on a fine-tuned lightweight\nVision-Language Model (VLM). REACT integrates infrastructure-provided hazard\nalerts with onboard sensor data, capturing intricate surrounding traffic\ndynamics and vehicle intents through visual embeddings, interpreting precise\nnumerical data from symbolic inputs, and employing contextual reasoning to\ngenerate optimized, safety-oriented trajectories. To ensure robust real-time\ndeployment on edge devices, REACT innovatively employs Residual Trajectory\nFusion (RTF) design and specialized edge-adaptation strategies to reduce model\ncomplexity and improve inference efficiency. Evaluated on the DeepAccident\nbenchmark, REACT achieves state-of-the-art performance, a 77% collision rate\nreduction, a 48.2% Video Panoptic Quality (VPQ), and a 0.57-second inference\nlatency on the Jetson AGX Orin. Ablation studies validate the contribution of\neach input, module, and edge adaptation strategy. These results highlight the\neffectiveness of lightweight VLMs in enabling real-time cooperative planning on\nedge platforms and underscore the potential of language-guided contextual\nreasoning for improving traffic safety and responsiveness."
                },
                "authors": [
                    {
                        "name": "Fengze Yang"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Xuewen Luo"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Chenxi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chenxi Liu"
                },
                "author": "Chenxi Liu",
                "arxiv_comment": "24 pages, 6 tables, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08887v1",
                "updated": "2025-08-12T12:24:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    24,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T12:24:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    24,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "A Dual Framework for Optimized Data Storage and Retrieval using\n  Lightweight Python Blockchain and Scalable Smart Contracts with IPFS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dual Framework for Optimized Data Storage and Retrieval using\n  Lightweight Python Blockchain and Scalable Smart Contracts with IPFS"
                },
                "summary": "The exponential growth of IoT data demands efficient, secure, and scalable\nstorage solutions on one hand, and efficient data migration and retrieval on\nthe other hand are essential for the systems to be practical and acceptable for\ndifferent applications. The traditional cloud-based models face latency,\nsecurity, and high operational costs, while existing bi-directional data\nstorage and retrieval-based IPFS models are not computationally efficient and\nincur high gas costs at the cost of a necessary blockchain deployment. To\novercome the challenges of efficient data migration, we initially developed a\n2-way data storage and retrieval system as well as a scalable framework that\ndynamically monitors and transfers device-generated data to IPFS, records the\ncontent identifier(CID) on a blockchain, and enables secure, real-time access\nvia smart contracts. Experimental results demonstrate that the existing work\nachieved an average data upload time of 117.12 sec for a file size of 500 MB;\nour framework achieves a faster upload time of 7.63 sec, marking a 93.47%\nimprovement. We further optimize the proposed framework to reduce the file\nupload time incurred from the smart contracts by introducing a\nblockchain-inspired, lightweight, and customizable Python framework that\nreplicates the storage and retrieval functionalities of a traditional\nblockchain, where the file upload time is 4.2 sec, further optimized by 45%\nfrom our previous approach, thus demonstrating its efficiency, security and\nsuitability for deploy ment in real-time and critical IoT applications and\noutperforming the existing IPFS-smart contract based solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of IoT data demands efficient, secure, and scalable\nstorage solutions on one hand, and efficient data migration and retrieval on\nthe other hand are essential for the systems to be practical and acceptable for\ndifferent applications. The traditional cloud-based models face latency,\nsecurity, and high operational costs, while existing bi-directional data\nstorage and retrieval-based IPFS models are not computationally efficient and\nincur high gas costs at the cost of a necessary blockchain deployment. To\novercome the challenges of efficient data migration, we initially developed a\n2-way data storage and retrieval system as well as a scalable framework that\ndynamically monitors and transfers device-generated data to IPFS, records the\ncontent identifier(CID) on a blockchain, and enables secure, real-time access\nvia smart contracts. Experimental results demonstrate that the existing work\nachieved an average data upload time of 117.12 sec for a file size of 500 MB;\nour framework achieves a faster upload time of 7.63 sec, marking a 93.47%\nimprovement. We further optimize the proposed framework to reduce the file\nupload time incurred from the smart contracts by introducing a\nblockchain-inspired, lightweight, and customizable Python framework that\nreplicates the storage and retrieval functionalities of a traditional\nblockchain, where the file upload time is 4.2 sec, further optimized by 45%\nfrom our previous approach, thus demonstrating its efficiency, security and\nsuitability for deploy ment in real-time and critical IoT applications and\noutperforming the existing IPFS-smart contract based solutions."
                },
                "authors": [
                    {
                        "name": "Vatsala Upadhyay"
                    },
                    {
                        "name": "J. Kokila"
                    },
                    {
                        "name": "Abhishek Vaish"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Vaish"
                },
                "author": "Abhishek Vaish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08879v1",
                "updated": "2025-08-12T12:05:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    5,
                    32,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T12:05:32Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    5,
                    32,
                    1,
                    224,
                    0
                ],
                "title": "Entangled in Representations: Mechanistic Investigation of Cultural\n  Biases in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entangled in Representations: Mechanistic Investigation of Cultural\n  Biases in Large Language Models"
                },
                "summary": "The growing deployment of large language models (LLMs) across diverse\ncultural contexts necessitates a better understanding of how the\novergeneralization of less documented cultures within LLMs' representations\nimpacts their cultural understanding. Prior work only performs extrinsic\nevaluation of LLMs' cultural competence, without accounting for how LLMs'\ninternal mechanisms lead to cultural (mis)representation. To bridge this gap,\nwe propose Culturescope, the first mechanistic interpretability-based method\nthat probes the internal representations of LLMs to elicit the underlying\ncultural knowledge space. CultureScope utilizes a patching method to extract\nthe cultural knowledge. We introduce a cultural flattening score as a measure\nof the intrinsic cultural biases. Additionally, we study how LLMs internalize\nWestern-dominance bias and cultural flattening, which allows us to trace how\ncultural biases emerge within LLMs. Our experimental results reveal that LLMs\nencode Western-dominance bias and cultural flattening in their cultural\nknowledge space. We find that low-resource cultures are less susceptible to\ncultural biases, likely due to their limited training resources. Our work\nprovides a foundation for future research on mitigating cultural biases and\nenhancing LLMs' cultural understanding. Our codes and data used for experiments\nare publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing deployment of large language models (LLMs) across diverse\ncultural contexts necessitates a better understanding of how the\novergeneralization of less documented cultures within LLMs' representations\nimpacts their cultural understanding. Prior work only performs extrinsic\nevaluation of LLMs' cultural competence, without accounting for how LLMs'\ninternal mechanisms lead to cultural (mis)representation. To bridge this gap,\nwe propose Culturescope, the first mechanistic interpretability-based method\nthat probes the internal representations of LLMs to elicit the underlying\ncultural knowledge space. CultureScope utilizes a patching method to extract\nthe cultural knowledge. We introduce a cultural flattening score as a measure\nof the intrinsic cultural biases. Additionally, we study how LLMs internalize\nWestern-dominance bias and cultural flattening, which allows us to trace how\ncultural biases emerge within LLMs. Our experimental results reveal that LLMs\nencode Western-dominance bias and cultural flattening in their cultural\nknowledge space. We find that low-resource cultures are less susceptible to\ncultural biases, likely due to their limited training resources. Our work\nprovides a foundation for future research on mitigating cultural biases and\nenhancing LLMs' cultural understanding. Our codes and data used for experiments\nare publicly available."
                },
                "authors": [
                    {
                        "name": "Haeun Yu"
                    },
                    {
                        "name": "Seogyeong Jeong"
                    },
                    {
                        "name": "Siddhesh Pawar"
                    },
                    {
                        "name": "Jisu Shin"
                    },
                    {
                        "name": "Jiho Jin"
                    },
                    {
                        "name": "Junho Myung"
                    },
                    {
                        "name": "Alice Oh"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "16 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08875v1",
                "updated": "2025-08-12T12:02:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    2,
                    53,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T12:02:53Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    2,
                    53,
                    1,
                    224,
                    0
                ],
                "title": "Oblivionis: A Lightweight Learning and Unlearning Framework for\n  Federated Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivionis: A Lightweight Learning and Unlearning Framework for\n  Federated Large Language Models"
                },
                "summary": "Large Language Models (LLMs) increasingly leverage Federated Learning (FL) to\nutilize private, task-specific datasets for fine-tuning while preserving data\nprivacy. However, while federated LLM frameworks effectively enable\ncollaborative training without raw data sharing, they critically lack built-in\nmechanisms for regulatory compliance like GDPR's right to be forgotten.\nIntegrating private data heightens concerns over data quality and long-term\ngovernance, yet existing distributed training frameworks offer no principled\nway to selectively remove specific client contributions post-training. Due to\ndistributed data silos, stringent privacy constraints, and the intricacies of\ninterdependent model aggregation, federated LLM unlearning is significantly\nmore complex than centralized LLM unlearning. To address this gap, we introduce\nOblivionis, a lightweight learning and unlearning framework that enables\nclients to selectively remove specific private data during federated LLM\ntraining, enhancing trustworthiness and regulatory compliance. By unifying FL\nand unlearning as a dual optimization objective, we incorporate 6 FL and 5\nunlearning algorithms for comprehensive evaluation and comparative analysis,\nestablishing a robust pipeline for federated LLM unlearning. Extensive\nexperiments demonstrate that Oblivionis outperforms local training, achieving a\nrobust balance between forgetting efficacy and model utility, with\ncross-algorithm comparisons providing clear directions for future LLM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly leverage Federated Learning (FL) to\nutilize private, task-specific datasets for fine-tuning while preserving data\nprivacy. However, while federated LLM frameworks effectively enable\ncollaborative training without raw data sharing, they critically lack built-in\nmechanisms for regulatory compliance like GDPR's right to be forgotten.\nIntegrating private data heightens concerns over data quality and long-term\ngovernance, yet existing distributed training frameworks offer no principled\nway to selectively remove specific client contributions post-training. Due to\ndistributed data silos, stringent privacy constraints, and the intricacies of\ninterdependent model aggregation, federated LLM unlearning is significantly\nmore complex than centralized LLM unlearning. To address this gap, we introduce\nOblivionis, a lightweight learning and unlearning framework that enables\nclients to selectively remove specific private data during federated LLM\ntraining, enhancing trustworthiness and regulatory compliance. By unifying FL\nand unlearning as a dual optimization objective, we incorporate 6 FL and 5\nunlearning algorithms for comprehensive evaluation and comparative analysis,\nestablishing a robust pipeline for federated LLM unlearning. Extensive\nexperiments demonstrate that Oblivionis outperforms local training, achieving a\nrobust balance between forgetting efficacy and model utility, with\ncross-algorithm comparisons providing clear directions for future LLM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Fuyao Zhang"
                    },
                    {
                        "name": "Xinyu Yan"
                    },
                    {
                        "name": "Tiantong Wu"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Tianxiang Chen"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Longtao Huang"
                    },
                    {
                        "name": "Wei Yang Bryan Lim"
                    },
                    {
                        "name": "Qiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yang"
                },
                "author": "Qiang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20977v2",
                "updated": "2025-08-12T11:56:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    56,
                    32,
                    1,
                    224,
                    0
                ],
                "published": "2024-12-30T14:31:01Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    31,
                    1,
                    0,
                    365,
                    0
                ],
                "title": "UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI"
                },
                "summary": "We introduce UnrealZoo, a collection of over 100 photo-realistic 3D virtual\nworlds built on Unreal Engine, designed to reflect the complexity and\nvariability of open-world environments. We also provide a rich variety of\nplayable entities, including humans, animals, robots, and vehicles for embodied\nAI research. We extend UnrealCV with optimized APIs and tools for data\ncollection, environment augmentation, distributed training, and benchmarking.\nThese improvements achieve significant improvements in the efficiency of\nrendering and communication, enabling advanced applications such as multi-agent\ninteractions. Our experimental evaluation across visual navigation and tracking\ntasks reveals two key insights: 1) environmental diversity provides substantial\nbenefits for developing generalizable reinforcement learning (RL) agents, and\n2) current embodied agents face persistent challenges in open-world scenarios,\nincluding navigation in unstructured terrain, adaptation to unseen\nmorphologies, and managing latency in the close-loop control systems for\ninteracting in highly dynamic objects. UnrealZoo thus serves as both a\ncomprehensive testing ground and a pathway toward developing more capable\nembodied AI systems for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce UnrealZoo, a collection of over 100 photo-realistic 3D virtual\nworlds built on Unreal Engine, designed to reflect the complexity and\nvariability of open-world environments. We also provide a rich variety of\nplayable entities, including humans, animals, robots, and vehicles for embodied\nAI research. We extend UnrealCV with optimized APIs and tools for data\ncollection, environment augmentation, distributed training, and benchmarking.\nThese improvements achieve significant improvements in the efficiency of\nrendering and communication, enabling advanced applications such as multi-agent\ninteractions. Our experimental evaluation across visual navigation and tracking\ntasks reveals two key insights: 1) environmental diversity provides substantial\nbenefits for developing generalizable reinforcement learning (RL) agents, and\n2) current embodied agents face persistent challenges in open-world scenarios,\nincluding navigation in unstructured terrain, adaptation to unseen\nmorphologies, and managing latency in the close-loop control systems for\ninteracting in highly dynamic objects. UnrealZoo thus serves as both a\ncomprehensive testing ground and a pathway toward developing more capable\nembodied AI systems for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Fangwei Zhong"
                    },
                    {
                        "name": "Kui Wu"
                    },
                    {
                        "name": "Churan Wang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Hai Ci"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Yizhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Wang"
                },
                "author": "Yizhou Wang",
                "arxiv_comment": "ICCV 2025 (Highlight), Project page: http://unrealzoo.site/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08855v1",
                "updated": "2025-08-12T11:23:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    23,
                    44,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T11:23:44Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    23,
                    44,
                    1,
                    224,
                    0
                ],
                "title": "BiasGym: Fantastic Biases and How to Find (and Remove) Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiasGym: Fantastic Biases and How to Find (and Remove) Them"
                },
                "summary": "Understanding biases and stereotypes encoded in the weights of Large Language\nModels (LLMs) is crucial for developing effective mitigation strategies. Biased\nbehaviour is often subtle and non-trivial to isolate, even when deliberately\nelicited, making systematic analysis and debiasing particularly challenging. To\naddress this, we introduce BiasGym, a simple, cost-effective, and generalizable\nframework for reliably injecting, analyzing, and mitigating conceptual\nassociations within LLMs. BiasGym consists of two components: BiasInject, which\ninjects specific biases into the model via token-based fine-tuning while\nkeeping the model frozen, and BiasScope, which leverages these injected signals\nto identify and steer the components responsible for biased behavior. Our\nmethod enables consistent bias elicitation for mechanistic analysis, supports\ntargeted debiasing without degrading performance on downstream tasks, and\ngeneralizes to biases unseen during training. We demonstrate the effectiveness\nof BiasGym in reducing real-world stereotypes (e.g., people from a country\nbeing `reckless drivers') and in probing fictional associations (e.g., people\nfrom a country having `blue skin'), showing its utility for both safety\ninterventions and interpretability research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding biases and stereotypes encoded in the weights of Large Language\nModels (LLMs) is crucial for developing effective mitigation strategies. Biased\nbehaviour is often subtle and non-trivial to isolate, even when deliberately\nelicited, making systematic analysis and debiasing particularly challenging. To\naddress this, we introduce BiasGym, a simple, cost-effective, and generalizable\nframework for reliably injecting, analyzing, and mitigating conceptual\nassociations within LLMs. BiasGym consists of two components: BiasInject, which\ninjects specific biases into the model via token-based fine-tuning while\nkeeping the model frozen, and BiasScope, which leverages these injected signals\nto identify and steer the components responsible for biased behavior. Our\nmethod enables consistent bias elicitation for mechanistic analysis, supports\ntargeted debiasing without degrading performance on downstream tasks, and\ngeneralizes to biases unseen during training. We demonstrate the effectiveness\nof BiasGym in reducing real-world stereotypes (e.g., people from a country\nbeing `reckless drivers') and in probing fictional associations (e.g., people\nfrom a country having `blue skin'), showing its utility for both safety\ninterventions and interpretability research."
                },
                "authors": [
                    {
                        "name": "Sekh Mainul Islam"
                    },
                    {
                        "name": "Nadav Borenstein"
                    },
                    {
                        "name": "Siddhesh Milind Pawar"
                    },
                    {
                        "name": "Haeun Yu"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20252v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20252v3",
                "updated": "2025-08-12T11:22:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    22,
                    33,
                    1,
                    224,
                    0
                ],
                "published": "2025-07-27T12:47:26Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    12,
                    47,
                    26,
                    6,
                    208,
                    0
                ],
                "title": "Post-Completion Learning for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Completion Learning for Language Models"
                },
                "summary": "Current language model training paradigms typically terminate learning upon\nreaching the end-of-sequence (<eos>) token, overlooking the potential learning\nopportunities in the post-completion space. We propose Post-Completion Learning\n(PCL), a novel training framework that systematically utilizes the sequence\nspace after model output completion, to enhance both the reasoning and\nself-evaluation abilities. PCL enables models to continue generating\nself-assessments and reward predictions during training, while maintaining\nefficient inference by stopping at the completion point.\n  To fully utilize this post-completion space, we design a white-box\nreinforcement learning method: let the model evaluate the output content\naccording to the reward rules, then calculate and align the score with the\nreward functions for supervision. We implement dual-track SFT to optimize both\nreasoning and evaluation capabilities, and mixed it with RL training to achieve\nmulti-objective hybrid optimization.\n  Experimental results on different datasets and models demonstrate consistent\nimprovements over traditional SFT and RL methods. Our method provides a new\ntechnical path for language model training that enhances output quality while\npreserving deployment efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current language model training paradigms typically terminate learning upon\nreaching the end-of-sequence (<eos>) token, overlooking the potential learning\nopportunities in the post-completion space. We propose Post-Completion Learning\n(PCL), a novel training framework that systematically utilizes the sequence\nspace after model output completion, to enhance both the reasoning and\nself-evaluation abilities. PCL enables models to continue generating\nself-assessments and reward predictions during training, while maintaining\nefficient inference by stopping at the completion point.\n  To fully utilize this post-completion space, we design a white-box\nreinforcement learning method: let the model evaluate the output content\naccording to the reward rules, then calculate and align the score with the\nreward functions for supervision. We implement dual-track SFT to optimize both\nreasoning and evaluation capabilities, and mixed it with RL training to achieve\nmulti-objective hybrid optimization.\n  Experimental results on different datasets and models demonstrate consistent\nimprovements over traditional SFT and RL methods. Our method provides a new\ntechnical path for language model training that enhances output quality while\npreserving deployment efficiency."
                },
                "authors": [
                    {
                        "name": "Xiang Fei"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Shu Wei"
                    },
                    {
                        "name": "Yuxiang Nie"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Can Huang"
                    }
                ],
                "author_detail": {
                    "name": "Can Huang"
                },
                "author": "Can Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20252v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20252v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08846v1",
                "updated": "2025-08-12T11:09:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    9,
                    3,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T11:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    11,
                    9,
                    3,
                    1,
                    224,
                    0
                ],
                "title": "Steering Towards Fairness: Mitigating Political Bias in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Towards Fairness: Mitigating Political Bias in LLMs"
                },
                "summary": "Recent advancements in large language models (LLMs) have enabled their\nwidespread use across diverse real-world applications. However, concerns remain\nabout their tendency to encode and reproduce ideological biases, particularly\nalong political and economic dimensions. In this paper, we propose a framework\nfor probing and mitigating such biases in decoder-based LLMs through analysis\nof internal model representations. Grounded in the Political Compass Test\n(PCT), our method uses contrastive pairs to extract and compare hidden layer\nactivations from models like Mistral and DeepSeek. We introduce a comprehensive\nactivation extraction pipeline capable of layer-wise analysis across multiple\nideological axes, revealing meaningful disparities linked to political framing.\nOur results show that decoder LLMs systematically encode representational bias\nacross layers, which can be leveraged for effective steering vector-based\nmitigation. This work provides new insights into how political bias is encoded\nin LLMs and offers a principled approach to debiasing beyond surface-level\noutput interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have enabled their\nwidespread use across diverse real-world applications. However, concerns remain\nabout their tendency to encode and reproduce ideological biases, particularly\nalong political and economic dimensions. In this paper, we propose a framework\nfor probing and mitigating such biases in decoder-based LLMs through analysis\nof internal model representations. Grounded in the Political Compass Test\n(PCT), our method uses contrastive pairs to extract and compare hidden layer\nactivations from models like Mistral and DeepSeek. We introduce a comprehensive\nactivation extraction pipeline capable of layer-wise analysis across multiple\nideological axes, revealing meaningful disparities linked to political framing.\nOur results show that decoder LLMs systematically encode representational bias\nacross layers, which can be leveraged for effective steering vector-based\nmitigation. This work provides new insights into how political bias is encoded\nin LLMs and offers a principled approach to debiasing beyond surface-level\noutput interventions."
                },
                "authors": [
                    {
                        "name": "Afrozah Nadeem"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08837v1",
                "updated": "2025-08-12T10:54:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    54,
                    8,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T10:54:08Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    54,
                    8,
                    1,
                    224,
                    0
                ],
                "title": "The Roots of International Perceptions: Simulating US Attitude Changes\n  Towards China with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Roots of International Perceptions: Simulating US Attitude Changes\n  Towards China with LLM Agents"
                },
                "summary": "The rise of LLMs poses new possibilities in modeling opinion evolution, a\nlong-standing task in simulation, by leveraging advanced reasoning abilities to\nrecreate complex, large-scale human cognitive trends. While most prior works\nfocus on opinion evolution surrounding specific isolated events or the views\nwithin a country, ours is the first to model the large-scale attitude evolution\nof a population representing an entire country towards another -- US citizens'\nperspectives towards China. To tackle the challenges of this broad scenario, we\npropose a framework that integrates media data collection, user profile\ncreation, and cognitive architecture for opinion updates to successfully\nreproduce the real trend of US attitudes towards China over a 20-year period\nfrom 2005 to today. We also leverage LLMs' capabilities to introduce debiased\nmedia exposure, extracting neutral events from typically subjective news\ncontents, to uncover the roots of polarized opinion formation, as well as a\ndevils advocate agent to help explain the rare reversal from negative to\npositive attitudes towards China, corresponding with changes in the way\nAmericans obtain information about the country. The simulation results, beyond\nvalidating our framework architecture, also reveal the impact of biased framing\nand selection bias in shaping attitudes. Overall, our work contributes to a new\nparadigm for LLM-based modeling of cognitive behaviors in a large-scale,\nlong-term, cross-border social context, providing insights into the formation\nof international biases and offering valuable implications for media consumers\nto better understand the factors shaping their perspectives, and ultimately\ncontributing to the larger social need for bias reduction and cross-cultural\ntolerance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of LLMs poses new possibilities in modeling opinion evolution, a\nlong-standing task in simulation, by leveraging advanced reasoning abilities to\nrecreate complex, large-scale human cognitive trends. While most prior works\nfocus on opinion evolution surrounding specific isolated events or the views\nwithin a country, ours is the first to model the large-scale attitude evolution\nof a population representing an entire country towards another -- US citizens'\nperspectives towards China. To tackle the challenges of this broad scenario, we\npropose a framework that integrates media data collection, user profile\ncreation, and cognitive architecture for opinion updates to successfully\nreproduce the real trend of US attitudes towards China over a 20-year period\nfrom 2005 to today. We also leverage LLMs' capabilities to introduce debiased\nmedia exposure, extracting neutral events from typically subjective news\ncontents, to uncover the roots of polarized opinion formation, as well as a\ndevils advocate agent to help explain the rare reversal from negative to\npositive attitudes towards China, corresponding with changes in the way\nAmericans obtain information about the country. The simulation results, beyond\nvalidating our framework architecture, also reveal the impact of biased framing\nand selection bias in shaping attitudes. Overall, our work contributes to a new\nparadigm for LLM-based modeling of cognitive behaviors in a large-scale,\nlong-term, cross-border social context, providing insights into the formation\nof international biases and offering valuable implications for media consumers\nto better understand the factors shaping their perspectives, and ultimately\ncontributing to the larger social need for bias reduction and cross-cultural\ntolerance."
                },
                "authors": [
                    {
                        "name": "Nicholas Sukiennik"
                    },
                    {
                        "name": "Yichuan Xu"
                    },
                    {
                        "name": "Yuqing Kan"
                    },
                    {
                        "name": "Jinghua Piao"
                    },
                    {
                        "name": "Yuwei Yan"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "Submitted to AAAI Social Impact 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08836v1",
                "updated": "2025-08-12T10:52:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    52,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T10:52:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    52,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "EditMF: Drawing an Invisible Fingerprint for Your Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EditMF: Drawing an Invisible Fingerprint for Your Large Language Models"
                },
                "summary": "Training large language models (LLMs) is resource-intensive and expensive,\nmaking protecting intellectual property (IP) for LLMs crucial. Recently,\nembedding fingerprints into LLMs has emerged as a prevalent method for\nestablishing model ownership. However, existing back-door-based methods suffer\nfrom limited stealth and efficiency. To simultaneously address these issues, we\npropose EditMF, a training-free fingerprinting paradigm that achieves highly\nimperceptible fingerprint embedding with minimal computational overhead.\nOwnership bits are mapped to compact, semantically coherent triples drawn from\nan encrypted artificial knowledge base (e.g., virtual author-novel-protagonist\nfacts). Causal tracing localizes the minimal set of layers influencing each\ntriple, and a zero-space update injects the fingerprint without perturbing\nunrelated knowledge. Verification requires only a single black-box query and\nsucceeds when the model returns the exact pre-embedded protagonist. Empirical\nresults on LLaMA and Qwen families show that EditMF combines high\nimperceptibility with negligible model's performance loss, while delivering\nrobustness far beyond LoRA-based fingerprinting and approaching that of SFT\nembeddings. Extensive experiments demonstrate that EditMF is an effective and\nlow-overhead solution for secure LLM ownership verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) is resource-intensive and expensive,\nmaking protecting intellectual property (IP) for LLMs crucial. Recently,\nembedding fingerprints into LLMs has emerged as a prevalent method for\nestablishing model ownership. However, existing back-door-based methods suffer\nfrom limited stealth and efficiency. To simultaneously address these issues, we\npropose EditMF, a training-free fingerprinting paradigm that achieves highly\nimperceptible fingerprint embedding with minimal computational overhead.\nOwnership bits are mapped to compact, semantically coherent triples drawn from\nan encrypted artificial knowledge base (e.g., virtual author-novel-protagonist\nfacts). Causal tracing localizes the minimal set of layers influencing each\ntriple, and a zero-space update injects the fingerprint without perturbing\nunrelated knowledge. Verification requires only a single black-box query and\nsucceeds when the model returns the exact pre-embedded protagonist. Empirical\nresults on LLaMA and Qwen families show that EditMF combines high\nimperceptibility with negligible model's performance loss, while delivering\nrobustness far beyond LoRA-based fingerprinting and approaching that of SFT\nembeddings. Extensive experiments demonstrate that EditMF is an effective and\nlow-overhead solution for secure LLM ownership verification."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Wu"
                    },
                    {
                        "name": "Yinghan Zhou"
                    },
                    {
                        "name": "Wanli Peng"
                    },
                    {
                        "name": "Yiming Xue"
                    },
                    {
                        "name": "Juan Wen"
                    },
                    {
                        "name": "Ping Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhong"
                },
                "author": "Ping Zhong",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08833v1",
                "updated": "2025-08-12T10:40:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    40,
                    33,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T10:40:33Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    40,
                    33,
                    1,
                    224,
                    0
                ],
                "title": "An Investigation of Robustness of LLMs in Mathematical Reasoning:\n  Benchmarking with Mathematically-Equivalent Transformation of Advanced\n  Mathematical Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Investigation of Robustness of LLMs in Mathematical Reasoning:\n  Benchmarking with Mathematically-Equivalent Transformation of Advanced\n  Mathematical Problems"
                },
                "summary": "In this paper, we introduce a systematic framework beyond conventional method\nto assess LLMs' mathematical-reasoning robustness by stress-testing them on\nadvanced math problems that are mathematically equivalent but with linguistic\nand parametric variation. These transformations allow us to measure the\nsensitivity of LLMs to non-mathematical perturbations, thereby enabling a more\naccurate evaluation of their mathematical reasoning capabilities. Using this\nnew evaluation methodology, we created PutnamGAP, a new benchmark dataset with\nmultiple mathematically-equivalent variations of competition-level math\nproblems. With the new dataset, we evaluate multiple families of representative\nLLMs and examine their robustness. Across 18 commercial and open-source models\nwe observe sharp performance degradation on the variants. OpenAI's flagship\nreasoning model, O3, scores 49 % on the originals but drops by 4 percentage\npoints on surface variants, and by 10.5 percentage points on core-step-based\nvariants, while smaller models fare far worse. Overall, the results show that\nthe proposed new evaluation methodology is effective for deepening our\nunderstanding of the robustness of LLMs and generating new insights for further\nimproving their mathematical reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a systematic framework beyond conventional method\nto assess LLMs' mathematical-reasoning robustness by stress-testing them on\nadvanced math problems that are mathematically equivalent but with linguistic\nand parametric variation. These transformations allow us to measure the\nsensitivity of LLMs to non-mathematical perturbations, thereby enabling a more\naccurate evaluation of their mathematical reasoning capabilities. Using this\nnew evaluation methodology, we created PutnamGAP, a new benchmark dataset with\nmultiple mathematically-equivalent variations of competition-level math\nproblems. With the new dataset, we evaluate multiple families of representative\nLLMs and examine their robustness. Across 18 commercial and open-source models\nwe observe sharp performance degradation on the variants. OpenAI's flagship\nreasoning model, O3, scores 49 % on the originals but drops by 4 percentage\npoints on surface variants, and by 10.5 percentage points on core-step-based\nvariants, while smaller models fare far worse. Overall, the results show that\nthe proposed new evaluation methodology is effective for deepening our\nunderstanding of the robustness of LLMs and generating new insights for further\nimproving their mathematical reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Yuren Hao"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Chengxiang Zhai"
                },
                "author": "Chengxiang Zhai",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08827v1",
                "updated": "2025-08-12T10:36:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    36,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T10:36:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    36,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "TiMoE: Time-Aware Mixture of Language Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TiMoE: Time-Aware Mixture of Language Experts"
                },
                "summary": "Large language models (LLMs) are typically trained on fixed snapshots of the\nweb, which means that their knowledge becomes stale and their predictions risk\ntemporal leakage: relying on information that lies in the future relative to a\nquery. We tackle this problem by pre-training from scratch a set of GPT-style\nexperts on disjoint two-year slices of a 2013-2024 corpus and combining them\nthrough TiMoE, a Time-aware Mixture of Language Experts. At inference time,\nTiMoE masks all experts whose training window ends after the query timestamp\nand merges the remaining log-probabilities in a shared space, guaranteeing\nstrict causal validity while retaining the breadth of multi-period knowledge.\nWe also release TSQA, a 10k-question benchmark whose alternatives are\nexplicitly labelled as past, future or irrelevant, allowing fine-grained\nmeasurement of temporal hallucinations. Experiments on eight standard NLP tasks\nplus TSQA show that a co-adapted TiMoE variant matches or exceeds the best\nsingle-period expert and cuts future-knowledge errors by up to 15%. Our results\ndemonstrate that modular, time-segmented pre-training paired with causal\nrouting is a simple yet effective path toward LLMs that stay chronologically\ngrounded without sacrificing general performance much. We open source our code\nat TiMoE (Github): https://github.com/epfml/TiMoE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically trained on fixed snapshots of the\nweb, which means that their knowledge becomes stale and their predictions risk\ntemporal leakage: relying on information that lies in the future relative to a\nquery. We tackle this problem by pre-training from scratch a set of GPT-style\nexperts on disjoint two-year slices of a 2013-2024 corpus and combining them\nthrough TiMoE, a Time-aware Mixture of Language Experts. At inference time,\nTiMoE masks all experts whose training window ends after the query timestamp\nand merges the remaining log-probabilities in a shared space, guaranteeing\nstrict causal validity while retaining the breadth of multi-period knowledge.\nWe also release TSQA, a 10k-question benchmark whose alternatives are\nexplicitly labelled as past, future or irrelevant, allowing fine-grained\nmeasurement of temporal hallucinations. Experiments on eight standard NLP tasks\nplus TSQA show that a co-adapted TiMoE variant matches or exceeds the best\nsingle-period expert and cuts future-knowledge errors by up to 15%. Our results\ndemonstrate that modular, time-segmented pre-training paired with causal\nrouting is a simple yet effective path toward LLMs that stay chronologically\ngrounded without sacrificing general performance much. We open source our code\nat TiMoE (Github): https://github.com/epfml/TiMoE"
                },
                "authors": [
                    {
                        "name": "Robin Faro"
                    },
                    {
                        "name": "Dongyang Fan"
                    },
                    {
                        "name": "Tamar Alphaidze"
                    },
                    {
                        "name": "Martin Jaggi"
                    }
                ],
                "author_detail": {
                    "name": "Martin Jaggi"
                },
                "author": "Martin Jaggi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10961v2",
                "updated": "2025-08-12T10:35:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    35,
                    5,
                    1,
                    224,
                    0
                ],
                "published": "2025-04-15T08:06:36Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    6,
                    36,
                    1,
                    105,
                    0
                ],
                "title": "Evaluating Trust in AI, Human, and Co-produced Feedback Among\n  Undergraduate Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Trust in AI, Human, and Co-produced Feedback Among\n  Undergraduate Students"
                },
                "summary": "As generative AI models, particularly large language models (LLMs), transform\neducational feedback practices in higher education (HE) contexts, understanding\nstudents' perceptions of different sources of feedback becomes crucial for\ntheir effective implementation and adoption. This study addresses a critical\ngap by comparing undergraduate students' trust in LLM, human, and human-AI\nco-produced feedback in their authentic HE context. More specifically, through\na within-subject experimental design involving 91 participants, we investigated\nfactors that predict students' ability to distinguish between feedback types,\ntheir perceptions of feedback quality, and potential biases related to the\nsource of feedback. Findings revealed that when the source was blinded,\nstudents generally preferred AI and co-produced feedback over human feedback\nregarding perceived usefulness and objectivity. However, they presented a\nstrong bias against AI when the source of feedback was disclosed. In addition,\nonly AI feedback suffered a decline in perceived genuineness when feedback\nsources were revealed, while co-produced feedback maintained its positive\nperception. Educational AI experience improved students' ability to identify\nLLM-generated feedback and increased their trust in all types of feedback. More\nyears of students' experience using AI for general purposes were associated\nwith lower perceived usefulness and credibility of feedback. These insights\noffer substantial evidence of the importance of source credibility and the need\nto enhance both feedback literacy and AI literacy to mitigate bias in student\nperceptions for AI-generated feedback to be adopted and impact education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As generative AI models, particularly large language models (LLMs), transform\neducational feedback practices in higher education (HE) contexts, understanding\nstudents' perceptions of different sources of feedback becomes crucial for\ntheir effective implementation and adoption. This study addresses a critical\ngap by comparing undergraduate students' trust in LLM, human, and human-AI\nco-produced feedback in their authentic HE context. More specifically, through\na within-subject experimental design involving 91 participants, we investigated\nfactors that predict students' ability to distinguish between feedback types,\ntheir perceptions of feedback quality, and potential biases related to the\nsource of feedback. Findings revealed that when the source was blinded,\nstudents generally preferred AI and co-produced feedback over human feedback\nregarding perceived usefulness and objectivity. However, they presented a\nstrong bias against AI when the source of feedback was disclosed. In addition,\nonly AI feedback suffered a decline in perceived genuineness when feedback\nsources were revealed, while co-produced feedback maintained its positive\nperception. Educational AI experience improved students' ability to identify\nLLM-generated feedback and increased their trust in all types of feedback. More\nyears of students' experience using AI for general purposes were associated\nwith lower perceived usefulness and credibility of feedback. These insights\noffer substantial evidence of the importance of source credibility and the need\nto enhance both feedback literacy and AI literacy to mitigate bias in student\nperceptions for AI-generated feedback to be adopted and impact education."
                },
                "authors": [
                    {
                        "name": "Audrey Zhang"
                    },
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Wannapon Suraworachet"
                    },
                    {
                        "name": "Tanya Nazaretsky"
                    },
                    {
                        "name": "Mutlu Cukurova"
                    }
                ],
                "author_detail": {
                    "name": "Mutlu Cukurova"
                },
                "author": "Mutlu Cukurova",
                "arxiv_comment": "35 pages, 6 figures. Under review at Assessment and Evaluation in\n  Higher Education",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08821v1",
                "updated": "2025-08-12T10:21:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    21,
                    59,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T10:21:59Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    21,
                    59,
                    1,
                    224,
                    0
                ],
                "title": "3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs"
                },
                "summary": "Recent Multi-Modal Large Language Models (MLLMs) have demonstrated strong\ncapabilities in learning joint representations from text and images. However,\ntheir spatial reasoning remains limited. We introduce 3DFroMLLM, a novel\nframework that enables the generation of 3D object prototypes directly from\nMLLMs, including geometry and part labels. Our pipeline is agentic, comprising\na designer, coder, and visual inspector operating in a refinement loop.\nNotably, our approach requires no additional training data or detailed user\ninstructions. Building on prior work in 2D generation, we demonstrate that\nrendered images produced by our framework can be effectively used for image\nclassification pretraining tasks and outperforms previous methods by 15%. As a\ncompelling real-world use case, we show that the generated prototypes can be\nleveraged to improve fine-grained vision-language models by using the rendered,\npart-labeled prototypes to fine-tune CLIP for part segmentation and achieving a\n55% accuracy improvement without relying on any additional human-labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multi-Modal Large Language Models (MLLMs) have demonstrated strong\ncapabilities in learning joint representations from text and images. However,\ntheir spatial reasoning remains limited. We introduce 3DFroMLLM, a novel\nframework that enables the generation of 3D object prototypes directly from\nMLLMs, including geometry and part labels. Our pipeline is agentic, comprising\na designer, coder, and visual inspector operating in a refinement loop.\nNotably, our approach requires no additional training data or detailed user\ninstructions. Building on prior work in 2D generation, we demonstrate that\nrendered images produced by our framework can be effectively used for image\nclassification pretraining tasks and outperforms previous methods by 15%. As a\ncompelling real-world use case, we show that the generated prototypes can be\nleveraged to improve fine-grained vision-language models by using the rendered,\npart-labeled prototypes to fine-tune CLIP for part segmentation and achieving a\n55% accuracy improvement without relying on any additional human-labeled data."
                },
                "authors": [
                    {
                        "name": "Noor Ahmed"
                    },
                    {
                        "name": "Cameron Braunstein"
                    },
                    {
                        "name": "Steffen Eger"
                    },
                    {
                        "name": "Eddy Ilg"
                    }
                ],
                "author_detail": {
                    "name": "Eddy Ilg"
                },
                "author": "Eddy Ilg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07107v2",
                "updated": "2025-08-12T10:20:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    20,
                    46,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-09T21:24:54Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    21,
                    24,
                    54,
                    5,
                    221,
                    0
                ],
                "title": "Designing a Feedback-Driven Decision Support System for Dynamic Student\n  Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing a Feedback-Driven Decision Support System for Dynamic Student\n  Intervention"
                },
                "summary": "Accurate prediction of student performance is essential for enabling timely\nacademic interventions. However, most machine learning models used in\neducational settings are static and lack the ability to adapt when new data\nsuch as post-intervention outcomes become available. To address this\nlimitation, we propose a Feedback-Driven Decision Support System (DSS) with a\nclosed-loop architecture that enables continuous model refinement. The system\nemploys a LightGBM-based regressor with incremental retraining, allowing\neducators to input updated student performance data, which automatically\ntriggers model updates. This adaptive mechanism enhances prediction accuracy by\nlearning from real-world academic progress over time.\n  The platform features a Flask-based web interface to support real-time\ninteraction and integrates SHAP (SHapley Additive exPlanations) for model\ninterpretability, ensuring transparency and trustworthiness in predictions.\nExperimental results demonstrate a 10.7% reduction in RMSE after retraining,\nwith consistent upward adjustments in predicted scores for students who\nreceived interventions. By transforming static predictive models into\nself-improving systems, our approach advances educational analytics toward\nhuman-centered, data-driven, and responsive artificial intelligence. The\nframework is designed for seamless integration into Learning Management Systems\n(LMS) and institutional dashboards, facilitating practical deployment in real\neducational environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate prediction of student performance is essential for enabling timely\nacademic interventions. However, most machine learning models used in\neducational settings are static and lack the ability to adapt when new data\nsuch as post-intervention outcomes become available. To address this\nlimitation, we propose a Feedback-Driven Decision Support System (DSS) with a\nclosed-loop architecture that enables continuous model refinement. The system\nemploys a LightGBM-based regressor with incremental retraining, allowing\neducators to input updated student performance data, which automatically\ntriggers model updates. This adaptive mechanism enhances prediction accuracy by\nlearning from real-world academic progress over time.\n  The platform features a Flask-based web interface to support real-time\ninteraction and integrates SHAP (SHapley Additive exPlanations) for model\ninterpretability, ensuring transparency and trustworthiness in predictions.\nExperimental results demonstrate a 10.7% reduction in RMSE after retraining,\nwith consistent upward adjustments in predicted scores for students who\nreceived interventions. By transforming static predictive models into\nself-improving systems, our approach advances educational analytics toward\nhuman-centered, data-driven, and responsive artificial intelligence. The\nframework is designed for seamless integration into Learning Management Systems\n(LMS) and institutional dashboards, facilitating practical deployment in real\neducational environments."
                },
                "authors": [
                    {
                        "name": "Timothy Oluwapelumi Adeyemi"
                    },
                    {
                        "name": "Nadiah Fahad AlOtaibi"
                    }
                ],
                "author_detail": {
                    "name": "Nadiah Fahad AlOtaibi"
                },
                "author": "Nadiah Fahad AlOtaibi",
                "arxiv_comment": "10 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.1; I.2.6; H.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03850v2",
                "updated": "2025-08-12T10:16:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    16,
                    47,
                    1,
                    224,
                    0
                ],
                "published": "2025-06-04T11:33:36Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    33,
                    36,
                    2,
                    155,
                    0
                ],
                "title": "Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful\n  Fine-Tuning"
                },
                "summary": "Harmful fine-tuning (HFT), performed directly on open-source LLMs or through\nFine-tuning-as-a-Service, breaks safety alignment and poses significant\nthreats. Existing methods aim to mitigate HFT risks by learning robust\nrepresentation on alignment data or making harmful data unlearnable, but they\ntreat each data sample equally, leaving data vulnerability patterns\nunderstudied. In this work, we reveal that certain subsets of alignment data\nare consistently more prone to forgetting during HFT across different\nfine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware\nAlignment (VAA), which estimates data vulnerability, partitions data into\n\"vulnerable\" and \"invulnerable\" groups, and encourages balanced learning using\na group distributionally robust optimization (Group DRO) framework.\nSpecifically, VAA learns an adversarial sampler that samples examples from the\ncurrently underperforming group and then applies group-dependent adversarial\nperturbations to the data during training, aiming to encourage a balanced\nlearning process across groups. Experiments across four fine-tuning tasks\ndemonstrate that VAA significantly reduces harmful scores while preserving\ndownstream task performance, outperforming state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmful fine-tuning (HFT), performed directly on open-source LLMs or through\nFine-tuning-as-a-Service, breaks safety alignment and poses significant\nthreats. Existing methods aim to mitigate HFT risks by learning robust\nrepresentation on alignment data or making harmful data unlearnable, but they\ntreat each data sample equally, leaving data vulnerability patterns\nunderstudied. In this work, we reveal that certain subsets of alignment data\nare consistently more prone to forgetting during HFT across different\nfine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware\nAlignment (VAA), which estimates data vulnerability, partitions data into\n\"vulnerable\" and \"invulnerable\" groups, and encourages balanced learning using\na group distributionally robust optimization (Group DRO) framework.\nSpecifically, VAA learns an adversarial sampler that samples examples from the\ncurrently underperforming group and then applies group-dependent adversarial\nperturbations to the data during training, aiming to encourage a balanced\nlearning process across groups. Experiments across four fine-tuning tasks\ndemonstrate that VAA significantly reduces harmful scores while preserving\ndownstream task performance, outperforming state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Xueting Han"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Jing Bai"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23486v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23486v3",
                "updated": "2025-08-13T08:51:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    51,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-31T12:10:00Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    12,
                    10,
                    0,
                    3,
                    212,
                    0
                ],
                "title": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and\n  Effectiveness in Clinical Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and\n  Effectiveness in Clinical Domains"
                },
                "summary": "Large language models (LLMs) hold promise in clinical decision support but\nface major challenges in safety evaluation and effectiveness validation. We\ndeveloped the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a\nmultidimensional framework built on clinical expert consensus, encompassing 30\ncriteria covering critical areas like critical illness recognition, guideline\nadherence, and medication safety, with weighted consequence measures.\nThirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A\nitems aligned with these criteria, spanning 26 clinical departments to simulate\nreal-world scenarios. Benchmark testing of six LLMs revealed moderate overall\nperformance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),\nwith a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).\nDomain-specific medical LLMs showed consistent performance advantages over\ngeneral-purpose models, with relatively higher top scores in safety (0.912) and\neffectiveness (0.861). The findings of this study not only provide a\nstandardized metric for evaluating the clinical application of medical LLMs,\nfacilitating comparative analyses, risk exposure identification, and\nimprovement directions across different scenarios, but also hold the potential\nto promote safer and more effective deployment of large language models in\nhealthcare environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hold promise in clinical decision support but\nface major challenges in safety evaluation and effectiveness validation. We\ndeveloped the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a\nmultidimensional framework built on clinical expert consensus, encompassing 30\ncriteria covering critical areas like critical illness recognition, guideline\nadherence, and medication safety, with weighted consequence measures.\nThirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A\nitems aligned with these criteria, spanning 26 clinical departments to simulate\nreal-world scenarios. Benchmark testing of six LLMs revealed moderate overall\nperformance (average total score 57.2%, safety 54.7%, effectiveness 62.3%),\nwith a significant 13.3% performance drop in high-risk scenarios (p < 0.0001).\nDomain-specific medical LLMs showed consistent performance advantages over\ngeneral-purpose models, with relatively higher top scores in safety (0.912) and\neffectiveness (0.861). The findings of this study not only provide a\nstandardized metric for evaluating the clinical application of medical LLMs,\nfacilitating comparative analyses, risk exposure identification, and\nimprovement directions across different scenarios, but also hold the potential\nto promote safer and more effective deployment of large language models in\nhealthcare environments."
                },
                "authors": [
                    {
                        "name": "Shirui Wang"
                    },
                    {
                        "name": "Zhihui Tang"
                    },
                    {
                        "name": "Huaxia Yang"
                    },
                    {
                        "name": "Qiuhong Gong"
                    },
                    {
                        "name": "Tiantian Gu"
                    },
                    {
                        "name": "Hongyang Ma"
                    },
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Wubin Sun"
                    },
                    {
                        "name": "Zeliang Lian"
                    },
                    {
                        "name": "Kehang Mao"
                    },
                    {
                        "name": "Yinan Jiang"
                    },
                    {
                        "name": "Zhicheng Huang"
                    },
                    {
                        "name": "Lingyun Ma"
                    },
                    {
                        "name": "Wenjie Shen"
                    },
                    {
                        "name": "Yajie Ji"
                    },
                    {
                        "name": "Yunhui Tan"
                    },
                    {
                        "name": "Chunbo Wang"
                    },
                    {
                        "name": "Yunlu Gao"
                    },
                    {
                        "name": "Qianling Ye"
                    },
                    {
                        "name": "Rui Lin"
                    },
                    {
                        "name": "Mingyu Chen"
                    },
                    {
                        "name": "Lijuan Niu"
                    },
                    {
                        "name": "Zhihao Wang"
                    },
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "Mengran Lang"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Huimin Zhang"
                    },
                    {
                        "name": "Haitao Shen"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Qiguang Zhao"
                    },
                    {
                        "name": "Si-Xuan Liu"
                    },
                    {
                        "name": "Lina Zhou"
                    },
                    {
                        "name": "Hua Gao"
                    },
                    {
                        "name": "Dongqiang Ye"
                    },
                    {
                        "name": "Lingmin Meng"
                    },
                    {
                        "name": "Youtao Yu"
                    },
                    {
                        "name": "Naixin Liang"
                    },
                    {
                        "name": "Jianxiong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jianxiong Wu"
                },
                "author": "Jianxiong Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23486v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23486v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00642v2",
                "updated": "2025-08-12T10:04:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    4,
                    27,
                    1,
                    224,
                    0
                ],
                "published": "2025-07-01T10:34:17Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    10,
                    34,
                    17,
                    1,
                    182,
                    0
                ],
                "title": "ChatHLS: Towards Systematic Design Automation and Optimization for\n  High-Level Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatHLS: Towards Systematic Design Automation and Optimization for\n  High-Level Synthesis"
                },
                "summary": "The increasing complexity of computational demands has spurred the adoption\nof domain-specific accelerators, yet traditional hardware design methodologies\nremain constrained by prolonged development and verification cycles. High-Level\nSynthesis (HLS) bridges the software-hardware gap by enabling hardware design\nfrom high-level languages. However, its widespread adoption is hindered by\nstrict coding constraints and intricate hardware-specific optimizations. To\naddress these challenges, we introduce ChatHLS, an agile HLS design automation\nworkflow that leverages fine-tuned LLMs integrated within a multi-agent\nframework for HLS-specific error correction and design optimization. Through\nnavigating LLM training with a novel verification-oriented data augmentation\nparadigm, ChatHLS achieves an average repair pass rate of 82.7% over 612 error\ncases. Furthermore, by enabling optimization reasoning within practical\ncomputational budgets, ChatHLS delivers performance improvements ranging from\n1.9$\\times$ to 14.8$\\times$ on resource-constrained kernels, attaining a\n3.6$\\times$ average speedup compared to SOTA approaches. These results\nunderscore the potential of ChatHLS in substantially expediting hardware\ndevelopment cycles while upholding rigorous standards of design reliability and\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of computational demands has spurred the adoption\nof domain-specific accelerators, yet traditional hardware design methodologies\nremain constrained by prolonged development and verification cycles. High-Level\nSynthesis (HLS) bridges the software-hardware gap by enabling hardware design\nfrom high-level languages. However, its widespread adoption is hindered by\nstrict coding constraints and intricate hardware-specific optimizations. To\naddress these challenges, we introduce ChatHLS, an agile HLS design automation\nworkflow that leverages fine-tuned LLMs integrated within a multi-agent\nframework for HLS-specific error correction and design optimization. Through\nnavigating LLM training with a novel verification-oriented data augmentation\nparadigm, ChatHLS achieves an average repair pass rate of 82.7% over 612 error\ncases. Furthermore, by enabling optimization reasoning within practical\ncomputational budgets, ChatHLS delivers performance improvements ranging from\n1.9$\\times$ to 14.8$\\times$ on resource-constrained kernels, attaining a\n3.6$\\times$ average speedup compared to SOTA approaches. These results\nunderscore the potential of ChatHLS in substantially expediting hardware\ndevelopment cycles while upholding rigorous standards of design reliability and\nquality."
                },
                "authors": [
                    {
                        "name": "Runkai Li"
                    },
                    {
                        "name": "Jia Xiong"
                    },
                    {
                        "name": "Xiuyuan He"
                    },
                    {
                        "name": "Jiaqi Lv"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Xi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xi Wang"
                },
                "author": "Xi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08800v1",
                "updated": "2025-08-12T09:57:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    57,
                    5,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T09:57:05Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    57,
                    5,
                    1,
                    224,
                    0
                ],
                "title": "Fault Tolerant Multi-Agent Learning with Adversarial Budget Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault Tolerant Multi-Agent Learning with Adversarial Budget Constraints"
                },
                "summary": "In multi-agent systems, the safe and reliable execution of tasks often\ndepends on agents correctly coordinating their actions. However, in real-world\ndeployments, failures of computational components are inevitable, presenting a\ncritical challenge: ensuring that multi-agent reinforcement learning (MARL)\npolicies remain effective even when some agents malfunction. We propose the\nMulti-Agent Robust Training Algorithm (MARTA), a plug-and-play framework for\ntraining MARL agents to be resilient to potentially severe faults. MARTA\noperates in cooperative multi-agent settings where agents may lose the ability\nto execute their intended actions. It learns to identify failure scenarios that\nare especially detrimental to system performance and equips agents with\nstrategies to mitigate their impact. At the heart of MARTA is a novel\nadversarial Markov game in which an adversary -- modelled via \\emph{Markov\nswitching controls} -- learns to disable agents in high-risk state regions,\nwhile the remaining agents are trained to \\emph{jointly} best-respond to such\ntargeted malfunctions. To ensure practicality, MARTA enforces a malfunction\nbudget, constraining the adversary to a fixed number of failures and learning\nrobust policies accordingly. We provide theoretical guarantees that MARTA\nconverges to a Markov perfect equilibrium, ensuring agents optimally counteract\nworst-case faults. Empirically, we show that MARTA achieves state-of-the-art\nfault-tolerant performance across benchmark environments, including Multi-Agent\nParticle World and Level-Based Foraging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems, the safe and reliable execution of tasks often\ndepends on agents correctly coordinating their actions. However, in real-world\ndeployments, failures of computational components are inevitable, presenting a\ncritical challenge: ensuring that multi-agent reinforcement learning (MARL)\npolicies remain effective even when some agents malfunction. We propose the\nMulti-Agent Robust Training Algorithm (MARTA), a plug-and-play framework for\ntraining MARL agents to be resilient to potentially severe faults. MARTA\noperates in cooperative multi-agent settings where agents may lose the ability\nto execute their intended actions. It learns to identify failure scenarios that\nare especially detrimental to system performance and equips agents with\nstrategies to mitigate their impact. At the heart of MARTA is a novel\nadversarial Markov game in which an adversary -- modelled via \\emph{Markov\nswitching controls} -- learns to disable agents in high-risk state regions,\nwhile the remaining agents are trained to \\emph{jointly} best-respond to such\ntargeted malfunctions. To ensure practicality, MARTA enforces a malfunction\nbudget, constraining the adversary to a fixed number of failures and learning\nrobust policies accordingly. We provide theoretical guarantees that MARTA\nconverges to a Markov perfect equilibrium, ensuring agents optimally counteract\nworst-case faults. Empirically, we show that MARTA achieves state-of-the-art\nfault-tolerant performance across benchmark environments, including Multi-Agent\nParticle World and Level-Based Foraging."
                },
                "authors": [
                    {
                        "name": "David Mguni"
                    },
                    {
                        "name": "Yaqi Sun"
                    },
                    {
                        "name": "Haojun Chen"
                    },
                    {
                        "name": "Amir Darabi"
                    },
                    {
                        "name": "Larry Olanrewaju Orimoloye"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08795v1",
                "updated": "2025-08-12T09:51:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    51,
                    39,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T09:51:39Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    51,
                    39,
                    1,
                    224,
                    0
                ],
                "title": "A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to\n  Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to\n  Functions"
                },
                "summary": "Large language models (LLMs) acquire vast knowledge from large text corpora,\nbut this information can become outdated or inaccurate. Since retraining is\ncomputationally expensive, knowledge editing offers an efficient alternative --\nmodifying internal knowledge without full retraining. These methods aim to\nupdate facts precisely while preserving the model's overall capabilities. While\nexisting surveys focus on the mechanism of editing (e.g., parameter changes vs.\nexternal memory), they often overlook the function of the knowledge being\nedited. This survey introduces a novel, complementary function-based taxonomy\nto provide a more holistic view. We examine how different mechanisms apply to\nvarious knowledge types -- factual, temporal, conceptual, commonsense, and\nsocial -- highlighting how editing effectiveness depends on the nature of the\ntarget knowledge. By organizing our review along these two axes, we map the\ncurrent landscape, outline the strengths and limitations of existing methods,\ndefine the problem formally, survey evaluation tasks and datasets, and conclude\nwith open challenges and future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) acquire vast knowledge from large text corpora,\nbut this information can become outdated or inaccurate. Since retraining is\ncomputationally expensive, knowledge editing offers an efficient alternative --\nmodifying internal knowledge without full retraining. These methods aim to\nupdate facts precisely while preserving the model's overall capabilities. While\nexisting surveys focus on the mechanism of editing (e.g., parameter changes vs.\nexternal memory), they often overlook the function of the knowledge being\nedited. This survey introduces a novel, complementary function-based taxonomy\nto provide a more holistic view. We examine how different mechanisms apply to\nvarious knowledge types -- factual, temporal, conceptual, commonsense, and\nsocial -- highlighting how editing effectiveness depends on the nature of the\ntarget knowledge. By organizing our review along these two axes, we map the\ncurrent landscape, outline the strengths and limitations of existing methods,\ndefine the problem formally, survey evaluation tasks and datasets, and conclude\nwith open challenges and future directions."
                },
                "authors": [
                    {
                        "name": "Amir Mohammad Salehoof"
                    },
                    {
                        "name": "Ali Ramezani"
                    },
                    {
                        "name": "Yadollah Yaghoobzadeh"
                    },
                    {
                        "name": "Majid Nili Ahmadabadi"
                    }
                ],
                "author_detail": {
                    "name": "Majid Nili Ahmadabadi"
                },
                "author": "Majid Nili Ahmadabadi",
                "arxiv_comment": "13 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08791v1",
                "updated": "2025-08-12T09:45:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    45,
                    19,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T09:45:19Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    45,
                    19,
                    1,
                    224,
                    0
                ],
                "title": "Feedback-Driven Tool-Use Improvements in Large Language Models via\n  Automated Build Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback-Driven Tool-Use Improvements in Large Language Models via\n  Automated Build Environments"
                },
                "summary": "Effective tool use is essential for large language models (LLMs) to interact\nmeaningfully with their environment. However, progress is limited by the lack\nof efficient reinforcement learning (RL) frameworks specifically designed for\ntool use, due to challenges in constructing stable training environments and\ndesigning verifiable reward mechanisms. To address this, we propose an\nautomated environment construction pipeline, incorporating scenario\ndecomposition, document generation, function integration, complexity scaling,\nand localized deployment. This enables the creation of high-quality training\nenvironments that provide detailed and measurable feedback without relying on\nexternal tools. Additionally, we introduce a verifiable reward mechanism that\nevaluates both the precision of tool use and the completeness of task\nexecution. When combined with trajectory data collected from the constructed\nenvironments, this mechanism integrates seamlessly with standard RL algorithms\nto facilitate feedback-driven model training. Experiments on LLMs of varying\nscales demonstrate that our approach significantly enhances the models'\ntool-use performance without degrading their general capabilities, regardless\nof inference modes or training algorithms. Our analysis suggests that these\ngains result from improved context understanding and reasoning, driven by\nupdates to the lower-layer MLP parameters in models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective tool use is essential for large language models (LLMs) to interact\nmeaningfully with their environment. However, progress is limited by the lack\nof efficient reinforcement learning (RL) frameworks specifically designed for\ntool use, due to challenges in constructing stable training environments and\ndesigning verifiable reward mechanisms. To address this, we propose an\nautomated environment construction pipeline, incorporating scenario\ndecomposition, document generation, function integration, complexity scaling,\nand localized deployment. This enables the creation of high-quality training\nenvironments that provide detailed and measurable feedback without relying on\nexternal tools. Additionally, we introduce a verifiable reward mechanism that\nevaluates both the precision of tool use and the completeness of task\nexecution. When combined with trajectory data collected from the constructed\nenvironments, this mechanism integrates seamlessly with standard RL algorithms\nto facilitate feedback-driven model training. Experiments on LLMs of varying\nscales demonstrate that our approach significantly enhances the models'\ntool-use performance without degrading their general capabilities, regardless\nof inference modes or training algorithms. Our analysis suggests that these\ngains result from improved context understanding and reasoning, driven by\nupdates to the lower-layer MLP parameters in models."
                },
                "authors": [
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Xuesong Yao"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Jiecao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiecao Chen"
                },
                "author": "Jiecao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08785v1",
                "updated": "2025-08-12T09:38:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    38,
                    21,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T09:38:21Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    38,
                    21,
                    1,
                    224,
                    0
                ],
                "title": "Privacy-protected Retrieval-Augmented Generation for Knowledge Graph\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-protected Retrieval-Augmented Generation for Knowledge Graph\n  Question Answering"
                },
                "summary": "LLMs often suffer from hallucinations and outdated or incomplete knowledge.\nRAG is proposed to address these issues by integrating external knowledge like\nthat in KGs into LLMs. However, leveraging private KGs in RAG systems poses\nsignificant privacy risks due to the black-box nature of LLMs and potential\ninsecure data transmission, especially when using third-party LLM APIs lacking\ntransparency and control. In this paper, we investigate the privacy-protected\nRAG scenario for the first time, where entities in KGs are anonymous for LLMs,\nthus preventing them from accessing entity semantics. Due to the loss of\nsemantics of entities, previous RAG systems cannot retrieve question-relevant\nknowledge from KGs by matching questions with the meaningless identifiers of\nanonymous entities. To realize an effective RAG system in this scenario, two\nkey challenges must be addressed: (1) How can anonymous entities be converted\ninto retrievable information. (2) How to retrieve question-relevant anonymous\nentities. Hence, we propose a novel ARoG framework including relation-centric\nabstraction and structure-oriented abstraction strategies. For challenge (1),\nthe first strategy abstracts entities into high-level concepts by dynamically\ncapturing the semantics of their adjacent relations. It supplements meaningful\nsemantics which can further support the retrieval process. For challenge (2),\nthe second strategy transforms unstructured natural language questions into\nstructured abstract concept paths. These paths can be more effectively aligned\nwith the abstracted concepts in KGs, thereby improving retrieval performance.\nTo guide LLMs to effectively retrieve knowledge from KGs, the two strategies\nstrictly protect privacy from being exposed to LLMs. Experiments on three\ndatasets demonstrate that ARoG achieves strong performance and\nprivacy-robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs often suffer from hallucinations and outdated or incomplete knowledge.\nRAG is proposed to address these issues by integrating external knowledge like\nthat in KGs into LLMs. However, leveraging private KGs in RAG systems poses\nsignificant privacy risks due to the black-box nature of LLMs and potential\ninsecure data transmission, especially when using third-party LLM APIs lacking\ntransparency and control. In this paper, we investigate the privacy-protected\nRAG scenario for the first time, where entities in KGs are anonymous for LLMs,\nthus preventing them from accessing entity semantics. Due to the loss of\nsemantics of entities, previous RAG systems cannot retrieve question-relevant\nknowledge from KGs by matching questions with the meaningless identifiers of\nanonymous entities. To realize an effective RAG system in this scenario, two\nkey challenges must be addressed: (1) How can anonymous entities be converted\ninto retrievable information. (2) How to retrieve question-relevant anonymous\nentities. Hence, we propose a novel ARoG framework including relation-centric\nabstraction and structure-oriented abstraction strategies. For challenge (1),\nthe first strategy abstracts entities into high-level concepts by dynamically\ncapturing the semantics of their adjacent relations. It supplements meaningful\nsemantics which can further support the retrieval process. For challenge (2),\nthe second strategy transforms unstructured natural language questions into\nstructured abstract concept paths. These paths can be more effectively aligned\nwith the abstracted concepts in KGs, thereby improving retrieval performance.\nTo guide LLMs to effectively retrieve knowledge from KGs, the two strategies\nstrictly protect privacy from being exposed to LLMs. Experiments on three\ndatasets demonstrate that ARoG achieves strong performance and\nprivacy-robustness."
                },
                "authors": [
                    {
                        "name": "Yunfeng Ning"
                    },
                    {
                        "name": "Mayi Xu"
                    },
                    {
                        "name": "Jintao Wen"
                    },
                    {
                        "name": "Qiankun Pi"
                    },
                    {
                        "name": "Yuanyuan Zhu"
                    },
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Tieyun Qian"
                    }
                ],
                "author_detail": {
                    "name": "Tieyun Qian"
                },
                "author": "Tieyun Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08783v1",
                "updated": "2025-08-12T09:37:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    37,
                    9,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T09:37:09Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    37,
                    9,
                    1,
                    224,
                    0
                ],
                "title": "DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal\n  Pose Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal\n  Pose Estimation"
                },
                "summary": "Animal pose estimation is a fundamental task in computer vision, with growing\nimportance in ecological monitoring, behavioral analysis, and intelligent\nlivestock management. Compared to human pose estimation, animal pose estimation\nis more challenging due to high interspecies morphological diversity, complex\nbody structures, and limited annotated data. In this work, we introduce\nDiffPose-Animal, a novel diffusion-based framework for top-down animal pose\nestimation. Unlike traditional heatmap regression methods, DiffPose-Animal\nreformulates pose estimation as a denoising process under the generative\nframework of diffusion models. To enhance semantic guidance during keypoint\ngeneration, we leverage large language models (LLMs) to extract both global\nanatomical priors and local keypoint-wise semantics based on species-specific\nprompts. These textual priors are encoded and fused with image features via\ncross-attention modules to provide biologically meaningful constraints\nthroughout the denoising process. Additionally, a diffusion-based keypoint\ndecoder is designed to progressively refine pose predictions, improving\nrobustness to occlusion and annotation sparsity. Extensive experiments on\npublic animal pose datasets demonstrate the effectiveness and generalization\ncapability of our method, especially under challenging scenarios with diverse\nspecies, cluttered backgrounds, and incomplete keypoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Animal pose estimation is a fundamental task in computer vision, with growing\nimportance in ecological monitoring, behavioral analysis, and intelligent\nlivestock management. Compared to human pose estimation, animal pose estimation\nis more challenging due to high interspecies morphological diversity, complex\nbody structures, and limited annotated data. In this work, we introduce\nDiffPose-Animal, a novel diffusion-based framework for top-down animal pose\nestimation. Unlike traditional heatmap regression methods, DiffPose-Animal\nreformulates pose estimation as a denoising process under the generative\nframework of diffusion models. To enhance semantic guidance during keypoint\ngeneration, we leverage large language models (LLMs) to extract both global\nanatomical priors and local keypoint-wise semantics based on species-specific\nprompts. These textual priors are encoded and fused with image features via\ncross-attention modules to provide biologically meaningful constraints\nthroughout the denoising process. Additionally, a diffusion-based keypoint\ndecoder is designed to progressively refine pose predictions, improving\nrobustness to occlusion and annotation sparsity. Extensive experiments on\npublic animal pose datasets demonstrate the effectiveness and generalization\ncapability of our method, especially under challenging scenarios with diverse\nspecies, cluttered backgrounds, and incomplete keypoints."
                },
                "authors": [
                    {
                        "name": "Tianyu Xiong"
                    },
                    {
                        "name": "Dayi Tan"
                    },
                    {
                        "name": "Wei Tian"
                    }
                ],
                "author_detail": {
                    "name": "Wei Tian"
                },
                "author": "Wei Tian",
                "arxiv_comment": "13pages,2figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08777v1",
                "updated": "2025-08-12T09:23:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    23,
                    35,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T09:23:35Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    23,
                    35,
                    1,
                    224,
                    0
                ],
                "title": "Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge"
                },
                "summary": "Evaluating personalized recommendations remains a central challenge,\nespecially in long-form audio domains like podcasts, where traditional offline\nmetrics suffer from exposure bias and online methods such as A/B testing are\ncostly and operationally constrained. In this paper, we propose a novel\nframework that leverages Large Language Models (LLMs) as offline judges to\nassess the quality of podcast recommendations in a scalable and interpretable\nmanner. Our two-stage profile-aware approach first constructs natural-language\nuser profiles distilled from 90 days of listening history. These profiles\nsummarize both topical interests and behavioral patterns, serving as compact,\ninterpretable representations of user preferences. Rather than prompting the\nLLM with raw data, we use these profiles to provide high-level, semantically\nrich context-enabling the LLM to reason more effectively about alignment\nbetween a user's interests and recommended episodes. This reduces input\ncomplexity and improves interpretability. The LLM is then prompted to deliver\nfine-grained pointwise and pairwise judgments based on the profile-episode\nmatch. In a controlled study with 47 participants, our profile-aware judge\nmatched human judgments with high fidelity and outperformed or matched a\nvariant using raw listening histories. The framework enables efficient,\nprofile-aware evaluation for iterative testing and model selection in\nrecommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating personalized recommendations remains a central challenge,\nespecially in long-form audio domains like podcasts, where traditional offline\nmetrics suffer from exposure bias and online methods such as A/B testing are\ncostly and operationally constrained. In this paper, we propose a novel\nframework that leverages Large Language Models (LLMs) as offline judges to\nassess the quality of podcast recommendations in a scalable and interpretable\nmanner. Our two-stage profile-aware approach first constructs natural-language\nuser profiles distilled from 90 days of listening history. These profiles\nsummarize both topical interests and behavioral patterns, serving as compact,\ninterpretable representations of user preferences. Rather than prompting the\nLLM with raw data, we use these profiles to provide high-level, semantically\nrich context-enabling the LLM to reason more effectively about alignment\nbetween a user's interests and recommended episodes. This reduces input\ncomplexity and improves interpretability. The LLM is then prompted to deliver\nfine-grained pointwise and pairwise judgments based on the profile-episode\nmatch. In a controlled study with 47 participants, our profile-aware judge\nmatched human judgments with high fidelity and outperformed or matched a\nvariant using raw listening histories. The framework enables efficient,\nprofile-aware evaluation for iterative testing and model selection in\nrecommender systems."
                },
                "authors": [
                    {
                        "name": "Francesco Fabbri"
                    },
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Edoardo D'Amico"
                    },
                    {
                        "name": "Alice Wang"
                    },
                    {
                        "name": "Marco De Nadai"
                    },
                    {
                        "name": "Jackie Doremus"
                    },
                    {
                        "name": "Paul Gigioli"
                    },
                    {
                        "name": "Andreas Damianou"
                    },
                    {
                        "name": "Oskar Stal"
                    },
                    {
                        "name": "Mounia Lalmas"
                    }
                ],
                "author_detail": {
                    "name": "Mounia Lalmas"
                },
                "author": "Mounia Lalmas",
                "arxiv_doi": "10.1145/3705328.3759305",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3705328.3759305",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.08777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at RecSys '25",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08765v1",
                "updated": "2025-08-12T09:11:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    11,
                    31,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T09:11:31Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    11,
                    31,
                    1,
                    224,
                    0
                ],
                "title": "Bridging the Gap: A Framework for Real-World Video Deepfake Detection\n  via Social Network Compression Emulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Gap: A Framework for Real-World Video Deepfake Detection\n  via Social Network Compression Emulation"
                },
                "summary": "The growing presence of AI-generated videos on social networks poses new\nchallenges for deepfake detection, as detectors trained under controlled\nconditions often fail to generalize to real-world scenarios. A key factor\nbehind this gap is the aggressive, proprietary compression applied by platforms\nlike YouTube and Facebook, which launder low-level forensic cues. However,\nreplicating these transformations at scale is difficult due to API limitations\nand data-sharing constraints. For these reasons, we propose a first framework\nthat emulates the video sharing pipelines of social networks by estimating\ncompression and resizing parameters from a small set of uploaded videos. These\nparameters enable a local emulator capable of reproducing platform-specific\nartifacts on large datasets without direct API access. Experiments on\nFaceForensics++ videos shared via social networks demonstrate that our emulated\ndata closely matches the degradation patterns of real uploads. Furthermore,\ndetectors fine-tuned on emulated videos achieve comparable performance to those\ntrained on actual shared media. Our approach offers a scalable and practical\nsolution for bridging the gap between lab-based training and real-world\ndeployment of deepfake detectors, particularly in the underexplored domain of\ncompressed video content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing presence of AI-generated videos on social networks poses new\nchallenges for deepfake detection, as detectors trained under controlled\nconditions often fail to generalize to real-world scenarios. A key factor\nbehind this gap is the aggressive, proprietary compression applied by platforms\nlike YouTube and Facebook, which launder low-level forensic cues. However,\nreplicating these transformations at scale is difficult due to API limitations\nand data-sharing constraints. For these reasons, we propose a first framework\nthat emulates the video sharing pipelines of social networks by estimating\ncompression and resizing parameters from a small set of uploaded videos. These\nparameters enable a local emulator capable of reproducing platform-specific\nartifacts on large datasets without direct API access. Experiments on\nFaceForensics++ videos shared via social networks demonstrate that our emulated\ndata closely matches the degradation patterns of real uploads. Furthermore,\ndetectors fine-tuned on emulated videos achieve comparable performance to those\ntrained on actual shared media. Our approach offers a scalable and practical\nsolution for bridging the gap between lab-based training and real-world\ndeployment of deepfake detectors, particularly in the underexplored domain of\ncompressed video content."
                },
                "authors": [
                    {
                        "name": "Andrea Montibeller"
                    },
                    {
                        "name": "Dasara Shullani"
                    },
                    {
                        "name": "Daniele Baracchi"
                    },
                    {
                        "name": "Alessandro Piva"
                    },
                    {
                        "name": "Giulia Boato"
                    }
                ],
                "author_detail": {
                    "name": "Giulia Boato"
                },
                "author": "Giulia Boato",
                "arxiv_doi": "10.1145/3746265.3759670",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746265.3759670",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.08765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08761v1",
                "updated": "2025-08-12T09:08:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    8,
                    29,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T09:08:29Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    8,
                    29,
                    1,
                    224,
                    0
                ],
                "title": "DevNous: An LLM-Based Multi-Agent System for Grounding IT Project\n  Management in Unstructured Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DevNous: An LLM-Based Multi-Agent System for Grounding IT Project\n  Management in Unstructured Conversation"
                },
                "summary": "The manual translation of unstructured team dialogue into the structured\nartifacts required for Information Technology (IT) project governance is a\ncritical bottleneck in modern information systems management. We introduce\nDevNous, a Large Language Model-based (LLM) multi-agent expert system, to\nautomate this unstructured-to-structured translation process. DevNous\nintegrates directly into team chat environments, identifying actionable intents\nfrom informal dialogue and managing stateful, multi-turn workflows for core\nadministrative tasks like automated task formalization and progress summary\nsynthesis. To quantitatively evaluate the system, we introduce a new benchmark\nof 160 realistic, interactive conversational turns. The dataset was manually\nannotated with a multi-label ground truth and is publicly available. On this\nbenchmark, DevNous achieves an exact match turn accuracy of 81.3\\% and a\nmultiset F1-Score of 0.845, providing strong evidence for its viability. The\nprimary contributions of this work are twofold: (1) a validated architectural\npattern for developing ambient administrative agents, and (2) the introduction\nof the first robust empirical baseline and public benchmark dataset for this\nchallenging problem domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The manual translation of unstructured team dialogue into the structured\nartifacts required for Information Technology (IT) project governance is a\ncritical bottleneck in modern information systems management. We introduce\nDevNous, a Large Language Model-based (LLM) multi-agent expert system, to\nautomate this unstructured-to-structured translation process. DevNous\nintegrates directly into team chat environments, identifying actionable intents\nfrom informal dialogue and managing stateful, multi-turn workflows for core\nadministrative tasks like automated task formalization and progress summary\nsynthesis. To quantitatively evaluate the system, we introduce a new benchmark\nof 160 realistic, interactive conversational turns. The dataset was manually\nannotated with a multi-label ground truth and is publicly available. On this\nbenchmark, DevNous achieves an exact match turn accuracy of 81.3\\% and a\nmultiset F1-Score of 0.845, providing strong evidence for its viability. The\nprimary contributions of this work are twofold: (1) a validated architectural\npattern for developing ambient administrative agents, and (2) the introduction\nof the first robust empirical baseline and public benchmark dataset for this\nchallenging problem domain."
                },
                "authors": [
                    {
                        "name": "Stavros Doropoulos"
                    },
                    {
                        "name": "Stavros Vologiannidis"
                    },
                    {
                        "name": "Ioannis Magnisalis"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Magnisalis"
                },
                "arxiv_affiliation": "DG Informatics, European Commission, Brussels, Belgium",
                "author": "Ioannis Magnisalis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06922v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06922v4",
                "updated": "2025-08-12T09:08:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    8,
                    2,
                    1,
                    224,
                    0
                ],
                "published": "2024-02-10T11:07:24Z",
                "published_parsed": [
                    2024,
                    2,
                    10,
                    11,
                    7,
                    24,
                    5,
                    41,
                    0
                ],
                "title": "Whispers in the Machine: Confidentiality in Agentic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whispers in the Machine: Confidentiality in Agentic Systems"
                },
                "summary": "The interaction between users and applications is increasingly shifted toward\nnatural language by deploying Large Language Models (LLMs) as the core\ninterface. The capabilities of these so-called agents become more capable the\nmore tools and services they serve as an interface for, ultimately leading to\nagentic systems. Agentic systems use LLM-based agents as interfaces for most\nuser interactions and various integrations with external tools and services.\nWhile these interfaces can significantly enhance the capabilities of the\nagentic system, they also introduce a new attack surface. Manipulated\nintegrations, for example, can exploit the internal LLM and compromise\nsensitive data accessed through other interfaces. While previous work primarily\nfocused on attacks targeting a model's alignment or the leakage of training\ndata, the security of data that is only available during inference has escaped\nscrutiny so far. In this work, we demonstrate how the integration of LLMs into\nsystems with external tool integration poses a risk similar to established\nprompt-based attacks, able to compromise the confidentiality of the entire\nsystem. Introducing a systematic approach to evaluate these confidentiality\nrisks, we identify two specific attack scenarios unique to these agentic\nsystems and formalize these into a tool-robustness framework designed to\nmeasure a model's ability to protect sensitive information. Our analysis\nreveals significant vulnerabilities across all tested models, highlighting an\nincreased risk when models are combined with external tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The interaction between users and applications is increasingly shifted toward\nnatural language by deploying Large Language Models (LLMs) as the core\ninterface. The capabilities of these so-called agents become more capable the\nmore tools and services they serve as an interface for, ultimately leading to\nagentic systems. Agentic systems use LLM-based agents as interfaces for most\nuser interactions and various integrations with external tools and services.\nWhile these interfaces can significantly enhance the capabilities of the\nagentic system, they also introduce a new attack surface. Manipulated\nintegrations, for example, can exploit the internal LLM and compromise\nsensitive data accessed through other interfaces. While previous work primarily\nfocused on attacks targeting a model's alignment or the leakage of training\ndata, the security of data that is only available during inference has escaped\nscrutiny so far. In this work, we demonstrate how the integration of LLMs into\nsystems with external tool integration poses a risk similar to established\nprompt-based attacks, able to compromise the confidentiality of the entire\nsystem. Introducing a systematic approach to evaluate these confidentiality\nrisks, we identify two specific attack scenarios unique to these agentic\nsystems and formalize these into a tool-robustness framework designed to\nmeasure a model's ability to protect sensitive information. Our analysis\nreveals significant vulnerabilities across all tested models, highlighting an\nincreased risk when models are combined with external tools."
                },
                "authors": [
                    {
                        "name": "Jonathan Evertz"
                    },
                    {
                        "name": "Merlin Chlosta"
                    },
                    {
                        "name": "Lea Schönherr"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Eisenhofer"
                },
                "author": "Thorsten Eisenhofer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06922v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06922v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04276v2",
                "updated": "2025-08-12T09:00:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    0,
                    7,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-06T10:01:26Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    10,
                    1,
                    26,
                    2,
                    218,
                    0
                ],
                "title": "A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on\n  Graph-based Retrieval-Augmented Generation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on\n  Graph-based Retrieval-Augmented Generation of Large Language Models"
                },
                "summary": "Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as\na promising paradigm for enhancing large language models (LLMs) by converting\nraw text into structured knowledge graphs, improving both accuracy and\nexplainability. However, GraphRAG relies on LLMs to extract knowledge from raw\ntext during graph construction, and this process can be maliciously manipulated\nto implant misleading information. Targeting this attack surface, we propose\ntwo knowledge poisoning attacks (KPAs) and demonstrate that modifying only a\nfew words in the source text can significantly change the constructed graph,\npoison the GraphRAG, and severely mislead downstream reasoning. The first\nattack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate\nvulnerable nodes in the generated graphs and rewrites the corresponding\nnarratives with LLMs, achieving precise control over specific\nquestion-answering (QA) outcomes with a success rate of 93.1\\%, while keeping\nthe poisoned text fluent and natural. The second attack, named Universal KPA\n(UKPA), exploits linguistic cues such as pronouns and dependency relations to\ndisrupt the structural integrity of the generated graph by altering globally\ninfluential words. With fewer than 0.05\\% of full text modified, the QA\naccuracy collapses from 95\\% to 50\\%. Furthermore, experiments show that\nstate-of-the-art defense methods fail to detect these attacks, highlighting\nthat securing GraphRAG pipelines against knowledge poisoning remains largely\nunexplored.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as\na promising paradigm for enhancing large language models (LLMs) by converting\nraw text into structured knowledge graphs, improving both accuracy and\nexplainability. However, GraphRAG relies on LLMs to extract knowledge from raw\ntext during graph construction, and this process can be maliciously manipulated\nto implant misleading information. Targeting this attack surface, we propose\ntwo knowledge poisoning attacks (KPAs) and demonstrate that modifying only a\nfew words in the source text can significantly change the constructed graph,\npoison the GraphRAG, and severely mislead downstream reasoning. The first\nattack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate\nvulnerable nodes in the generated graphs and rewrites the corresponding\nnarratives with LLMs, achieving precise control over specific\nquestion-answering (QA) outcomes with a success rate of 93.1\\%, while keeping\nthe poisoned text fluent and natural. The second attack, named Universal KPA\n(UKPA), exploits linguistic cues such as pronouns and dependency relations to\ndisrupt the structural integrity of the generated graph by altering globally\ninfluential words. With fewer than 0.05\\% of full text modified, the QA\naccuracy collapses from 95\\% to 50\\%. Furthermore, experiments show that\nstate-of-the-art defense methods fail to detect these attacks, highlighting\nthat securing GraphRAG pipelines against knowledge poisoning remains largely\nunexplored."
                },
                "authors": [
                    {
                        "name": "Jiayi Wen"
                    },
                    {
                        "name": "Tianxin Chen"
                    },
                    {
                        "name": "Zhirun Zheng"
                    },
                    {
                        "name": "Cheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Huang"
                },
                "author": "Cheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08746v1",
                "updated": "2025-08-12T08:41:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    41,
                    0,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T08:41:00Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    41,
                    0,
                    1,
                    224,
                    0
                ],
                "title": "Interpretable Reward Model via Sparse Autoencoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Reward Model via Sparse Autoencoder"
                },
                "summary": "Large language models (LLMs) have been widely deployed across numerous\nfields. Reinforcement Learning from Human Feedback (RLHF) leverages reward\nmodels (RMs) as proxies for human preferences to align LLM behaviors with human\nvalues, making the accuracy, reliability, and interpretability of RMs critical\nfor effective alignment. However, traditional RMs lack interpretability, offer\nlimited insight into the reasoning behind reward assignments, and are\ninflexible toward user preference shifts. While recent multidimensional RMs aim\nfor improved interpretability, they often fail to provide feature-level\nattribution and require costly annotations. To overcome these limitations, we\nintroduce the Sparse Autoencoder-enhanced Reward Model (\\textbf{SARM}), a novel\narchitecture that integrates a pretrained Sparse Autoencoder (SAE) into a\nreward model. SARM maps the hidden activations of LLM-based RM into an\ninterpretable, sparse, and monosemantic feature space, from which a scalar head\naggregates feature activations to produce transparent and conceptually\nmeaningful reward scores. Empirical evaluations demonstrate that SARM\nfacilitates direct feature-level attribution of reward assignments, allows\ndynamic adjustment to preference shifts, and achieves superior alignment\nperformance compared to conventional reward models. Our code is available at\nhttps://github.com/schrieffer-z/sarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed across numerous\nfields. Reinforcement Learning from Human Feedback (RLHF) leverages reward\nmodels (RMs) as proxies for human preferences to align LLM behaviors with human\nvalues, making the accuracy, reliability, and interpretability of RMs critical\nfor effective alignment. However, traditional RMs lack interpretability, offer\nlimited insight into the reasoning behind reward assignments, and are\ninflexible toward user preference shifts. While recent multidimensional RMs aim\nfor improved interpretability, they often fail to provide feature-level\nattribution and require costly annotations. To overcome these limitations, we\nintroduce the Sparse Autoencoder-enhanced Reward Model (\\textbf{SARM}), a novel\narchitecture that integrates a pretrained Sparse Autoencoder (SAE) into a\nreward model. SARM maps the hidden activations of LLM-based RM into an\ninterpretable, sparse, and monosemantic feature space, from which a scalar head\naggregates feature activations to produce transparent and conceptually\nmeaningful reward scores. Empirical evaluations demonstrate that SARM\nfacilitates direct feature-level attribution of reward assignments, allows\ndynamic adjustment to preference shifts, and achieves superior alignment\nperformance compared to conventional reward models. Our code is available at\nhttps://github.com/schrieffer-z/sarm."
                },
                "authors": [
                    {
                        "name": "Shuyi Zhang"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Jiayi Liao"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Hengxing Cai"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08745v1",
                "updated": "2025-08-12T08:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    40,
                    52,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T08:40:52Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    40,
                    52,
                    1,
                    224,
                    0
                ],
                "title": "Comprehensive Comparison Network: a framework for locality-aware,\n  routes-comparable and interpretable route recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Comparison Network: a framework for locality-aware,\n  routes-comparable and interpretable route recommendation"
                },
                "summary": "Route recommendation (RR) is a core task of route planning in the Amap app,\nwith the goal of recommending the optimal route among candidate routes to\nusers. Unlike traditional recommendation methods, insights into the local\nquality of routes and comparisons between candidate routes are crucial for\nenhancing recommendation performance but often overlooked in previous studies.\nTo achieve these, we propose a novel model called Comprehensive Comparison\nNetwork (CCN). CCN not only uses query-level features (e.g. user features) and\nitem-level features (e.g. route features, item embedding) that are common in\ntraditional recommendations, but also introduces comparison-level features\nwhich describe the non-overlapping segments between different routes to capture\nthe local quality of routes. The key component Comprehensive Comparison Block\n(CCB) in CCN is designed to enable comparisons between routes. CCB includes a\nComprehensive Comparison Operator (CCO) and a multi-scenario MLP, which can\nupdate the representations of candidate routes based on a comprehensive\ncomparison. By stacking multiple CCBs, CCN can determine the final scores of\ncandidate routes and recommend the optimal one to the user. Additionally, since\nroutes directly affect the costs and risks experienced by users, the RR model\nmust be interpretable for online deployment. Therefore, we designed an\ninterpretable pair scoring network to achieve interpretability. Both offline\nand online experiments demonstrate that CCN significantly improves RR\nperformance and exhibits strong interpretability. CCN has been fully deployed\nin the Amap app for over a year, providing stable and optimal benefits for\nroute recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Route recommendation (RR) is a core task of route planning in the Amap app,\nwith the goal of recommending the optimal route among candidate routes to\nusers. Unlike traditional recommendation methods, insights into the local\nquality of routes and comparisons between candidate routes are crucial for\nenhancing recommendation performance but often overlooked in previous studies.\nTo achieve these, we propose a novel model called Comprehensive Comparison\nNetwork (CCN). CCN not only uses query-level features (e.g. user features) and\nitem-level features (e.g. route features, item embedding) that are common in\ntraditional recommendations, but also introduces comparison-level features\nwhich describe the non-overlapping segments between different routes to capture\nthe local quality of routes. The key component Comprehensive Comparison Block\n(CCB) in CCN is designed to enable comparisons between routes. CCB includes a\nComprehensive Comparison Operator (CCO) and a multi-scenario MLP, which can\nupdate the representations of candidate routes based on a comprehensive\ncomparison. By stacking multiple CCBs, CCN can determine the final scores of\ncandidate routes and recommend the optimal one to the user. Additionally, since\nroutes directly affect the costs and risks experienced by users, the RR model\nmust be interpretable for online deployment. Therefore, we designed an\ninterpretable pair scoring network to achieve interpretability. Both offline\nand online experiments demonstrate that CCN significantly improves RR\nperformance and exhibits strong interpretability. CCN has been fully deployed\nin the Amap app for over a year, providing stable and optimal benefits for\nroute recommendations."
                },
                "authors": [
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Longfei Xu"
                    },
                    {
                        "name": "Hanyu Guo"
                    },
                    {
                        "name": "Chengzhang Wang"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Kaikui Liu"
                    },
                    {
                        "name": "Xiangxiang Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangxiang Chu"
                },
                "author": "Xiangxiang Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08742v1",
                "updated": "2025-08-12T08:36:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    36,
                    23,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T08:36:23Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    36,
                    23,
                    1,
                    224,
                    0
                ],
                "title": "SciRerankBench: Benchmarking Rerankers Towards Scientific\n  Retrieval-Augmented Generated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciRerankBench: Benchmarking Rerankers Towards Scientific\n  Retrieval-Augmented Generated LLMs"
                },
                "summary": "Scientific literature question answering is a pivotal step towards new\nscientific discoveries. Recently, \\textit{two-stage} retrieval-augmented\ngenerated large language models (RAG-LLMs) have shown impressive advancements\nin this domain. Such a two-stage framework, especially the second stage\n(reranker), is particularly essential in the scientific domain, where subtle\ndifferences in terminology may have a greatly negative impact on the final\nfactual-oriented or knowledge-intensive answers. Despite this significant\nprogress, the potential and limitations of these works remain unexplored. In\nthis work, we present a Scientific Rerank-oriented RAG Benchmark\n(SciRerankBench), for evaluating rerankers within RAG-LLMs systems, spanning\nfive scientific subjects. To rigorously assess the reranker performance in\nterms of noise resilience, relevance disambiguation, and factual consistency,\nwe develop three types of question-context-answer (Q-C-A) pairs, i.e., Noisy\nContexts (NC), Semantically Similar but Logically Irrelevant Contexts (SSLI),\nand Counterfactual Contexts (CC). Through systematic evaluation of 13 widely\nused rerankers on five families of LLMs, we provide detailed insights into\ntheir relative strengths and limitations. To the best of our knowledge,\nSciRerankBench is the first benchmark specifically developed to evaluate\nrerankers within RAG-LLMs, which provides valuable observations and guidance\nfor their future development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific literature question answering is a pivotal step towards new\nscientific discoveries. Recently, \\textit{two-stage} retrieval-augmented\ngenerated large language models (RAG-LLMs) have shown impressive advancements\nin this domain. Such a two-stage framework, especially the second stage\n(reranker), is particularly essential in the scientific domain, where subtle\ndifferences in terminology may have a greatly negative impact on the final\nfactual-oriented or knowledge-intensive answers. Despite this significant\nprogress, the potential and limitations of these works remain unexplored. In\nthis work, we present a Scientific Rerank-oriented RAG Benchmark\n(SciRerankBench), for evaluating rerankers within RAG-LLMs systems, spanning\nfive scientific subjects. To rigorously assess the reranker performance in\nterms of noise resilience, relevance disambiguation, and factual consistency,\nwe develop three types of question-context-answer (Q-C-A) pairs, i.e., Noisy\nContexts (NC), Semantically Similar but Logically Irrelevant Contexts (SSLI),\nand Counterfactual Contexts (CC). Through systematic evaluation of 13 widely\nused rerankers on five families of LLMs, we provide detailed insights into\ntheir relative strengths and limitations. To the best of our knowledge,\nSciRerankBench is the first benchmark specifically developed to evaluate\nrerankers within RAG-LLMs, which provides valuable observations and guidance\nfor their future development."
                },
                "authors": [
                    {
                        "name": "Haotian Chen"
                    },
                    {
                        "name": "Qingqing Long"
                    },
                    {
                        "name": "Meng Xiao"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Wei Ju"
                    },
                    {
                        "name": "Chengrui Wang"
                    },
                    {
                        "name": "Xuezhi Wang"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Hengshu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hengshu Zhu"
                },
                "author": "Hengshu Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05496v2",
                "updated": "2025-08-12T08:26:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    26,
                    0,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-07T15:34:06Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    34,
                    6,
                    3,
                    219,
                    0
                ],
                "title": "InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs\n  to Enhance Reasoning Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs\n  to Enhance Reasoning Capabilities"
                },
                "summary": "Large language models (LLMs) have exhibited impressive reasoning abilities on\na wide range of complex tasks. However, enhancing these capabilities through\npost-training remains resource intensive, particularly in terms of data and\ncomputational cost. Although recent efforts have sought to improve sample\nefficiency through selective data curation, existing methods often rely on\nheuristic or task-specific strategies that hinder scalability. In this work, we\nintroduce InfiAlign, a scalable and sample-efficient post-training framework\nthat integrates supervised fine-tuning (SFT) with Direct Preference\nOptimization (DPO) to align LLMs for enhanced reasoning. At the core of\nInfiAlign is a robust data selection pipeline that automatically curates\nhigh-quality alignment data from open-source reasoning datasets using\nmultidimensional quality metrics. This pipeline enables significant performance\ngains while drastically reducing data requirements and remains extensible to\nnew data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model\nachieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only\napproximately 12% of the training data, and demonstrates strong generalization\nacross diverse reasoning tasks. Additional improvements are obtained through\nthe application of DPO, with particularly notable gains in mathematical\nreasoning tasks. The model achieves an average improvement of 3.89% on AIME\n24/25 benchmarks. Our results highlight the effectiveness of combining\nprincipled data selection with full-stage post-training, offering a practical\nsolution for aligning large reasoning models in a scalable and data-efficient\nmanner. The model checkpoints are available at\nhttps://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited impressive reasoning abilities on\na wide range of complex tasks. However, enhancing these capabilities through\npost-training remains resource intensive, particularly in terms of data and\ncomputational cost. Although recent efforts have sought to improve sample\nefficiency through selective data curation, existing methods often rely on\nheuristic or task-specific strategies that hinder scalability. In this work, we\nintroduce InfiAlign, a scalable and sample-efficient post-training framework\nthat integrates supervised fine-tuning (SFT) with Direct Preference\nOptimization (DPO) to align LLMs for enhanced reasoning. At the core of\nInfiAlign is a robust data selection pipeline that automatically curates\nhigh-quality alignment data from open-source reasoning datasets using\nmultidimensional quality metrics. This pipeline enables significant performance\ngains while drastically reducing data requirements and remains extensible to\nnew data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model\nachieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only\napproximately 12% of the training data, and demonstrates strong generalization\nacross diverse reasoning tasks. Additional improvements are obtained through\nthe application of DPO, with particularly notable gains in mathematical\nreasoning tasks. The model achieves an average improvement of 3.89% on AIME\n24/25 benchmarks. Our results highlight the effectiveness of combining\nprincipled data selection with full-stage post-training, offering a practical\nsolution for aligning large reasoning models in a scalable and data-efficient\nmanner. The model checkpoints are available at\nhttps://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT."
                },
                "authors": [
                    {
                        "name": "Shuo Cai"
                    },
                    {
                        "name": "Su Lu"
                    },
                    {
                        "name": "Qi Zhou"
                    },
                    {
                        "name": "Kejing Yang"
                    },
                    {
                        "name": "Zhijie Sang"
                    },
                    {
                        "name": "Congkai Xie"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08731v1",
                "updated": "2025-08-12T08:22:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    22,
                    4,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T08:22:04Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    22,
                    4,
                    1,
                    224,
                    0
                ],
                "title": "Caption: Generating Informative Content Labels for Image Buttons Using\n  Next-Screen Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caption: Generating Informative Content Labels for Image Buttons Using\n  Next-Screen Context"
                },
                "summary": "We present Caption, an LLM-powered content label generation tool for visual\ninteractive elements on mobile devices. Content labels are essential for screen\nreaders to provide announcements for image-based elements, but are often\nmissing or uninformative due to developer neglect. Automated captioning systems\nattempt to address this, but are limited to on-screen context, often resulting\nin inaccurate or unspecific labels. To generate more accurate and descriptive\nlabels, Caption collects next-screen context on interactive elements by\nnavigating to the destination screen that appears after an interaction and\nincorporating information from both the origin and destination screens.\nPreliminary results show Caption generates more accurate labels than both human\nannotators and an LLM baseline. We expect Caption to empower developers by\nproviding actionable accessibility suggestions and directly support on-demand\nrepairs by screen reader users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Caption, an LLM-powered content label generation tool for visual\ninteractive elements on mobile devices. Content labels are essential for screen\nreaders to provide announcements for image-based elements, but are often\nmissing or uninformative due to developer neglect. Automated captioning systems\nattempt to address this, but are limited to on-screen context, often resulting\nin inaccurate or unspecific labels. To generate more accurate and descriptive\nlabels, Caption collects next-screen context on interactive elements by\nnavigating to the destination screen that appears after an interaction and\nincorporating information from both the origin and destination screens.\nPreliminary results show Caption generates more accurate labels than both human\nannotators and an LLM baseline. We expect Caption to empower developers by\nproviding actionable accessibility suggestions and directly support on-demand\nrepairs by screen reader users."
                },
                "authors": [
                    {
                        "name": "Mingyuan Zhong"
                    },
                    {
                        "name": "Ajit Mallavarapu"
                    },
                    {
                        "name": "Qing Nie"
                    }
                ],
                "author_detail": {
                    "name": "Qing Nie"
                },
                "author": "Qing Nie",
                "arxiv_doi": "10.1145/3746058.3758413",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746058.3758413",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.08731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08730v1",
                "updated": "2025-08-12T08:21:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    21,
                    58,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T08:21:58Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    21,
                    58,
                    1,
                    224,
                    0
                ],
                "title": "Magical: Medical Lay Language Generation via Semantic Invariance and\n  Layperson-tailored Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magical: Medical Lay Language Generation via Semantic Invariance and\n  Layperson-tailored Adaptation"
                },
                "summary": "Medical Lay Language Generation (MLLG) plays a vital role in improving the\naccessibility of complex scientific content for broader audiences. Recent\nliterature to MLLG commonly employ parameter-efficient fine-tuning methods such\nas Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using\npaired expert-lay language datasets. However, LoRA struggles with the\nchallenges posed by multi-source heterogeneous MLLG datasets. Specifically,\nthrough a series of exploratory experiments, we reveal that standard LoRA fail\nto meet the requirement for semantic fidelity and diverse lay-style generation\nin MLLG task. To address these limitations, we propose Magical, an asymmetric\nLoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical\nemploys a shared matrix $A$ for abstractive summarization, along with multiple\nisolated matrices $B$ for diverse lay-style generation. To preserve semantic\nfidelity during the lay language generation process, Magical introduces a\nSemantic Invariance Constraint to mitigate semantic subspace shifts on matrix\n$A$. Furthermore, to better adapt to diverse lay-style generation, Magical\nincorporates the Recommendation-guided Switch, an externally interface to\nprompt the LLM to switch between different matrices $B$. Experimental results\non three real-world lay language generation datasets demonstrate that Magical\nconsistently outperforms prompt-based methods, vanilla LoRA, and its recent\nvariants, while also reducing trainable parameters by 31.66%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Lay Language Generation (MLLG) plays a vital role in improving the\naccessibility of complex scientific content for broader audiences. Recent\nliterature to MLLG commonly employ parameter-efficient fine-tuning methods such\nas Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using\npaired expert-lay language datasets. However, LoRA struggles with the\nchallenges posed by multi-source heterogeneous MLLG datasets. Specifically,\nthrough a series of exploratory experiments, we reveal that standard LoRA fail\nto meet the requirement for semantic fidelity and diverse lay-style generation\nin MLLG task. To address these limitations, we propose Magical, an asymmetric\nLoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical\nemploys a shared matrix $A$ for abstractive summarization, along with multiple\nisolated matrices $B$ for diverse lay-style generation. To preserve semantic\nfidelity during the lay language generation process, Magical introduces a\nSemantic Invariance Constraint to mitigate semantic subspace shifts on matrix\n$A$. Furthermore, to better adapt to diverse lay-style generation, Magical\nincorporates the Recommendation-guided Switch, an externally interface to\nprompt the LLM to switch between different matrices $B$. Experimental results\non three real-world lay language generation datasets demonstrate that Magical\nconsistently outperforms prompt-based methods, vanilla LoRA, and its recent\nvariants, while also reducing trainable parameters by 31.66%."
                },
                "authors": [
                    {
                        "name": "Weibin Liao"
                    },
                    {
                        "name": "Tianlong Wang"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Yasha Wang"
                    },
                    {
                        "name": "Junyi Gao"
                    },
                    {
                        "name": "Liantao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Liantao Ma"
                },
                "author": "Liantao Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08726v1",
                "updated": "2025-08-12T08:14:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    14,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T08:14:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    14,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "Simulating Generative Social Agents via Theory-Informed Workflow Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Generative Social Agents via Theory-Informed Workflow Design"
                },
                "summary": "Recent advances in large language models have demonstrated strong reasoning\nand role-playing capabilities, opening new opportunities for agent-based social\nsimulations. However, most existing agents' implementations are\nscenario-tailored, without a unified framework to guide the design. This lack\nof a general social agent limits their ability to generalize across different\nsocial contexts and to produce consistent, realistic behaviors. To address this\nchallenge, we propose a theory-informed framework that provides a systematic\ndesign process for LLM-based social agents. Our framework is grounded in\nprinciples from Social Cognition Theory and introduces three key modules:\nmotivation, action planning, and learning. These modules jointly enable agents\nto reason about their goals, plan coherent actions, and adapt their behavior\nover time, leading to more flexible and contextually appropriate responses.\nComprehensive experiments demonstrate that our theory-driven agents reproduce\nrealistic human behavior patterns under complex conditions, achieving up to 75%\nlower deviation from real-world behavioral data across multiple fidelity\nmetrics compared to classical generative baselines. Ablation studies further\nshow that removing motivation, planning, or learning modules increases errors\nby 1.5 to 3.2 times, confirming their distinct and essential contributions to\ngenerating realistic and coherent social behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated strong reasoning\nand role-playing capabilities, opening new opportunities for agent-based social\nsimulations. However, most existing agents' implementations are\nscenario-tailored, without a unified framework to guide the design. This lack\nof a general social agent limits their ability to generalize across different\nsocial contexts and to produce consistent, realistic behaviors. To address this\nchallenge, we propose a theory-informed framework that provides a systematic\ndesign process for LLM-based social agents. Our framework is grounded in\nprinciples from Social Cognition Theory and introduces three key modules:\nmotivation, action planning, and learning. These modules jointly enable agents\nto reason about their goals, plan coherent actions, and adapt their behavior\nover time, leading to more flexible and contextually appropriate responses.\nComprehensive experiments demonstrate that our theory-driven agents reproduce\nrealistic human behavior patterns under complex conditions, achieving up to 75%\nlower deviation from real-world behavioral data across multiple fidelity\nmetrics compared to classical generative baselines. Ablation studies further\nshow that removing motivation, planning, or learning modules increases errors\nby 1.5 to 3.2 times, confirming their distinct and essential contributions to\ngenerating realistic and coherent social behaviors."
                },
                "authors": [
                    {
                        "name": "Yuwei Yan"
                    },
                    {
                        "name": "Jinghua Piao"
                    },
                    {
                        "name": "Xiaochong Lan"
                    },
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Pan Hui"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08722v1",
                "updated": "2025-08-12T08:06:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    6,
                    29,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T08:06:29Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    6,
                    29,
                    1,
                    224,
                    0
                ],
                "title": "Continuous-variable quantum key distribution over 50.4 km fiber using\n  integrated silicon photonic transmitter and receiver",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous-variable quantum key distribution over 50.4 km fiber using\n  integrated silicon photonic transmitter and receiver"
                },
                "summary": "Quantum key distribution (QKD) is the fastest-growing and relatively mature\ntechnology in the field of quantum information, enabling\ninformation-theoretically secure key distribution between two remote users.\nAlthough QKD based on off-the-shelf telecom components has been validated in\nboth laboratory and field tests, its high cost and large volume remain major\nobstacles to large-scale deployment. Photonic integration, featured by its\ncompact size and low cost, offers an effective approach to addressing the above\nchallenges faced by QKD. Here, we implement a high-performance, integrated\nlocal local oscillator continuous-variable (CV) QKD system based on an\nintegrated silicon photonic transmitter and receiver. By employing a high-speed\nsilicon photonic integrated in-phase and quadrature modulator, a low-noise and\nhigh bandwidth silicon photonic integrated heterodyne detector, and digital\nsignal processing, our CV-QKD system achieves a symbol rate of up to 1.5625\nGBaud. Furthermore, the system achieves asymptotic secret key rates of 31.05\nand 5.05 Mbps over 25.8 and 50.4 km standard single-mode fiber, respectively,\nusing an 8-phase-shift keying discrete modulation. Our integrated CV-QKD system\nwith high symbol rate and long transmission distance pays the way for the\nquantum secure communication network at metropolitan area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum key distribution (QKD) is the fastest-growing and relatively mature\ntechnology in the field of quantum information, enabling\ninformation-theoretically secure key distribution between two remote users.\nAlthough QKD based on off-the-shelf telecom components has been validated in\nboth laboratory and field tests, its high cost and large volume remain major\nobstacles to large-scale deployment. Photonic integration, featured by its\ncompact size and low cost, offers an effective approach to addressing the above\nchallenges faced by QKD. Here, we implement a high-performance, integrated\nlocal local oscillator continuous-variable (CV) QKD system based on an\nintegrated silicon photonic transmitter and receiver. By employing a high-speed\nsilicon photonic integrated in-phase and quadrature modulator, a low-noise and\nhigh bandwidth silicon photonic integrated heterodyne detector, and digital\nsignal processing, our CV-QKD system achieves a symbol rate of up to 1.5625\nGBaud. Furthermore, the system achieves asymptotic secret key rates of 31.05\nand 5.05 Mbps over 25.8 and 50.4 km standard single-mode fiber, respectively,\nusing an 8-phase-shift keying discrete modulation. Our integrated CV-QKD system\nwith high symbol rate and long transmission distance pays the way for the\nquantum secure communication network at metropolitan area."
                },
                "authors": [
                    {
                        "name": "Shuaishuai Liu"
                    },
                    {
                        "name": "Yanxiang Jia"
                    },
                    {
                        "name": "Yuqi Shi"
                    },
                    {
                        "name": "Yizhuo Hou"
                    },
                    {
                        "name": "Pu Wang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shiwei Yang"
                    },
                    {
                        "name": "Zhenguo Lu"
                    },
                    {
                        "name": "Xuyang Wang"
                    },
                    {
                        "name": "Yongmin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongmin Li"
                },
                "author": "Yongmin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]