[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v1",
                "updated": "2025-01-07T17:00:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v2",
                "updated": "2025-01-06T23:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03322v1",
                "updated": "2025-01-06T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "title": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method"
                },
                "summary": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available."
                },
                "authors": [
                    {
                        "name": "Suwei Wang"
                    },
                    {
                        "name": "Lile Wang"
                    },
                    {
                        "name": "Subo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Subo Dong"
                },
                "author": "Subo Dong",
                "arxiv_comment": "Accepted by ApJS, GitHub link:\n  https://github.com/AsterLight0626/Twinkle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v2",
                "updated": "2025-01-06T15:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    59,
                    23,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02803v1",
                "updated": "2025-01-06T06:44:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:44:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism"
                },
                "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Yimin Tang"
                    },
                    {
                        "name": "Zhenghong Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "T. K. Satish Kumar"
                    },
                    {
                        "name": "Jiaoyang Li"
                    },
                    {
                        "name": "Sven Koenig"
                    }
                ],
                "author_detail": {
                    "name": "Sven Koenig"
                },
                "author": "Sven Koenig",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v2",
                "updated": "2025-01-06T01:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    26,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02524v1",
                "updated": "2025-01-05T12:51:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T12:51:08Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "title": "A Full-System Simulation Framework for CXL-Based SSD Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-System Simulation Framework for CXL-Based SSD Memory System"
                },
                "summary": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim."
                },
                "authors": [
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Fanfeng Meng"
                    },
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Xuran Ge"
                    },
                    {
                        "name": "Jijun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jijun Cao"
                },
                "author": "Jijun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v1",
                "updated": "2025-01-05T07:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "11 pages, 15 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v1",
                "updated": "2025-01-04T20:59:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v1",
                "updated": "2025-01-03T13:32:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v1",
                "updated": "2024-12-29T17:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha Rösler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v1",
                "updated": "2024-12-28T14:38:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v1",
                "updated": "2024-12-26T15:45:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    }
                ],
                "author_detail": {
                    "name": "Heung-Yeung Shum"
                },
                "author": "Heung-Yeung Shum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermüller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v1",
                "updated": "2024-12-15T21:02:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15246v1",
                "updated": "2024-12-14T06:47:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T06:47:56Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "title": "Accelerating Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Retrieval-Augmented Generation"
                },
                "summary": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded."
                },
                "authors": [
                    {
                        "name": "Derrick Quinn"
                    },
                    {
                        "name": "Mohammad Nouri"
                    },
                    {
                        "name": "Neel Patel"
                    },
                    {
                        "name": "John Salihu"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Sukhan Lee"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Mohammad Alian"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alian"
                },
                "author": "Mohammad Alian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Köstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thießen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v1",
                "updated": "2024-12-13T02:26:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_comment": "Conference submission for IPCCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.04001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04001v1",
                "updated": "2025-01-07T18:58:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    58,
                    54,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:58:54Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    58,
                    54,
                    1,
                    7,
                    0
                ],
                "title": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos"
                },
                "summary": "This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications."
                },
                "authors": [
                    {
                        "name": "Haobo Yuan"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Zilong Huang"
                    },
                    {
                        "name": "Shilin Xu"
                    },
                    {
                        "name": "Shunping Ji"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Lu Qi"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Hsuan Yang"
                },
                "author": "Ming-Hsuan Yang",
                "arxiv_comment": "Project page: https://lxtgh.github.io/project/sa2va",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03995v1",
                "updated": "2025-01-07T18:52:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    52,
                    5,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:52:05Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    52,
                    5,
                    1,
                    7,
                    0
                ],
                "title": "RAG-Check: Evaluating Multimodal Retrieval Augmented Generation\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Check: Evaluating Multimodal Retrieval Augmented Generation\n  Performance"
                },
                "summary": "Retrieval-augmented generation (RAG) improves large language models (LLMs) by\nusing external knowledge to guide response generation, reducing hallucinations.\nHowever, RAG, particularly multi-modal RAG, can introduce new hallucination\nsources: (i) the retrieval process may select irrelevant pieces (e.g.,\ndocuments, images) as raw context from the database, and (ii) retrieved images\nare processed into text-based context via vision-language models (VLMs) or\ndirectly used by multi-modal language models (MLLMs) like GPT-4o, which may\nhallucinate. To address this, we propose a novel framework to evaluate the\nreliability of multi-modal RAG using two performance measures: (i) the\nrelevancy score (RS), assessing the relevance of retrieved entries to the\nquery, and (ii) the correctness score (CS), evaluating the accuracy of the\ngenerated response. We train RS and CS models using a ChatGPT-derived database\nand human evaluator samples. Results show that both models achieve ~88%\naccuracy on test data. Additionally, we construct a 5000-sample human-annotated\ndatabase evaluating the relevancy of retrieved pieces and the correctness of\nresponse statements. Our RS model aligns with human preferences 20% more often\nthan CLIP in retrieval, and our CS model matches human preferences ~91% of the\ntime. Finally, we assess various RAG systems' selection and generation\nperformances using RS and CS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves large language models (LLMs) by\nusing external knowledge to guide response generation, reducing hallucinations.\nHowever, RAG, particularly multi-modal RAG, can introduce new hallucination\nsources: (i) the retrieval process may select irrelevant pieces (e.g.,\ndocuments, images) as raw context from the database, and (ii) retrieved images\nare processed into text-based context via vision-language models (VLMs) or\ndirectly used by multi-modal language models (MLLMs) like GPT-4o, which may\nhallucinate. To address this, we propose a novel framework to evaluate the\nreliability of multi-modal RAG using two performance measures: (i) the\nrelevancy score (RS), assessing the relevance of retrieved entries to the\nquery, and (ii) the correctness score (CS), evaluating the accuracy of the\ngenerated response. We train RS and CS models using a ChatGPT-derived database\nand human evaluator samples. Results show that both models achieve ~88%\naccuracy on test data. Additionally, we construct a 5000-sample human-annotated\ndatabase evaluating the relevancy of retrieved pieces and the correctness of\nresponse statements. Our RS model aligns with human preferences 20% more often\nthan CLIP in retrieval, and our CS model matches human preferences ~91% of the\ntime. Finally, we assess various RAG systems' selection and generation\nperformances using RS and CS."
                },
                "authors": [
                    {
                        "name": "Matin Mortaheb"
                    },
                    {
                        "name": "Mohammad A. Amir Khojastepour"
                    },
                    {
                        "name": "Srimat T. Chakradhar"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03992v1",
                "updated": "2025-01-07T18:50:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    50,
                    6,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:50:06Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    50,
                    6,
                    1,
                    7,
                    0
                ],
                "title": "NeuralSVG: An Implicit Representation for Text-to-Vector Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuralSVG: An Implicit Representation for Text-to-Vector Generation"
                },
                "summary": "Vector graphics are essential in design, providing artists with a versatile\nmedium for creating resolution-independent and highly editable visual content.\nRecent advancements in vision-language and diffusion models have fueled\ninterest in text-to-vector graphics generation. However, existing approaches\noften suffer from over-parameterized outputs or treat the layered structure - a\ncore feature of vector graphics - as a secondary goal, diminishing their\npractical use. Recognizing the importance of layered SVG representations, we\npropose NeuralSVG, an implicit neural representation for generating vector\ngraphics from text prompts. Inspired by Neural Radiance Fields (NeRFs),\nNeuralSVG encodes the entire scene into the weights of a small MLP network,\noptimized using Score Distillation Sampling (SDS). To encourage a layered\nstructure in the generated SVG, we introduce a dropout-based regularization\ntechnique that strengthens the standalone meaning of each shape. We\nadditionally demonstrate that utilizing a neural representation provides an\nadded benefit of inference-time control, enabling users to dynamically adapt\nthe generated SVG based on user-provided inputs, all with a single learned\nrepresentation. Through extensive qualitative and quantitative evaluations, we\ndemonstrate that NeuralSVG outperforms existing methods in generating\nstructured and flexible SVG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector graphics are essential in design, providing artists with a versatile\nmedium for creating resolution-independent and highly editable visual content.\nRecent advancements in vision-language and diffusion models have fueled\ninterest in text-to-vector graphics generation. However, existing approaches\noften suffer from over-parameterized outputs or treat the layered structure - a\ncore feature of vector graphics - as a secondary goal, diminishing their\npractical use. Recognizing the importance of layered SVG representations, we\npropose NeuralSVG, an implicit neural representation for generating vector\ngraphics from text prompts. Inspired by Neural Radiance Fields (NeRFs),\nNeuralSVG encodes the entire scene into the weights of a small MLP network,\noptimized using Score Distillation Sampling (SDS). To encourage a layered\nstructure in the generated SVG, we introduce a dropout-based regularization\ntechnique that strengthens the standalone meaning of each shape. We\nadditionally demonstrate that utilizing a neural representation provides an\nadded benefit of inference-time control, enabling users to dynamically adapt\nthe generated SVG based on user-provided inputs, all with a single learned\nrepresentation. Through extensive qualitative and quantitative evaluations, we\ndemonstrate that NeuralSVG outperforms existing methods in generating\nstructured and flexible SVG."
                },
                "authors": [
                    {
                        "name": "Sagi Polaczek"
                    },
                    {
                        "name": "Yuval Alaluf"
                    },
                    {
                        "name": "Elad Richardson"
                    },
                    {
                        "name": "Yael Vinker"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Cohen-Or"
                },
                "author": "Daniel Cohen-Or",
                "arxiv_comment": "Project Page: https://sagipolaczek.github.io/NeuralSVG/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03991v1",
                "updated": "2025-01-07T18:48:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    48,
                    42,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:48:42Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    48,
                    42,
                    1,
                    7,
                    0
                ],
                "title": "Influences on LLM Calibration: A Study of Response Agreement, Loss\n  Functions, and Prompt Styles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influences on LLM Calibration: A Study of Response Agreement, Loss\n  Functions, and Prompt Styles"
                },
                "summary": "Calibration, the alignment between model confidence and prediction accuracy,\nis critical for the reliable deployment of large language models (LLMs).\nExisting works neglect to measure the generalization of their methods to other\nprompt styles and different sizes of LLMs. To address this, we define a\ncontrolled experimental setting covering 12 LLMs and four prompt styles. We\nadditionally investigate if incorporating the response agreement of multiple\nLLMs and an appropriate loss function can improve calibration performance.\nConcretely, we build Calib-n, a novel framework that trains an auxiliary model\nfor confidence estimation that aggregates responses from multiple LLMs to\ncapture inter-model agreement. To optimize calibration, we integrate focal and\nAUC surrogate losses alongside binary cross-entropy. Experiments across four\ndatasets demonstrate that both response agreement and focal loss improve\ncalibration from baselines. We find that few-shot prompts are the most\neffective for auxiliary model-based methods, and auxiliary models demonstrate\nrobust calibration performance across accuracy variations, outperforming LLMs'\ninternal probabilities and verbalized confidences. These insights deepen the\nunderstanding of influence factors in LLM calibration, supporting their\nreliable deployment in diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibration, the alignment between model confidence and prediction accuracy,\nis critical for the reliable deployment of large language models (LLMs).\nExisting works neglect to measure the generalization of their methods to other\nprompt styles and different sizes of LLMs. To address this, we define a\ncontrolled experimental setting covering 12 LLMs and four prompt styles. We\nadditionally investigate if incorporating the response agreement of multiple\nLLMs and an appropriate loss function can improve calibration performance.\nConcretely, we build Calib-n, a novel framework that trains an auxiliary model\nfor confidence estimation that aggregates responses from multiple LLMs to\ncapture inter-model agreement. To optimize calibration, we integrate focal and\nAUC surrogate losses alongside binary cross-entropy. Experiments across four\ndatasets demonstrate that both response agreement and focal loss improve\ncalibration from baselines. We find that few-shot prompts are the most\neffective for auxiliary model-based methods, and auxiliary models demonstrate\nrobust calibration performance across accuracy variations, outperforming LLMs'\ninternal probabilities and verbalized confidences. These insights deepen the\nunderstanding of influence factors in LLM calibration, supporting their\nreliable deployment in diverse applications."
                },
                "authors": [
                    {
                        "name": "Yuxi Xia"
                    },
                    {
                        "name": "Pedro Henrique Luz de Araujo"
                    },
                    {
                        "name": "Klim Zaporojets"
                    },
                    {
                        "name": "Benjamin Roth"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Roth"
                },
                "author": "Benjamin Roth",
                "arxiv_comment": "24 pages, 11 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03989v1",
                "updated": "2025-01-07T18:46:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    46,
                    34,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:46:34Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    46,
                    34,
                    1,
                    7,
                    0
                ],
                "title": "(De)-Indexing and the Right to be Forgotten",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "(De)-Indexing and the Right to be Forgotten"
                },
                "summary": "In the digital age, the challenge of forgetfulness has emerged as a\nsignificant concern, particularly regarding the management of personal data and\nits accessibility online. The right to be forgotten (RTBF) allows individuals\nto request the removal of outdated or harmful information from public access,\nyet implementing this right poses substantial technical difficulties for search\nengines. This paper aims to introduce non-experts to the foundational concepts\nof information retrieval (IR) and de-indexing, which are critical for\nunderstanding how search engines can effectively \"forget\" certain content. We\nwill explore various IR models, including boolean, probabilistic, vector space,\nand embedding-based approaches, as well as the role of Large Language Models\n(LLMs) in enhancing data processing capabilities. By providing this overview,\nwe seek to highlight the complexities involved in balancing individual privacy\nrights with the operational challenges faced by search engines in managing\ninformation visibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the digital age, the challenge of forgetfulness has emerged as a\nsignificant concern, particularly regarding the management of personal data and\nits accessibility online. The right to be forgotten (RTBF) allows individuals\nto request the removal of outdated or harmful information from public access,\nyet implementing this right poses substantial technical difficulties for search\nengines. This paper aims to introduce non-experts to the foundational concepts\nof information retrieval (IR) and de-indexing, which are critical for\nunderstanding how search engines can effectively \"forget\" certain content. We\nwill explore various IR models, including boolean, probabilistic, vector space,\nand embedding-based approaches, as well as the role of Large Language Models\n(LLMs) in enhancing data processing capabilities. By providing this overview,\nwe seek to highlight the complexities involved in balancing individual privacy\nrights with the operational challenges faced by search engines in managing\ninformation visibility."
                },
                "authors": [
                    {
                        "name": "Salvatore Vilella"
                    },
                    {
                        "name": "Giancarlo Ruffo"
                    }
                ],
                "author_detail": {
                    "name": "Giancarlo Ruffo"
                },
                "author": "Giancarlo Ruffo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4; H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03982v1",
                "updated": "2025-01-07T18:39:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    39,
                    28,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:39:28Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    39,
                    28,
                    1,
                    7,
                    0
                ],
                "title": "Sequentializing a Test: Anytime Validity is Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequentializing a Test: Anytime Validity is Free"
                },
                "summary": "An anytime valid sequential test permits us to peek at observations as they\narrive. This means we can stop, continue or adapt the testing process based on\nthe current data, without invalidating the inference. Given a maximum number of\nobservations $N$, one may believe that this benefit must be paid for in terms\nof power when compared to a conventional test that waits until all $N$\nobservations have arrived. Our key contribution is to show that this is false:\nfor any valid test based on $N$ observations, we derive an anytime valid\nsequential test that matches it after $N$ observations. In addition, we show\nthat the value of the sequential test before a rejection is attained can be\ndirectly used as a significance level for a subsequent test. We illustrate this\nfor the $z$-test. There, we find that the current state-of-the-art based on\nlog-optimal $e$-values can be obtained as a special limiting case that\nreplicates a $z$-test with level $\\alpha \\to 0$ as $N \\to \\infty$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An anytime valid sequential test permits us to peek at observations as they\narrive. This means we can stop, continue or adapt the testing process based on\nthe current data, without invalidating the inference. Given a maximum number of\nobservations $N$, one may believe that this benefit must be paid for in terms\nof power when compared to a conventional test that waits until all $N$\nobservations have arrived. Our key contribution is to show that this is false:\nfor any valid test based on $N$ observations, we derive an anytime valid\nsequential test that matches it after $N$ observations. In addition, we show\nthat the value of the sequential test before a rejection is attained can be\ndirectly used as a significance level for a subsequent test. We illustrate this\nfor the $z$-test. There, we find that the current state-of-the-art based on\nlog-optimal $e$-values can be obtained as a special limiting case that\nreplicates a $z$-test with level $\\alpha \\to 0$ as $N \\to \\infty$."
                },
                "authors": [
                    {
                        "name": "Nick W. Koning"
                    },
                    {
                        "name": "Sam van Meer"
                    }
                ],
                "author_detail": {
                    "name": "Sam van Meer"
                },
                "author": "Sam van Meer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13510v2",
                "updated": "2025-01-07T18:16:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    16,
                    17,
                    1,
                    7,
                    0
                ],
                "published": "2024-08-24T08:12:22Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    8,
                    12,
                    22,
                    5,
                    237,
                    0
                ],
                "title": "Intelligent Router for LLM Workloads: Improving Performance Through\n  Workload-Aware Load Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Router for LLM Workloads: Improving Performance Through\n  Workload-Aware Load Balancing"
                },
                "summary": "Large Language Model (LLM) workloads have distinct prefill and decode phases\nwith different compute and memory requirements which should ideally be\naccounted for when scheduling input queries across different LLM instances in a\ncluster. However existing scheduling algorithms treat LLM workloads as\nmonolithic jobs without considering the distinct characteristics of the two\nphases in each workload. This leads to sub-optimal scheduling and increased\nresponse latency. In this work, we start by characterizing factors affecting\nthe response latency during LLM inference serving. We establish that better\nload balancing of inference requests across the available LLM instances can\nimprove the end-to-end latency to a larger extent than merely focusing on\noptimizing the instance-level scheduler. Motivated by our findings, we propose\na heuristic-guided reinforcement learning-based intelligent router for\ndata-driven and workload-aware scheduling. Our router schedules queries across\nLLM instances by leveraging a trainable response-length predictor, and a novel\nformulation for estimating the impact of mixing different workloads and\nachieves over 11% lower end-to-end latency than existing approaches on a mix of\npublic datasets and 7.8% lower end-to-end latency on real workload data with\ndiverse input and output trends from Cloud Provider X. Additionally, the\nproposed framework can also serve as a standard for benchmarking different LLM\ninference schedulers since it provides the best latency for a given model,\nhardware, and instance-level scheduler combination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) workloads have distinct prefill and decode phases\nwith different compute and memory requirements which should ideally be\naccounted for when scheduling input queries across different LLM instances in a\ncluster. However existing scheduling algorithms treat LLM workloads as\nmonolithic jobs without considering the distinct characteristics of the two\nphases in each workload. This leads to sub-optimal scheduling and increased\nresponse latency. In this work, we start by characterizing factors affecting\nthe response latency during LLM inference serving. We establish that better\nload balancing of inference requests across the available LLM instances can\nimprove the end-to-end latency to a larger extent than merely focusing on\noptimizing the instance-level scheduler. Motivated by our findings, we propose\na heuristic-guided reinforcement learning-based intelligent router for\ndata-driven and workload-aware scheduling. Our router schedules queries across\nLLM instances by leveraging a trainable response-length predictor, and a novel\nformulation for estimating the impact of mixing different workloads and\nachieves over 11% lower end-to-end latency than existing approaches on a mix of\npublic datasets and 7.8% lower end-to-end latency on real workload data with\ndiverse input and output trends from Cloud Provider X. Additionally, the\nproposed framework can also serve as a standard for benchmarking different LLM\ninference schedulers since it provides the best latency for a given model,\nhardware, and instance-level scheduler combination."
                },
                "authors": [
                    {
                        "name": "Kunal Jain"
                    },
                    {
                        "name": "Anjaly Parayil"
                    },
                    {
                        "name": "Ankur Mallick"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Xiaoting Qin"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Rujia Wang"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Anoop Kulkarni"
                    },
                    {
                        "name": "Steve Kofsky"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03968v1",
                "updated": "2025-01-07T18:06:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    6,
                    27,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:06:27Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    6,
                    27,
                    1,
                    7,
                    0
                ],
                "title": "VLM-driven Behavior Tree for Context-aware Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM-driven Behavior Tree for Context-aware Task Planning"
                },
                "summary": "The use of Large Language Models (LLMs) for generating Behavior Trees (BTs)\nhas recently gained attention in the robotics community, yet remains in its\nearly stages of development. In this paper, we propose a novel framework that\nleverages Vision-Language Models (VLMs) to interactively generate and edit BTs\nthat address visual conditions, enabling context-aware robot operations in\nvisually complex environments. A key feature of our approach lies in the\nconditional control through self-prompted visual conditions. Specifically, the\nVLM generates BTs with visual condition nodes, where conditions are expressed\nas free-form text. Another VLM process integrates the text into its prompt and\nevaluates the conditions against real-world images during robot execution. We\nvalidated our framework in a real-world cafe scenario, demonstrating both its\nfeasibility and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) for generating Behavior Trees (BTs)\nhas recently gained attention in the robotics community, yet remains in its\nearly stages of development. In this paper, we propose a novel framework that\nleverages Vision-Language Models (VLMs) to interactively generate and edit BTs\nthat address visual conditions, enabling context-aware robot operations in\nvisually complex environments. A key feature of our approach lies in the\nconditional control through self-prompted visual conditions. Specifically, the\nVLM generates BTs with visual condition nodes, where conditions are expressed\nas free-form text. Another VLM process integrates the text into its prompt and\nevaluates the conditions against real-world images during robot execution. We\nvalidated our framework in a real-world cafe scenario, demonstrating both its\nfeasibility and limitations."
                },
                "authors": [
                    {
                        "name": "Naoki Wake"
                    },
                    {
                        "name": "Atsushi Kanehira"
                    },
                    {
                        "name": "Jun Takamatsu"
                    },
                    {
                        "name": "Kazuhiro Sasabuchi"
                    },
                    {
                        "name": "Katsushi Ikeuchi"
                    }
                ],
                "author_detail": {
                    "name": "Katsushi Ikeuchi"
                },
                "author": "Katsushi Ikeuchi",
                "arxiv_comment": "10 pages, 11 figures, 5 tables. Last updated on January 7th, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03957v1",
                "updated": "2025-01-07T17:37:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    37,
                    57,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:37:57Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    37,
                    57,
                    1,
                    7,
                    0
                ],
                "title": "Vision Language Models as Values Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models as Values Detectors"
                },
                "summary": "Large Language Models integrating textual and visual inputs have introduced\nnew possibilities for interpreting complex data. Despite their remarkable\nability to generate coherent and contextually relevant text based on visual\nstimuli, the alignment of these models with human perception in identifying\nrelevant elements in images requires further exploration. This paper\ninvestigates the alignment between state-of-the-art LLMs and human annotators\nin detecting elements of relevance within home environment scenarios. We\ncreated a set of twelve images depicting various domestic scenarios and\nenlisted fourteen annotators to identify the key element in each image. We then\ncompared these human responses with outputs from five different LLMs, including\nGPT-4o and four LLaVA variants. Our findings reveal a varied degree of\nalignment, with LLaVA 34B showing the highest performance but still scoring\nlow. However, an analysis of the results highlights the models' potential to\ndetect value-laden elements in images, suggesting that with improved training\nand refined prompts, LLMs could enhance applications in social robotics,\nassistive technologies, and human-computer interaction by providing deeper\ninsights and more contextually relevant responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models integrating textual and visual inputs have introduced\nnew possibilities for interpreting complex data. Despite their remarkable\nability to generate coherent and contextually relevant text based on visual\nstimuli, the alignment of these models with human perception in identifying\nrelevant elements in images requires further exploration. This paper\ninvestigates the alignment between state-of-the-art LLMs and human annotators\nin detecting elements of relevance within home environment scenarios. We\ncreated a set of twelve images depicting various domestic scenarios and\nenlisted fourteen annotators to identify the key element in each image. We then\ncompared these human responses with outputs from five different LLMs, including\nGPT-4o and four LLaVA variants. Our findings reveal a varied degree of\nalignment, with LLaVA 34B showing the highest performance but still scoring\nlow. However, an analysis of the results highlights the models' potential to\ndetect value-laden elements in images, suggesting that with improved training\nand refined prompts, LLMs could enhance applications in social robotics,\nassistive technologies, and human-computer interaction by providing deeper\ninsights and more contextually relevant responses."
                },
                "authors": [
                    {
                        "name": "Giulio Antonio Abbo"
                    },
                    {
                        "name": "Tony Belpaeme"
                    }
                ],
                "author_detail": {
                    "name": "Tony Belpaeme"
                },
                "author": "Tony Belpaeme",
                "arxiv_comment": "13 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00568v2",
                "updated": "2025-01-07T17:36:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    36,
                    14,
                    1,
                    7,
                    0
                ],
                "published": "2024-11-01T13:26:13Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    13,
                    26,
                    13,
                    4,
                    306,
                    0
                ],
                "title": "Constrained Sampling with Primal-Dual Langevin Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained Sampling with Primal-Dual Langevin Monte Carlo"
                },
                "summary": "This work considers the problem of sampling from a probability distribution\nknown up to a normalization constant while satisfying a set of statistical\nconstraints specified by the expected values of general nonlinear functions.\nThis problem finds applications in, e.g., Bayesian inference, where it can\nconstrain moments to evaluate counterfactual scenarios or enforce desiderata\nsuch as prediction fairness. Methods developed to handle support constraints,\nsuch as those based on mirror maps, barriers, and penalties, are not suited for\nthis task. This work therefore relies on gradient descent-ascent dynamics in\nWasserstein space to put forward a discrete-time primal-dual Langevin Monte\nCarlo algorithm (PD-LMC) that simultaneously constrains the target distribution\nand samples from it. We analyze the convergence of PD-LMC under standard\nassumptions on the target distribution and constraints, namely (strong)\nconvexity and log-Sobolev inequalities. To do so, we bring classical\noptimization arguments for saddle-point algorithms to the geometry of\nWasserstein space. We illustrate the relevance and effectiveness of PD-LMC in\nseveral applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work considers the problem of sampling from a probability distribution\nknown up to a normalization constant while satisfying a set of statistical\nconstraints specified by the expected values of general nonlinear functions.\nThis problem finds applications in, e.g., Bayesian inference, where it can\nconstrain moments to evaluate counterfactual scenarios or enforce desiderata\nsuch as prediction fairness. Methods developed to handle support constraints,\nsuch as those based on mirror maps, barriers, and penalties, are not suited for\nthis task. This work therefore relies on gradient descent-ascent dynamics in\nWasserstein space to put forward a discrete-time primal-dual Langevin Monte\nCarlo algorithm (PD-LMC) that simultaneously constrains the target distribution\nand samples from it. We analyze the convergence of PD-LMC under standard\nassumptions on the target distribution and constraints, namely (strong)\nconvexity and log-Sobolev inequalities. To do so, we bring classical\noptimization arguments for saddle-point algorithms to the geometry of\nWasserstein space. We illustrate the relevance and effectiveness of PD-LMC in\nseveral applications."
                },
                "authors": [
                    {
                        "name": "Luiz F. O. Chamon"
                    },
                    {
                        "name": "Mohammad Reza Karimi"
                    },
                    {
                        "name": "Anna Korba"
                    }
                ],
                "author_detail": {
                    "name": "Anna Korba"
                },
                "author": "Anna Korba",
                "arxiv_comment": "39 pages, 14 figures. Published at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08110v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08110v6",
                "updated": "2025-01-07T17:26:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    26,
                    26,
                    1,
                    7,
                    0
                ],
                "published": "2023-01-19T15:01:00Z",
                "published_parsed": [
                    2023,
                    1,
                    19,
                    15,
                    1,
                    0,
                    3,
                    19,
                    0
                ],
                "title": "AtMan: Understanding Transformer Predictions Through Memory Efficient\n  Attention Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AtMan: Understanding Transformer Predictions Through Memory Efficient\n  Attention Manipulation"
                },
                "summary": "Generative transformer models have become increasingly complex, with large\nnumbers of parameters and the ability to process multiple input modalities.\nCurrent methods for explaining their predictions are resource-intensive. Most\ncrucially, they require prohibitively large amounts of extra memory, since they\nrely on backpropagation which allocates almost twice as much GPU memory as the\nforward pass. This makes it difficult, if not impossible, to use them in\nproduction. We present AtMan that provides explanations of generative\ntransformer models at almost no extra cost. Specifically, AtMan is a\nmodality-agnostic perturbation method that manipulates the attention mechanisms\nof transformers to produce relevance maps for the input with respect to the\noutput prediction. Instead of using backpropagation, AtMan applies a\nparallelizable token-based search method based on cosine similarity\nneighborhood in the embedding space. Our exhaustive experiments on text and\nimage-text benchmarks demonstrate that AtMan outperforms current\nstate-of-the-art gradient-based methods on several metrics while being\ncomputationally efficient. As such, AtMan is suitable for use in large model\ninference deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative transformer models have become increasingly complex, with large\nnumbers of parameters and the ability to process multiple input modalities.\nCurrent methods for explaining their predictions are resource-intensive. Most\ncrucially, they require prohibitively large amounts of extra memory, since they\nrely on backpropagation which allocates almost twice as much GPU memory as the\nforward pass. This makes it difficult, if not impossible, to use them in\nproduction. We present AtMan that provides explanations of generative\ntransformer models at almost no extra cost. Specifically, AtMan is a\nmodality-agnostic perturbation method that manipulates the attention mechanisms\nof transformers to produce relevance maps for the input with respect to the\noutput prediction. Instead of using backpropagation, AtMan applies a\nparallelizable token-based search method based on cosine similarity\nneighborhood in the embedding space. Our exhaustive experiments on text and\nimage-text benchmarks demonstrate that AtMan outperforms current\nstate-of-the-art gradient-based methods on several metrics while being\ncomputationally efficient. As such, AtMan is suitable for use in large model\ninference deployments."
                },
                "authors": [
                    {
                        "name": "Björn Deiseroth"
                    },
                    {
                        "name": "Mayukh Deb"
                    },
                    {
                        "name": "Samuel Weinbach"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08110v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08110v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03952v1",
                "updated": "2025-01-07T17:24:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    24,
                    17,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:24:17Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    24,
                    17,
                    1,
                    7,
                    0
                ],
                "title": "Localizing AI: Evaluating Open-Weight Language Models for Languages of\n  Baltic States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localizing AI: Evaluating Open-Weight Language Models for Languages of\n  Baltic States"
                },
                "summary": "Although large language models (LLMs) have transformed our expectations of\nmodern language technologies, concerns over data privacy often restrict the use\nof commercially available LLMs hosted outside of EU jurisdictions. This limits\ntheir application in governmental, defence, and other data-sensitive sectors.\nIn this work, we evaluate the extent to which locally deployable open-weight\nLLMs support lesser-spoken languages such as Lithuanian, Latvian, and Estonian.\nWe examine various size and precision variants of the top-performing\nmultilingual open-weight models, Llama~3, Gemma~2, Phi, and NeMo, on machine\ntranslation, multiple-choice question answering, and free-form text generation.\nThe results indicate that while certain models like Gemma~2 perform close to\nthe top commercially available models, many LLMs struggle with these languages.\nMost surprisingly, however, we find that these models, while showing close to\nstate-of-the-art translation performance, are still prone to lexical\nhallucinations with errors in at least 1 in 20 words for all open-weight\nmultilingual LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have transformed our expectations of\nmodern language technologies, concerns over data privacy often restrict the use\nof commercially available LLMs hosted outside of EU jurisdictions. This limits\ntheir application in governmental, defence, and other data-sensitive sectors.\nIn this work, we evaluate the extent to which locally deployable open-weight\nLLMs support lesser-spoken languages such as Lithuanian, Latvian, and Estonian.\nWe examine various size and precision variants of the top-performing\nmultilingual open-weight models, Llama~3, Gemma~2, Phi, and NeMo, on machine\ntranslation, multiple-choice question answering, and free-form text generation.\nThe results indicate that while certain models like Gemma~2 perform close to\nthe top commercially available models, many LLMs struggle with these languages.\nMost surprisingly, however, we find that these models, while showing close to\nstate-of-the-art translation performance, are still prone to lexical\nhallucinations with errors in at least 1 in 20 words for all open-weight\nmultilingual LLMs."
                },
                "authors": [
                    {
                        "name": "Jurgita Kapočiūtė-Dzikienė"
                    },
                    {
                        "name": "Toms Bergmanis"
                    },
                    {
                        "name": "Mārcis Pinnis"
                    }
                ],
                "author_detail": {
                    "name": "Mārcis Pinnis"
                },
                "author": "Mārcis Pinnis",
                "arxiv_comment": "This paper is accepted to NoDaLiDa/Baltic-HLT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03950v1",
                "updated": "2025-01-07T17:19:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    19,
                    5,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:19:05Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    19,
                    5,
                    1,
                    7,
                    0
                ],
                "title": "Scalable calibration for partially observed individual-based epidemic\n  models through categorical approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable calibration for partially observed individual-based epidemic\n  models through categorical approximations"
                },
                "summary": "The computational cost of exact likelihood evaluation for partially observed\nand highly-heterogeneous individual-based models grows exponentially with the\npopulation size, therefore inference relies on approximations. Sampling-based\napproaches to this problem such as Sequential Monte Carlo or Approximate\nBayesian Computation usually require simulation of every individual in the\npopulation multiple times and are heavily reliant on the design of bespoke\nproposal distributions or summary statistics, and can still scale poorly with\npopulation size. To overcome this, we propose a deterministic recursive\napproach to approximating the likelihood function using categorical\ndistributions. The resulting algorithm has a computational cost as low as\nlinear in the population size and is amenable to automatic differentiation,\nleading to simple algorithms for maximizing this approximate likelihood or\nsampling from posterior distributions. We prove consistency of the maximum\napproximate likelihood estimator of model parameters. We empirically test our\napproach on a range of models with various flavors of heterogeneity: different\nsets of disease states, individual-specific susceptibility and infectivity,\nspatial interaction mechanisms, under-reporting and mis-reporting. We\ndemonstrate strong calibration performance, in terms of log-likelihood variance\nand ground truth recovery, and computational advantages over competitor\nmethods. We conclude by illustrating the effectiveness of our approach in a\nreal-world large-scale application using Foot-and-Mouth data from the 2001\noutbreak in the United Kingdom.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational cost of exact likelihood evaluation for partially observed\nand highly-heterogeneous individual-based models grows exponentially with the\npopulation size, therefore inference relies on approximations. Sampling-based\napproaches to this problem such as Sequential Monte Carlo or Approximate\nBayesian Computation usually require simulation of every individual in the\npopulation multiple times and are heavily reliant on the design of bespoke\nproposal distributions or summary statistics, and can still scale poorly with\npopulation size. To overcome this, we propose a deterministic recursive\napproach to approximating the likelihood function using categorical\ndistributions. The resulting algorithm has a computational cost as low as\nlinear in the population size and is amenable to automatic differentiation,\nleading to simple algorithms for maximizing this approximate likelihood or\nsampling from posterior distributions. We prove consistency of the maximum\napproximate likelihood estimator of model parameters. We empirically test our\napproach on a range of models with various flavors of heterogeneity: different\nsets of disease states, individual-specific susceptibility and infectivity,\nspatial interaction mechanisms, under-reporting and mis-reporting. We\ndemonstrate strong calibration performance, in terms of log-likelihood variance\nand ground truth recovery, and computational advantages over competitor\nmethods. We conclude by illustrating the effectiveness of our approach in a\nreal-world large-scale application using Foot-and-Mouth data from the 2001\noutbreak in the United Kingdom."
                },
                "authors": [
                    {
                        "name": "Lorenzo Rimella"
                    },
                    {
                        "name": "Nick Whiteley"
                    },
                    {
                        "name": "Chris Jewell"
                    },
                    {
                        "name": "Paul Fearnhead"
                    },
                    {
                        "name": "Michael Whitehouse"
                    }
                ],
                "author_detail": {
                    "name": "Michael Whitehouse"
                },
                "author": "Michael Whitehouse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v1",
                "updated": "2025-01-07T17:00:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14746v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14746v3",
                "updated": "2025-01-07T16:58:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    58,
                    5,
                    1,
                    7,
                    0
                ],
                "published": "2024-02-22T18:06:19Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    18,
                    6,
                    19,
                    3,
                    53,
                    0
                ],
                "title": "Scaling Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Efficient LLMs"
                },
                "summary": "Trained LLMs are typically sparse in that most of the parameters are zero,\nraising questions on efficiency. In response, we inquire into efficient LLMs,\ni.e. those with the fewest parameters that achieve the desired accuracy on a\ntraining corpus. Specifically, we compare theoretical and empirical estimates\nfor training loss to obtain upper and lower bounds on the number of unique\nsequences in a natural training corpus as a function of its size. Our result\nimplies (1) to double the number of skills represented in a training corpus,\nthe corpus must scale more than four fold (2) for efficient LLMs, the number of\nparameters N and the size D of a natural training corpus scale as $N \\propto\nD^{0.44}$; (3) if the number of parameters of an LLM is smaller than the number\nof unique sequences in the training corpus, scaling up can uncover emergent\nskills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trained LLMs are typically sparse in that most of the parameters are zero,\nraising questions on efficiency. In response, we inquire into efficient LLMs,\ni.e. those with the fewest parameters that achieve the desired accuracy on a\ntraining corpus. Specifically, we compare theoretical and empirical estimates\nfor training loss to obtain upper and lower bounds on the number of unique\nsequences in a natural training corpus as a function of its size. Our result\nimplies (1) to double the number of skills represented in a training corpus,\nthe corpus must scale more than four fold (2) for efficient LLMs, the number of\nparameters N and the size D of a natural training corpus scale as $N \\propto\nD^{0.44}$; (3) if the number of parameters of an LLM is smaller than the number\nof unique sequences in the training corpus, scaling up can uncover emergent\nskills."
                },
                "authors": [
                    {
                        "name": "B. N. Kausik"
                    }
                ],
                "author_detail": {
                    "name": "B. N. Kausik"
                },
                "author": "B. N. Kausik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14746v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14746v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.13798v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.13798v2",
                "updated": "2025-01-07T16:55:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    55,
                    39,
                    1,
                    7,
                    0
                ],
                "published": "2023-07-25T20:05:01Z",
                "published_parsed": [
                    2023,
                    7,
                    25,
                    20,
                    5,
                    1,
                    1,
                    206,
                    0
                ],
                "title": "Estimates of the reproduction ratio from epidemic surveillance may be\n  biased in spatially structured populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimates of the reproduction ratio from epidemic surveillance may be\n  biased in spatially structured populations"
                },
                "summary": "Accurate estimates of the reproduction ratio are crucial to project\ninfectious disease epidemic evolution and guide public health response. Here,\nwe prove that estimates of the reproduction ratio based on inference from\nsurveillance data can be inaccurate if the population comprises spatially\ndistinct communities, as the space-mobility interplay may hide the true\nepidemic evolution from surveillance data. Consequently, surveillance may\nunderestimate the reproduction ratio over long periods, even mistaking growing\nepidemics as subsiding. To address this, we use the spectral properties of the\nmatrix describing the spatial epidemic spread to reweigh surveillance data. We\npropose a correction that removes the bias across all epidemic phases. We\nvalidate this correction against simulated epidemics and use COVID-19 as a case\nstudy. However, our results apply to any epidemic where mobility is a driver of\ncirculation. Our findings will help improve epidemic monitoring and\nsurveillance and inform strategies for public health response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate estimates of the reproduction ratio are crucial to project\ninfectious disease epidemic evolution and guide public health response. Here,\nwe prove that estimates of the reproduction ratio based on inference from\nsurveillance data can be inaccurate if the population comprises spatially\ndistinct communities, as the space-mobility interplay may hide the true\nepidemic evolution from surveillance data. Consequently, surveillance may\nunderestimate the reproduction ratio over long periods, even mistaking growing\nepidemics as subsiding. To address this, we use the spectral properties of the\nmatrix describing the spatial epidemic spread to reweigh surveillance data. We\npropose a correction that removes the bias across all epidemic phases. We\nvalidate this correction against simulated epidemics and use COVID-19 as a case\nstudy. However, our results apply to any epidemic where mobility is a driver of\ncirculation. Our findings will help improve epidemic monitoring and\nsurveillance and inform strategies for public health response."
                },
                "authors": [
                    {
                        "name": "Piero Birello"
                    },
                    {
                        "name": "Michele Re Fiorentin"
                    },
                    {
                        "name": "Boxuan Wang"
                    },
                    {
                        "name": "Vittoria Colizza"
                    },
                    {
                        "name": "Eugenio Valdano"
                    }
                ],
                "author_detail": {
                    "name": "Eugenio Valdano"
                },
                "author": "Eugenio Valdano",
                "arxiv_doi": "10.1038/s41567-024-02471-7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41567-024-02471-7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.13798v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.13798v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, 4 figures, plus References and Supplementary Information",
                "arxiv_journal_ref": "Nature Physics 20, 1204-1210 (2024)",
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10020v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10020v4",
                "updated": "2025-01-07T16:48:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    48,
                    36,
                    1,
                    7,
                    0
                ],
                "published": "2024-11-15T07:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    54,
                    19,
                    4,
                    320,
                    0
                ],
                "title": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?"
                },
                "summary": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BERT in terms of performance, generalizability, computational\nresources, and throughput to BERT. Results: LLaMA models outperformed BERT\nacross datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BERT by 7% (F1) on\nNER and 4% on RE. However, LLaMA models required more computing resources and\nran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE package\nfeaturing both models, available at https://kiwi.clinicalnlp.org/. Conclusion:\nThis study is among the first to develop and evaluate a comprehensive clinical\nIE system using open-source LLMs. Results indicate that LLaMA models outperform\nBERT for clinical NER and RE but with higher computational costs and lower\nthroughputs. These findings highlight that choosing between LLMs and\ntraditional deep learning methods for clinical IE applications should remain\ntask-specific, taking into account both performance metrics and practical\nconsiderations such as available computing resources and the intended use case\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BERT in terms of performance, generalizability, computational\nresources, and throughput to BERT. Results: LLaMA models outperformed BERT\nacross datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BERT by 7% (F1) on\nNER and 4% on RE. However, LLaMA models required more computing resources and\nran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE package\nfeaturing both models, available at https://kiwi.clinicalnlp.org/. Conclusion:\nThis study is among the first to develop and evaluate a comprehensive clinical\nIE system using open-source LLMs. Results indicate that LLaMA models outperform\nBERT for clinical NER and RE but with higher computational costs and lower\nthroughputs. These findings highlight that choosing between LLMs and\ntraditional deep learning methods for clinical IE applications should remain\ntask-specific, taking into account both performance metrics and practical\nconsiderations such as available computing resources and the intended use case\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Xu Zuo"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Vipina K. Keloth"
                    },
                    {
                        "name": "Vincent J. Zhang"
                    },
                    {
                        "name": "Ruey-Ling Weng"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Xiaoqian Jiang"
                    },
                    {
                        "name": "Kirk E. Roberts"
                    },
                    {
                        "name": "Hua Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xu"
                },
                "author": "Hua Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10020v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10020v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03927v1",
                "updated": "2025-01-07T16:43:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    43,
                    51,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T16:43:51Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    43,
                    51,
                    1,
                    7,
                    0
                ],
                "title": "Optimal Estimation of Temperature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Estimation of Temperature"
                },
                "summary": "Over the past century, the Boltzmann entropy has been widely accepted as the\nstandard definition of entropy for an isolated system. However, it coexists\nwith controversial alternatives, such as the Gibbs entropy. These definitions,\nincluding the Boltzmann entropy, exhibit certain inconsistencies, both\nmathematically and thermodynamically. To address this challenge, we introduce\nthe estimation theory in statistical inference into the study of thermodynamics\nand statistical physics for finite-sized systems. By regarding the finite-sized\nsystem as a thermometer used to measure the temperature of the heat reservoir,\nwe show that optimal estimation of temperature yields the corresponding entropy\nformula for an isolated system. In the single-sample case, optimal estimation\nof inverse temperature (or temperature) corresponds to the Boltzmann entropy\n(or Gibbs entropy). These different definitions of entropy, rather than being\ncontradictory, apply to optimal estimation of different parameters.\nFurthermore, via the Laplace transform, we identify a complementarity between\nestimation of temperature and system's energy, a concept suggested by Niels\nBohr. We also correct the energy-temperature uncertainty relation, as expressed\nby the Cram\\'{e}r-Rao bound, in the large-$N$ limit. In the multiple-sample\ncase, we generalize the definitions of both Boltzmann entropy and Gibbs entropy\nto achieve optimal estimation of temperature, revealing the tight connection\nbetween statistical inference and Terrell Hill's nanothermodynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past century, the Boltzmann entropy has been widely accepted as the\nstandard definition of entropy for an isolated system. However, it coexists\nwith controversial alternatives, such as the Gibbs entropy. These definitions,\nincluding the Boltzmann entropy, exhibit certain inconsistencies, both\nmathematically and thermodynamically. To address this challenge, we introduce\nthe estimation theory in statistical inference into the study of thermodynamics\nand statistical physics for finite-sized systems. By regarding the finite-sized\nsystem as a thermometer used to measure the temperature of the heat reservoir,\nwe show that optimal estimation of temperature yields the corresponding entropy\nformula for an isolated system. In the single-sample case, optimal estimation\nof inverse temperature (or temperature) corresponds to the Boltzmann entropy\n(or Gibbs entropy). These different definitions of entropy, rather than being\ncontradictory, apply to optimal estimation of different parameters.\nFurthermore, via the Laplace transform, we identify a complementarity between\nestimation of temperature and system's energy, a concept suggested by Niels\nBohr. We also correct the energy-temperature uncertainty relation, as expressed\nby the Cram\\'{e}r-Rao bound, in the large-$N$ limit. In the multiple-sample\ncase, we generalize the definitions of both Boltzmann entropy and Gibbs entropy\nto achieve optimal estimation of temperature, revealing the tight connection\nbetween statistical inference and Terrell Hill's nanothermodynamics."
                },
                "authors": [
                    {
                        "name": "Shaoyong Zhang"
                    },
                    {
                        "name": "Zhaoyu Fei"
                    },
                    {
                        "name": "Xiaoguang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoguang Wang"
                },
                "author": "Xiaoguang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03921v1",
                "updated": "2025-01-07T16:34:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    34,
                    47,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T16:34:47Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    34,
                    47,
                    1,
                    7,
                    0
                ],
                "title": "Cosmological Parameter Estimation with Sequential Linear\n  Simulation-based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmological Parameter Estimation with Sequential Linear\n  Simulation-based Inference"
                },
                "summary": "We develop the framework of Linear Simulation-based Inference (LSBI), an\napplication of simulation-based inference where the likelihood is approximated\nby a Gaussian linear function of its parameters. We obtain analytical\nexpressions for the posterior distributions of hyper-parameters of the linear\nlikelihood in terms of samples drawn from a simulator, for both uniform and\nconjugate priors. This method is applied sequentially to several toy-models and\ntested on emulated datasets for the Cosmic Microwave Background temperature\npower spectrum. We find that convergence is achieved after four or five rounds\nof $\\mathcal{O}(10^4)$ simulations, which is competitive with state-of-the-art\nneural density estimation methods. Therefore, we demonstrate that it is\npossible to obtain significant information gain and generate posteriors that\nagree with the underlying parameters while maintaining explainability and\nintellectual oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop the framework of Linear Simulation-based Inference (LSBI), an\napplication of simulation-based inference where the likelihood is approximated\nby a Gaussian linear function of its parameters. We obtain analytical\nexpressions for the posterior distributions of hyper-parameters of the linear\nlikelihood in terms of samples drawn from a simulator, for both uniform and\nconjugate priors. This method is applied sequentially to several toy-models and\ntested on emulated datasets for the Cosmic Microwave Background temperature\npower spectrum. We find that convergence is achieved after four or five rounds\nof $\\mathcal{O}(10^4)$ simulations, which is competitive with state-of-the-art\nneural density estimation methods. Therefore, we demonstrate that it is\npossible to obtain significant information gain and generate posteriors that\nagree with the underlying parameters while maintaining explainability and\nintellectual oversight."
                },
                "authors": [
                    {
                        "name": "Nicolas Mediato-Diaz"
                    },
                    {
                        "name": "Will Handley"
                    }
                ],
                "author_detail": {
                    "name": "Will Handley"
                },
                "author": "Will Handley",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03914v1",
                "updated": "2025-01-07T16:28:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    28,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T16:28:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    28,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Active Learning Techniques for Pomset Recognizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Learning Techniques for Pomset Recognizers"
                },
                "summary": "Pomsets are a promising formalism for concurrent programs based on partially\nordered sets. Among this class, series-parallel pomsets admit a convenient\nlinear representation and can be recognized by simple algebraic structures\nknown as pomset recognizers. Active learning consists in inferring a formal\nmodel of a recognizable language by asking membership and equivalence queries\nto a minimally adequate teacher (MAT). We improve existing learning algorithms\nfor pomset recognizers by 1. introducing a new counter-example analysis\nprocedure that is in the best case scenario exponentially more efficient than\nexisting methods 2. adapting the state-of-the-art $L^{\\lambda}$ algorithm to\nminimize the impact of exceedingly verbose counter-examples and remove\nredundant queries 3. designing a suitable finite test suite that ensures\ngeneral equivalence between two pomset recognizers by extending the well-known\nW-method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pomsets are a promising formalism for concurrent programs based on partially\nordered sets. Among this class, series-parallel pomsets admit a convenient\nlinear representation and can be recognized by simple algebraic structures\nknown as pomset recognizers. Active learning consists in inferring a formal\nmodel of a recognizable language by asking membership and equivalence queries\nto a minimally adequate teacher (MAT). We improve existing learning algorithms\nfor pomset recognizers by 1. introducing a new counter-example analysis\nprocedure that is in the best case scenario exponentially more efficient than\nexisting methods 2. adapting the state-of-the-art $L^{\\lambda}$ algorithm to\nminimize the impact of exceedingly verbose counter-examples and remove\nredundant queries 3. designing a suitable finite test suite that ensures\ngeneral equivalence between two pomset recognizers by extending the well-known\nW-method."
                },
                "authors": [
                    {
                        "name": "Adrien Pommellet"
                    },
                    {
                        "name": "Amazigh Amrane"
                    },
                    {
                        "name": "Edgar Delaporte"
                    }
                ],
                "author_detail": {
                    "name": "Edgar Delaporte"
                },
                "author": "Edgar Delaporte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03907v1",
                "updated": "2025-01-07T16:22:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    22,
                    12,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T16:22:12Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    22,
                    12,
                    1,
                    7,
                    0
                ],
                "title": "Implicit Coordination using Active Epistemic Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Coordination using Active Epistemic Inference"
                },
                "summary": "A Multi-robot system (MRS) provides significant advantages for intricate\ntasks such as environmental monitoring, underwater inspections, and space\nmissions. However, addressing potential communication failures or the lack of\ncommunication infrastructure in these fields remains a challenge. A significant\nportion of MRS research presumes that the system can maintain communication\nwith proximity constraints, but this approach does not solve situations where\ncommunication is either non-existent, unreliable, or poses a security risk.\nSome approaches tackle this issue using predictions about other robots while\nnot communicating, but these methods generally only permit agents to utilize\nfirst-order reasoning, which involves reasoning based purely on their own\nobservations. In contrast, to deal with this problem, our proposed framework\nutilizes Theory of Mind (ToM), employing higher-order reasoning by shifting a\nrobot's perspective to reason about a belief of others observations. Our\napproach has two main phases: i) an efficient runtime plan adaptation using\nactive inference to signal intentions and reason about a robot's own belief and\nthe beliefs of others in the system, and ii) a hierarchical epistemic planning\nframework to iteratively reason about the current MRS mission state. The\nproposed framework outperforms greedy and first-order reasoning approaches and\nis validated using simulations and experiments with heterogeneous robotic\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-robot system (MRS) provides significant advantages for intricate\ntasks such as environmental monitoring, underwater inspections, and space\nmissions. However, addressing potential communication failures or the lack of\ncommunication infrastructure in these fields remains a challenge. A significant\nportion of MRS research presumes that the system can maintain communication\nwith proximity constraints, but this approach does not solve situations where\ncommunication is either non-existent, unreliable, or poses a security risk.\nSome approaches tackle this issue using predictions about other robots while\nnot communicating, but these methods generally only permit agents to utilize\nfirst-order reasoning, which involves reasoning based purely on their own\nobservations. In contrast, to deal with this problem, our proposed framework\nutilizes Theory of Mind (ToM), employing higher-order reasoning by shifting a\nrobot's perspective to reason about a belief of others observations. Our\napproach has two main phases: i) an efficient runtime plan adaptation using\nactive inference to signal intentions and reason about a robot's own belief and\nthe beliefs of others in the system, and ii) a hierarchical epistemic planning\nframework to iteratively reason about the current MRS mission state. The\nproposed framework outperforms greedy and first-order reasoning approaches and\nis validated using simulations and experiments with heterogeneous robotic\nsystems."
                },
                "authors": [
                    {
                        "name": "Lauren Bramblett"
                    },
                    {
                        "name": "Jonathan Reasoner"
                    },
                    {
                        "name": "Nicola Bezzo"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Bezzo"
                },
                "author": "Nicola Bezzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19223v2",
                "updated": "2025-01-07T16:20:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    20,
                    17,
                    1,
                    7,
                    0
                ],
                "published": "2024-06-27T14:49:08Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    14,
                    49,
                    8,
                    3,
                    179,
                    0
                ],
                "title": "T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse\n  Representations for Memory-Efficient Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse\n  Representations for Memory-Efficient Embeddings"
                },
                "summary": "Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced\neffectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our\nexhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced\neffectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our\nexhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning."
                },
                "authors": [
                    {
                        "name": "Björn Deiseroth"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Samuel Weinbach"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Weinbach"
                },
                "author": "Samuel Weinbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03904v1",
                "updated": "2025-01-07T16:18:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    18,
                    55,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T16:18:55Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    18,
                    55,
                    1,
                    7,
                    0
                ],
                "title": "Exploring the Potential of Large Language Models in Public\n  Transportation: San Antonio Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Large Language Models in Public\n  Transportation: San Antonio Case Study"
                },
                "summary": "The integration of large language models (LLMs) into public transit systems\npresents a transformative opportunity to enhance urban mobility. This study\nexplores the potential of LLMs to revolutionize public transportation\nmanagement within the context of San Antonio's transit system. Leveraging the\ncapabilities of LLMs in natural language processing and data analysis, we\ninvestigate their capabilities to optimize route planning, reduce wait times,\nand provide personalized travel assistance. By utilizing the General Transit\nFeed Specification (GTFS) and other relevant data, this research aims to\ndemonstrate how LLMs can potentially improve resource allocation, elevate\npassenger satisfaction, and inform data-driven decision-making in transit\noperations. A comparative analysis of different ChatGPT models was conducted to\nassess their ability to understand transportation information, retrieve\nrelevant data, and provide comprehensive responses. Findings from this study\nsuggest that while LLMs hold immense promise for public transit, careful\nengineering and fine-tuning are essential to realizing their full potential.\nSan Antonio serves as a case study to inform the development of LLM-powered\ntransit systems in other urban environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) into public transit systems\npresents a transformative opportunity to enhance urban mobility. This study\nexplores the potential of LLMs to revolutionize public transportation\nmanagement within the context of San Antonio's transit system. Leveraging the\ncapabilities of LLMs in natural language processing and data analysis, we\ninvestigate their capabilities to optimize route planning, reduce wait times,\nand provide personalized travel assistance. By utilizing the General Transit\nFeed Specification (GTFS) and other relevant data, this research aims to\ndemonstrate how LLMs can potentially improve resource allocation, elevate\npassenger satisfaction, and inform data-driven decision-making in transit\noperations. A comparative analysis of different ChatGPT models was conducted to\nassess their ability to understand transportation information, retrieve\nrelevant data, and provide comprehensive responses. Findings from this study\nsuggest that while LLMs hold immense promise for public transit, careful\nengineering and fine-tuning are essential to realizing their full potential.\nSan Antonio serves as a case study to inform the development of LLM-powered\ntransit systems in other urban environments."
                },
                "authors": [
                    {
                        "name": "Ramya Jonnala"
                    },
                    {
                        "name": "Gongbo Liang"
                    },
                    {
                        "name": "Jeong Yang"
                    },
                    {
                        "name": "Izzat Alsmadi"
                    }
                ],
                "author_detail": {
                    "name": "Izzat Alsmadi"
                },
                "author": "Izzat Alsmadi",
                "arxiv_comment": "This work is accepted to AAAI 2025 Workshop on AI for Urban Planning.\n  arXiv admin note: substantial text overlap with arXiv:2407.11003",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16315v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16315v4",
                "updated": "2025-01-07T16:05:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    5,
                    16,
                    1,
                    7,
                    0
                ],
                "published": "2024-02-26T05:43:51Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    5,
                    43,
                    51,
                    0,
                    57,
                    0
                ],
                "title": "Finer: Investigating and Enhancing Fine-Grained Visual Concept\n  Recognition in Large Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finer: Investigating and Enhancing Fine-Grained Visual Concept\n  Recognition in Large Vision Language Models"
                },
                "summary": "Recent advances in instruction-tuned Large Vision-Language Models (LVLMs)\nhave imbued the models with the ability to generate high-level, image-grounded\nexplanations with ease. While such capability is largely attributed to the rich\nworld knowledge contained within the Large Language Models (LLMs), our work\nreveals their shortcomings in fine-grained visual categorization (FGVC) across\nsix different benchmark settings. Most recent state-of-the-art LVLMs like\nLLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of\nclassification performance, e.g., average drop of 65.58 in EM for Stanford Dogs\nfor LLaVA-1.5, but also struggle to generate an accurate explanation with\ndetailed attributes based on the concept that appears within an input image\ndespite their capability to generate holistic image-level descriptions.\nIn-depth analyses show that instruction-tuned LVLMs exhibit modality gap,\nshowing discrepancy when given textual and visual inputs that correspond to the\nsame concept, preventing the image modality from leveraging the rich parametric\nknowledge within the LLMs. In an effort to further the community's endeavor in\nthis direction, we propose a multiple granularity attribute-centric evaluation\nbenchmark, Finer, which aims to establish a ground to evaluate LVLMs'\nfine-grained visual comprehension ability and provide significantly improved\nexplainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in instruction-tuned Large Vision-Language Models (LVLMs)\nhave imbued the models with the ability to generate high-level, image-grounded\nexplanations with ease. While such capability is largely attributed to the rich\nworld knowledge contained within the Large Language Models (LLMs), our work\nreveals their shortcomings in fine-grained visual categorization (FGVC) across\nsix different benchmark settings. Most recent state-of-the-art LVLMs like\nLLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of\nclassification performance, e.g., average drop of 65.58 in EM for Stanford Dogs\nfor LLaVA-1.5, but also struggle to generate an accurate explanation with\ndetailed attributes based on the concept that appears within an input image\ndespite their capability to generate holistic image-level descriptions.\nIn-depth analyses show that instruction-tuned LVLMs exhibit modality gap,\nshowing discrepancy when given textual and visual inputs that correspond to the\nsame concept, preventing the image modality from leveraging the rich parametric\nknowledge within the LLMs. In an effort to further the community's endeavor in\nthis direction, we propose a multiple granularity attribute-centric evaluation\nbenchmark, Finer, which aims to establish a ground to evaluate LVLMs'\nfine-grained visual comprehension ability and provide significantly improved\nexplainability."
                },
                "authors": [
                    {
                        "name": "Jeonghwan Kim"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_doi": "10.18653/v1/2024.emnlp-main.356",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.emnlp-main.356",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.16315v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16315v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "EMNLP 2024; Main Conference",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03895v1",
                "updated": "2025-01-07T16:03:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    3,
                    14,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T16:03:14Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    3,
                    14,
                    1,
                    7,
                    0
                ],
                "title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One\n  Vision Token",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One\n  Vision Token"
                },
                "summary": "The advent of real-time large multimodal models (LMMs) like GPT-4o has\nsparked considerable interest in efficient LMMs. LMM frameworks typically\nencode visual inputs into vision tokens (continuous representations) and\nintegrate them and textual instructions into the context of large language\nmodels (LLMs), where large-scale parameters and numerous context tokens\n(predominantly vision tokens) result in substantial computational overhead.\nPrevious efforts towards efficient LMMs always focus on replacing the LLM\nbackbone with smaller models, while neglecting the crucial issue of token\nquantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal\nvision tokens. To achieve a high compression ratio of vision tokens while\npreserving visual information, we first analyze how LMMs understand vision\ntokens and find that most vision tokens only play a crucial role in the early\nlayers of LLM backbone, where they mainly fuse visual information into text\ntokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to\nfuse visual information into text tokens in advance, thereby facilitating the\nextreme compression of vision tokens fed to LLM backbone into one token.\nLLaVA-Mini is a unified large multimodal model that can support the\nunderstanding of images, high-resolution images, and videos in an efficient\nmanner. Experiments across 11 image-based and 7 video-based benchmarks\ndemonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token\ninstead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by\n77%, deliver low-latency responses within 40 milliseconds, and process over\n10,000 frames of video on the GPU hardware with 24GB of memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of real-time large multimodal models (LMMs) like GPT-4o has\nsparked considerable interest in efficient LMMs. LMM frameworks typically\nencode visual inputs into vision tokens (continuous representations) and\nintegrate them and textual instructions into the context of large language\nmodels (LLMs), where large-scale parameters and numerous context tokens\n(predominantly vision tokens) result in substantial computational overhead.\nPrevious efforts towards efficient LMMs always focus on replacing the LLM\nbackbone with smaller models, while neglecting the crucial issue of token\nquantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal\nvision tokens. To achieve a high compression ratio of vision tokens while\npreserving visual information, we first analyze how LMMs understand vision\ntokens and find that most vision tokens only play a crucial role in the early\nlayers of LLM backbone, where they mainly fuse visual information into text\ntokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to\nfuse visual information into text tokens in advance, thereby facilitating the\nextreme compression of vision tokens fed to LLM backbone into one token.\nLLaVA-Mini is a unified large multimodal model that can support the\nunderstanding of images, high-resolution images, and videos in an efficient\nmanner. Experiments across 11 image-based and 7 video-based benchmarks\ndemonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token\ninstead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by\n77%, deliver low-latency responses within 40 milliseconds, and process over\n10,000 frames of video on the GPU hardware with 24GB of memory."
                },
                "authors": [
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "Code: https://github.com/ictnlp/LLaVA-Mini; Model:\n  https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03892v1",
                "updated": "2025-01-07T16:00:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    0,
                    40,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T16:00:40Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    0,
                    40,
                    1,
                    7,
                    0
                ],
                "title": "LEAP: LLM-powered End-to-end Automatic Library for Processing Social\n  Science Queries on Unstructured Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEAP: LLM-powered End-to-end Automatic Library for Processing Social\n  Science Queries on Unstructured Data"
                },
                "summary": "Social scientists are increasingly interested in analyzing the semantic\ninformation (e.g., emotion) of unstructured data (e.g., Tweets), where the\nsemantic information is not natively present. Performing this analysis in a\ncost-efficient manner requires using machine learning (ML) models to extract\nthe semantic information and subsequently analyze the now structured data.\nHowever, this process remains challenging for domain experts.\n  To demonstrate the challenges in social science analytics, we collect a\ndataset, QUIET-ML, of 120 real-world social science queries in natural language\nand their ground truth answers. Existing systems struggle with these queries\nsince (1) they require selecting and applying ML models, and (2) more than a\nquarter of these queries are vague, making standard tools like natural language\nto SQL systems unsuited. To address these issues, we develop LEAP, an\nend-to-end library that answers social science queries in natural language with\nML. LEAP filters vague queries to ensure that the answers are deterministic and\nselects from internally supported and user-defined ML functions to extend the\nunstructured data to structured tables with necessary annotations. LEAP further\ngenerates and executes code to respond to these natural language queries. LEAP\nachieves a 100% pass @ 3 and 92% pass @ 1 on QUIET-ML, with a \\$1.06 average\nend-to-end cost, of which code generation costs \\$0.02.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social scientists are increasingly interested in analyzing the semantic\ninformation (e.g., emotion) of unstructured data (e.g., Tweets), where the\nsemantic information is not natively present. Performing this analysis in a\ncost-efficient manner requires using machine learning (ML) models to extract\nthe semantic information and subsequently analyze the now structured data.\nHowever, this process remains challenging for domain experts.\n  To demonstrate the challenges in social science analytics, we collect a\ndataset, QUIET-ML, of 120 real-world social science queries in natural language\nand their ground truth answers. Existing systems struggle with these queries\nsince (1) they require selecting and applying ML models, and (2) more than a\nquarter of these queries are vague, making standard tools like natural language\nto SQL systems unsuited. To address these issues, we develop LEAP, an\nend-to-end library that answers social science queries in natural language with\nML. LEAP filters vague queries to ensure that the answers are deterministic and\nselects from internally supported and user-defined ML functions to extend the\nunstructured data to structured tables with necessary annotations. LEAP further\ngenerates and executes code to respond to these natural language queries. LEAP\nachieves a 100% pass @ 3 and 92% pass @ 1 on QUIET-ML, with a \\$1.06 average\nend-to-end cost, of which code generation costs \\$0.02."
                },
                "authors": [
                    {
                        "name": "Chuxuan Hu"
                    },
                    {
                        "name": "Austin Peters"
                    },
                    {
                        "name": "Daniel Kang"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kang"
                },
                "author": "Daniel Kang",
                "arxiv_doi": "10.14778/3705829.3705843",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3705829.3705843",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.03892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to VLDB 2025",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03884v1",
                "updated": "2025-01-07T15:46:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    46,
                    42,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T15:46:42Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    46,
                    42,
                    1,
                    7,
                    0
                ],
                "title": "AlphaPO -- Reward shape matters for LLM alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaPO -- Reward shape matters for LLM alignment"
                },
                "summary": "Reinforcement Learning with Human Feedback (RLHF) and its variants have made\nhuge strides toward the effective alignment of large language models (LLMs) to\nfollow instructions and reflect human values. More recently, Direct Alignment\nAlgorithms (DAAs) have emerged in which the reward modeling stage of RLHF is\nskipped by characterizing the reward directly as a function of the policy being\nlearned. Examples include Direct Preference Optimization (DPO) and Simple\nPreference Optimization (SimPO). These methods often suffer from likelihood\ndisplacement, a phenomenon by which the probabilities of preferred responses\nare often reduced undesirably.\n  In this paper, we argue that, for DAAs the reward (function) shape matters.\nWe introduce AlphaPO, a new DAA method that leverages an $\\alpha$-parameter to\nhelp change the shape of the reward function beyond the standard log reward.\nAlphaPO helps maintain fine-grained control over likelihood displacement and\nover-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO\nleads to about 7\\% to 10\\% relative improvement in alignment performance for\nthe instruct versions of Mistral-7B and Llama3-8B. The analysis and results\npresented highlight the importance of the reward shape, and how one can\nsystematically change it to affect training dynamics, as well as improve\nalignment performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Human Feedback (RLHF) and its variants have made\nhuge strides toward the effective alignment of large language models (LLMs) to\nfollow instructions and reflect human values. More recently, Direct Alignment\nAlgorithms (DAAs) have emerged in which the reward modeling stage of RLHF is\nskipped by characterizing the reward directly as a function of the policy being\nlearned. Examples include Direct Preference Optimization (DPO) and Simple\nPreference Optimization (SimPO). These methods often suffer from likelihood\ndisplacement, a phenomenon by which the probabilities of preferred responses\nare often reduced undesirably.\n  In this paper, we argue that, for DAAs the reward (function) shape matters.\nWe introduce AlphaPO, a new DAA method that leverages an $\\alpha$-parameter to\nhelp change the shape of the reward function beyond the standard log reward.\nAlphaPO helps maintain fine-grained control over likelihood displacement and\nover-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO\nleads to about 7\\% to 10\\% relative improvement in alignment performance for\nthe instruct versions of Mistral-7B and Llama3-8B. The analysis and results\npresented highlight the importance of the reward shape, and how one can\nsystematically change it to affect training dynamics, as well as improve\nalignment performance."
                },
                "authors": [
                    {
                        "name": "Aman Gupta"
                    },
                    {
                        "name": "Shao Tang"
                    },
                    {
                        "name": "Qingquan Song"
                    },
                    {
                        "name": "Sirou Zhu"
                    },
                    {
                        "name": "Jiwoo Hong"
                    },
                    {
                        "name": "Ankan Saha"
                    },
                    {
                        "name": "Viral Gupta"
                    },
                    {
                        "name": "Noah Lee"
                    },
                    {
                        "name": "Eunki Kim"
                    },
                    {
                        "name": "Jason Zhu"
                    },
                    {
                        "name": "Natesh Pillai"
                    },
                    {
                        "name": "S. Sathiya Keerthi"
                    }
                ],
                "author_detail": {
                    "name": "S. Sathiya Keerthi"
                },
                "author": "S. Sathiya Keerthi",
                "arxiv_comment": "Preprint. Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03879v1",
                "updated": "2025-01-07T15:42:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    42,
                    32,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T15:42:32Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    42,
                    32,
                    1,
                    7,
                    0
                ],
                "title": "CL3DOR: Contrastive Learning for 3D Large Multimodal Models via Odds\n  Ratio on High-Resolution Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CL3DOR: Contrastive Learning for 3D Large Multimodal Models via Odds\n  Ratio on High-Resolution Point Clouds"
                },
                "summary": "Recent research has demonstrated that Large Language Models (LLMs) are not\nlimited to text-only tasks but can also function as multimodal models across\nvarious modalities, including audio, images, and videos. In particular,\nresearch on 3D Large Multimodal Models (3D LMMs) is making notable strides,\ndriven by the potential of processing higher-dimensional data like point\nclouds. However, upon closer examination, we find that the visual and textual\ncontent within each sample of existing training datasets lacks both high\ninformational granularity and clarity, which serve as a bottleneck for precise\ncross-modal understanding. To address these issues, we propose CL3DOR,\nContrastive Learning for 3D large multimodal models via Odds ratio on\nhigh-Resolution point clouds, designed to ensure greater specificity and\nclarity in both visual and textual content. Specifically, we increase the\ndensity of point clouds per object and construct informative hard negative\nresponses in the training dataset to penalize unwanted responses. To leverage\nhard negative responses, we incorporate the odds ratio as an auxiliary term for\ncontrastive learning into the conventional language modeling loss. CL3DOR\nachieves state-of-the-art performance in 3D scene understanding and reasoning\nbenchmarks. Additionally, we demonstrate the effectiveness of CL3DOR's key\ncomponents through extensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has demonstrated that Large Language Models (LLMs) are not\nlimited to text-only tasks but can also function as multimodal models across\nvarious modalities, including audio, images, and videos. In particular,\nresearch on 3D Large Multimodal Models (3D LMMs) is making notable strides,\ndriven by the potential of processing higher-dimensional data like point\nclouds. However, upon closer examination, we find that the visual and textual\ncontent within each sample of existing training datasets lacks both high\ninformational granularity and clarity, which serve as a bottleneck for precise\ncross-modal understanding. To address these issues, we propose CL3DOR,\nContrastive Learning for 3D large multimodal models via Odds ratio on\nhigh-Resolution point clouds, designed to ensure greater specificity and\nclarity in both visual and textual content. Specifically, we increase the\ndensity of point clouds per object and construct informative hard negative\nresponses in the training dataset to penalize unwanted responses. To leverage\nhard negative responses, we incorporate the odds ratio as an auxiliary term for\ncontrastive learning into the conventional language modeling loss. CL3DOR\nachieves state-of-the-art performance in 3D scene understanding and reasoning\nbenchmarks. Additionally, we demonstrate the effectiveness of CL3DOR's key\ncomponents through extensive experiments."
                },
                "authors": [
                    {
                        "name": "Keonwoo Kim"
                    },
                    {
                        "name": "Yeongjae Cho"
                    },
                    {
                        "name": "Taebaek Hwang"
                    },
                    {
                        "name": "Minsoo Jo"
                    },
                    {
                        "name": "Sangdo Han"
                    }
                ],
                "author_detail": {
                    "name": "Sangdo Han"
                },
                "author": "Sangdo Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03875v1",
                "updated": "2025-01-07T15:39:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    39,
                    2,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T15:39:02Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    39,
                    2,
                    1,
                    7,
                    0
                ],
                "title": "ZDySS -- Zero-Shot Dynamic Scene Stylization using Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZDySS -- Zero-Shot Dynamic Scene Stylization using Gaussian Splatting"
                },
                "summary": "Stylizing a dynamic scene based on an exemplar image is critical for various\nreal-world applications, including gaming, filmmaking, and augmented and\nvirtual reality. However, achieving consistent stylization across both spatial\nand temporal dimensions remains a significant challenge. Most existing methods\nare designed for static scenes and often require an optimization process for\neach style image, limiting their adaptability. We introduce ZDySS, a zero-shot\nstylization framework for dynamic scenes, allowing our model to generalize to\npreviously unseen style images at inference. Our approach employs Gaussian\nsplatting for scene representation, linking each Gaussian to a learned feature\nvector that renders a feature map for any given view and timestamp. By applying\nstyle transfer on the learned feature vectors instead of the rendered feature\nmap, we enhance spatio-temporal consistency across frames. Our method\ndemonstrates superior performance and coherence over state-of-the-art baselines\nin tests on real-world dynamic scenes, making it a robust solution for\npractical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stylizing a dynamic scene based on an exemplar image is critical for various\nreal-world applications, including gaming, filmmaking, and augmented and\nvirtual reality. However, achieving consistent stylization across both spatial\nand temporal dimensions remains a significant challenge. Most existing methods\nare designed for static scenes and often require an optimization process for\neach style image, limiting their adaptability. We introduce ZDySS, a zero-shot\nstylization framework for dynamic scenes, allowing our model to generalize to\npreviously unseen style images at inference. Our approach employs Gaussian\nsplatting for scene representation, linking each Gaussian to a learned feature\nvector that renders a feature map for any given view and timestamp. By applying\nstyle transfer on the learned feature vectors instead of the rendered feature\nmap, we enhance spatio-temporal consistency across frames. Our method\ndemonstrates superior performance and coherence over state-of-the-art baselines\nin tests on real-world dynamic scenes, making it a robust solution for\npractical applications."
                },
                "authors": [
                    {
                        "name": "Abhishek Saroha"
                    },
                    {
                        "name": "Florian Hofherr"
                    },
                    {
                        "name": "Mariia Gladkova"
                    },
                    {
                        "name": "Cecilia Curreli"
                    },
                    {
                        "name": "Or Litany"
                    },
                    {
                        "name": "Daniel Cremers"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Cremers"
                },
                "author": "Daniel Cremers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12359v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12359v2",
                "updated": "2025-01-07T15:36:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    36,
                    54,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-16T21:14:11Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    21,
                    14,
                    11,
                    0,
                    351,
                    0
                ],
                "title": "LLaVA Steering: Visual Instruction Tuning with 500x Fewer Parameters\n  through Modality Linear Representation-Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA Steering: Visual Instruction Tuning with 500x Fewer Parameters\n  through Modality Linear Representation-Steering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have significantly advanced visual\ntasks by integrating visual representations into large language models (LLMs).\nThe textual modality, inherited from LLMs, equips MLLMs with abilities like\ninstruction following and in-context learning. In contrast, the visual modality\nenhances performance in downstream tasks by leveraging rich semantic content,\nspatial information, and grounding capabilities. These intrinsic modalities\nwork synergistically across various visual tasks. Our research initially\nreveals a persistent imbalance between these modalities, with text often\ndominating output generation during visual instruction tuning. This imbalance\noccurs when using both full fine-tuning and parameter-efficient fine-tuning\n(PEFT) methods. We then found that re-balancing these modalities can\nsignificantly reduce the number of trainable parameters required, inspiring a\ndirection for further optimizing visual instruction tuning. We introduce\nModality Linear Representation-Steering (MoReS) to achieve the goal. MoReS\neffectively re-balances the intrinsic modalities throughout the model, where\nthe key idea is to steer visual representations through linear transformations\nin the visual subspace across each model layer. To validate our solution, we\ncomposed LLaVA Steering, a suite of models integrated with the proposed MoReS\nmethod. Evaluation results show that the composed LLaVA Steering models\nrequire, on average, 500 times fewer trainable parameters than LoRA needs while\nstill achieving comparable performance across three visual benchmarks and eight\nvisual question-answering tasks. Last, we present the LLaVA Steering Factory,\nan in-house developed platform that enables researchers to quickly customize\nvarious MLLMs with component-based architecture for seamlessly integrating\nstate-of-the-art models, and evaluate their intrinsic modality imbalance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have significantly advanced visual\ntasks by integrating visual representations into large language models (LLMs).\nThe textual modality, inherited from LLMs, equips MLLMs with abilities like\ninstruction following and in-context learning. In contrast, the visual modality\nenhances performance in downstream tasks by leveraging rich semantic content,\nspatial information, and grounding capabilities. These intrinsic modalities\nwork synergistically across various visual tasks. Our research initially\nreveals a persistent imbalance between these modalities, with text often\ndominating output generation during visual instruction tuning. This imbalance\noccurs when using both full fine-tuning and parameter-efficient fine-tuning\n(PEFT) methods. We then found that re-balancing these modalities can\nsignificantly reduce the number of trainable parameters required, inspiring a\ndirection for further optimizing visual instruction tuning. We introduce\nModality Linear Representation-Steering (MoReS) to achieve the goal. MoReS\neffectively re-balances the intrinsic modalities throughout the model, where\nthe key idea is to steer visual representations through linear transformations\nin the visual subspace across each model layer. To validate our solution, we\ncomposed LLaVA Steering, a suite of models integrated with the proposed MoReS\nmethod. Evaluation results show that the composed LLaVA Steering models\nrequire, on average, 500 times fewer trainable parameters than LoRA needs while\nstill achieving comparable performance across three visual benchmarks and eight\nvisual question-answering tasks. Last, we present the LLaVA Steering Factory,\nan in-house developed platform that enables researchers to quickly customize\nvarious MLLMs with component-based architecture for seamlessly integrating\nstate-of-the-art models, and evaluate their intrinsic modality imbalance."
                },
                "authors": [
                    {
                        "name": "Jinhe Bi"
                    },
                    {
                        "name": "Yujun Wang"
                    },
                    {
                        "name": "Haokun Chen"
                    },
                    {
                        "name": "Xun Xiao"
                    },
                    {
                        "name": "Artur Hecker"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12359v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12359v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14841v2",
                "updated": "2025-01-07T15:30:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    30,
                    56,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-19T13:34:14Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    34,
                    14,
                    3,
                    354,
                    0
                ],
                "title": "Helping LLMs Improve Code Generation Using Feedback from Testing and\n  Static Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helping LLMs Improve Code Generation Using Feedback from Testing and\n  Static Analysis"
                },
                "summary": "Large Language Models (LLMs) are one of the most promising developments in\nthe field of artificial intelligence, and the software engineering community\nhas readily noticed their potential role in the software development\nlife-cycle. Developers routinely ask LLMs to generate code snippets, increasing\nproductivity but also potentially introducing ownership, privacy, correctness,\nand security issues. Previous work highlighted how code generated by mainstream\ncommercial LLMs is often not safe, containing vulnerabilities, bugs, and code\nsmells. In this paper, we present a framework that leverages testing and static\nanalysis to assess the quality, and guide the self-improvement, of code\ngenerated by general-purpose, open-source LLMs.\n  First, we ask LLMs to generate C code to solve a number of programming tasks.\nThen we employ ground-truth tests to assess the (in)correctness of the\ngenerated code, and a static analysis tool to detect potential safety\nvulnerabilities. Next, we assess the models ability to evaluate the generated\ncode, by asking them to detect errors and vulnerabilities. Finally, we test the\nmodels ability to fix the generated code, providing the reports produced during\nthe static analysis and incorrectness evaluation phases as feedback.\n  Our results show that models often produce incorrect code, and that the\ngenerated code can include safety issues. Moreover, they perform very poorly at\ndetecting either issue. On the positive side, we observe a substantial ability\nto fix flawed code when provided with information about failed tests or\npotential vulnerabilities, indicating a promising avenue for improving the\nsafety of LLM-based code generation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are one of the most promising developments in\nthe field of artificial intelligence, and the software engineering community\nhas readily noticed their potential role in the software development\nlife-cycle. Developers routinely ask LLMs to generate code snippets, increasing\nproductivity but also potentially introducing ownership, privacy, correctness,\nand security issues. Previous work highlighted how code generated by mainstream\ncommercial LLMs is often not safe, containing vulnerabilities, bugs, and code\nsmells. In this paper, we present a framework that leverages testing and static\nanalysis to assess the quality, and guide the self-improvement, of code\ngenerated by general-purpose, open-source LLMs.\n  First, we ask LLMs to generate C code to solve a number of programming tasks.\nThen we employ ground-truth tests to assess the (in)correctness of the\ngenerated code, and a static analysis tool to detect potential safety\nvulnerabilities. Next, we assess the models ability to evaluate the generated\ncode, by asking them to detect errors and vulnerabilities. Finally, we test the\nmodels ability to fix the generated code, providing the reports produced during\nthe static analysis and incorrectness evaluation phases as feedback.\n  Our results show that models often produce incorrect code, and that the\ngenerated code can include safety issues. Moreover, they perform very poorly at\ndetecting either issue. On the positive side, we observe a substantial ability\nto fix flawed code when provided with information about failed tests or\npotential vulnerabilities, indicating a promising avenue for improving the\nsafety of LLM-based code generation tools."
                },
                "authors": [
                    {
                        "name": "Greta Dolcetti"
                    },
                    {
                        "name": "Vincenzo Arceri"
                    },
                    {
                        "name": "Eleonora Iotti"
                    },
                    {
                        "name": "Sergio Maffeis"
                    },
                    {
                        "name": "Agostino Cortesi"
                    },
                    {
                        "name": "Enea Zaffanella"
                    }
                ],
                "author_detail": {
                    "name": "Enea Zaffanella"
                },
                "author": "Enea Zaffanella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19155v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19155v3",
                "updated": "2025-01-07T15:30:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    30,
                    2,
                    1,
                    7,
                    0
                ],
                "published": "2024-10-24T20:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    20,
                    49,
                    22,
                    3,
                    298,
                    0
                ],
                "title": "Lived Experience Not Found: LLMs Struggle to Align with Experts on\n  Addressing Adverse Drug Reactions from Psychiatric Medication Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lived Experience Not Found: LLMs Struggle to Align with Experts on\n  Addressing Adverse Drug Reactions from Psychiatric Medication Use"
                },
                "summary": "Adverse Drug Reactions (ADRs) from psychiatric medications are the leading\ncause of hospitalizations among mental health patients. With healthcare systems\nand online communities facing limitations in resolving ADR-related issues,\nLarge Language Models (LLMs) have the potential to fill this gap. Despite the\nincreasing capabilities of LLMs, past research has not explored their\ncapabilities in detecting ADRs related to psychiatric medications or in\nproviding effective harm reduction strategies. To address this, we introduce\nthe Psych-ADR benchmark and the Adverse Drug Reaction Response Assessment\n(ADRA) framework to systematically evaluate LLM performance in detecting ADR\nexpressions and delivering expert-aligned mitigation strategies. Our analyses\nshow that LLMs struggle with understanding the nuances of ADRs and\ndifferentiating between types of ADRs. While LLMs align with experts in terms\nof expressed emotions and tone of the text, their responses are more complex,\nharder to read, and only 70.86% aligned with expert strategies. Furthermore,\nthey provide less actionable advice by a margin of 12.32% on average. Our work\nprovides a comprehensive benchmark and evaluation framework for assessing LLMs\nin strategy-driven tasks within high-risk domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adverse Drug Reactions (ADRs) from psychiatric medications are the leading\ncause of hospitalizations among mental health patients. With healthcare systems\nand online communities facing limitations in resolving ADR-related issues,\nLarge Language Models (LLMs) have the potential to fill this gap. Despite the\nincreasing capabilities of LLMs, past research has not explored their\ncapabilities in detecting ADRs related to psychiatric medications or in\nproviding effective harm reduction strategies. To address this, we introduce\nthe Psych-ADR benchmark and the Adverse Drug Reaction Response Assessment\n(ADRA) framework to systematically evaluate LLM performance in detecting ADR\nexpressions and delivering expert-aligned mitigation strategies. Our analyses\nshow that LLMs struggle with understanding the nuances of ADRs and\ndifferentiating between types of ADRs. While LLMs align with experts in terms\nof expressed emotions and tone of the text, their responses are more complex,\nharder to read, and only 70.86% aligned with expert strategies. Furthermore,\nthey provide less actionable advice by a margin of 12.32% on average. Our work\nprovides a comprehensive benchmark and evaluation framework for assessing LLMs\nin strategy-driven tasks within high-risk domains."
                },
                "authors": [
                    {
                        "name": "Mohit Chandra"
                    },
                    {
                        "name": "Siddharth Sriraman"
                    },
                    {
                        "name": "Gaurav Verma"
                    },
                    {
                        "name": "Harneet Singh Khanuja"
                    },
                    {
                        "name": "Jose Suarez Campayo"
                    },
                    {
                        "name": "Zihang Li"
                    },
                    {
                        "name": "Michael L. Birnbaum"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Munmun De Choudhury"
                },
                "author": "Munmun De Choudhury",
                "arxiv_comment": "30 pages, 8 figures, 16 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19155v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19155v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03857v1",
                "updated": "2025-01-07T15:14:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    14,
                    37,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T15:14:37Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    14,
                    37,
                    1,
                    7,
                    0
                ],
                "title": "Progressive Document-level Text Simplification via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Document-level Text Simplification via Large Language Models"
                },
                "summary": "Research on text simplification has primarily focused on lexical and\nsentence-level changes. Long document-level simplification (DS) is still\nrelatively unexplored. Large Language Models (LLMs), like ChatGPT, have\nexcelled in many natural language processing tasks. However, their performance\non DS tasks is unsatisfactory, as they often treat DS as merely document\nsummarization. For the DS task, the generated long sequences not only must\nmaintain consistency with the original document throughout, but complete\nmoderate simplification operations encompassing discourses, sentences, and\nword-level simplifications. Human editors employ a hierarchical complexity\nsimplification strategy to simplify documents. This study delves into\nsimulating this strategy through the utilization of a multi-stage collaboration\nusing LLMs. We propose a progressive simplification method (ProgDS) by\nhierarchically decomposing the task, including the discourse-level,\ntopic-level, and lexical-level simplification. Experimental results demonstrate\nthat ProgDS significantly outperforms existing smaller models or direct\nprompting with LLMs, advancing the state-of-the-art in the document\nsimplification task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on text simplification has primarily focused on lexical and\nsentence-level changes. Long document-level simplification (DS) is still\nrelatively unexplored. Large Language Models (LLMs), like ChatGPT, have\nexcelled in many natural language processing tasks. However, their performance\non DS tasks is unsatisfactory, as they often treat DS as merely document\nsummarization. For the DS task, the generated long sequences not only must\nmaintain consistency with the original document throughout, but complete\nmoderate simplification operations encompassing discourses, sentences, and\nword-level simplifications. Human editors employ a hierarchical complexity\nsimplification strategy to simplify documents. This study delves into\nsimulating this strategy through the utilization of a multi-stage collaboration\nusing LLMs. We propose a progressive simplification method (ProgDS) by\nhierarchically decomposing the task, including the discourse-level,\ntopic-level, and lexical-level simplification. Experimental results demonstrate\nthat ProgDS significantly outperforms existing smaller models or direct\nprompting with LLMs, advancing the state-of-the-art in the document\nsimplification task."
                },
                "authors": [
                    {
                        "name": "Dengzhao Fang"
                    },
                    {
                        "name": "Jipeng Qiang"
                    },
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Yunhao Yuan"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Liu"
                },
                "author": "Yan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15460v3",
                "updated": "2025-01-07T14:56:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    56,
                    42,
                    1,
                    7,
                    0
                ],
                "published": "2024-10-20T18:18:23Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    18,
                    18,
                    23,
                    6,
                    294,
                    0
                ],
                "title": "Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model\n  Training"
                },
                "summary": "As large language models (LLMs) are increasingly deployed across various\nindustries, concerns regarding their reliability, particularly due to\nhallucinations - outputs that are factually inaccurate or irrelevant to user\ninput - have grown. Our research investigates the relationship between the\ntraining process and the emergence of hallucinations to address a key gap in\nexisting research that focuses primarily on post hoc detection and mitigation\nstrategies. Using models from the Pythia suite (70M - 12B parameters) and\nseveral hallucination detection metrics, we analyze hallucination trends\nthroughout training and explore LLM internal dynamics. We introduce Sensitivity\nDropout (SenD), a novel training protocol designed to mitigate hallucinations\nby reducing variance during training. SenD achieves this by deterministically\ndropping embedding indices with significant variability, referred to as\nSensitive Embedding Indices. In addition, we develop an unsupervised\nhallucination detection metric, Efficient EigenScore (EES), which approximates\nthe traditional EigenScore at 2x speed. This efficient metric is integrated\ninto our protocol, allowing SenD to be both computationally scalable and\neffective at reducing hallucinations. Our empirical evaluation demonstrates\nthat our approach improves LLM reliability at test time by up to 40% compared\nto normal training while also providing an efficient method to improve factual\naccuracy when adapting LLMs to Wikipedia, Medical, and LegalBench domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed across various\nindustries, concerns regarding their reliability, particularly due to\nhallucinations - outputs that are factually inaccurate or irrelevant to user\ninput - have grown. Our research investigates the relationship between the\ntraining process and the emergence of hallucinations to address a key gap in\nexisting research that focuses primarily on post hoc detection and mitigation\nstrategies. Using models from the Pythia suite (70M - 12B parameters) and\nseveral hallucination detection metrics, we analyze hallucination trends\nthroughout training and explore LLM internal dynamics. We introduce Sensitivity\nDropout (SenD), a novel training protocol designed to mitigate hallucinations\nby reducing variance during training. SenD achieves this by deterministically\ndropping embedding indices with significant variability, referred to as\nSensitive Embedding Indices. In addition, we develop an unsupervised\nhallucination detection metric, Efficient EigenScore (EES), which approximates\nthe traditional EigenScore at 2x speed. This efficient metric is integrated\ninto our protocol, allowing SenD to be both computationally scalable and\neffective at reducing hallucinations. Our empirical evaluation demonstrates\nthat our approach improves LLM reliability at test time by up to 40% compared\nto normal training while also providing an efficient method to improve factual\naccuracy when adapting LLMs to Wikipedia, Medical, and LegalBench domains."
                },
                "authors": [
                    {
                        "name": "Shahrad Mohammadzadeh"
                    },
                    {
                        "name": "Juan David Guerra"
                    },
                    {
                        "name": "Marco Bonizzato"
                    },
                    {
                        "name": "Reihaneh Rabbany"
                    },
                    {
                        "name": "Golnoosh Farnadi"
                    }
                ],
                "author_detail": {
                    "name": "Golnoosh Farnadi"
                },
                "author": "Golnoosh Farnadi",
                "arxiv_comment": "23 pages, 15 figures, under review at ICLR, accepted to Safe\n  Generative AI Workshop @ NeurIPS 2024, resubmitting to change name to\n  appropriate name",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03835v1",
                "updated": "2025-01-07T14:45:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    45,
                    30,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T14:45:30Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    45,
                    30,
                    1,
                    7,
                    0
                ],
                "title": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial\n  Product Attribute Value Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial\n  Product Attribute Value Identification"
                },
                "summary": "Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendations, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity to the item embedding. It leverages contrastive training with\ntaxonomy-aware hard negative sampling and employs adaptive inference with\ndynamic thresholds. TACLR offers three key advantages: (1) it effectively\nhandles implicit and OOD values while producing normalized outputs; (2) it\nscales to thousands of categories, tens of thousands of attributes, and\nmillions of values; and (3) it supports efficient inference for high-load\nindustrial scenarios. Extensive experiments on proprietary and public datasets\nvalidate the effectiveness and efficiency of TACLR. Moreover, it has been\nsuccessfully deployed in a real-world e-commerce platform, processing millions\nof product listings daily while supporting dynamic, large-scale attribute\ntaxonomies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendations, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity to the item embedding. It leverages contrastive training with\ntaxonomy-aware hard negative sampling and employs adaptive inference with\ndynamic thresholds. TACLR offers three key advantages: (1) it effectively\nhandles implicit and OOD values while producing normalized outputs; (2) it\nscales to thousands of categories, tens of thousands of attributes, and\nmillions of values; and (3) it supports efficient inference for high-load\nindustrial scenarios. Extensive experiments on proprietary and public datasets\nvalidate the effectiveness and efficiency of TACLR. Moreover, it has been\nsuccessfully deployed in a real-world e-commerce platform, processing millions\nof product listings daily while supporting dynamic, large-scale attribute\ntaxonomies."
                },
                "authors": [
                    {
                        "name": "Yindu Su"
                    },
                    {
                        "name": "Huike Zou"
                    },
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Haiyang Yang"
                    },
                    {
                        "name": "Liyu Chen"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Qingheng Zhang"
                    },
                    {
                        "name": "Shuguang Han"
                    },
                    {
                        "name": "Jufeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jufeng Chen"
                },
                "author": "Jufeng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12968v2",
                "updated": "2025-01-07T14:45:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    45,
                    4,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-17T14:53:38Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    53,
                    38,
                    1,
                    352,
                    0
                ],
                "title": "On Local Overfitting and Forgetting in Deep Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Local Overfitting and Forgetting in Deep Neural Networks"
                },
                "summary": "The infrequent occurrence of overfitting in deep neural networks is\nperplexing: contrary to theoretical expectations, increasing model size often\nenhances performance in practice. But what if overfitting does occur, though\nrestricted to specific sub-regions of the data space? In this work, we propose\na novel score that captures the forgetting rate of deep models on validation\ndata. We posit that this score quantifies local overfitting: a decline in\nperformance confined to certain regions of the data space. We then show\nempirically that local overfitting occurs regardless of the presence of\ntraditional overfitting. Using the framework of deep over-parametrized linear\nmodels, we offer a certain theoretical characterization of forgotten knowledge,\nand show that it correlates with knowledge forgotten by real deep models.\nFinally, we devise a new ensemble method that aims to recover forgotten\nknowledge, relying solely on the training history of a single network. When\ncombined with self-distillation, this method enhances the performance of any\ntrained model without adding inference costs. Extensive empirical evaluations\ndemonstrate the efficacy of our method across multiple datasets, contemporary\nneural network architectures, and training protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The infrequent occurrence of overfitting in deep neural networks is\nperplexing: contrary to theoretical expectations, increasing model size often\nenhances performance in practice. But what if overfitting does occur, though\nrestricted to specific sub-regions of the data space? In this work, we propose\na novel score that captures the forgetting rate of deep models on validation\ndata. We posit that this score quantifies local overfitting: a decline in\nperformance confined to certain regions of the data space. We then show\nempirically that local overfitting occurs regardless of the presence of\ntraditional overfitting. Using the framework of deep over-parametrized linear\nmodels, we offer a certain theoretical characterization of forgotten knowledge,\nand show that it correlates with knowledge forgotten by real deep models.\nFinally, we devise a new ensemble method that aims to recover forgotten\nknowledge, relying solely on the training history of a single network. When\ncombined with self-distillation, this method enhances the performance of any\ntrained model without adding inference costs. Extensive empirical evaluations\ndemonstrate the efficacy of our method across multiple datasets, contemporary\nneural network architectures, and training protocols."
                },
                "authors": [
                    {
                        "name": "Uri Stern"
                    },
                    {
                        "name": "Tomer Yaacoby"
                    },
                    {
                        "name": "Daphna Weinshall"
                    }
                ],
                "author_detail": {
                    "name": "Daphna Weinshall"
                },
                "author": "Daphna Weinshall",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2310.11094",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03825v1",
                "updated": "2025-01-07T14:37:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    37,
                    14,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T14:37:14Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    37,
                    14,
                    1,
                    7,
                    0
                ],
                "title": "Deep Sylvester Posterior Inference for Adaptive Compressed Sensing in\n  Ultrasound Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Sylvester Posterior Inference for Adaptive Compressed Sensing in\n  Ultrasound Imaging"
                },
                "summary": "Ultrasound images are commonly formed by sequential acquisition of\nbeam-steered scan-lines. Minimizing the number of required scan-lines can\nsignificantly enhance frame rate, field of view, energy efficiency, and data\ntransfer speeds. Existing approaches typically use static subsampling schemes\nin combination with sparsity-based or, more recently, deep-learning-based\nrecovery. In this work, we introduce an adaptive subsampling method that\nmaximizes intrinsic information gain in-situ, employing a Sylvester Normalizing\nFlow encoder to infer an approximate Bayesian posterior under partial\nobservation in real-time. Using the Bayesian posterior and a deep generative\nmodel for future observations, we determine the subsampling scheme that\nmaximizes the mutual information between the subsampled observations, and the\nnext frame of the video. We evaluate our approach using the EchoNet cardiac\nultrasound video dataset and demonstrate that our active sampling method\noutperforms competitive baselines, including uniform and variable-density\nrandom sampling, as well as equidistantly spaced scan-lines, improving mean\nabsolute reconstruction error by 15%. Moreover, posterior inference and the\nsampling scheme generation are performed in just 0.015 seconds (66Hz), making\nit fast enough for real-time 2D ultrasound imaging applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrasound images are commonly formed by sequential acquisition of\nbeam-steered scan-lines. Minimizing the number of required scan-lines can\nsignificantly enhance frame rate, field of view, energy efficiency, and data\ntransfer speeds. Existing approaches typically use static subsampling schemes\nin combination with sparsity-based or, more recently, deep-learning-based\nrecovery. In this work, we introduce an adaptive subsampling method that\nmaximizes intrinsic information gain in-situ, employing a Sylvester Normalizing\nFlow encoder to infer an approximate Bayesian posterior under partial\nobservation in real-time. Using the Bayesian posterior and a deep generative\nmodel for future observations, we determine the subsampling scheme that\nmaximizes the mutual information between the subsampled observations, and the\nnext frame of the video. We evaluate our approach using the EchoNet cardiac\nultrasound video dataset and demonstrate that our active sampling method\noutperforms competitive baselines, including uniform and variable-density\nrandom sampling, as well as equidistantly spaced scan-lines, improving mean\nabsolute reconstruction error by 15%. Moreover, posterior inference and the\nsampling scheme generation are performed in just 0.015 seconds (66Hz), making\nit fast enough for real-time 2D ultrasound imaging applications."
                },
                "authors": [
                    {
                        "name": "Simon W. Penninga"
                    },
                    {
                        "name": "Hans van Gorp"
                    },
                    {
                        "name": "Ruud J. G. van Sloun"
                    }
                ],
                "author_detail": {
                    "name": "Ruud J. G. van Sloun"
                },
                "author": "Ruud J. G. van Sloun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03820v1",
                "updated": "2025-01-07T14:35:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    35,
                    3,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T14:35:03Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    35,
                    3,
                    1,
                    7,
                    0
                ],
                "title": "Reconstructing ecological community dynamics from limited observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing ecological community dynamics from limited observations"
                },
                "summary": "Ecosystems tend to fluctuate around stable equilibria in response to internal\ndynamics and environmental factors. Occasionally, they enter an unstable\ntipping region and collapse into an alternative stable state. Our understanding\nof how ecological communities vary over time and respond to perturbations\ndepends on our ability to quantify and predict these dynamics. However, the\nscarcity of long, dense time series data poses a severe bottleneck for\ncharacterising community dynamics using existing methods. We overcome this\nlimitation by combining information across multiple short time series using\nBayesian inference. By decomposing dynamics into deterministic and stochastic\ncomponents using Gaussian process priors, we predict stable and tipping regions\nalong the community landscape and quantify resilience while addressing\nuncertainty. After validation with simulated and real ecological time series,\nwe use the model to question common assumptions underlying classical potential\nanalysis and re-evaluate the stability of previously proposed \"tipping\nelements\" in the human gut microbiota.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ecosystems tend to fluctuate around stable equilibria in response to internal\ndynamics and environmental factors. Occasionally, they enter an unstable\ntipping region and collapse into an alternative stable state. Our understanding\nof how ecological communities vary over time and respond to perturbations\ndepends on our ability to quantify and predict these dynamics. However, the\nscarcity of long, dense time series data poses a severe bottleneck for\ncharacterising community dynamics using existing methods. We overcome this\nlimitation by combining information across multiple short time series using\nBayesian inference. By decomposing dynamics into deterministic and stochastic\ncomponents using Gaussian process priors, we predict stable and tipping regions\nalong the community landscape and quantify resilience while addressing\nuncertainty. After validation with simulated and real ecological time series,\nwe use the model to question common assumptions underlying classical potential\nanalysis and re-evaluate the stability of previously proposed \"tipping\nelements\" in the human gut microbiota."
                },
                "authors": [
                    {
                        "name": "Chandler Ross"
                    },
                    {
                        "name": "Ville Laitinen"
                    },
                    {
                        "name": "Moein Khalighi"
                    },
                    {
                        "name": "Jarkko Salojärvi"
                    },
                    {
                        "name": "Willem de Vos"
                    },
                    {
                        "name": "Guilhem Sommeria-Klein"
                    },
                    {
                        "name": "Leo Lahti"
                    }
                ],
                "author_detail": {
                    "name": "Leo Lahti"
                },
                "author": "Leo Lahti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03813v1",
                "updated": "2025-01-07T14:25:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    25,
                    53,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T14:25:53Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    25,
                    53,
                    1,
                    7,
                    0
                ],
                "title": "High-cadence stellar variability studies of RR Lyrae stars with DECam:\n  New multi-band templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-cadence stellar variability studies of RR Lyrae stars with DECam:\n  New multi-band templates"
                },
                "summary": "We present the most extensive set to date of high-quality RR Lyrae light\ncurve templates in the griz bands, based on time-series observations of the\nDark Energy Camera Plane Survey (DECaPS) East field, located in the Galactic\nbulge at coordinates (RA, DEC)(J2000) = (18:03:34, -29:32:02), obtained with\nthe Dark Energy Camera (DECam) on the 4-m Blanco telescope at the Cerro Tololo\nInter-American Observatory (CTIO). Our templates, which cover both\nfundamental-mode (RRab) and first-overtone (RRc) pulsators, can be especially\nuseful when there is insufficient data for accurately calculating the average\nmagnitudes and colors, hence distances, as well as to inform multi-band light\ncurve classifiers, as will be required in the case of the Vera C. Rubin\nObservatory's Legacy Survey of Space and Time (LSST). In this paper, we\ndescribe in detail the procedures that were adopted in producing these\ntemplates, including a novel approach to account for the presence of outliers\nin photometry. Our final sample comprises 136 RRab and 144 RRc templates, all\nof which are publicly available. Lastly, in this paper we study the inferred\nFourier parameters and other light curve descriptors, including rise\ntime,skewness, and kurtosis, as well as their correlations with the pulsation\nmode, period, and effective wavelength.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the most extensive set to date of high-quality RR Lyrae light\ncurve templates in the griz bands, based on time-series observations of the\nDark Energy Camera Plane Survey (DECaPS) East field, located in the Galactic\nbulge at coordinates (RA, DEC)(J2000) = (18:03:34, -29:32:02), obtained with\nthe Dark Energy Camera (DECam) on the 4-m Blanco telescope at the Cerro Tololo\nInter-American Observatory (CTIO). Our templates, which cover both\nfundamental-mode (RRab) and first-overtone (RRc) pulsators, can be especially\nuseful when there is insufficient data for accurately calculating the average\nmagnitudes and colors, hence distances, as well as to inform multi-band light\ncurve classifiers, as will be required in the case of the Vera C. Rubin\nObservatory's Legacy Survey of Space and Time (LSST). In this paper, we\ndescribe in detail the procedures that were adopted in producing these\ntemplates, including a novel approach to account for the presence of outliers\nin photometry. Our final sample comprises 136 RRab and 144 RRc templates, all\nof which are publicly available. Lastly, in this paper we study the inferred\nFourier parameters and other light curve descriptors, including rise\ntime,skewness, and kurtosis, as well as their correlations with the pulsation\nmode, period, and effective wavelength."
                },
                "authors": [
                    {
                        "name": "K. Baeza-Villagra"
                    },
                    {
                        "name": "N. Rodriguez-Segovia"
                    },
                    {
                        "name": "M. Catelan"
                    },
                    {
                        "name": "A. Rest"
                    },
                    {
                        "name": "A. Papageorgiou"
                    },
                    {
                        "name": "C. E. Martinez-Vazquez"
                    },
                    {
                        "name": "A. A. R. Valcarce"
                    },
                    {
                        "name": "C. E. Ferreira Lopes"
                    },
                    {
                        "name": "F. B. Bianco"
                    }
                ],
                "author_detail": {
                    "name": "F. B. Bianco"
                },
                "author": "F. B. Bianco",
                "arxiv_comment": "19 pages, 17 figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03810v1",
                "updated": "2025-01-07T14:23:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    23,
                    7,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T14:23:07Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    23,
                    7,
                    1,
                    7,
                    0
                ],
                "title": "Concentration of Empirical First-Passage Times",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concentration of Empirical First-Passage Times"
                },
                "summary": "First-passage properties are central to the kinetics of target-search\nprocesses. Theoretical approaches so far primarily focused on predicting\nfirst-passage statistics for a given process or model. In practice, however,\none faces the reverse problem of inferring first-passage statistics from,\ntypically sub-sampled, experimental or simulation data. Obtaining trustworthy\nestimates from under-sampled data and unknown underlying dynamics remains a\ndaunting task, and the assessment of the uncertainty is imperative. In this\nchapter, we highlight recent progress in understanding and controlling\nfinite-sample effects in empirical first-passage times of reversible Markov\nprocesses. Precisely, we present concentration inequalities bounding from above\nthe deviations of the sample mean for any sample size from the true mean\nfirst-passage time and construct non-asymptotic confidence intervals. Moreover,\nwe present two-sided bounds on the range of fluctuations, i.e, deviations of\nthe expected maximum and minimum from the mean in any given sample, which\ncontrol uncertainty even in situations where the mean is a priori not a\nsufficient statistic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First-passage properties are central to the kinetics of target-search\nprocesses. Theoretical approaches so far primarily focused on predicting\nfirst-passage statistics for a given process or model. In practice, however,\none faces the reverse problem of inferring first-passage statistics from,\ntypically sub-sampled, experimental or simulation data. Obtaining trustworthy\nestimates from under-sampled data and unknown underlying dynamics remains a\ndaunting task, and the assessment of the uncertainty is imperative. In this\nchapter, we highlight recent progress in understanding and controlling\nfinite-sample effects in empirical first-passage times of reversible Markov\nprocesses. Precisely, we present concentration inequalities bounding from above\nthe deviations of the sample mean for any sample size from the true mean\nfirst-passage time and construct non-asymptotic confidence intervals. Moreover,\nwe present two-sided bounds on the range of fluctuations, i.e, deviations of\nthe expected maximum and minimum from the mean in any given sample, which\ncontrol uncertainty even in situations where the mean is a priori not a\nsufficient statistic."
                },
                "authors": [
                    {
                        "name": "Rick Bebon"
                    },
                    {
                        "name": "Aljaz Godec"
                    }
                ],
                "author_detail": {
                    "name": "Aljaz Godec"
                },
                "author": "Aljaz Godec",
                "arxiv_doi": "10.1007/978-3-031-67802-8_2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-67802-8_2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.03810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 5 figures, Chapter 2 in \"Target Search Problems\" edited by\n  D. Grebenkov, R. Metzler, Gleb Oshanin (Springer Nature Switzerland, 2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01460v2",
                "updated": "2025-01-07T14:19:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    19,
                    35,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-31T10:43:19Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    10,
                    43,
                    19,
                    1,
                    366,
                    0
                ],
                "title": "GDSR: Global-Detail Integration through Dual-Branch Network with Wavelet\n  Losses for Remote Sensing Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GDSR: Global-Detail Integration through Dual-Branch Network with Wavelet\n  Losses for Remote Sensing Image Super-Resolution"
                },
                "summary": "In recent years, deep neural networks, including Convolutional Neural\nNetworks, Transformers, and State Space Models, have achieved significant\nprogress in Remote Sensing Image (RSI) Super-Resolution (SR). However, existing\nSR methods typically overlook the complementary relationship between global and\nlocal dependencies. These methods either focus on capturing local information\nor prioritize global information, which results in models that are unable to\neffectively capture both global and local features simultaneously. Moreover,\ntheir computational cost becomes prohibitive when applied to large-scale RSIs.\nTo address these challenges, we introduce the novel application of Receptance\nWeighted Key Value (RWKV) to RSI-SR, which captures long-range dependencies\nwith linear complexity. To simultaneously model global and local features, we\npropose the Global-Detail dual-branch structure, GDSR, which performs SR\nreconstruction by paralleling RWKV and convolutional operations to handle\nlarge-scale RSIs. Furthermore, we introduce the Global-Detail Reconstruction\nModule (GDRM) as an intermediary between the two branches to bridge their\ncomplementary roles. In addition, we propose Wavelet Loss, a loss function that\neffectively captures high-frequency detail information in images, thereby\nenhancing the visual quality of SR, particularly in terms of detail\nreconstruction. Extensive experiments on several benchmarks, including AID,\nAID_CDM, RSSRD-QH, and RSSRD-QH_CDM, demonstrate that GSDR outperforms the\nstate-of-the-art Transformer-based method HAT by an average of 0.05 dB in PSNR,\nwhile using only 63% of its parameters and 51% of its FLOPs, achieving an\ninference speed 2.9 times faster. Furthermore, the Wavelet Loss shows excellent\ngeneralization across various architectures, providing a novel perspective for\nRSI-SR enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, deep neural networks, including Convolutional Neural\nNetworks, Transformers, and State Space Models, have achieved significant\nprogress in Remote Sensing Image (RSI) Super-Resolution (SR). However, existing\nSR methods typically overlook the complementary relationship between global and\nlocal dependencies. These methods either focus on capturing local information\nor prioritize global information, which results in models that are unable to\neffectively capture both global and local features simultaneously. Moreover,\ntheir computational cost becomes prohibitive when applied to large-scale RSIs.\nTo address these challenges, we introduce the novel application of Receptance\nWeighted Key Value (RWKV) to RSI-SR, which captures long-range dependencies\nwith linear complexity. To simultaneously model global and local features, we\npropose the Global-Detail dual-branch structure, GDSR, which performs SR\nreconstruction by paralleling RWKV and convolutional operations to handle\nlarge-scale RSIs. Furthermore, we introduce the Global-Detail Reconstruction\nModule (GDRM) as an intermediary between the two branches to bridge their\ncomplementary roles. In addition, we propose Wavelet Loss, a loss function that\neffectively captures high-frequency detail information in images, thereby\nenhancing the visual quality of SR, particularly in terms of detail\nreconstruction. Extensive experiments on several benchmarks, including AID,\nAID_CDM, RSSRD-QH, and RSSRD-QH_CDM, demonstrate that GSDR outperforms the\nstate-of-the-art Transformer-based method HAT by an average of 0.05 dB in PSNR,\nwhile using only 63% of its parameters and 51% of its FLOPs, achieving an\ninference speed 2.9 times faster. Furthermore, the Wavelet Loss shows excellent\ngeneralization across various architectures, providing a novel perspective for\nRSI-SR enhancement."
                },
                "authors": [
                    {
                        "name": "Qiwei Zhu"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Guojing Zhang"
                    },
                    {
                        "name": "Xiaoying Wang"
                    },
                    {
                        "name": "Jianqiang Huang"
                    },
                    {
                        "name": "Xilai Li"
                    }
                ],
                "author_detail": {
                    "name": "Xilai Li"
                },
                "author": "Xilai Li",
                "arxiv_comment": "The experiments were conducted using private datasets that were\n  incomplete as they did not include all the necessary copyrights.\n  Additionally, the conclusions require further exploration as the work is\n  still in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10486v2",
                "updated": "2025-01-07T14:09:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    9,
                    22,
                    1,
                    7,
                    0
                ],
                "published": "2024-07-15T07:14:56Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    7,
                    14,
                    56,
                    0,
                    197,
                    0
                ],
                "title": "IDEAL: Leveraging Infinite and Dynamic Characterizations of Large\n  Language Models for Query-focused Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IDEAL: Leveraging Infinite and Dynamic Characterizations of Large\n  Language Models for Query-focused Summarization"
                },
                "summary": "Query-focused summarization (QFS) aims to produce summaries that answer\nparticular questions of interest, enabling greater user control and\npersonalization. With the advent of large language models (LLMs), shows their\nimpressive capability of textual understanding through large-scale pretraining,\nwhich implies the great potential of extractive snippet generation. In this\npaper, we systematically investigated two indispensable characteristics that\nthe LLMs-based QFS models should be harnessed, Lengthy Document Summarization\nand Efficiently Fine-grained Query-LLM Alignment, respectively.\nCorrespondingly, we propose two modules called Query-aware HyperExpert and\nQuery-focused Infini-attention to access the aforementioned characteristics.\nThese innovations pave the way for broader application and accessibility in the\nfield of QFS technology. Extensive experiments conducted on existing QFS\nbenchmarks indicate the effectiveness and generalizability of the proposed\napproach. Our code is publicly available at\nhttps://github.com/DCDmllm/IDEAL_Summary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-focused summarization (QFS) aims to produce summaries that answer\nparticular questions of interest, enabling greater user control and\npersonalization. With the advent of large language models (LLMs), shows their\nimpressive capability of textual understanding through large-scale pretraining,\nwhich implies the great potential of extractive snippet generation. In this\npaper, we systematically investigated two indispensable characteristics that\nthe LLMs-based QFS models should be harnessed, Lengthy Document Summarization\nand Efficiently Fine-grained Query-LLM Alignment, respectively.\nCorrespondingly, we propose two modules called Query-aware HyperExpert and\nQuery-focused Infini-attention to access the aforementioned characteristics.\nThese innovations pave the way for broader application and accessibility in the\nfield of QFS technology. Extensive experiments conducted on existing QFS\nbenchmarks indicate the effectiveness and generalizability of the proposed\napproach. Our code is publicly available at\nhttps://github.com/DCDmllm/IDEAL_Summary."
                },
                "authors": [
                    {
                        "name": "Jie Cao"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Qiang Yan"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19153v3",
                "updated": "2025-01-07T13:41:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    13,
                    41,
                    26,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-26T10:17:21Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    10,
                    17,
                    21,
                    3,
                    361,
                    0
                ],
                "title": "Sketch-MoMa: Teleoperation for Mobile Manipulator via Interpretation of\n  Hand-Drawn Sketches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sketch-MoMa: Teleoperation for Mobile Manipulator via Interpretation of\n  Hand-Drawn Sketches"
                },
                "summary": "To use assistive robots in everyday life, a remote control system with common\ndevices, such as 2D devices, is helpful to control the robots anytime and\nanywhere as intended. Hand-drawn sketches are one of the intuitive ways to\ncontrol robots with 2D devices. However, since similar sketches have different\nintentions from scene to scene, existing work needs additional modalities to\nset the sketches' semantics. This requires complex operations for users and\nleads to decreasing usability. In this paper, we propose Sketch-MoMa, a\nteleoperation system using the user-given hand-drawn sketches as instructions\nto control a robot. We use Vision-Language Models (VLMs) to understand the\nuser-given sketches superimposed on an observation image and infer drawn shapes\nand low-level tasks of the robot. We utilize the sketches and the generated\nshapes for recognition and motion planning of the generated low-level tasks for\nprecise and intuitive operations. We validate our approach using\nstate-of-the-art VLMs with 7 tasks and 5 sketch shapes. We also demonstrate\nthat our approach effectively specifies the detailed motions, such as how to\ngrasp and how much to rotate. Moreover, we show the competitive usability of\nour approach compared with the existing 2D interface through a user experiment\nwith 14 participants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To use assistive robots in everyday life, a remote control system with common\ndevices, such as 2D devices, is helpful to control the robots anytime and\nanywhere as intended. Hand-drawn sketches are one of the intuitive ways to\ncontrol robots with 2D devices. However, since similar sketches have different\nintentions from scene to scene, existing work needs additional modalities to\nset the sketches' semantics. This requires complex operations for users and\nleads to decreasing usability. In this paper, we propose Sketch-MoMa, a\nteleoperation system using the user-given hand-drawn sketches as instructions\nto control a robot. We use Vision-Language Models (VLMs) to understand the\nuser-given sketches superimposed on an observation image and infer drawn shapes\nand low-level tasks of the robot. We utilize the sketches and the generated\nshapes for recognition and motion planning of the generated low-level tasks for\nprecise and intuitive operations. We validate our approach using\nstate-of-the-art VLMs with 7 tasks and 5 sketch shapes. We also demonstrate\nthat our approach effectively specifies the detailed motions, such as how to\ngrasp and how much to rotate. Moreover, we show the competitive usability of\nour approach compared with the existing 2D interface through a user experiment\nwith 14 participants."
                },
                "authors": [
                    {
                        "name": "Kosei Tanada"
                    },
                    {
                        "name": "Yuka Iwanaga"
                    },
                    {
                        "name": "Masayoshi Tsuchinaga"
                    },
                    {
                        "name": "Yuji Nakamura"
                    },
                    {
                        "name": "Takemitsu Mori"
                    },
                    {
                        "name": "Remi Sakai"
                    },
                    {
                        "name": "Takashi Yamamoto"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Yamamoto"
                },
                "author": "Takashi Yamamoto",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Project Page: https://toyotafrc.github.io/SketchMoMa-Proj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11805v2",
                "updated": "2025-01-07T13:34:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    13,
                    34,
                    6,
                    1,
                    7,
                    0
                ],
                "published": "2024-10-15T17:33:43Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    33,
                    43,
                    1,
                    289,
                    0
                ],
                "title": "NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) combined with tool learning have gained\nimpressive results in real-world applications. During tool learning, LLMs may\ncall multiple tools in nested orders, where the latter tool call may take the\nformer response as its input parameters. However, current research on the\nnested tool learning capabilities is still under-explored, since the existing\nbenchmarks lack relevant data instances. To address this problem, we introduce\nNesTools to bridge the current gap in comprehensive nested tool learning\nevaluations. NesTools comprises a novel automatic data generation method to\nconstruct large-scale nested tool calls with different nesting structures. With\nmanual review and refinement, the dataset is in high quality and closely\naligned with real-world scenarios. Therefore, NesTools can serve as a new\nbenchmark to evaluate the nested tool learning abilities of LLMs. We conduct\nextensive experiments on 22 LLMs, and provide in-depth analyses with NesTools,\nwhich shows that current LLMs still suffer from the complex nested tool\nlearning task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) combined with tool learning have gained\nimpressive results in real-world applications. During tool learning, LLMs may\ncall multiple tools in nested orders, where the latter tool call may take the\nformer response as its input parameters. However, current research on the\nnested tool learning capabilities is still under-explored, since the existing\nbenchmarks lack relevant data instances. To address this problem, we introduce\nNesTools to bridge the current gap in comprehensive nested tool learning\nevaluations. NesTools comprises a novel automatic data generation method to\nconstruct large-scale nested tool calls with different nesting structures. With\nmanual review and refinement, the dataset is in high quality and closely\naligned with real-world scenarios. Therefore, NesTools can serve as a new\nbenchmark to evaluate the nested tool learning abilities of LLMs. We conduct\nextensive experiments on 22 LLMs, and provide in-depth analyses with NesTools,\nwhich shows that current LLMs still suffer from the complex nested tool\nlearning task."
                },
                "authors": [
                    {
                        "name": "Han Han"
                    },
                    {
                        "name": "Tong Zhu"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Mengsong Wu"
                    },
                    {
                        "name": "Hao Xiong"
                    },
                    {
                        "name": "Wenliang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenliang Chen"
                },
                "author": "Wenliang Chen",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04460v3",
                "updated": "2025-01-07T13:31:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    13,
                    31,
                    23,
                    1,
                    7,
                    0
                ],
                "published": "2024-04-06T01:06:25Z",
                "published_parsed": [
                    2024,
                    4,
                    6,
                    1,
                    6,
                    25,
                    5,
                    97,
                    0
                ],
                "title": "Transdimensional inference for gravitational-wave astronomy with Bilby",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transdimensional inference for gravitational-wave astronomy with Bilby"
                },
                "summary": "It has become increasingly useful to answer questions in gravitational-wave\nastronomy using transdimensional models where the number of free parameters can\nbe varied depending on the complexity required to fit the data. Given the\ngrowing interest in transdimensional inference, we introduce a new package for\nthe Bayesian inference Library (Bilby) called tBilby. The tBilby package allows\nusers to set up transdimensional inference calculations using the existing\nBilby architecture with off-the-shelf nested samplers and/or Markov Chain Monte\nCarlo algorithms. Transdimensional models are particularly helpful when we seek\nto test theoretically uncertain predictions described by phenomenological\nmodels. For example, bursts of gravitational waves can be modelled using a\nsuperposition of N wavelets where N is itself a free parameter. Short pulses\nare modelled with small values of N whereas longer, more complicated signals\nare represented with a large number of wavelets stitched together. Other\ntransdimensional models have found use describing instrumental noise and the\npopulation properties of gravitational-wave sources. We provide a few\ndemonstrations of tBilby, including fitting the gravitational-wave signal\nGW150914 with a superposition of N sine-Gaussian wavelets. We outline our plans\nto further develop the tbilby code suite for a broader range of\ntransdimensional problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has become increasingly useful to answer questions in gravitational-wave\nastronomy using transdimensional models where the number of free parameters can\nbe varied depending on the complexity required to fit the data. Given the\ngrowing interest in transdimensional inference, we introduce a new package for\nthe Bayesian inference Library (Bilby) called tBilby. The tBilby package allows\nusers to set up transdimensional inference calculations using the existing\nBilby architecture with off-the-shelf nested samplers and/or Markov Chain Monte\nCarlo algorithms. Transdimensional models are particularly helpful when we seek\nto test theoretically uncertain predictions described by phenomenological\nmodels. For example, bursts of gravitational waves can be modelled using a\nsuperposition of N wavelets where N is itself a free parameter. Short pulses\nare modelled with small values of N whereas longer, more complicated signals\nare represented with a large number of wavelets stitched together. Other\ntransdimensional models have found use describing instrumental noise and the\npopulation properties of gravitational-wave sources. We provide a few\ndemonstrations of tBilby, including fitting the gravitational-wave signal\nGW150914 with a superposition of N sine-Gaussian wavelets. We outline our plans\nto further develop the tbilby code suite for a broader range of\ntransdimensional problems."
                },
                "authors": [
                    {
                        "name": "Hui Tong"
                    },
                    {
                        "name": "Nir Guttman"
                    },
                    {
                        "name": "Teagan A. Clarke"
                    },
                    {
                        "name": "Paul D. Lasky"
                    },
                    {
                        "name": "Eric Thrane"
                    },
                    {
                        "name": "Ethan Payne"
                    },
                    {
                        "name": "Rowina Nathan"
                    },
                    {
                        "name": "Ben Farr"
                    },
                    {
                        "name": "Maya Fishbach"
                    },
                    {
                        "name": "Gregory Ashton"
                    },
                    {
                        "name": "Valentina Di Marco"
                    }
                ],
                "author_detail": {
                    "name": "Valentina Di Marco"
                },
                "author": "Valentina Di Marco",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.00546v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.00546v3",
                "updated": "2025-01-07T13:31:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    13,
                    31,
                    1,
                    1,
                    7,
                    0
                ],
                "published": "2023-12-31T17:21:02Z",
                "published_parsed": [
                    2023,
                    12,
                    31,
                    17,
                    21,
                    2,
                    6,
                    365,
                    0
                ],
                "title": "AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with\n  Ten Modalities via Language as a Reference Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with\n  Ten Modalities via Language as a Reference Framework"
                },
                "summary": "Leveraging multimodal data is an inherent requirement for comprehending\ngeographic objects. However, due to the high heterogeneity in structure and\nsemantics among various spatio-temporal modalities, the joint interpretation of\nmultimodal spatio-temporal data has long been an extremely challenging problem.\nThe primary challenge resides in striking a trade-off between the cohesion and\nautonomy of diverse modalities. This trade-off becomes progressively nonlinear\nas the number of modalities expands. Inspired by the human cognitive system and\nlinguistic philosophy, where perceptual signals from the five senses converge\ninto language, we introduce the Language as Reference Framework (LaRF), a\nfundamental principle for constructing a multimodal unified model. Building\nupon this, we propose AllSpark, a multimodal spatio-temporal general artificial\nintelligence model. Our model integrates ten different modalities into a\nunified framework. To achieve modal cohesion, AllSpark introduces a modal\nbridge and multimodal large language model (LLM) to map diverse modal features\ninto the language feature space. To maintain modality autonomy, AllSpark uses\nmodality-specific encoders to extract the tokens of various spatio-temporal\nmodalities. Finally, observing a gap between the model's interpretability and\ndownstream tasks, we designed modality-specific prompts and task heads,\nenhancing the model's generalization capability across specific tasks.\nExperiments indicate that the incorporation of language enables AllSpark to\nexcel in few-shot classification tasks for RGB and point cloud modalities\nwithout additional training, surpassing baseline performance by up to 41.82\\%.\nThe source code is available at https://github.com/GeoX-Lab/AllSpark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging multimodal data is an inherent requirement for comprehending\ngeographic objects. However, due to the high heterogeneity in structure and\nsemantics among various spatio-temporal modalities, the joint interpretation of\nmultimodal spatio-temporal data has long been an extremely challenging problem.\nThe primary challenge resides in striking a trade-off between the cohesion and\nautonomy of diverse modalities. This trade-off becomes progressively nonlinear\nas the number of modalities expands. Inspired by the human cognitive system and\nlinguistic philosophy, where perceptual signals from the five senses converge\ninto language, we introduce the Language as Reference Framework (LaRF), a\nfundamental principle for constructing a multimodal unified model. Building\nupon this, we propose AllSpark, a multimodal spatio-temporal general artificial\nintelligence model. Our model integrates ten different modalities into a\nunified framework. To achieve modal cohesion, AllSpark introduces a modal\nbridge and multimodal large language model (LLM) to map diverse modal features\ninto the language feature space. To maintain modality autonomy, AllSpark uses\nmodality-specific encoders to extract the tokens of various spatio-temporal\nmodalities. Finally, observing a gap between the model's interpretability and\ndownstream tasks, we designed modality-specific prompts and task heads,\nenhancing the model's generalization capability across specific tasks.\nExperiments indicate that the incorporation of language enables AllSpark to\nexcel in few-shot classification tasks for RGB and point cloud modalities\nwithout additional training, surpassing baseline performance by up to 41.82\\%.\nThe source code is available at https://github.com/GeoX-Lab/AllSpark."
                },
                "authors": [
                    {
                        "name": "Run Shao"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Qiujun Li"
                    },
                    {
                        "name": "Qing Zhu"
                    },
                    {
                        "name": "Yongjun Zhang"
                    },
                    {
                        "name": "YanSheng Li"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Yong Tang"
                    },
                    {
                        "name": "Dapeng Liu"
                    },
                    {
                        "name": "Shizhong Yang"
                    },
                    {
                        "name": "Haifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Li"
                },
                "author": "Haifeng Li",
                "arxiv_doi": "10.1109/TGRS.2025.3526725",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TGRS.2025.3526725",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.00546v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.00546v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "19 pages, 19 tables, 3 figures",
                "arxiv_journal_ref": "IEEE Transactions on Geoscience and Remote Sensing. 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15857v2",
                "updated": "2025-01-07T13:28:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    13,
                    28,
                    0,
                    1,
                    7,
                    0
                ],
                "published": "2024-07-08T06:38:50Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    6,
                    38,
                    50,
                    0,
                    190,
                    0
                ],
                "title": "BoRA: Bayesian Hierarchical Low-Rank Adaption for Multi-Task Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BoRA: Bayesian Hierarchical Low-Rank Adaption for Multi-Task Large\n  Language Models"
                },
                "summary": "This paper introduces Bayesian Hierarchical Low-Rank Adaption (BoRA), a novel\nmethod for finetuning multi-task Large Language Models (LLMs). Current\nfinetuning approaches, such as Low-Rank Adaption (LoRA), perform exeptionally\nwell in reducing training parameters and memory usage but face limitations when\napplied to multiple similar tasks. Practitioners usually have to choose between\ntraining separate models for each task or a single model for all tasks, both of\nwhich come with trade-offs in specialization and data utilization. BoRA\naddresses these trade-offs by leveraging a Bayesian hierarchical model that\nallows tasks to share information through global hierarchical priors. This\nenables tasks with limited data to benefit from the overall structure derived\nfrom related tasks while allowing tasks with more data to specialize. Our\nexperimental results show that BoRA outperforms both individual and unified\nmodel approaches, achieving lower perplexity and better generalization across\ntasks. This method provides a scalable and efficient solution for multi-task\nLLM finetuning, with significant practical implications for diverse\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Bayesian Hierarchical Low-Rank Adaption (BoRA), a novel\nmethod for finetuning multi-task Large Language Models (LLMs). Current\nfinetuning approaches, such as Low-Rank Adaption (LoRA), perform exeptionally\nwell in reducing training parameters and memory usage but face limitations when\napplied to multiple similar tasks. Practitioners usually have to choose between\ntraining separate models for each task or a single model for all tasks, both of\nwhich come with trade-offs in specialization and data utilization. BoRA\naddresses these trade-offs by leveraging a Bayesian hierarchical model that\nallows tasks to share information through global hierarchical priors. This\nenables tasks with limited data to benefit from the overall structure derived\nfrom related tasks while allowing tasks with more data to specialize. Our\nexperimental results show that BoRA outperforms both individual and unified\nmodel approaches, achieving lower perplexity and better generalization across\ntasks. This method provides a scalable and efficient solution for multi-task\nLLM finetuning, with significant practical implications for diverse\napplications."
                },
                "authors": [
                    {
                        "name": "Simen Eide"
                    },
                    {
                        "name": "Arnoldo Frigessi"
                    }
                ],
                "author_detail": {
                    "name": "Arnoldo Frigessi"
                },
                "author": "Arnoldo Frigessi",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09212v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09212v3",
                "updated": "2025-01-07T13:24:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    13,
                    24,
                    58,
                    1,
                    7,
                    0
                ],
                "published": "2024-06-13T15:15:13Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    15,
                    15,
                    13,
                    3,
                    165,
                    0
                ],
                "title": "To curve, or not to curve: Is curvature-assisted quintessence\n  observationally viable?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To curve, or not to curve: Is curvature-assisted quintessence\n  observationally viable?"
                },
                "summary": "Single-field models of accelerated expansion with nearly flat potentials,\ndespite being able to provide observationally viable explanations for the\nearly-time cosmic inflation and the late-time cosmic acceleration, are in\nstrong tension with string theory evidence and the associated de Sitter\nswampland constraints. It has recently been argued that in an open universe,\nwhere the spatial curvature is negative (i.e., with $\\Omega_k>0$), a new stable\nfixed point arises, which may lead to viable single-field-based accelerated\nexpansion with an arbitrarily steep potential. Here, we show, through a\ndynamical systems analysis and a Bayesian statistical inference of cosmological\nparameters, that the additional cosmological solutions based on the new fixed\npoint do not render steep-potential, single-field, accelerated expansion\nobservationally viable. We mainly focus on quintessence models of dark energy,\nbut we also argue that a similar conclusion can be drawn for cosmic inflation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-field models of accelerated expansion with nearly flat potentials,\ndespite being able to provide observationally viable explanations for the\nearly-time cosmic inflation and the late-time cosmic acceleration, are in\nstrong tension with string theory evidence and the associated de Sitter\nswampland constraints. It has recently been argued that in an open universe,\nwhere the spatial curvature is negative (i.e., with $\\Omega_k>0$), a new stable\nfixed point arises, which may lead to viable single-field-based accelerated\nexpansion with an arbitrarily steep potential. Here, we show, through a\ndynamical systems analysis and a Bayesian statistical inference of cosmological\nparameters, that the additional cosmological solutions based on the new fixed\npoint do not render steep-potential, single-field, accelerated expansion\nobservationally viable. We mainly focus on quintessence models of dark energy,\nbut we also argue that a similar conclusion can be drawn for cosmic inflation."
                },
                "authors": [
                    {
                        "name": "George Alestas"
                    },
                    {
                        "name": "Matilda Delgado"
                    },
                    {
                        "name": "Ignacio Ruiz"
                    },
                    {
                        "name": "Yashar Akrami"
                    },
                    {
                        "name": "Miguel Montero"
                    },
                    {
                        "name": "Savvas Nesseris"
                    }
                ],
                "author_detail": {
                    "name": "Savvas Nesseris"
                },
                "author": "Savvas Nesseris",
                "arxiv_doi": "10.1103/PhysRevD.110.106010",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.106010",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.09212v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09212v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 6 figures. v3: version published in PRD",
                "arxiv_journal_ref": "Phys. Rev. D 110, 106010 (2024)",
                "arxiv_primary_category": {
                    "term": "hep-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02893v2",
                "updated": "2025-01-07T13:21:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    13,
                    21,
                    10,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-06T10:15:21Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    10,
                    15,
                    21,
                    0,
                    6,
                    0
                ],
                "title": "A Volumetric Approach to Privacy of Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Volumetric Approach to Privacy of Dynamical Systems"
                },
                "summary": "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. For linear scalar systems, we derive an explicit formulation\nof the uncertainty set. For multi-dimensional linear systems, we develop an\napproximate computation method leveraging interval analysis. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\nthe inference attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. For linear scalar systems, we derive an explicit formulation\nof the uncertainty set. For multi-dimensional linear systems, we develop an\napproximate computation method leveraging interval analysis. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\nthe inference attack."
                },
                "authors": [
                    {
                        "name": "Chuanghong Weng"
                    },
                    {
                        "name": "Ehsan Nekouei"
                    }
                ],
                "author_detail": {
                    "name": "Ehsan Nekouei"
                },
                "author": "Ehsan Nekouei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24222v2",
                "updated": "2025-01-07T13:04:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    13,
                    4,
                    51,
                    1,
                    7,
                    0
                ],
                "published": "2024-10-31T17:59:56Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    59,
                    56,
                    3,
                    305,
                    0
                ],
                "title": "Robust Gaussian Processes via Relevance Pursuit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Gaussian Processes via Relevance Pursuit"
                },
                "summary": "Gaussian processes (GPs) are non-parametric probabilistic regression models\nthat are popular due to their flexibility, data efficiency, and well-calibrated\nuncertainty estimates. However, standard GP models assume homoskedastic\nGaussian noise, while many real-world applications are subject to non-Gaussian\ncorruptions. Variants of GPs that are more robust to alternative noise models\nhave been proposed, and entail significant trade-offs between accuracy and\nrobustness, and between computational requirements and theoretical guarantees.\nIn this work, we propose and study a GP model that achieves robustness against\nsparse outliers by inferring data-point-specific noise levels with a sequential\nselection procedure maximizing the log marginal likelihood that we refer to as\nrelevance pursuit. We show, surprisingly, that the model can be parameterized\nsuch that the associated log marginal likelihood is strongly concave in the\ndata-point-specific noise variances, a property rarely found in either robust\nregression objectives or GP marginal likelihoods. This in turn implies the weak\nsubmodularity of the corresponding subset selection problem, and thereby proves\napproximation guarantees for the proposed algorithm. We compare the model's\nperformance relative to other approaches on diverse regression and Bayesian\noptimization tasks, including the challenging but common setting of sparse\ncorruptions of the labels within or close to the function range.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian processes (GPs) are non-parametric probabilistic regression models\nthat are popular due to their flexibility, data efficiency, and well-calibrated\nuncertainty estimates. However, standard GP models assume homoskedastic\nGaussian noise, while many real-world applications are subject to non-Gaussian\ncorruptions. Variants of GPs that are more robust to alternative noise models\nhave been proposed, and entail significant trade-offs between accuracy and\nrobustness, and between computational requirements and theoretical guarantees.\nIn this work, we propose and study a GP model that achieves robustness against\nsparse outliers by inferring data-point-specific noise levels with a sequential\nselection procedure maximizing the log marginal likelihood that we refer to as\nrelevance pursuit. We show, surprisingly, that the model can be parameterized\nsuch that the associated log marginal likelihood is strongly concave in the\ndata-point-specific noise variances, a property rarely found in either robust\nregression objectives or GP marginal likelihoods. This in turn implies the weak\nsubmodularity of the corresponding subset selection problem, and thereby proves\napproximation guarantees for the proposed algorithm. We compare the model's\nperformance relative to other approaches on diverse regression and Bayesian\noptimization tasks, including the challenging but common setting of sparse\ncorruptions of the labels within or close to the function range."
                },
                "authors": [
                    {
                        "name": "Sebastian Ament"
                    },
                    {
                        "name": "Elizabeth Santorella"
                    },
                    {
                        "name": "David Eriksson"
                    },
                    {
                        "name": "Ben Letham"
                    },
                    {
                        "name": "Maximilian Balandat"
                    },
                    {
                        "name": "Eytan Bakshy"
                    }
                ],
                "author_detail": {
                    "name": "Eytan Bakshy"
                },
                "author": "Eytan Bakshy",
                "arxiv_comment": "NeurIPS 2024 Article (https://openreview.net/forum?id=5FATPIlWUJ)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03761v1",
                "updated": "2025-01-07T13:03:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    13,
                    3,
                    56,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T13:03:56Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    13,
                    3,
                    56,
                    1,
                    7,
                    0
                ],
                "title": "Implications of the intriguing constant inner mass surface density\n  observed in dark matter halos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implications of the intriguing constant inner mass surface density\n  observed in dark matter halos"
                },
                "summary": "It is known for long that the observed mass surface density of cored dark\nmatter (DM) halos is approximately constant, independently of the galaxy mass\n(i.e., rhoc X rc simeq constant}, with rhoc and rc the central volume density\nand the radius of the core, respectively). Here we review the evidence\nsupporting this empirical fact as well as its theoretical interpretation. It\nseems to be an emergent law resulting from the concentration-halo mass relation\npredicted by the current cosmological model, where the DM is made of\ncollisionless cold DM particles (CDM). We argue that the prediction rhoc X rc\nsimeq constant is not specific to this particular model of DM but holds for any\nother DM model (e.g., self-interacting) or process (e.g., stellar or AGN\nfeedback) that redistributes the DM within halos conserving its CDM mass. In\naddition, the fact that rhoc X rc simeq constant is shown to allow the estimate\nof the core DM mass and baryon fraction from stellar photometry alone,\nparticularly useful when the observationally-expensive conventional\nspectroscopic techniques are unfeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is known for long that the observed mass surface density of cored dark\nmatter (DM) halos is approximately constant, independently of the galaxy mass\n(i.e., rhoc X rc simeq constant}, with rhoc and rc the central volume density\nand the radius of the core, respectively). Here we review the evidence\nsupporting this empirical fact as well as its theoretical interpretation. It\nseems to be an emergent law resulting from the concentration-halo mass relation\npredicted by the current cosmological model, where the DM is made of\ncollisionless cold DM particles (CDM). We argue that the prediction rhoc X rc\nsimeq constant is not specific to this particular model of DM but holds for any\nother DM model (e.g., self-interacting) or process (e.g., stellar or AGN\nfeedback) that redistributes the DM within halos conserving its CDM mass. In\naddition, the fact that rhoc X rc simeq constant is shown to allow the estimate\nof the core DM mass and baryon fraction from stellar photometry alone,\nparticularly useful when the observationally-expensive conventional\nspectroscopic techniques are unfeasible."
                },
                "authors": [
                    {
                        "name": "Jorge Sanchez Almeida"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Sanchez Almeida"
                },
                "arxiv_affiliation": "Departamento de Astrofisica, Universidad de La Laguna, Spain",
                "author": "Jorge Sanchez Almeida",
                "arxiv_comment": "Accepted for publication in \"Galaxies\". Recipe to infer dark matter\n  mass from photometry alone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03756v1",
                "updated": "2025-01-07T12:56:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    12,
                    56,
                    31,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T12:56:31Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    12,
                    56,
                    31,
                    1,
                    7,
                    0
                ],
                "title": "Conditions for radiative zones in the molecular hydrogen envelope of\n  Jupiter and Saturn: The role of alkali metals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditions for radiative zones in the molecular hydrogen envelope of\n  Jupiter and Saturn: The role of alkali metals"
                },
                "summary": "Interior models of gas giants in the Solar System traditionally assume a\nfully convective molecular hydrogen envelope. However, recent observations from\nthe Juno mission suggest a possible depletion of alkali metals in Jupiter's\nmolecular hydrogen envelope, indicating that a stable radiative layer could\nexist at the kilobar level. Recent studies propose that deep stable layers help\nreconcile various Jupiter observations, including its atmospheric water and CO\nabundances and the depth of its zonal winds. However, opacity tables used to\ninfer stable layers are often outdated and incomplete, leaving the precise\nmolecular hydrogen envelope composition required for a deep radiative zone\nuncertain. In this paper, we determine atmospheric compositions that can lead\nto the formation of a radiative zone at the kilobar level in Jupiter and Saturn\ntoday. We computed radiative opacity tables covering pressures up to $10^5$\nbar, including the most abundant molecules present in the gas giants of the\nSolar System, as well as contributions from free electrons, metal hydrides,\noxides, and atomic species, using the most up-to-date line lists published in\nthe literature. These tables were used to calculate Rosseland-mean opacities\nfor the molecular hydrogen envelopes of Jupiter and Saturn, which were then\ncompared to the critical mean opacity required to maintain convection. We find\nthat the presence of a radiative zone is controlled by the existence of K, Na,\nand NaH in the atmosphere of Jupiter and Saturn. For Jupiter, the elemental\nabundance of K and Na must be less than $\\sim 10^{-3}$ times solar to form a\nradiative zone. In contrast, for Saturn, the required abundance for K and Na is\nbelow $\\sim 10^{-4}$ times solar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interior models of gas giants in the Solar System traditionally assume a\nfully convective molecular hydrogen envelope. However, recent observations from\nthe Juno mission suggest a possible depletion of alkali metals in Jupiter's\nmolecular hydrogen envelope, indicating that a stable radiative layer could\nexist at the kilobar level. Recent studies propose that deep stable layers help\nreconcile various Jupiter observations, including its atmospheric water and CO\nabundances and the depth of its zonal winds. However, opacity tables used to\ninfer stable layers are often outdated and incomplete, leaving the precise\nmolecular hydrogen envelope composition required for a deep radiative zone\nuncertain. In this paper, we determine atmospheric compositions that can lead\nto the formation of a radiative zone at the kilobar level in Jupiter and Saturn\ntoday. We computed radiative opacity tables covering pressures up to $10^5$\nbar, including the most abundant molecules present in the gas giants of the\nSolar System, as well as contributions from free electrons, metal hydrides,\noxides, and atomic species, using the most up-to-date line lists published in\nthe literature. These tables were used to calculate Rosseland-mean opacities\nfor the molecular hydrogen envelopes of Jupiter and Saturn, which were then\ncompared to the critical mean opacity required to maintain convection. We find\nthat the presence of a radiative zone is controlled by the existence of K, Na,\nand NaH in the atmosphere of Jupiter and Saturn. For Jupiter, the elemental\nabundance of K and Na must be less than $\\sim 10^{-3}$ times solar to form a\nradiative zone. In contrast, for Saturn, the required abundance for K and Na is\nbelow $\\sim 10^{-4}$ times solar."
                },
                "authors": [
                    {
                        "name": "Louis Siebenaler"
                    },
                    {
                        "name": "Yamila Miguel"
                    },
                    {
                        "name": "Sam de Regt"
                    },
                    {
                        "name": "Tristan Guillot"
                    }
                ],
                "author_detail": {
                    "name": "Tristan Guillot"
                },
                "author": "Tristan Guillot",
                "arxiv_comment": "Accepted for publication in A&A, 14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17481v2",
                "updated": "2025-01-07T12:48:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    12,
                    48,
                    22,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-23T11:11:51Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    11,
                    51,
                    0,
                    358,
                    0
                ],
                "title": "A Survey on LLM-based Multi-Agent System: Recent Advances and New\n  Frontiers in Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LLM-based Multi-Agent System: Recent Advances and New\n  Frontiers in Application"
                },
                "summary": "LLM-based Multi-Agent Systems ( LLM-MAS ) have become a research hotspot\nsince the rise of large language models (LLMs). However, with the continuous\ninflux of new related works, the existing reviews struggle to capture them\ncomprehensively. This paper presents a comprehensive survey of these studies.\nWe first discuss the definition of LLM-MAS, a framework encompassing much of\nprevious work. We provide an overview of the various applications of LLM-MAS in\n(i) solving complex tasks, (ii) simulating specific scenarios, and (iii)\nevaluating generative agents. Building on previous studies, we also highlight\nseveral challenges and propose future directions for research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Multi-Agent Systems ( LLM-MAS ) have become a research hotspot\nsince the rise of large language models (LLMs). However, with the continuous\ninflux of new related works, the existing reviews struggle to capture them\ncomprehensively. This paper presents a comprehensive survey of these studies.\nWe first discuss the definition of LLM-MAS, a framework encompassing much of\nprevious work. We provide an overview of the various applications of LLM-MAS in\n(i) solving complex tasks, (ii) simulating specific scenarios, and (iii)\nevaluating generative agents. Building on previous studies, we also highlight\nseveral challenges and propose future directions for research in this field."
                },
                "authors": [
                    {
                        "name": "Shuaihang Chen"
                    },
                    {
                        "name": "Yuanxing Liu"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "13 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.04482v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.04482v3",
                "updated": "2025-01-07T12:40:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    12,
                    40,
                    58,
                    1,
                    7,
                    0
                ],
                "published": "2024-01-09T10:39:17Z",
                "published_parsed": [
                    2024,
                    1,
                    9,
                    10,
                    39,
                    17,
                    1,
                    9,
                    0
                ],
                "title": "Continuously Learning New Words in Automatic Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuously Learning New Words in Automatic Speech Recognition"
                },
                "summary": "Despite recent advances, Automatic Speech Recognition (ASR) systems are still\nfar from perfect. Typical errors include acronyms, named entities, and\ndomain-specific special words for which little or no labeled data is available.\nTo address the problem of recognizing these words, we propose a self-supervised\ncontinual learning approach: Given the audio of a lecture talk with the\ncorresponding slides, we bias the model towards decoding new words from the\nslides by using a memory-enhanced ASR model from the literature. Then, we\nperform inference on the talk, collecting utterances that contain detected new\nwords into an adaptation data set. Continual learning is then performed by\ntraining adaptation weights added to the model on this data set. The whole\nprocedure is iterated for many talks. We show that with this approach, we\nobtain increasing performance on the new words when they occur more frequently\n(more than 80% recall) while preserving the general performance of the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances, Automatic Speech Recognition (ASR) systems are still\nfar from perfect. Typical errors include acronyms, named entities, and\ndomain-specific special words for which little or no labeled data is available.\nTo address the problem of recognizing these words, we propose a self-supervised\ncontinual learning approach: Given the audio of a lecture talk with the\ncorresponding slides, we bias the model towards decoding new words from the\nslides by using a memory-enhanced ASR model from the literature. Then, we\nperform inference on the talk, collecting utterances that contain detected new\nwords into an adaptation data set. Continual learning is then performed by\ntraining adaptation weights added to the model on this data set. The whole\nprocedure is iterated for many talks. We show that with this approach, we\nobtain increasing performance on the new words when they occur more frequently\n(more than 80% recall) while preserving the general performance of the model."
                },
                "authors": [
                    {
                        "name": "Christian Huber"
                    },
                    {
                        "name": "Alexander Waibel"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Waibel"
                },
                "author": "Alexander Waibel",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.04482v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.04482v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03747v1",
                "updated": "2025-01-07T12:40:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    12,
                    40,
                    35,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T12:40:35Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    12,
                    40,
                    35,
                    1,
                    7,
                    0
                ],
                "title": "Context-Alignment: Activating and Enhancing LLM Capabilities in Time\n  Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Alignment: Activating and Enhancing LLM Capabilities in Time\n  Series"
                },
                "summary": "Recently, leveraging pre-trained Large Language Models (LLMs) for time series\n(TS) tasks has gained increasing attention, which involves activating and\nenhancing LLMs' capabilities. Many methods aim to activate LLMs' capabilities\nbased on token-level alignment but overlook LLMs' inherent strength on natural\nlanguage processing -- their deep understanding of linguistic logic and\nstructure rather than superficial embedding processing. We propose\nContext-Alignment, a new paradigm that aligns TS with a linguistic component in\nthe language environments familiar to LLMs to enable LLMs to contextualize and\ncomprehend TS data, thereby activating their capabilities. Specifically, such\ncontext-level alignment comprises structural alignment and logical alignment,\nwhich is achieved by a Dual-Scale Context-Alignment GNNs (DSCA-GNNs) applied to\nTS-language multimodal inputs. Structural alignment utilizes dual-scale nodes\nto describe hierarchical structure in TS-language, enabling LLMs treat long TS\ndata as a whole linguistic component while preserving intrinsic token features.\nLogical alignment uses directed edges to guide logical relationships, ensuring\ncoherence in the contextual semantics. Demonstration examples prompt are\nemployed to construct Demonstration Examples based Context-Alignment (DECA)\nfollowing DSCA-GNNs framework. DECA can be flexibly and repeatedly integrated\ninto various layers of pre-trained LLMs to improve awareness of logic and\nstructure, thereby enhancing performance. Extensive experiments show the\neffectiveness of DECA and the importance of Context-Alignment across tasks,\nparticularly in few-shot and zero-shot forecasting, confirming that\nContext-Alignment provide powerful prior knowledge on context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, leveraging pre-trained Large Language Models (LLMs) for time series\n(TS) tasks has gained increasing attention, which involves activating and\nenhancing LLMs' capabilities. Many methods aim to activate LLMs' capabilities\nbased on token-level alignment but overlook LLMs' inherent strength on natural\nlanguage processing -- their deep understanding of linguistic logic and\nstructure rather than superficial embedding processing. We propose\nContext-Alignment, a new paradigm that aligns TS with a linguistic component in\nthe language environments familiar to LLMs to enable LLMs to contextualize and\ncomprehend TS data, thereby activating their capabilities. Specifically, such\ncontext-level alignment comprises structural alignment and logical alignment,\nwhich is achieved by a Dual-Scale Context-Alignment GNNs (DSCA-GNNs) applied to\nTS-language multimodal inputs. Structural alignment utilizes dual-scale nodes\nto describe hierarchical structure in TS-language, enabling LLMs treat long TS\ndata as a whole linguistic component while preserving intrinsic token features.\nLogical alignment uses directed edges to guide logical relationships, ensuring\ncoherence in the contextual semantics. Demonstration examples prompt are\nemployed to construct Demonstration Examples based Context-Alignment (DECA)\nfollowing DSCA-GNNs framework. DECA can be flexibly and repeatedly integrated\ninto various layers of pre-trained LLMs to improve awareness of logic and\nstructure, thereby enhancing performance. Extensive experiments show the\neffectiveness of DECA and the importance of Context-Alignment across tasks,\nparticularly in few-shot and zero-shot forecasting, confirming that\nContext-Alignment provide powerful prior knowledge on context."
                },
                "authors": [
                    {
                        "name": "Yuxiao Hu"
                    },
                    {
                        "name": "Qian Li"
                    },
                    {
                        "name": "Dongxiao Zhang"
                    },
                    {
                        "name": "Jinyue Yan"
                    },
                    {
                        "name": "Yuntian Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuntian Chen"
                },
                "author": "Yuntian Chen",
                "arxiv_comment": "no comment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10936v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10936v2",
                "updated": "2025-01-07T12:15:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    12,
                    15,
                    1,
                    1,
                    7,
                    0
                ],
                "published": "2024-05-17T17:47:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    17,
                    47,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "A Survey on Large Language Models with Multilingualism: Recent Advances\n  and New Frontiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Models with Multilingualism: Recent Advances\n  and New Frontiers"
                },
                "summary": "The rapid development of Large Language Models (LLMs) demonstrates remarkable\nmultilingual capabilities in natural language processing, attracting global\nattention in both academia and industry. To mitigate potential discrimination\nand enhance the overall usability and accessibility for diverse language user\ngroups, it is important for the development of language-fair technology.\nDespite the breakthroughs of LLMs, the investigation into the multilingual\nscenario remains insufficient, where a comprehensive survey to summarize recent\napproaches, developments, limitations, and potential solutions is desirable. To\nthis end, we provide a survey with multiple perspectives on the utilization of\nLLMs in the multilingual scenario. We first rethink the transitions between\nprevious and current research on pre-trained language models. Then we introduce\nseveral perspectives on the multilingualism of LLMs, including training and\ninference methods, information retrieval, model security, multi-domain with\nlanguage culture, and usage of datasets. We also discuss the major challenges\nthat arise in these aspects, along with possible solutions. Besides, we\nhighlight future research directions that aim at further enhancing LLMs with\nmultilingualism. The survey aims to help the research community address\nmultilingual problems and provide a comprehensive understanding of the core\nconcepts, key techniques, and latest developments in multilingual natural\nlanguage processing based on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) demonstrates remarkable\nmultilingual capabilities in natural language processing, attracting global\nattention in both academia and industry. To mitigate potential discrimination\nand enhance the overall usability and accessibility for diverse language user\ngroups, it is important for the development of language-fair technology.\nDespite the breakthroughs of LLMs, the investigation into the multilingual\nscenario remains insufficient, where a comprehensive survey to summarize recent\napproaches, developments, limitations, and potential solutions is desirable. To\nthis end, we provide a survey with multiple perspectives on the utilization of\nLLMs in the multilingual scenario. We first rethink the transitions between\nprevious and current research on pre-trained language models. Then we introduce\nseveral perspectives on the multilingualism of LLMs, including training and\ninference methods, information retrieval, model security, multi-domain with\nlanguage culture, and usage of datasets. We also discuss the major challenges\nthat arise in these aspects, along with possible solutions. Besides, we\nhighlight future research directions that aim at further enhancing LLMs with\nmultilingualism. The survey aims to help the research community address\nmultilingual problems and provide a comprehensive understanding of the core\nconcepts, key techniques, and latest developments in multilingual natural\nlanguage processing based on LLMs."
                },
                "authors": [
                    {
                        "name": "Kaiyu Huang"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Hongliang Li"
                    },
                    {
                        "name": "You Li"
                    },
                    {
                        "name": "Yuanchi Zhang"
                    },
                    {
                        "name": "Weijian Yi"
                    },
                    {
                        "name": "Yulong Mao"
                    },
                    {
                        "name": "Jinchen Liu"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Jinan Xu"
                    },
                    {
                        "name": "Jian-Yun Nie"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "65 pages, Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10936v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12761v2",
                "updated": "2025-01-07T11:18:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    11,
                    18,
                    8,
                    1,
                    7,
                    0
                ],
                "published": "2024-03-19T14:27:31Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    14,
                    27,
                    31,
                    1,
                    79,
                    0
                ],
                "title": "BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight\n  LLMs"
                },
                "summary": "This paper presents a novel approach to generating behavior trees for robots\nusing lightweight large language models (LLMs) with a maximum of 7 billion\nparameters. The study demonstrates that it is possible to achieve satisfying\nresults with compact LLMs when fine-tuned on a specific dataset. The key\ncontributions of this research include the creation of a fine-tuning dataset\nbased on existing behavior trees using GPT-3.5 and a comprehensive comparison\nof multiple LLMs (namely llama2, llama-chat, and code-llama) across nine\ndistinct tasks. To be thorough, we evaluated the generated behavior trees using\nstatic syntactical analysis, a validation system, a simulated environment, and\na real robot. Furthermore, this work opens the possibility of deploying such\nsolutions directly on the robot, enhancing its practical applicability.\nFindings from this study demonstrate the potential of LLMs with a limited\nnumber of parameters in generating effective and efficient robot behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to generating behavior trees for robots\nusing lightweight large language models (LLMs) with a maximum of 7 billion\nparameters. The study demonstrates that it is possible to achieve satisfying\nresults with compact LLMs when fine-tuned on a specific dataset. The key\ncontributions of this research include the creation of a fine-tuning dataset\nbased on existing behavior trees using GPT-3.5 and a comprehensive comparison\nof multiple LLMs (namely llama2, llama-chat, and code-llama) across nine\ndistinct tasks. To be thorough, we evaluated the generated behavior trees using\nstatic syntactical analysis, a validation system, a simulated environment, and\na real robot. Furthermore, this work opens the possibility of deploying such\nsolutions directly on the robot, enhancing its practical applicability.\nFindings from this study demonstrate the potential of LLMs with a limited\nnumber of parameters in generating effective and efficient robot behaviors."
                },
                "authors": [
                    {
                        "name": "Riccardo Andrea Izzo"
                    },
                    {
                        "name": "Gianluca Bardaro"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_doi": "10.1109/IROS58592.2024.10802304",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IROS58592.2024.10802304",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.12761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11543v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11543v3",
                "updated": "2025-01-07T11:09:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    11,
                    9,
                    52,
                    1,
                    7,
                    0
                ],
                "published": "2024-11-18T13:01:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    1,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "PSA-VLM: Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PSA-VLM: Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment"
                },
                "summary": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark."
                },
                "authors": [
                    {
                        "name": "Zhendong Liu"
                    },
                    {
                        "name": "Yuanbi Nie"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Qiushi Cui"
                    },
                    {
                        "name": "Chongjun Wang"
                    },
                    {
                        "name": "Xiaoyong Zhu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2405.13581",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11543v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11543v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03699v1",
                "updated": "2025-01-07T11:03:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    11,
                    3,
                    43,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T11:03:43Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    11,
                    3,
                    43,
                    1,
                    7,
                    0
                ],
                "title": "Motion-Aware Generative Frame Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion-Aware Generative Frame Interpolation"
                },
                "summary": "Generative frame interpolation, empowered by large-scale pre-trained video\ngeneration models, has demonstrated remarkable advantages in complex scenes.\nHowever, existing methods heavily rely on the generative model to independently\ninfer the correspondences between input frames, an ability that is inadequately\ndeveloped during pre-training. In this work, we propose a novel framework,\ntermed Motion-aware Generative frame interpolation (MoG), to significantly\nenhance the model's motion awareness by integrating explicit motion guidance.\nSpecifically we investigate two key questions: what can serve as an effective\nmotion guidance, and how we can seamlessly embed this guidance into the\ngenerative model. For the first question, we reveal that the intermediate flow\nfrom flow-based interpolation models could efficiently provide task-oriented\nmotion guidance. Regarding the second, we first obtain guidance-based\nrepresentations of intermediate frames by warping input frames' representations\nusing guidance, and then integrate them into the model at both latent and\nfeature levels. To demonstrate the versatility of our method, we train MoG on\nboth real-world and animation datasets. Comprehensive evaluations show that our\nMoG significantly outperforms the existing methods in both domains, achieving\nsuperior video quality and improved fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative frame interpolation, empowered by large-scale pre-trained video\ngeneration models, has demonstrated remarkable advantages in complex scenes.\nHowever, existing methods heavily rely on the generative model to independently\ninfer the correspondences between input frames, an ability that is inadequately\ndeveloped during pre-training. In this work, we propose a novel framework,\ntermed Motion-aware Generative frame interpolation (MoG), to significantly\nenhance the model's motion awareness by integrating explicit motion guidance.\nSpecifically we investigate two key questions: what can serve as an effective\nmotion guidance, and how we can seamlessly embed this guidance into the\ngenerative model. For the first question, we reveal that the intermediate flow\nfrom flow-based interpolation models could efficiently provide task-oriented\nmotion guidance. Regarding the second, we first obtain guidance-based\nrepresentations of intermediate frames by warping input frames' representations\nusing guidance, and then integrate them into the model at both latent and\nfeature levels. To demonstrate the versatility of our method, we train MoG on\nboth real-world and animation datasets. Comprehensive evaluations show that our\nMoG significantly outperforms the existing methods in both domains, achieving\nsuperior video quality and improved fidelity."
                },
                "authors": [
                    {
                        "name": "Guozhen Zhang"
                    },
                    {
                        "name": "Yuhan Zhu"
                    },
                    {
                        "name": "Yutao Cui"
                    },
                    {
                        "name": "Xiaotong Zhao"
                    },
                    {
                        "name": "Kai Ma"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06096v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06096v4",
                "updated": "2025-01-07T10:45:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    10,
                    45,
                    58,
                    1,
                    7,
                    0
                ],
                "published": "2024-09-09T22:16:48Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    22,
                    16,
                    48,
                    0,
                    253,
                    0
                ],
                "title": "Latent Diffusion Bridges for Unsupervised Musical Audio Timbre Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Diffusion Bridges for Unsupervised Musical Audio Timbre Transfer"
                },
                "summary": "Music timbre transfer is a challenging task that involves modifying the\ntimbral characteristics of an audio signal while preserving its melodic\nstructure. In this paper, we propose a novel method based on dual diffusion\nbridges, trained using the CocoChorales Dataset, which consists of unpaired\nmonophonic single-instrument audio data. Each diffusion model is trained on a\nspecific instrument with a Gaussian prior. During inference, a model is\ndesignated as the source model to map the input audio to its corresponding\nGaussian prior, and another model is designated as the target model to\nreconstruct the target audio from this Gaussian prior, thereby facilitating\ntimbre transfer. We compare our approach against existing unsupervised timbre\ntransfer models such as VAEGAN and Gaussian Flow Bridges (GFB). Experimental\nresults demonstrate that our method achieves both better Fr\\'echet Audio\nDistance (FAD) and melody preservation, as reflected by lower pitch distances\n(DPD) compared to VAEGAN and GFB. Additionally, we discover that the noise\nlevel from the Gaussian prior, $\\sigma$, can be adjusted to control the degree\nof melody preservation and amount of timbre transferred.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Music timbre transfer is a challenging task that involves modifying the\ntimbral characteristics of an audio signal while preserving its melodic\nstructure. In this paper, we propose a novel method based on dual diffusion\nbridges, trained using the CocoChorales Dataset, which consists of unpaired\nmonophonic single-instrument audio data. Each diffusion model is trained on a\nspecific instrument with a Gaussian prior. During inference, a model is\ndesignated as the source model to map the input audio to its corresponding\nGaussian prior, and another model is designated as the target model to\nreconstruct the target audio from this Gaussian prior, thereby facilitating\ntimbre transfer. We compare our approach against existing unsupervised timbre\ntransfer models such as VAEGAN and Gaussian Flow Bridges (GFB). Experimental\nresults demonstrate that our method achieves both better Fr\\'echet Audio\nDistance (FAD) and melody preservation, as reflected by lower pitch distances\n(DPD) compared to VAEGAN and GFB. Additionally, we discover that the noise\nlevel from the Gaussian prior, $\\sigma$, can be adjusted to control the degree\nof melody preservation and amount of timbre transferred."
                },
                "authors": [
                    {
                        "name": "Michele Mancusi"
                    },
                    {
                        "name": "Yurii Halychanskyi"
                    },
                    {
                        "name": "Kin Wai Cheuk"
                    },
                    {
                        "name": "Eloi Moliner"
                    },
                    {
                        "name": "Chieh-Hsin Lai"
                    },
                    {
                        "name": "Stefan Uhlich"
                    },
                    {
                        "name": "Junghyun Koo"
                    },
                    {
                        "name": "Marco A. Martínez-Ramírez"
                    },
                    {
                        "name": "Wei-Hsiang Liao"
                    },
                    {
                        "name": "Giorgio Fabbro"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06096v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06096v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03681v1",
                "updated": "2025-01-07T10:29:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    10,
                    29,
                    43,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T10:29:43Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    10,
                    29,
                    43,
                    1,
                    7,
                    0
                ],
                "title": "SLAM: Towards Efficient Multilingual Reasoning via Selective Language\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM: Towards Efficient Multilingual Reasoning via Selective Language\n  Alignment"
                },
                "summary": "Despite the significant improvements achieved by large language models (LLMs)\nin English reasoning tasks, these models continue to struggle with multilingual\nreasoning. Recent studies leverage a full-parameter and two-stage training\nparadigm to teach models to first understand non-English questions and then\nreason. However, this method suffers from both substantial computational\nresource computing and catastrophic forgetting. The fundamental cause is that,\nwith the primary goal of enhancing multilingual comprehension, an excessive\nnumber of irrelevant layers and parameters are tuned during the first stage.\nGiven our findings that the representation learning of languages is merely\nconducted in lower-level layers, we propose an efficient multilingual reasoning\nalignment approach that precisely identifies and fine-tunes the layers\nresponsible for handling multilingualism. Experimental results show that our\nmethod, SLAM, only tunes 6 layers' feed-forward sub-layers including 6.5-8% of\nall parameters within 7B and 13B LLMs, achieving superior average performance\nthan all strong baselines across 10 languages. Meanwhile, SLAM only involves\none training stage, reducing training time by 4.1-11.9 compared to the\ntwo-stage method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the significant improvements achieved by large language models (LLMs)\nin English reasoning tasks, these models continue to struggle with multilingual\nreasoning. Recent studies leverage a full-parameter and two-stage training\nparadigm to teach models to first understand non-English questions and then\nreason. However, this method suffers from both substantial computational\nresource computing and catastrophic forgetting. The fundamental cause is that,\nwith the primary goal of enhancing multilingual comprehension, an excessive\nnumber of irrelevant layers and parameters are tuned during the first stage.\nGiven our findings that the representation learning of languages is merely\nconducted in lower-level layers, we propose an efficient multilingual reasoning\nalignment approach that precisely identifies and fine-tunes the layers\nresponsible for handling multilingualism. Experimental results show that our\nmethod, SLAM, only tunes 6 layers' feed-forward sub-layers including 6.5-8% of\nall parameters within 7B and 13B LLMs, achieving superior average performance\nthan all strong baselines across 10 languages. Meanwhile, SLAM only involves\none training stage, reducing training time by 4.1-11.9 compared to the\ntwo-stage method."
                },
                "authors": [
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yilin Wang"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Junhao Ruan"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Accepted by COLING 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03675v1",
                "updated": "2025-01-07T10:21:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    10,
                    21,
                    21,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T10:21:21Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    10,
                    21,
                    21,
                    1,
                    7,
                    0
                ],
                "title": "SMIR: Efficient Synthetic Data Pipeline To Improve Multi-Image Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMIR: Efficient Synthetic Data Pipeline To Improve Multi-Image Reasoning"
                },
                "summary": "Vision-Language Models (VLMs) have shown strong performance in understanding\nsingle images, aided by numerous high-quality instruction datasets. However,\nmulti-image reasoning tasks are still under-explored in the open-source\ncommunity due to two main challenges: (1) scaling datasets with multiple\ncorrelated images and complex reasoning instructions is resource-intensive and\nmaintaining quality is difficult, and (2) there is a lack of robust evaluation\nbenchmarks for multi-image tasks. To address these issues, we introduce SMIR,\nan efficient synthetic data-generation pipeline for multi-image reasoning, and\na high-quality dataset generated using this pipeline. Our pipeline efficiently\nextracts highly correlated images using multimodal embeddings, combining visual\nand descriptive information and leverages open-source LLMs to generate quality\ninstructions. Using this pipeline, we generated 160K synthetic training\nsamples, offering a cost-effective alternative to expensive closed-source\nsolutions. Additionally, we present SMIR-BENCH, a novel multi-image reasoning\nevaluation benchmark comprising 200 diverse examples across 7 complex\nmulti-image reasoning tasks. SMIR-BENCH is multi-turn and utilizes a VLM judge\nto evaluate free-form responses, providing a comprehensive assessment of model\nexpressiveness and reasoning capability across modalities. We demonstrate the\neffectiveness of SMIR dataset by fine-tuning several open-source VLMs and\nevaluating their performance on SMIR-BENCH. Our results show that models\ntrained on our dataset outperform baseline models in multi-image reasoning\ntasks up to 8% with a much more scalable data pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have shown strong performance in understanding\nsingle images, aided by numerous high-quality instruction datasets. However,\nmulti-image reasoning tasks are still under-explored in the open-source\ncommunity due to two main challenges: (1) scaling datasets with multiple\ncorrelated images and complex reasoning instructions is resource-intensive and\nmaintaining quality is difficult, and (2) there is a lack of robust evaluation\nbenchmarks for multi-image tasks. To address these issues, we introduce SMIR,\nan efficient synthetic data-generation pipeline for multi-image reasoning, and\na high-quality dataset generated using this pipeline. Our pipeline efficiently\nextracts highly correlated images using multimodal embeddings, combining visual\nand descriptive information and leverages open-source LLMs to generate quality\ninstructions. Using this pipeline, we generated 160K synthetic training\nsamples, offering a cost-effective alternative to expensive closed-source\nsolutions. Additionally, we present SMIR-BENCH, a novel multi-image reasoning\nevaluation benchmark comprising 200 diverse examples across 7 complex\nmulti-image reasoning tasks. SMIR-BENCH is multi-turn and utilizes a VLM judge\nto evaluate free-form responses, providing a comprehensive assessment of model\nexpressiveness and reasoning capability across modalities. We demonstrate the\neffectiveness of SMIR dataset by fine-tuning several open-source VLMs and\nevaluating their performance on SMIR-BENCH. Our results show that models\ntrained on our dataset outperform baseline models in multi-image reasoning\ntasks up to 8% with a much more scalable data pipeline."
                },
                "authors": [
                    {
                        "name": "Andrew Li"
                    },
                    {
                        "name": "Rahul Thapa"
                    },
                    {
                        "name": "Rahul Chalamala"
                    },
                    {
                        "name": "Qingyang Wu"
                    },
                    {
                        "name": "Kezhen Chen"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11887v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11887v3",
                "updated": "2025-01-07T10:09:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    10,
                    9,
                    33,
                    1,
                    7,
                    0
                ],
                "published": "2024-10-12T04:03:36Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    4,
                    3,
                    36,
                    5,
                    286,
                    0
                ],
                "title": "Thermal Comfort in Sight: Thermal Affordance and its Visual Assessment\n  for Sustainable Streetscape Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thermal Comfort in Sight: Thermal Affordance and its Visual Assessment\n  for Sustainable Streetscape Design"
                },
                "summary": "In response to climate change and urban heat island effects, enhancing human\nthermal comfort in cities is crucial for sustainable urban development.\nTraditional methods for investigating the urban thermal environment and\ncorresponding human thermal comfort level are often resource intensive,\ninefficient, and limited in scope. To address these challenges, we (1)\nintroduce a new concept named thermal affordance, which formalizes the\nintegrated inherent capacity of a streetscape to influence human thermal\ncomfort based on its visual and physical features; and (2) an efficient method\nto evaluate it (visual assessment of thermal affordance -- VATA), which\ncombines street view imagery (SVI), online and in-field surveys, and\nstatistical learning algorithms. VATA extracts five categories of image\nfeatures from SVI data and establishes 19 visual-perceptual indicators for\nstreetscape visual assessment. Using a multi-task neural network and elastic\nnet regression, we model their chained relationship to predict and comprehend\nthermal affordance for Singapore. VATA predictions are validated with\nfield-investigated OTC data, providing a cost-effective, scalable, and\ntransferable method to assess the thermal comfort potential of urban\nstreetscape. Moreover, we demonstrate its utility by generating a geospatially\nexplicit mapping of thermal affordance, outlining a model update workflow for\nlong-term urban-scale analysis, and implementing a two-stage prediction and\ninference approach (IF-VPI-VATA) to guide future streetscape improvements. This\nframework can inform streetscape design to support sustainable, liveable, and\nresilient urban environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to climate change and urban heat island effects, enhancing human\nthermal comfort in cities is crucial for sustainable urban development.\nTraditional methods for investigating the urban thermal environment and\ncorresponding human thermal comfort level are often resource intensive,\ninefficient, and limited in scope. To address these challenges, we (1)\nintroduce a new concept named thermal affordance, which formalizes the\nintegrated inherent capacity of a streetscape to influence human thermal\ncomfort based on its visual and physical features; and (2) an efficient method\nto evaluate it (visual assessment of thermal affordance -- VATA), which\ncombines street view imagery (SVI), online and in-field surveys, and\nstatistical learning algorithms. VATA extracts five categories of image\nfeatures from SVI data and establishes 19 visual-perceptual indicators for\nstreetscape visual assessment. Using a multi-task neural network and elastic\nnet regression, we model their chained relationship to predict and comprehend\nthermal affordance for Singapore. VATA predictions are validated with\nfield-investigated OTC data, providing a cost-effective, scalable, and\ntransferable method to assess the thermal comfort potential of urban\nstreetscape. Moreover, we demonstrate its utility by generating a geospatially\nexplicit mapping of thermal affordance, outlining a model update workflow for\nlong-term urban-scale analysis, and implementing a two-stage prediction and\ninference approach (IF-VPI-VATA) to guide future streetscape improvements. This\nframework can inform streetscape design to support sustainable, liveable, and\nresilient urban environments."
                },
                "authors": [
                    {
                        "name": "Sijie Yang"
                    },
                    {
                        "name": "Adrian Chong"
                    },
                    {
                        "name": "Pengyuan Liu"
                    },
                    {
                        "name": "Filip Biljecki"
                    }
                ],
                "author_detail": {
                    "name": "Filip Biljecki"
                },
                "author": "Filip Biljecki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11887v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11887v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14887v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14887v3",
                "updated": "2025-01-07T09:55:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    55,
                    57,
                    1,
                    7,
                    0
                ],
                "published": "2024-09-23T10:35:57Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    10,
                    35,
                    57,
                    0,
                    267,
                    0
                ],
                "title": "Deploying Open-Source Large Language Models: A performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Open-Source Large Language Models: A performance Analysis"
                },
                "summary": "Since the release of ChatGPT in November 2022, large language models (LLMs)\nhave seen considerable success, including in the open-source community, with\nmany open-weight models available. However, the requirements to deploy such a\nservice are often unknown and difficult to evaluate in advance. To facilitate\nthis process, we conducted numerous tests at the Centre Inria de l'Universit\\'e\nde Bordeaux. In this article, we propose a comparison of the performance of\nseveral models of different sizes (mainly Mistral and LLaMa) depending on the\navailable GPUs, using vLLM, a Python library designed to optimize the inference\nof these models. Our results provide valuable information for private and\npublic groups wishing to deploy LLMs, allowing them to evaluate the performance\nof different models based on their available hardware. This study thus\ncontributes to facilitating the adoption and use of these large language models\nin various application domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the release of ChatGPT in November 2022, large language models (LLMs)\nhave seen considerable success, including in the open-source community, with\nmany open-weight models available. However, the requirements to deploy such a\nservice are often unknown and difficult to evaluate in advance. To facilitate\nthis process, we conducted numerous tests at the Centre Inria de l'Universit\\'e\nde Bordeaux. In this article, we propose a comparison of the performance of\nseveral models of different sizes (mainly Mistral and LLaMa) depending on the\navailable GPUs, using vLLM, a Python library designed to optimize the inference\nof these models. Our results provide valuable information for private and\npublic groups wishing to deploy LLMs, allowing them to evaluate the performance\nof different models based on their available hardware. This study thus\ncontributes to facilitating the adoption and use of these large language models\nin various application domains."
                },
                "authors": [
                    {
                        "name": "Yannis Bendi-Ouis"
                    },
                    {
                        "name": "Dan Dutartre"
                    },
                    {
                        "name": "Xavier Hinaut"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Hinaut"
                },
                "author": "Xavier Hinaut",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14887v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14887v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03659v1",
                "updated": "2025-01-07T09:47:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    47,
                    46,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T09:47:46Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    47,
                    46,
                    1,
                    7,
                    0
                ],
                "title": "DehazeGS: Seeing Through Fog with 3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DehazeGS: Seeing Through Fog with 3D Gaussian Splatting"
                },
                "summary": "Current novel view synthesis tasks primarily rely on high-quality and clear\nimages. However, in foggy scenes, scattering and attenuation can significantly\ndegrade the reconstruction and rendering quality. Although NeRF-based dehazing\nreconstruction algorithms have been developed, their use of deep fully\nconnected neural networks and per-ray sampling strategies leads to high\ncomputational costs. Moreover, NeRF's implicit representation struggles to\nrecover fine details from hazy scenes. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction by explicitly\nmodeling point clouds into 3D Gaussians. In this paper, we propose leveraging\nthe explicit Gaussian representation to explain the foggy image formation\nprocess through a physically accurate forward rendering process. We introduce\nDehazeGS, a method capable of decomposing and rendering a fog-free background\nfrom participating media using only muti-view foggy images as input. We model\nthe transmission within each Gaussian distribution to simulate the formation of\nfog. During this process, we jointly learn the atmospheric light and scattering\ncoefficient while optimizing the Gaussian representation of the hazy scene. In\nthe inference stage, we eliminate the effects of scattering and attenuation on\nthe Gaussians and directly project them onto a 2D plane to obtain a clear view.\nExperiments on both synthetic and real-world foggy datasets demonstrate that\nDehazeGS achieves state-of-the-art performance in terms of both rendering\nquality and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current novel view synthesis tasks primarily rely on high-quality and clear\nimages. However, in foggy scenes, scattering and attenuation can significantly\ndegrade the reconstruction and rendering quality. Although NeRF-based dehazing\nreconstruction algorithms have been developed, their use of deep fully\nconnected neural networks and per-ray sampling strategies leads to high\ncomputational costs. Moreover, NeRF's implicit representation struggles to\nrecover fine details from hazy scenes. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction by explicitly\nmodeling point clouds into 3D Gaussians. In this paper, we propose leveraging\nthe explicit Gaussian representation to explain the foggy image formation\nprocess through a physically accurate forward rendering process. We introduce\nDehazeGS, a method capable of decomposing and rendering a fog-free background\nfrom participating media using only muti-view foggy images as input. We model\nthe transmission within each Gaussian distribution to simulate the formation of\nfog. During this process, we jointly learn the atmospheric light and scattering\ncoefficient while optimizing the Gaussian representation of the hazy scene. In\nthe inference stage, we eliminate the effects of scattering and attenuation on\nthe Gaussians and directly project them onto a 2D plane to obtain a clear view.\nExperiments on both synthetic and real-world foggy datasets demonstrate that\nDehazeGS achieves state-of-the-art performance in terms of both rendering\nquality and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Jinze Yu"
                    },
                    {
                        "name": "Yiqun Wang"
                    },
                    {
                        "name": "Zhengda Lu"
                    },
                    {
                        "name": "Jianwei Guo"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Hongxing Qin"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "arxiv_comment": "9 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2107.06124v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2107.06124v3",
                "updated": "2025-01-07T09:46:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    46,
                    58,
                    1,
                    7,
                    0
                ],
                "published": "2021-07-13T14:24:33Z",
                "published_parsed": [
                    2021,
                    7,
                    13,
                    14,
                    24,
                    33,
                    1,
                    194,
                    0
                ],
                "title": "On Doubly Robust Inference for Double Machine Learning in Semiparametric\n  Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Doubly Robust Inference for Double Machine Learning in Semiparametric\n  Regression"
                },
                "summary": "Due to concerns about parametric model misspecification, there is interest in\nusing machine learning to adjust for confounding when evaluating the causal\neffect of an exposure on an outcome. Unfortunately, exposure effect estimators\nthat rely on machine learning predictions are generally subject to so-called\nplug-in bias, which can render naive $p$-values and confidence intervals\ninvalid. Progress has been made via proposals like targeted minimum loss\nestimation and more recently double machine learning, which rely on learning\nthe conditional mean of both the outcome and exposure. Valid inference can then\nbe obtained so long as both predictions converge (sufficiently fast) to the\ntruth. Focusing on partially linear regression models, we show that a specific\nimplementation of the machine learning techniques can yield exposure effect\nestimators that have small bias even when one of the first-stage predictions\ndoes not converge to the truth. The resulting tests and confidence intervals\nare doubly robust. We also show that the proposed estimators may fail to be\nregular when only one nuisance parameter is consistently estimated;\nnevertheless, we observe in simulation studies that our proposal can lead to\nreduced bias and improved confidence interval coverage in moderate-to-large\nsamples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to concerns about parametric model misspecification, there is interest in\nusing machine learning to adjust for confounding when evaluating the causal\neffect of an exposure on an outcome. Unfortunately, exposure effect estimators\nthat rely on machine learning predictions are generally subject to so-called\nplug-in bias, which can render naive $p$-values and confidence intervals\ninvalid. Progress has been made via proposals like targeted minimum loss\nestimation and more recently double machine learning, which rely on learning\nthe conditional mean of both the outcome and exposure. Valid inference can then\nbe obtained so long as both predictions converge (sufficiently fast) to the\ntruth. Focusing on partially linear regression models, we show that a specific\nimplementation of the machine learning techniques can yield exposure effect\nestimators that have small bias even when one of the first-stage predictions\ndoes not converge to the truth. The resulting tests and confidence intervals\nare doubly robust. We also show that the proposed estimators may fail to be\nregular when only one nuisance parameter is consistently estimated;\nnevertheless, we observe in simulation studies that our proposal can lead to\nreduced bias and improved confidence interval coverage in moderate-to-large\nsamples."
                },
                "authors": [
                    {
                        "name": "Oliver Dukes"
                    },
                    {
                        "name": "Stijn Vansteelandt"
                    },
                    {
                        "name": "David Whitney"
                    }
                ],
                "author_detail": {
                    "name": "David Whitney"
                },
                "author": "David Whitney",
                "arxiv_comment": "46 pages, 9 figures",
                "arxiv_journal_ref": "Journal of Machine Learning Research, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2107.06124v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2107.06124v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17624v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17624v2",
                "updated": "2025-01-07T09:46:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    46,
                    18,
                    1,
                    7,
                    0
                ],
                "published": "2024-07-24T20:30:55Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    20,
                    30,
                    55,
                    2,
                    206,
                    0
                ],
                "title": "Forecasting Credit Ratings: A Case Study where Traditional Methods\n  Outperform Generative LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting Credit Ratings: A Case Study where Traditional Methods\n  Outperform Generative LLMs"
                },
                "summary": "Large Language Models (LLMs) have been shown to perform well for many\ndownstream tasks. Transfer learning can enable LLMs to acquire skills that were\nnot targeted during pre-training. In financial contexts, LLMs can sometimes\nbeat well-established benchmarks. This paper investigates how well LLMs perform\nin the task of forecasting corporate credit ratings. We show that while LLMs\nare very good at encoding textual information, traditional methods are still\nvery competitive when it comes to encoding numeric and multimodal data. For our\ntask, current LLMs perform worse than a more traditional XGBoost architecture\nthat combines fundamental and macroeconomic data with high-density text-based\nembedding features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been shown to perform well for many\ndownstream tasks. Transfer learning can enable LLMs to acquire skills that were\nnot targeted during pre-training. In financial contexts, LLMs can sometimes\nbeat well-established benchmarks. This paper investigates how well LLMs perform\nin the task of forecasting corporate credit ratings. We show that while LLMs\nare very good at encoding textual information, traditional methods are still\nvery competitive when it comes to encoding numeric and multimodal data. For our\ntask, current LLMs perform worse than a more traditional XGBoost architecture\nthat combines fundamental and macroeconomic data with high-density text-based\nembedding features."
                },
                "authors": [
                    {
                        "name": "Felix Drinkall"
                    },
                    {
                        "name": "Janet B. Pierrehumbert"
                    },
                    {
                        "name": "Stefan Zohren"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Zohren"
                },
                "author": "Stefan Zohren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17624v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17624v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.RM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03656v1",
                "updated": "2025-01-07T09:41:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    41,
                    59,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T09:41:59Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    41,
                    59,
                    1,
                    7,
                    0
                ],
                "title": "The fine-scale structure of polar coronal holes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fine-scale structure of polar coronal holes"
                },
                "summary": "Coronal holes are thought to be composed of relatively broad columnar\nstructures known as plumes. Here we demonstrate that the plumes (and\ninter-plumes) in polar coronal holes are composed of fine-scale filamentary\nstructure, with average scales of 2-10$^{\\arcsec}$. The fine structure is the\noff-limb analogue of the previously found 'plumelets' of \\cite{Uritsky_2021}.\nThe off-limb observations enable an examination of the fine-structure without\nthe influence of the underlying atmosphere along the line of sight. Hence, we\nshow that the fine-scale structure is present at least until the edge of the\nfield of view of the Solar Dynamics Observatory. The fine structure is found to\nhave spatial distribution that follows a $k^{-1}$ power law perpendicular to\nthe inferred magnetic field direction. For a small sample of the fine\nstructure, the cross-sectional profiles are measured as a function of height.\nIn some cases, the measurements indicate that the fine structure expands\nsuper-radially, consistent with existing models of polar field expansion and\nthe expansion of the plumes. We discuss the implications of the presence of the\nfine structure with respect to understanding wave propagation in the coronal\nholes and their contribution to powering the solar wind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coronal holes are thought to be composed of relatively broad columnar\nstructures known as plumes. Here we demonstrate that the plumes (and\ninter-plumes) in polar coronal holes are composed of fine-scale filamentary\nstructure, with average scales of 2-10$^{\\arcsec}$. The fine structure is the\noff-limb analogue of the previously found 'plumelets' of \\cite{Uritsky_2021}.\nThe off-limb observations enable an examination of the fine-structure without\nthe influence of the underlying atmosphere along the line of sight. Hence, we\nshow that the fine-scale structure is present at least until the edge of the\nfield of view of the Solar Dynamics Observatory. The fine structure is found to\nhave spatial distribution that follows a $k^{-1}$ power law perpendicular to\nthe inferred magnetic field direction. For a small sample of the fine\nstructure, the cross-sectional profiles are measured as a function of height.\nIn some cases, the measurements indicate that the fine structure expands\nsuper-radially, consistent with existing models of polar field expansion and\nthe expansion of the plumes. We discuss the implications of the presence of the\nfine structure with respect to understanding wave propagation in the coronal\nholes and their contribution to powering the solar wind."
                },
                "authors": [
                    {
                        "name": "Richard J. Morton"
                    },
                    {
                        "name": "R. Cunningham"
                    }
                ],
                "author_detail": {
                    "name": "R. Cunningham"
                },
                "author": "R. Cunningham",
                "arxiv_doi": "10.3847/1538-4357/acea7c",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/acea7c",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.03656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 10 figues",
                "arxiv_journal_ref": "Astrophysical Journal, Volume 954, Issue 1, id.90, 11 pp. 2023",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12928v2",
                "updated": "2025-01-07T09:39:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    39,
                    15,
                    1,
                    7,
                    0
                ],
                "published": "2024-08-23T09:14:58Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    14,
                    58,
                    4,
                    236,
                    0
                ],
                "title": "ParGo: Bridging Vision-Language with Partial and Global Views",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParGo: Bridging Vision-Language with Partial and Global Views"
                },
                "summary": "This work presents ParGo, a novel Partial-Global projector designed to\nconnect the vision and language modalities for Multimodal Large Language Models\n(MLLMs). Unlike previous works that rely on global attention-based projectors,\nour ParGo bridges the representation gap between the separately pre-trained\nvision encoders and the LLMs by integrating global and partial views, which\nalleviates the overemphasis on prominent regions. To facilitate the effective\ntraining of ParGo, we collect a large-scale detail-captioned image-text dataset\nnamed ParGoCap-1M-PT, consisting of 1 million images paired with high-quality\ncaptions. Extensive experiments on several MLLM benchmarks demonstrate the\neffectiveness of our ParGo, highlighting its superiority in aligning vision and\nlanguage modalities. Compared to conventional Q-Former projector, our ParGo\nachieves an improvement of 259.96 in MME benchmark. Furthermore, our\nexperiments reveal that ParGo significantly outperforms other projectors,\nparticularly in tasks that emphasize detail perception ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents ParGo, a novel Partial-Global projector designed to\nconnect the vision and language modalities for Multimodal Large Language Models\n(MLLMs). Unlike previous works that rely on global attention-based projectors,\nour ParGo bridges the representation gap between the separately pre-trained\nvision encoders and the LLMs by integrating global and partial views, which\nalleviates the overemphasis on prominent regions. To facilitate the effective\ntraining of ParGo, we collect a large-scale detail-captioned image-text dataset\nnamed ParGoCap-1M-PT, consisting of 1 million images paired with high-quality\ncaptions. Extensive experiments on several MLLM benchmarks demonstrate the\neffectiveness of our ParGo, highlighting its superiority in aligning vision and\nlanguage modalities. Compared to conventional Q-Former projector, our ParGo\nachieves an improvement of 259.96 in MME benchmark. Furthermore, our\nexperiments reveal that ParGo significantly outperforms other projectors,\nparticularly in tasks that emphasize detail perception ability."
                },
                "authors": [
                    {
                        "name": "An-Lan Wang"
                    },
                    {
                        "name": "Bin Shan"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Kun-Yu Lin"
                    },
                    {
                        "name": "Xiang Fei"
                    },
                    {
                        "name": "Guozhi Tang"
                    },
                    {
                        "name": "Lei Liao"
                    },
                    {
                        "name": "Jingqun Tang"
                    },
                    {
                        "name": "Can Huang"
                    },
                    {
                        "name": "Wei-Shi Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Shi Zheng"
                },
                "author": "Wei-Shi Zheng",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10678v2",
                "updated": "2025-01-07T09:33:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    33,
                    20,
                    1,
                    7,
                    0
                ],
                "published": "2024-08-20T09:28:30Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    9,
                    28,
                    30,
                    1,
                    233,
                    0
                ],
                "title": "A machine-learning classifier for the postmerger remnant of binary\n  neutron stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A machine-learning classifier for the postmerger remnant of binary\n  neutron stars"
                },
                "summary": "Knowing the kind of remnant produced after the merger of a binary neutron\nstar system, e.g., if a black hole forms or not, would not only shed light on\nthe equation of state describing the extremely dense matter inside neutron\nstars, but also help understand the physical processes involved in the\npostmerger phase. Moreover, in the event of a gravitational-wave detection,\npredicting the presence of a neutron star remnant is crucial in order to advise\npotential electromagnetic follow-up campaigns. In this work, we use Gradient\nBoosted Decision Trees and publicly available data from numerical-relativity\nsimulations to construct a classifier that predicts the outcome of binary\nneutron star mergers, based on the binary's parameters inferred from\ngravitational-wave inspiral signals: total mass, mass-weighted tidal\ndeformability, mass ratio, and effective inspiral spin. Employing parameters\nthat can be estimated from the inspiral part of the signal only allows us to\npredict the remnant independently on the detection of a postmerger\ngravitational-wave signal. We build three different classifiers to distinguish\nbetween various potential scenarios, we estimate their accuracy and the\nconfidence of their predictions. Finally, we apply the developed classifiers to\nreal events data, finding that GW170817 most likely led to the formation of a\nhypermassive neutron star, while GW190425 to a prompt collapse to a black hole.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing the kind of remnant produced after the merger of a binary neutron\nstar system, e.g., if a black hole forms or not, would not only shed light on\nthe equation of state describing the extremely dense matter inside neutron\nstars, but also help understand the physical processes involved in the\npostmerger phase. Moreover, in the event of a gravitational-wave detection,\npredicting the presence of a neutron star remnant is crucial in order to advise\npotential electromagnetic follow-up campaigns. In this work, we use Gradient\nBoosted Decision Trees and publicly available data from numerical-relativity\nsimulations to construct a classifier that predicts the outcome of binary\nneutron star mergers, based on the binary's parameters inferred from\ngravitational-wave inspiral signals: total mass, mass-weighted tidal\ndeformability, mass ratio, and effective inspiral spin. Employing parameters\nthat can be estimated from the inspiral part of the signal only allows us to\npredict the remnant independently on the detection of a postmerger\ngravitational-wave signal. We build three different classifiers to distinguish\nbetween various potential scenarios, we estimate their accuracy and the\nconfidence of their predictions. Finally, we apply the developed classifiers to\nreal events data, finding that GW170817 most likely led to the formation of a\nhypermassive neutron star, while GW190425 to a prompt collapse to a black hole."
                },
                "authors": [
                    {
                        "name": "Anna Puecher"
                    },
                    {
                        "name": "Tim Dietrich"
                    }
                ],
                "author_detail": {
                    "name": "Tim Dietrich"
                },
                "author": "Tim Dietrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00698v2",
                "updated": "2025-01-07T09:26:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    26,
                    3,
                    1,
                    7,
                    0
                ],
                "published": "2024-09-01T11:39:13Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    11,
                    39,
                    13,
                    6,
                    245,
                    0
                ],
                "title": "Enhancing Remote Sensing Vision-Language Models for Zero-Shot Scene\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Remote Sensing Vision-Language Models for Zero-Shot Scene\n  Classification"
                },
                "summary": "Vision-Language Models for remote sensing have shown promising uses thanks to\ntheir extensive pretraining. However, their conventional usage in zero-shot\nscene classification methods still involves dividing large images into patches\nand making independent predictions, i.e., inductive inference, thereby limiting\ntheir effectiveness by ignoring valuable contextual information. Our approach\ntackles this issue by utilizing initial predictions based on text prompting and\npatch affinity relationships from the image encoder to enhance zero-shot\ncapabilities through transductive inference, all without the need for\nsupervision and at a minor computational cost. Experiments on 10 remote sensing\ndatasets with state-of-the-art Vision-Language Models demonstrate significant\naccuracy improvements over inductive zero-shot classification. Our source code\nis publicly available on Github: https://github.com/elkhouryk/RS-TransCLIP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models for remote sensing have shown promising uses thanks to\ntheir extensive pretraining. However, their conventional usage in zero-shot\nscene classification methods still involves dividing large images into patches\nand making independent predictions, i.e., inductive inference, thereby limiting\ntheir effectiveness by ignoring valuable contextual information. Our approach\ntackles this issue by utilizing initial predictions based on text prompting and\npatch affinity relationships from the image encoder to enhance zero-shot\ncapabilities through transductive inference, all without the need for\nsupervision and at a minor computational cost. Experiments on 10 remote sensing\ndatasets with state-of-the-art Vision-Language Models demonstrate significant\naccuracy improvements over inductive zero-shot classification. Our source code\nis publicly available on Github: https://github.com/elkhouryk/RS-TransCLIP"
                },
                "authors": [
                    {
                        "name": "Karim El Khoury"
                    },
                    {
                        "name": "Maxime Zanella"
                    },
                    {
                        "name": "Benoît Gérin"
                    },
                    {
                        "name": "Tiffanie Godelaine"
                    },
                    {
                        "name": "Benoît Macq"
                    },
                    {
                        "name": "Saïd Mahmoudi"
                    },
                    {
                        "name": "Christophe De Vleeschouwer"
                    },
                    {
                        "name": "Ismail Ben Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Ismail Ben Ayed"
                },
                "author": "Ismail Ben Ayed",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16020v2",
                "updated": "2025-01-07T09:15:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    15,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2024-10-21T13:50:32Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    13,
                    50,
                    32,
                    0,
                    295,
                    0
                ],
                "title": "START: A Generalized State Space Model with Saliency-Driven Token-Aware\n  Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "START: A Generalized State Space Model with Saliency-Driven Token-Aware\n  Transformation"
                },
                "summary": "Domain Generalization (DG) aims to enable models to generalize to unseen\ntarget domains by learning from multiple source domains. Existing DG methods\nprimarily rely on convolutional neural networks (CNNs), which inherently learn\ntexture biases due to their limited receptive fields, making them prone to\noverfitting source domains. While some works have introduced transformer-based\nmethods (ViTs) for DG to leverage the global receptive field, these methods\nincur high computational costs due to the quadratic complexity of\nself-attention. Recently, advanced state space models (SSMs), represented by\nMamba, have shown promising results in supervised learning tasks by achieving\nlinear complexity in sequence length during training and fast RNN-like\ncomputation during inference. Inspired by this, we investigate the\ngeneralization ability of the Mamba model under domain shifts and find that\ninput-dependent matrices within SSMs could accumulate and amplify\ndomain-specific features, thus hindering model generalization. To address this\nissue, we propose a novel SSM-based architecture with saliency-based\ntoken-aware transformation (namely START), which achieves state-of-the-art\n(SOTA) performances and offers a competitive alternative to CNNs and ViTs. Our\nSTART can selectively perturb and suppress domain-specific features in salient\ntokens within the input-dependent matrices of SSMs, thus effectively reducing\nthe discrepancy between different domains. Extensive experiments on five\nbenchmarks demonstrate that START outperforms existing SOTA DG methods with\nefficient linear complexity. Our code is available at\nhttps://github.com/lingeringlight/START.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Generalization (DG) aims to enable models to generalize to unseen\ntarget domains by learning from multiple source domains. Existing DG methods\nprimarily rely on convolutional neural networks (CNNs), which inherently learn\ntexture biases due to their limited receptive fields, making them prone to\noverfitting source domains. While some works have introduced transformer-based\nmethods (ViTs) for DG to leverage the global receptive field, these methods\nincur high computational costs due to the quadratic complexity of\nself-attention. Recently, advanced state space models (SSMs), represented by\nMamba, have shown promising results in supervised learning tasks by achieving\nlinear complexity in sequence length during training and fast RNN-like\ncomputation during inference. Inspired by this, we investigate the\ngeneralization ability of the Mamba model under domain shifts and find that\ninput-dependent matrices within SSMs could accumulate and amplify\ndomain-specific features, thus hindering model generalization. To address this\nissue, we propose a novel SSM-based architecture with saliency-based\ntoken-aware transformation (namely START), which achieves state-of-the-art\n(SOTA) performances and offers a competitive alternative to CNNs and ViTs. Our\nSTART can selectively perturb and suppress domain-specific features in salient\ntokens within the input-dependent matrices of SSMs, thus effectively reducing\nthe discrepancy between different domains. Extensive experiments on five\nbenchmarks demonstrate that START outperforms existing SOTA DG methods with\nefficient linear complexity. Our code is available at\nhttps://github.com/lingeringlight/START."
                },
                "authors": [
                    {
                        "name": "Jintao Guo"
                    },
                    {
                        "name": "Lei Qi"
                    },
                    {
                        "name": "Yinghuan Shi"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "arxiv_comment": "Accepted by NeurIPS2024. The code is available at\n  https://github.com/lingeringlight/START",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02506v2",
                "updated": "2025-01-07T09:13:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    13,
                    35,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-05T11:06:55Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    11,
                    6,
                    55,
                    6,
                    5,
                    0
                ],
                "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use"
                },
                "summary": "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https://huggingface.co/datasets/bytedance-research/ToolHop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https://huggingface.co/datasets/bytedance-research/ToolHop."
                },
                "authors": [
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Xuesong Yao"
                    },
                    {
                        "name": "Weijian Lin"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Zaiyuan Wang"
                    },
                    {
                        "name": "Sining Zhu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Jiecao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiecao Chen"
                },
                "author": "Jiecao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.01154v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.01154v4",
                "updated": "2025-01-07T09:12:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    12,
                    47,
                    1,
                    7,
                    0
                ],
                "published": "2024-01-02T11:08:39Z",
                "published_parsed": [
                    2024,
                    1,
                    2,
                    11,
                    8,
                    39,
                    1,
                    2,
                    0
                ],
                "title": "Applying Bayesian Data Analysis for Causal Inference about Requirements\n  Quality: A Controlled Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying Bayesian Data Analysis for Causal Inference about Requirements\n  Quality: A Controlled Experiment"
                },
                "summary": "It is commonly accepted that the quality of requirements specifications\nimpacts subsequent software engineering activities. However, we still lack\nempirical evidence to support organizations in deciding whether their\nrequirements are good enough or impede subsequent activities. We aim to\ncontribute empirical evidence to the effect that requirements quality defects\nhave on a software engineering activity that depends on this requirement. We\nconduct a controlled experiment in which 25 participants from industry and\nuniversity generate domain models from four natural language requirements\ncontaining different quality defects. We evaluate the resulting models using\nboth frequentist and Bayesian data analysis. Contrary to our expectations, our\nresults show that the use of passive voice only has a minor impact on the\nresulting domain models. The use of ambiguous pronouns, however, shows a strong\neffect on various properties of the resulting domain models. Most notably,\nambiguous pronouns lead to incorrect associations in domain models. Despite\nbeing equally advised against by literature and frequentist methods, the\nBayesian data analysis shows that the two investigated quality defects have\nvastly different impacts on software engineering activities and, hence, deserve\ndifferent levels of attention. Our employed method can be further utilized by\nresearchers to improve reliable, detailed empirical evidence on requirements\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is commonly accepted that the quality of requirements specifications\nimpacts subsequent software engineering activities. However, we still lack\nempirical evidence to support organizations in deciding whether their\nrequirements are good enough or impede subsequent activities. We aim to\ncontribute empirical evidence to the effect that requirements quality defects\nhave on a software engineering activity that depends on this requirement. We\nconduct a controlled experiment in which 25 participants from industry and\nuniversity generate domain models from four natural language requirements\ncontaining different quality defects. We evaluate the resulting models using\nboth frequentist and Bayesian data analysis. Contrary to our expectations, our\nresults show that the use of passive voice only has a minor impact on the\nresulting domain models. The use of ambiguous pronouns, however, shows a strong\neffect on various properties of the resulting domain models. Most notably,\nambiguous pronouns lead to incorrect associations in domain models. Despite\nbeing equally advised against by literature and frequentist methods, the\nBayesian data analysis shows that the two investigated quality defects have\nvastly different impacts on software engineering activities and, hence, deserve\ndifferent levels of attention. Our employed method can be further utilized by\nresearchers to improve reliable, detailed empirical evidence on requirements\nquality."
                },
                "authors": [
                    {
                        "name": "Julian Frattini"
                    },
                    {
                        "name": "Davide Fucci"
                    },
                    {
                        "name": "Richard Torkar"
                    },
                    {
                        "name": "Lloyd Montgomery"
                    },
                    {
                        "name": "Michael Unterkalmsteiner"
                    },
                    {
                        "name": "Jannik Fischbach"
                    },
                    {
                        "name": "Daniel Mendez"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Mendez"
                },
                "author": "Daniel Mendez",
                "arxiv_doi": "10.1007/s10664-024-10582-1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10664-024-10582-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.01154v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.01154v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07594v2",
                "updated": "2025-01-07T09:00:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    0,
                    7,
                    1,
                    7,
                    0
                ],
                "published": "2024-08-14T14:49:25Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    49,
                    25,
                    2,
                    227,
                    0
                ],
                "title": "Crossover Designs in Software Engineering Experiments: Review of the\n  State of Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crossover Designs in Software Engineering Experiments: Review of the\n  State of Analysis"
                },
                "summary": "Experimentation is an essential method for causal inference in any empirical\ndiscipline. Crossover-design experiments are common in Software Engineering\n(SE) research. In these, subjects apply more than one treatment in different\norders. This design increases the amount of obtained data and deals with\nsubject variability but introduces threats to internal validity like the\nlearning and carryover effect. Vegas et al. reviewed the state of practice for\ncrossover designs in SE research and provided guidelines on how to address its\nthreats during data analysis while still harnessing its benefits. In this\npaper, we reflect on the impact of these guidelines and review the state of\nanalysis of crossover design experiments in SE publications between 2015 and\nMarch 2024. To this end, by conducting a forward snowballing of the guidelines,\nwe survey 136 publications reporting 67 crossover-design experiments and\nevaluate their data analysis against the provided guidelines. The results show\nthat the validity of data analyses has improved compared to the original state\nof analysis. Still, despite the explicit guidelines, only 29.5% of all threats\nto validity were addressed properly. While the maturation and the optimal\nsequence threats are properly addressed in 35.8% and 38.8% of all studies in\nour sample respectively, the carryover threat is only modeled in about 3% of\nthe observed cases. The lack of adherence to the analysis guidelines threatens\nthe validity of the conclusions drawn from crossover design experiments",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimentation is an essential method for causal inference in any empirical\ndiscipline. Crossover-design experiments are common in Software Engineering\n(SE) research. In these, subjects apply more than one treatment in different\norders. This design increases the amount of obtained data and deals with\nsubject variability but introduces threats to internal validity like the\nlearning and carryover effect. Vegas et al. reviewed the state of practice for\ncrossover designs in SE research and provided guidelines on how to address its\nthreats during data analysis while still harnessing its benefits. In this\npaper, we reflect on the impact of these guidelines and review the state of\nanalysis of crossover design experiments in SE publications between 2015 and\nMarch 2024. To this end, by conducting a forward snowballing of the guidelines,\nwe survey 136 publications reporting 67 crossover-design experiments and\nevaluate their data analysis against the provided guidelines. The results show\nthat the validity of data analyses has improved compared to the original state\nof analysis. Still, despite the explicit guidelines, only 29.5% of all threats\nto validity were addressed properly. While the maturation and the optimal\nsequence threats are properly addressed in 35.8% and 38.8% of all studies in\nour sample respectively, the carryover threat is only modeled in about 3% of\nthe observed cases. The lack of adherence to the analysis guidelines threatens\nthe validity of the conclusions drawn from crossover design experiments"
                },
                "authors": [
                    {
                        "name": "Julian Frattini"
                    },
                    {
                        "name": "Davide Fucci"
                    },
                    {
                        "name": "Sira Vegas"
                    }
                ],
                "author_detail": {
                    "name": "Sira Vegas"
                },
                "author": "Sira Vegas",
                "arxiv_doi": "10.1145/3674805.3690754",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3674805.3690754",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.07594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03630v1",
                "updated": "2025-01-07T09:00:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    0,
                    7,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T09:00:07Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    0,
                    7,
                    1,
                    7,
                    0
                ],
                "title": "MC-VTON: Minimal Control Virtual Try-On Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC-VTON: Minimal Control Virtual Try-On Diffusion Transformer"
                },
                "summary": "Virtual try-on methods based on diffusion models achieve realistic try-on\neffects. They use an extra reference network or an additional image encoder to\nprocess multiple conditional image inputs, which results in high training\ncosts. Besides, they require more than 25 inference steps, bringing a long\ninference time. In this work, with the development of diffusion transformer\n(DiT), we rethink the necessity of reference network or image encoder, then\npropose MC-VTON, enabling DiT to integrate minimal conditional try-on inputs by\nutilizing its intrinsic backbone. Compared to existing methods, the superiority\nof MC-VTON is demonstrated in four aspects: (1)Superior detail fidelity. Our\nDiT-based MC-VTON exhibits superior fidelity in preserving fine-grained\ndetails. (2)Simplified network and inputs. We remove any extra reference\nnetwork or image encoder. We also remove unnecessary conditions like the long\nprompt, pose estimation, human parsing, and depth map. We require only the\nmasked person image and the garment image. (3)Parameter-efficient training. To\nprocess the try-on task, we fine-tune the FLUX.1-dev with only 39.7M additional\nparameters 0.33% of the backbone parameters). (4)Less inference steps. We apply\ndistillation diffusion on MC-VTON and only need 8 steps to generate a realistic\ntry-on image, with only 86.8M additional parameters (0.72% of the backbone\nparameters). Experiments show that MC-VTON achieves superior qualitative and\nquantitative results with fewer condition inputs, fewer inference steps, and\nfewer trainable parameters than baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual try-on methods based on diffusion models achieve realistic try-on\neffects. They use an extra reference network or an additional image encoder to\nprocess multiple conditional image inputs, which results in high training\ncosts. Besides, they require more than 25 inference steps, bringing a long\ninference time. In this work, with the development of diffusion transformer\n(DiT), we rethink the necessity of reference network or image encoder, then\npropose MC-VTON, enabling DiT to integrate minimal conditional try-on inputs by\nutilizing its intrinsic backbone. Compared to existing methods, the superiority\nof MC-VTON is demonstrated in four aspects: (1)Superior detail fidelity. Our\nDiT-based MC-VTON exhibits superior fidelity in preserving fine-grained\ndetails. (2)Simplified network and inputs. We remove any extra reference\nnetwork or image encoder. We also remove unnecessary conditions like the long\nprompt, pose estimation, human parsing, and depth map. We require only the\nmasked person image and the garment image. (3)Parameter-efficient training. To\nprocess the try-on task, we fine-tune the FLUX.1-dev with only 39.7M additional\nparameters 0.33% of the backbone parameters). (4)Less inference steps. We apply\ndistillation diffusion on MC-VTON and only need 8 steps to generate a realistic\ntry-on image, with only 86.8M additional parameters (0.72% of the backbone\nparameters). Experiments show that MC-VTON achieves superior qualitative and\nquantitative results with fewer condition inputs, fewer inference steps, and\nfewer trainable parameters than baseline methods."
                },
                "authors": [
                    {
                        "name": "Junsheng Luan"
                    },
                    {
                        "name": "Guangyuan Li"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Wei Xing"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xing"
                },
                "author": "Wei Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03626v1",
                "updated": "2025-01-07T08:52:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    8,
                    52,
                    55,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T08:52:55Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    8,
                    52,
                    55,
                    1,
                    7,
                    0
                ],
                "title": "CommitShield: Tracking Vulnerability Introduction and Fix in Version\n  Control Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommitShield: Tracking Vulnerability Introduction and Fix in Version\n  Control Systems"
                },
                "summary": "Version control systems are commonly used to manage open-source software, in\nwhich each commit may introduce new vulnerabilities or fix existing ones.\nResearchers have developed various tools for detecting vulnerabilities in code\ncommits, but their performance is limited by factors such as neglecting\ndescriptive data and challenges in accurately identifying vulnerability\nintroductions. To overcome these limitations, we propose CommitShield, which\ncombines the code analysis capabilities of static analysis tools with the\nnatural language and code understanding capabilities of large language models\n(LLMs) to enhance the accuracy of vulnerability introduction and fix detection\nby generating precise descriptions and obtaining rich patch contexts. We\nevaluate CommitShield using the newly constructed vulnerability repair dataset,\nCommitVulFix, and a cleaned vulnerability introduction dataset. Experimental\nresults indicate that CommitShield improves recall by 76%-87% over\nstate-of-the-art methods in the vulnerability fix detection task, and its\nF1-score improves by 15%-27% in the vulnerability introduction detection task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Version control systems are commonly used to manage open-source software, in\nwhich each commit may introduce new vulnerabilities or fix existing ones.\nResearchers have developed various tools for detecting vulnerabilities in code\ncommits, but their performance is limited by factors such as neglecting\ndescriptive data and challenges in accurately identifying vulnerability\nintroductions. To overcome these limitations, we propose CommitShield, which\ncombines the code analysis capabilities of static analysis tools with the\nnatural language and code understanding capabilities of large language models\n(LLMs) to enhance the accuracy of vulnerability introduction and fix detection\nby generating precise descriptions and obtaining rich patch contexts. We\nevaluate CommitShield using the newly constructed vulnerability repair dataset,\nCommitVulFix, and a cleaned vulnerability introduction dataset. Experimental\nresults indicate that CommitShield improves recall by 76%-87% over\nstate-of-the-art methods in the vulnerability fix detection task, and its\nF1-score improves by 15%-27% in the vulnerability introduction detection task."
                },
                "authors": [
                    {
                        "name": "Zhaonan Wu"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Zirui Wan"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03624v1",
                "updated": "2025-01-07T08:49:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    8,
                    49,
                    4,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T08:49:04Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    8,
                    49,
                    4,
                    1,
                    7,
                    0
                ],
                "title": "LlaMADRS: Prompting Large Language Models for Interview-Based Depression\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlaMADRS: Prompting Large Language Models for Interview-Based Depression\n  Assessment"
                },
                "summary": "This study introduces LlaMADRS, a novel framework leveraging open-source\nLarge Language Models (LLMs) to automate depression severity assessment using\nthe Montgomery-Asberg Depression Rating Scale (MADRS). We employ a zero-shot\nprompting strategy with carefully designed cues to guide the model in\ninterpreting and scoring transcribed clinical interviews. Our approach, tested\non 236 real-world interviews from the Context-Adaptive Multimodal Informatics\n(CAMI) dataset, demonstrates strong correlations with clinician assessments.\nThe Qwen 2.5--72b model achieves near-human level agreement across most MADRS\nitems, with Intraclass Correlation Coefficients (ICC) closely approaching those\nbetween human raters. We provide a comprehensive analysis of model performance\nacross different MADRS items, highlighting strengths and current limitations.\nOur findings suggest that LLMs, with appropriate prompting, can serve as\nefficient tools for mental health assessment, potentially increasing\naccessibility in resource-limited settings. However, challenges remain,\nparticularly in assessing symptoms that rely on non-verbal cues, underscoring\nthe need for multimodal approaches in future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces LlaMADRS, a novel framework leveraging open-source\nLarge Language Models (LLMs) to automate depression severity assessment using\nthe Montgomery-Asberg Depression Rating Scale (MADRS). We employ a zero-shot\nprompting strategy with carefully designed cues to guide the model in\ninterpreting and scoring transcribed clinical interviews. Our approach, tested\non 236 real-world interviews from the Context-Adaptive Multimodal Informatics\n(CAMI) dataset, demonstrates strong correlations with clinician assessments.\nThe Qwen 2.5--72b model achieves near-human level agreement across most MADRS\nitems, with Intraclass Correlation Coefficients (ICC) closely approaching those\nbetween human raters. We provide a comprehensive analysis of model performance\nacross different MADRS items, highlighting strengths and current limitations.\nOur findings suggest that LLMs, with appropriate prompting, can serve as\nefficient tools for mental health assessment, potentially increasing\naccessibility in resource-limited settings. However, challenges remain,\nparticularly in assessing symptoms that rely on non-verbal cues, underscoring\nthe need for multimodal approaches in future work."
                },
                "authors": [
                    {
                        "name": "Gaoussou Youssouf Kebe"
                    },
                    {
                        "name": "Jeffrey M. Girard"
                    },
                    {
                        "name": "Einat Liebenthal"
                    },
                    {
                        "name": "Justin Baker"
                    },
                    {
                        "name": "Fernando De la Torre"
                    },
                    {
                        "name": "Louis-Philippe Morency"
                    }
                ],
                "author_detail": {
                    "name": "Louis-Philippe Morency"
                },
                "author": "Louis-Philippe Morency",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04783v2",
                "updated": "2025-01-07T08:23:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    8,
                    23,
                    43,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-06T05:20:08Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    5,
                    20,
                    8,
                    4,
                    341,
                    0
                ],
                "title": "KNN-MMD: Cross Domain Wireless Sensing via Local Distribution Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KNN-MMD: Cross Domain Wireless Sensing via Local Distribution Alignment"
                },
                "summary": "Wireless sensing has recently found widespread applications in diverse\nenvironments, including homes, offices, and public spaces. By analyzing\npatterns in channel state information (CSI), it is possible to infer human\nactions for tasks such as person identification, gesture recognition, and fall\ndetection. However, CSI is highly sensitive to environmental changes, where\neven minor alterations can significantly distort the CSI patterns. This\nsensitivity often leads to performance degradation or outright failure when\napplying wireless sensing models trained in one environment to another. To\naddress this challenge, Domain Alignment (DAL) has been widely adopted for\ncross-domain classification tasks, as it focuses on aligning the global\ndistributions of the source and target domains in feature space. Despite its\npopularity, DAL often neglects inter-category relationships, which can lead to\nmisalignment between categories across domains, even when global alignment is\nachieved. To overcome these limitations, we propose K-Nearest Neighbors Maximum\nMean Discrepancy (KNN-MMD), a novel few-shot method for cross-domain wireless\nsensing. Our approach begins by constructing a help set using KNN from the\ntarget domain, enabling local alignment between the source and target domains\nwithin each category using MMD. Additionally, we address a key instability\nissue commonly observed in cross-domain methods, where model performance\nfluctuates sharply between epochs. Further, most existing methods struggle to\ndetermine an optimal stopping point during training due to the absence of\nlabeled data from the target domain. Our method resolves this by excluding the\nsupport set from the target domain during training and employing it as a\nvalidation set to determine the stopping criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless sensing has recently found widespread applications in diverse\nenvironments, including homes, offices, and public spaces. By analyzing\npatterns in channel state information (CSI), it is possible to infer human\nactions for tasks such as person identification, gesture recognition, and fall\ndetection. However, CSI is highly sensitive to environmental changes, where\neven minor alterations can significantly distort the CSI patterns. This\nsensitivity often leads to performance degradation or outright failure when\napplying wireless sensing models trained in one environment to another. To\naddress this challenge, Domain Alignment (DAL) has been widely adopted for\ncross-domain classification tasks, as it focuses on aligning the global\ndistributions of the source and target domains in feature space. Despite its\npopularity, DAL often neglects inter-category relationships, which can lead to\nmisalignment between categories across domains, even when global alignment is\nachieved. To overcome these limitations, we propose K-Nearest Neighbors Maximum\nMean Discrepancy (KNN-MMD), a novel few-shot method for cross-domain wireless\nsensing. Our approach begins by constructing a help set using KNN from the\ntarget domain, enabling local alignment between the source and target domains\nwithin each category using MMD. Additionally, we address a key instability\nissue commonly observed in cross-domain methods, where model performance\nfluctuates sharply between epochs. Further, most existing methods struggle to\ndetermine an optimal stopping point during training due to the absence of\nlabeled data from the target domain. Our method resolves this by excluding the\nsupport set from the target domain during training and employing it as a\nvalidation set to determine the stopping criterion."
                },
                "authors": [
                    {
                        "name": "Zijian Zhao"
                    },
                    {
                        "name": "Zhijie Cai"
                    },
                    {
                        "name": "Tingwei Chen"
                    },
                    {
                        "name": "Xiaoyang Li"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Qimei Chen"
                    },
                    {
                        "name": "Guangxu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Guangxu Zhu"
                },
                "author": "Guangxu Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10625v2",
                "updated": "2025-01-07T07:58:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    7,
                    58,
                    58,
                    1,
                    7,
                    0
                ],
                "published": "2024-09-16T18:00:40Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    0,
                    40,
                    0,
                    260,
                    0
                ],
                "title": "Two transitions in complex eigenvalue statistics: Hermiticity and\n  integrability breaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two transitions in complex eigenvalue statistics: Hermiticity and\n  integrability breaking"
                },
                "summary": "Open quantum systems have complex energy eigenvalues which are expected to\nfollow non-Hermitian random matrix statistics when chaotic, or 2-dimensional\n(2d) Poisson statistics when integrable. We investigate the spectral properties\nof a many-body quantum spin chain, the Hermitian XXZ Heisenberg model with\nimaginary disorder. Its rich complex eigenvalue statistics is found to\nseparately break both Hermiticity and integrability at different scales of the\ndisorder strength. With no disorder, the system is integrable and Hermitian,\nwith spectral statistics corresponding to the 1d Poisson point process. At very\nsmall disorder, we find a transition from 1d Poisson statistics to an effective\n$D$-dimensional Poisson point process, showing Hermiticity breaking. At\nintermediate disorder we find integrability breaking, as inferred from the\nstatistics matching that of non-Hermitian complex symmetric random matrices in\nclass AI$^\\dag$. For large disorder, as the spins align, we recover the\nexpected integrability (now in the non-Hermitian setup), indicated by 2d\nPoisson statistics. These conclusions are based on fitting the spin chain data\nof numerically generated nearest and next-to-nearest neighbour spacing\ndistributions to an effective 2d Coulomb gas description at inverse temperature\n$\\beta$. We confirm such an effective description of random matrices also\napplies in class AI$^\\dag$ and AII$^\\dag$ up to next-to-nearest neighbour\nspacings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open quantum systems have complex energy eigenvalues which are expected to\nfollow non-Hermitian random matrix statistics when chaotic, or 2-dimensional\n(2d) Poisson statistics when integrable. We investigate the spectral properties\nof a many-body quantum spin chain, the Hermitian XXZ Heisenberg model with\nimaginary disorder. Its rich complex eigenvalue statistics is found to\nseparately break both Hermiticity and integrability at different scales of the\ndisorder strength. With no disorder, the system is integrable and Hermitian,\nwith spectral statistics corresponding to the 1d Poisson point process. At very\nsmall disorder, we find a transition from 1d Poisson statistics to an effective\n$D$-dimensional Poisson point process, showing Hermiticity breaking. At\nintermediate disorder we find integrability breaking, as inferred from the\nstatistics matching that of non-Hermitian complex symmetric random matrices in\nclass AI$^\\dag$. For large disorder, as the spins align, we recover the\nexpected integrability (now in the non-Hermitian setup), indicated by 2d\nPoisson statistics. These conclusions are based on fitting the spin chain data\nof numerically generated nearest and next-to-nearest neighbour spacing\ndistributions to an effective 2d Coulomb gas description at inverse temperature\n$\\beta$. We confirm such an effective description of random matrices also\napplies in class AI$^\\dag$ and AII$^\\dag$ up to next-to-nearest neighbour\nspacings."
                },
                "authors": [
                    {
                        "name": "G. Akemann"
                    },
                    {
                        "name": "F. Balducci"
                    },
                    {
                        "name": "A. Chenu"
                    },
                    {
                        "name": "P. Päßler"
                    },
                    {
                        "name": "F. Roccati"
                    },
                    {
                        "name": "R. Shir"
                    }
                ],
                "author_detail": {
                    "name": "R. Shir"
                },
                "author": "R. Shir",
                "arxiv_comment": "12+2 pages, 12+2 figures. Accepted for publication in Phys. Rev.\n  Research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03814v2",
                "updated": "2025-01-07T07:46:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    7,
                    46,
                    16,
                    1,
                    7,
                    0
                ],
                "published": "2024-11-06T10:32:09Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    32,
                    9,
                    2,
                    311,
                    0
                ],
                "title": "MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue"
                },
                "summary": "Large Language Models (LLMs) demonstrate outstanding performance in their\nreservoir of knowledge and understanding capabilities, but they have also been\nshown to be prone to illegal or unethical reactions when subjected to jailbreak\nattacks. To ensure their responsible deployment in critical applications, it is\ncrucial to understand the safety capabilities and vulnerabilities of LLMs.\nPrevious works mainly focus on jailbreak in single-round dialogue, overlooking\nthe potential jailbreak risks in multi-round dialogues, which are a vital way\nhumans interact with and extract information from LLMs. Some studies have\nincreasingly concentrated on the risks associated with jailbreak in multi-round\ndialogues. These efforts typically involve the use of manually crafted\ntemplates or prompt engineering techniques. However, due to the inherent\ncomplexity of multi-round dialogues, their jailbreak performance is limited. To\nsolve this problem, we propose a novel multi-round dialogue jailbreaking agent,\nemphasizing the importance of stealthiness in identifying and mitigating\npotential threats to human values posed by LLMs. We propose a risk\ndecomposition strategy that distributes risks across multiple rounds of queries\nand utilizes psychological strategies to enhance attack strength. Extensive\nexperiments show that our proposed method surpasses other attack methods and\nachieves state-of-the-art attack success rate. We will make the corresponding\ncode and dataset available for future research. The code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate outstanding performance in their\nreservoir of knowledge and understanding capabilities, but they have also been\nshown to be prone to illegal or unethical reactions when subjected to jailbreak\nattacks. To ensure their responsible deployment in critical applications, it is\ncrucial to understand the safety capabilities and vulnerabilities of LLMs.\nPrevious works mainly focus on jailbreak in single-round dialogue, overlooking\nthe potential jailbreak risks in multi-round dialogues, which are a vital way\nhumans interact with and extract information from LLMs. Some studies have\nincreasingly concentrated on the risks associated with jailbreak in multi-round\ndialogues. These efforts typically involve the use of manually crafted\ntemplates or prompt engineering techniques. However, due to the inherent\ncomplexity of multi-round dialogues, their jailbreak performance is limited. To\nsolve this problem, we propose a novel multi-round dialogue jailbreaking agent,\nemphasizing the importance of stealthiness in identifying and mitigating\npotential threats to human values posed by LLMs. We propose a risk\ndecomposition strategy that distributes risks across multiple rounds of queries\nand utilizes psychological strategies to enhance attack strength. Extensive\nexperiments show that our proposed method surpasses other attack methods and\nachieves state-of-the-art attack success rate. We will make the corresponding\ncode and dataset available for future research. The code will be released soon."
                },
                "authors": [
                    {
                        "name": "Fengxiang Wang"
                    },
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Peng Xiao"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Shiji Zhao"
                    },
                    {
                        "name": "Cheng Wei"
                    },
                    {
                        "name": "YueFeng Chen"
                    },
                    {
                        "name": "Chongwen Wang"
                    },
                    {
                        "name": "Jialing Tao"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Hui Xue"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xue"
                },
                "author": "Hui Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15507v2",
                "updated": "2025-01-07T07:35:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    7,
                    35,
                    10,
                    1,
                    7,
                    0
                ],
                "published": "2024-07-22T09:44:35Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    9,
                    44,
                    35,
                    0,
                    204,
                    0
                ],
                "title": "SpotDiffusion: A Fast Approach For Seamless Panorama Generation Over\n  Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpotDiffusion: A Fast Approach For Seamless Panorama Generation Over\n  Time"
                },
                "summary": "Generating high-resolution images with generative models has recently been\nmade widely accessible by leveraging diffusion models pre-trained on\nlarge-scale datasets. Various techniques, such as MultiDiffusion and\nSyncDiffusion, have further pushed image generation beyond training\nresolutions, i.e., from square images to panorama, by merging multiple\noverlapping diffusion paths or employing gradient descent to maintain\nperceptual coherence. However, these methods suffer from significant\ncomputational inefficiencies due to generating and averaging numerous\npredictions, which is required in practice to produce high-quality and seamless\nimages. This work addresses this limitation and presents a novel approach that\neliminates the need to generate and average numerous overlapping denoising\npredictions. Our method shifts non-overlapping denoising windows over time,\nensuring that seams in one timestep are corrected in the next. This results in\ncoherent, high-resolution images with fewer overall steps. We demonstrate the\neffectiveness of our approach through qualitative and quantitative evaluations,\ncomparing it with MultiDiffusion, SyncDiffusion, and StitchDiffusion. Our\nmethod offers several key benefits, including improved computational efficiency\nand faster inference times while producing comparable or better image quality.\nLink to code https://github.com/stanifrolov/spotdiffusion",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-resolution images with generative models has recently been\nmade widely accessible by leveraging diffusion models pre-trained on\nlarge-scale datasets. Various techniques, such as MultiDiffusion and\nSyncDiffusion, have further pushed image generation beyond training\nresolutions, i.e., from square images to panorama, by merging multiple\noverlapping diffusion paths or employing gradient descent to maintain\nperceptual coherence. However, these methods suffer from significant\ncomputational inefficiencies due to generating and averaging numerous\npredictions, which is required in practice to produce high-quality and seamless\nimages. This work addresses this limitation and presents a novel approach that\neliminates the need to generate and average numerous overlapping denoising\npredictions. Our method shifts non-overlapping denoising windows over time,\nensuring that seams in one timestep are corrected in the next. This results in\ncoherent, high-resolution images with fewer overall steps. We demonstrate the\neffectiveness of our approach through qualitative and quantitative evaluations,\ncomparing it with MultiDiffusion, SyncDiffusion, and StitchDiffusion. Our\nmethod offers several key benefits, including improved computational efficiency\nand faster inference times while producing comparable or better image quality.\nLink to code https://github.com/stanifrolov/spotdiffusion"
                },
                "authors": [
                    {
                        "name": "Stanislav Frolov"
                    },
                    {
                        "name": "Brian B. Moser"
                    },
                    {
                        "name": "Andreas Dengel"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Dengel"
                },
                "author": "Andreas Dengel",
                "arxiv_comment": "Project page: https://spotdiffusion.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03572v1",
                "updated": "2025-01-07T06:51:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    51,
                    46,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T06:51:46Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    51,
                    46,
                    1,
                    7,
                    0
                ],
                "title": "From Code to Compliance: Assessing ChatGPT's Utility in Designing an\n  Accessible Webpage -- A Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Code to Compliance: Assessing ChatGPT's Utility in Designing an\n  Accessible Webpage -- A Case Study"
                },
                "summary": "Web accessibility ensures that individuals with disabilities can access and\ninteract with digital content without barriers, yet a significant majority of\nmost used websites fail to meet accessibility standards. This study evaluates\nChatGPT's (GPT-4o) ability to generate and improve web pages in line with Web\nContent Accessibility Guidelines (WCAG). While ChatGPT can effectively address\naccessibility issues when prompted, its default code often lacks compliance,\nreflecting limitations in its training data and prevailing inaccessible web\npractices. Automated and manual testing revealed strengths in resolving simple\nissues but challenges with complex tasks, requiring human oversight and\nadditional iterations. Unlike prior studies, we incorporate manual evaluation,\ndynamic elements, and use the visual reasoning capability of ChatGPT along with\nthe prompts to fix accessibility issues. Providing screenshots alongside\nprompts enhances the LLM's ability to address accessibility issues by allowing\nit to analyze surrounding components, such as determining appropriate contrast\ncolors. We found that effective prompt engineering, such as providing concise,\nstructured feedback and incorporating visual aids, significantly enhances\nChatGPT's performance. These findings highlight the potential and limitations\nof large language models for accessible web development, offering practical\nguidance for developers to create more inclusive websites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web accessibility ensures that individuals with disabilities can access and\ninteract with digital content without barriers, yet a significant majority of\nmost used websites fail to meet accessibility standards. This study evaluates\nChatGPT's (GPT-4o) ability to generate and improve web pages in line with Web\nContent Accessibility Guidelines (WCAG). While ChatGPT can effectively address\naccessibility issues when prompted, its default code often lacks compliance,\nreflecting limitations in its training data and prevailing inaccessible web\npractices. Automated and manual testing revealed strengths in resolving simple\nissues but challenges with complex tasks, requiring human oversight and\nadditional iterations. Unlike prior studies, we incorporate manual evaluation,\ndynamic elements, and use the visual reasoning capability of ChatGPT along with\nthe prompts to fix accessibility issues. Providing screenshots alongside\nprompts enhances the LLM's ability to address accessibility issues by allowing\nit to analyze surrounding components, such as determining appropriate contrast\ncolors. We found that effective prompt engineering, such as providing concise,\nstructured feedback and incorporating visual aids, significantly enhances\nChatGPT's performance. These findings highlight the potential and limitations\nof large language models for accessible web development, offering practical\nguidance for developers to create more inclusive websites."
                },
                "authors": [
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Margarida Fresco"
                    },
                    {
                        "name": "Fredrik Forsberg"
                    },
                    {
                        "name": "Hallvard Grotli"
                    }
                ],
                "author_detail": {
                    "name": "Hallvard Grotli"
                },
                "author": "Hallvard Grotli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.1.2; F.3.1; F.4.1; D.3.2; H.1.2; H.5.2; D.2.2; H.1.2; I.3.6;\n  H.5.4; H.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03571v1",
                "updated": "2025-01-07T06:51:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    51,
                    17,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T06:51:17Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    51,
                    17,
                    1,
                    7,
                    0
                ],
                "title": "AADNet: Exploring EEG Spatiotemporal Information for Fast and Accurate\n  Orientation and Timbre Detection of Auditory Attention Based on A Cue-Masked\n  Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AADNet: Exploring EEG Spatiotemporal Information for Fast and Accurate\n  Orientation and Timbre Detection of Auditory Attention Based on A Cue-Masked\n  Paradigm"
                },
                "summary": "Auditory attention decoding from electroencephalogram (EEG) could infer to\nwhich source the user is attending in noisy environments. Decoding algorithms\nand experimental paradigm designs are crucial for the development of technology\nin practical applications. To simulate real-world scenarios, this study\nproposed a cue-masked auditory attention paradigm to avoid information leakage\nbefore the experiment. To obtain high decoding accuracy with low latency, an\nend-to-end deep learning model, AADNet, was proposed to exploit the\nspatiotemporal information from the short time window of EEG signals. The\nresults showed that with a 0.5-second EEG window, AADNet achieved an average\naccuracy of 93.46% and 91.09% in decoding auditory orientation attention (OA)\nand timbre attention (TA), respectively. It significantly outperformed five\nprevious methods and did not need the knowledge of the original audio source.\nThis work demonstrated that it was possible to detect the orientation and\ntimbre of auditory attention from EEG signals fast and accurately. The results\nare promising for the real-time multi-property auditory attention decoding,\nfacilitating the application of the neuro-steered hearing aids and other\nassistive listening devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditory attention decoding from electroencephalogram (EEG) could infer to\nwhich source the user is attending in noisy environments. Decoding algorithms\nand experimental paradigm designs are crucial for the development of technology\nin practical applications. To simulate real-world scenarios, this study\nproposed a cue-masked auditory attention paradigm to avoid information leakage\nbefore the experiment. To obtain high decoding accuracy with low latency, an\nend-to-end deep learning model, AADNet, was proposed to exploit the\nspatiotemporal information from the short time window of EEG signals. The\nresults showed that with a 0.5-second EEG window, AADNet achieved an average\naccuracy of 93.46% and 91.09% in decoding auditory orientation attention (OA)\nand timbre attention (TA), respectively. It significantly outperformed five\nprevious methods and did not need the knowledge of the original audio source.\nThis work demonstrated that it was possible to detect the orientation and\ntimbre of auditory attention from EEG signals fast and accurately. The results\nare promising for the real-time multi-property auditory attention decoding,\nfacilitating the application of the neuro-steered hearing aids and other\nassistive listening devices."
                },
                "authors": [
                    {
                        "name": "Keren Shi"
                    },
                    {
                        "name": "Xu Liu"
                    },
                    {
                        "name": "Xue Yuan"
                    },
                    {
                        "name": "Haijie Shang"
                    },
                    {
                        "name": "Ruiting Dai"
                    },
                    {
                        "name": "Hanbin Wang"
                    },
                    {
                        "name": "Yunfa Fu"
                    },
                    {
                        "name": "Ning Jiang"
                    },
                    {
                        "name": "Jiayuan He"
                    }
                ],
                "author_detail": {
                    "name": "Jiayuan He"
                },
                "author": "Jiayuan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09498v2",
                "updated": "2025-01-07T06:49:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    49,
                    9,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-12T17:47:08Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    47,
                    8,
                    3,
                    347,
                    0
                ],
                "title": "Gradient descent inference in empirical risk minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient descent inference in empirical risk minimization"
                },
                "summary": "Gradient descent is one of the most widely used iterative algorithms in\nmodern statistical learning. However, its precise algorithmic dynamics in\nhigh-dimensional settings remain only partially understood, which has therefore\nlimited its broader potential for statistical inference applications.\n  This paper provides a precise, non-asymptotic distributional characterization\nof gradient descent iterates in a broad class of empirical risk minimization\nproblems, in the so-called mean-field regime where the sample size is\nproportional to the signal dimension. Our non-asymptotic state evolution theory\nholds for both general non-convex loss functions and non-Gaussian data, and\nreveals the central role of two Onsager correction matrices that precisely\ncharacterize the non-trivial dependence among all gradient descent iterates in\nthe mean-field regime.\n  Although the Onsager correction matrices are typically analytically\nintractable, our state evolution theory facilitates a generic gradient descent\ninference algorithm that consistently estimates these matrices across a broad\nclass of models. Leveraging this algorithm, we show that the state evolution\ncan be inverted to construct (i) data-driven estimators for the generalization\nerror of gradient descent iterates and (ii) debiased gradient descent iterates\nfor inference of the unknown signal. Detailed applications to two canonical\nmodels--linear regression and (generalized) logistic regression--are worked out\nto illustrate model-specific features of our general theory and inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient descent is one of the most widely used iterative algorithms in\nmodern statistical learning. However, its precise algorithmic dynamics in\nhigh-dimensional settings remain only partially understood, which has therefore\nlimited its broader potential for statistical inference applications.\n  This paper provides a precise, non-asymptotic distributional characterization\nof gradient descent iterates in a broad class of empirical risk minimization\nproblems, in the so-called mean-field regime where the sample size is\nproportional to the signal dimension. Our non-asymptotic state evolution theory\nholds for both general non-convex loss functions and non-Gaussian data, and\nreveals the central role of two Onsager correction matrices that precisely\ncharacterize the non-trivial dependence among all gradient descent iterates in\nthe mean-field regime.\n  Although the Onsager correction matrices are typically analytically\nintractable, our state evolution theory facilitates a generic gradient descent\ninference algorithm that consistently estimates these matrices across a broad\nclass of models. Leveraging this algorithm, we show that the state evolution\ncan be inverted to construct (i) data-driven estimators for the generalization\nerror of gradient descent iterates and (ii) debiased gradient descent iterates\nfor inference of the unknown signal. Detailed applications to two canonical\nmodels--linear regression and (generalized) logistic regression--are worked out\nto illustrate model-specific features of our general theory and inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Qiyang Han"
                    },
                    {
                        "name": "Xiaocong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaocong Xu"
                },
                "author": "Xiaocong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03569v1",
                "updated": "2025-01-07T06:44:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    44,
                    41,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T06:44:41Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    44,
                    41,
                    1,
                    7,
                    0
                ],
                "title": "What Does a Software Engineer Look Like? Exploring Societal Stereotypes\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Does a Software Engineer Look Like? Exploring Societal Stereotypes\n  in LLMs"
                },
                "summary": "Large language models (LLMs) have rapidly gained popularity and are being\nembedded into professional applications due to their capabilities in generating\nhuman-like content. However, unquestioned reliance on their outputs and\nrecommendations can be problematic as LLMs can reinforce societal biases and\nstereotypes. This study investigates how LLMs, specifically OpenAI's GPT-4 and\nMicrosoft Copilot, can reinforce gender and racial stereotypes within the\nsoftware engineering (SE) profession through both textual and graphical\noutputs. We used each LLM to generate 300 profiles, consisting of 100\ngender-based and 50 gender-neutral profiles, for a recruitment scenario in SE\nroles. Recommendations were generated for each profile and evaluated against\nthe job requirements for four distinct SE positions. Each LLM was asked to\nselect the top 5 candidates and subsequently the best candidate for each role.\nEach LLM was also asked to generate images for the top 5 candidates, providing\na dataset for analysing potential biases in both text-based selections and\nvisual representations. Our analysis reveals that both models preferred male\nand Caucasian profiles, particularly for senior roles, and favoured images\nfeaturing traits such as lighter skin tones, slimmer body types, and younger\nappearances. These findings highlight underlying societal biases influence the\noutputs of LLMs, contributing to narrow, exclusionary stereotypes that can\nfurther limit diversity and perpetuate inequities in the SE field. As LLMs are\nincreasingly adopted within SE research and professional practices, awareness\nof these biases is crucial to prevent the reinforcement of discriminatory norms\nand to ensure that AI tools are leveraged to promote an inclusive and equitable\nengineering culture rather than hinder it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have rapidly gained popularity and are being\nembedded into professional applications due to their capabilities in generating\nhuman-like content. However, unquestioned reliance on their outputs and\nrecommendations can be problematic as LLMs can reinforce societal biases and\nstereotypes. This study investigates how LLMs, specifically OpenAI's GPT-4 and\nMicrosoft Copilot, can reinforce gender and racial stereotypes within the\nsoftware engineering (SE) profession through both textual and graphical\noutputs. We used each LLM to generate 300 profiles, consisting of 100\ngender-based and 50 gender-neutral profiles, for a recruitment scenario in SE\nroles. Recommendations were generated for each profile and evaluated against\nthe job requirements for four distinct SE positions. Each LLM was asked to\nselect the top 5 candidates and subsequently the best candidate for each role.\nEach LLM was also asked to generate images for the top 5 candidates, providing\na dataset for analysing potential biases in both text-based selections and\nvisual representations. Our analysis reveals that both models preferred male\nand Caucasian profiles, particularly for senior roles, and favoured images\nfeaturing traits such as lighter skin tones, slimmer body types, and younger\nappearances. These findings highlight underlying societal biases influence the\noutputs of LLMs, contributing to narrow, exclusionary stereotypes that can\nfurther limit diversity and perpetuate inequities in the SE field. As LLMs are\nincreasingly adopted within SE research and professional practices, awareness\nof these biases is crucial to prevent the reinforcement of discriminatory norms\nand to ensure that AI tools are leveraged to promote an inclusive and equitable\nengineering culture rather than hinder it."
                },
                "authors": [
                    {
                        "name": "Muneera Bano"
                    },
                    {
                        "name": "Hashini Gunatilake"
                    },
                    {
                        "name": "Rashina Hoda"
                    }
                ],
                "author_detail": {
                    "name": "Rashina Hoda"
                },
                "author": "Rashina Hoda",
                "arxiv_comment": "Accepted for publication in Software Engineering in Society (SEIS) in\n  ICSE 2025",
                "arxiv_journal_ref": "Software Engineering in Society (SEIS) track in ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03568v1",
                "updated": "2025-01-07T06:43:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    43,
                    18,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T06:43:18Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    43,
                    18,
                    1,
                    7,
                    0
                ],
                "title": "Advanced Tutorial: Label-Efficient Two-Sample Tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Tutorial: Label-Efficient Two-Sample Tests"
                },
                "summary": "Hypothesis testing is a statistical inference approach used to determine\nwhether data supports a specific hypothesis. An important type is the\ntwo-sample test, which evaluates whether two sets of data points are from\nidentical distributions. This test is widely used, such as by clinical\nresearchers comparing treatment effectiveness. This tutorial explores\ntwo-sample testing in a context where an analyst has many features from two\nsamples, but determining the sample membership (or labels) of these features is\ncostly. In machine learning, a similar scenario is studied in active learning.\nThis tutorial extends active learning concepts to two-sample testing within\nthis \\textit{label-costly} setting while maintaining statistical validity and\nhigh testing power. Additionally, the tutorial discusses practical applications\nof these label-efficient two-sample tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypothesis testing is a statistical inference approach used to determine\nwhether data supports a specific hypothesis. An important type is the\ntwo-sample test, which evaluates whether two sets of data points are from\nidentical distributions. This test is widely used, such as by clinical\nresearchers comparing treatment effectiveness. This tutorial explores\ntwo-sample testing in a context where an analyst has many features from two\nsamples, but determining the sample membership (or labels) of these features is\ncostly. In machine learning, a similar scenario is studied in active learning.\nThis tutorial extends active learning concepts to two-sample testing within\nthis \\textit{label-costly} setting while maintaining statistical validity and\nhigh testing power. Additionally, the tutorial discusses practical applications\nof these label-efficient two-sample tests."
                },
                "authors": [
                    {
                        "name": "Weizhi Li"
                    },
                    {
                        "name": "Visar Berisha"
                    },
                    {
                        "name": "Gautam Dasarathy"
                    }
                ],
                "author_detail": {
                    "name": "Gautam Dasarathy"
                },
                "author": "Gautam Dasarathy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03566v1",
                "updated": "2025-01-07T06:34:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    34,
                    17,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T06:34:17Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    34,
                    17,
                    1,
                    7,
                    0
                ],
                "title": "Applying Large Language Models in Knowledge Graph-based Enterprise\n  Modeling: Challenges and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying Large Language Models in Knowledge Graph-based Enterprise\n  Modeling: Challenges and Opportunities"
                },
                "summary": "The role of large language models (LLMs) in enterprise modeling has recently\nstarted to shift from academic research to that of industrial applications.\nThereby, LLMs represent a further building block for the machine-supported\ngeneration of enterprise models. In this paper we employ a knowledge\ngraph-based approach for enterprise modeling and investigate the potential\nbenefits of LLMs in this context. In addition, the findings of an expert survey\nand ChatGPT-4o-based experiments demonstrate that LLM-based model generations\nexhibit minimal variability, yet remain constrained to specific tasks, with\nreliability declining for more intricate tasks. The survey results further\nsuggest that the supervision and intervention of human modeling experts are\nessential to ensure the accuracy and integrity of the generated models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The role of large language models (LLMs) in enterprise modeling has recently\nstarted to shift from academic research to that of industrial applications.\nThereby, LLMs represent a further building block for the machine-supported\ngeneration of enterprise models. In this paper we employ a knowledge\ngraph-based approach for enterprise modeling and investigate the potential\nbenefits of LLMs in this context. In addition, the findings of an expert survey\nand ChatGPT-4o-based experiments demonstrate that LLM-based model generations\nexhibit minimal variability, yet remain constrained to specific tasks, with\nreliability declining for more intricate tasks. The survey results further\nsuggest that the supervision and intervention of human modeling experts are\nessential to ensure the accuracy and integrity of the generated models."
                },
                "authors": [
                    {
                        "name": "Benedikt Reitemeyer"
                    },
                    {
                        "name": "Hans-Georg Fill"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Georg Fill"
                },
                "author": "Hans-Georg Fill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02155v2",
                "updated": "2025-01-07T06:30:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    30,
                    24,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-03T04:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "CausalMob: Causal Human Mobility Prediction with LLMs-derived Human\n  Intentions toward Public Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalMob: Causal Human Mobility Prediction with LLMs-derived Human\n  Intentions toward Public Events"
                },
                "summary": "Large-scale human mobility exhibits spatial and temporal patterns that can\nassist policymakers in decision making. Although traditional prediction models\nattempt to capture these patterns, they often interfered by non-periodic public\nevents, such as disasters and occasional celebrations. Since regular human\nmobility patterns are heavily affected by these events, estimating their causal\neffects is critical to accurate mobility predictions. Although news articles\nprovide unique perspectives on these events in an unstructured format,\nprocessing is a challenge. In this study, we propose a causality-augmented\nprediction model, called CausalMob, to analyze the causal effects of public\nevents. We first utilize large language models (LLMs) to extract human\nintentions from news articles and transform them into features that act as\ncausal treatments. Next, the model learns representations of spatio-temporal\nregional covariates from multiple data sources to serve as confounders for\ncausal inference. Finally, we present a causal effect estimation framework to\nensure event features remain independent of confounders during prediction.\nBased on large-scale real-world data, the experimental results show that the\nproposed model excels in human mobility prediction, outperforming\nstate-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale human mobility exhibits spatial and temporal patterns that can\nassist policymakers in decision making. Although traditional prediction models\nattempt to capture these patterns, they often interfered by non-periodic public\nevents, such as disasters and occasional celebrations. Since regular human\nmobility patterns are heavily affected by these events, estimating their causal\neffects is critical to accurate mobility predictions. Although news articles\nprovide unique perspectives on these events in an unstructured format,\nprocessing is a challenge. In this study, we propose a causality-augmented\nprediction model, called CausalMob, to analyze the causal effects of public\nevents. We first utilize large language models (LLMs) to extract human\nintentions from news articles and transform them into features that act as\ncausal treatments. Next, the model learns representations of spatio-temporal\nregional covariates from multiple data sources to serve as confounders for\ncausal inference. Finally, we present a causal effect estimation framework to\nensure event features remain independent of confounders during prediction.\nBased on large-scale real-world data, the experimental results show that the\nproposed model excels in human mobility prediction, outperforming\nstate-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Xiaojie Yang"
                    },
                    {
                        "name": "Hangli Ge"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Zipei Fan"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Ryosuke Shibasaki"
                    },
                    {
                        "name": "Noboru Koshizuka"
                    }
                ],
                "author_detail": {
                    "name": "Noboru Koshizuka"
                },
                "author": "Noboru Koshizuka",
                "arxiv_comment": "Accepted by KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10207v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10207v6",
                "updated": "2025-01-07T06:28:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    28,
                    56,
                    1,
                    7,
                    0
                ],
                "published": "2023-10-16T09:19:18Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    9,
                    19,
                    18,
                    0,
                    289,
                    0
                ],
                "title": "Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in\n  the Real World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in\n  the Real World"
                },
                "summary": "We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world\nfew-shot reasoning for machine vision. It originates from the classical Bongard\nProblems (BPs): Given two sets of images (positive and negative), the model\nneeds to identify the set that query images belong to by inducing the visual\nconcepts, which is exclusively depicted by images from the positive set. Our\nbenchmark inherits the few-shot concept induction of the original BPs while\nadding the two novel layers of challenge: 1) open-world free-form concepts, as\nthe visual concepts in Bongard-OpenWorld are unique compositions of terms from\nan open vocabulary, ranging from object categories to abstract visual\nattributes and commonsense factual knowledge; 2) real-world images, as opposed\nto the synthetic diagrams used by many counterparts. In our exploration,\nBongard-OpenWorld already imposes a significant challenge to current few-shot\nreasoning algorithms. We further investigate to which extent the recently\nintroduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nsolve our task, by directly probing VLMs, and combining VLMs and LLMs in an\ninteractive reasoning scheme. We even conceived a neuro-symbolic reasoning\napproach that reconciles LLMs & VLMs with logical reasoning to emulate the\nhuman problem-solving process for Bongard Problems. However, none of these\napproaches manage to close the human-machine gap, as the best learner achieves\n64% accuracy while human participants easily reach 91%. We hope\nBongard-OpenWorld can help us better understand the limitations of current\nvisual intelligence and facilitate future research on visual agents with\nstronger few-shot visual reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world\nfew-shot reasoning for machine vision. It originates from the classical Bongard\nProblems (BPs): Given two sets of images (positive and negative), the model\nneeds to identify the set that query images belong to by inducing the visual\nconcepts, which is exclusively depicted by images from the positive set. Our\nbenchmark inherits the few-shot concept induction of the original BPs while\nadding the two novel layers of challenge: 1) open-world free-form concepts, as\nthe visual concepts in Bongard-OpenWorld are unique compositions of terms from\nan open vocabulary, ranging from object categories to abstract visual\nattributes and commonsense factual knowledge; 2) real-world images, as opposed\nto the synthetic diagrams used by many counterparts. In our exploration,\nBongard-OpenWorld already imposes a significant challenge to current few-shot\nreasoning algorithms. We further investigate to which extent the recently\nintroduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nsolve our task, by directly probing VLMs, and combining VLMs and LLMs in an\ninteractive reasoning scheme. We even conceived a neuro-symbolic reasoning\napproach that reconciles LLMs & VLMs with logical reasoning to emulate the\nhuman problem-solving process for Bongard Problems. However, none of these\napproaches manage to close the human-machine gap, as the best learner achieves\n64% accuracy while human participants easily reach 91%. We hope\nBongard-OpenWorld can help us better understand the limitations of current\nvisual intelligence and facilitate future research on visual agents with\nstronger few-shot visual reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Rujie Wu"
                    },
                    {
                        "name": "Xiaojian Ma"
                    },
                    {
                        "name": "Zhenliang Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Yizhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Wang"
                },
                "author": "Yizhou Wang",
                "arxiv_comment": "Accepted to ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10207v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10207v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03553v1",
                "updated": "2025-01-07T06:00:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    0,
                    53,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T06:00:53Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    0,
                    53,
                    1,
                    7,
                    0
                ],
                "title": "Persistent Homology with Path-Representable Distances on Graph Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Homology with Path-Representable Distances on Graph Data"
                },
                "summary": "Topological data analysis over graph has been actively studied to understand\nthe underlying topological structure of data. However, limited research has\nbeen conducted on how different distance definitions impact persistent homology\nand the corresponding topological inference. To address this, we introduce the\nconcept of path-representable distance in a general form and prove the main\ntheorem for the case of cost-dominated distances. We found that a particular\ninjection exists among the $1$-dimensional persistence barcodes of these\ndistances with a certain condition. We prove that such an injection relation\nexists for $0$- and $1$-dimensional homology. For higher dimensions, we provide\nthe counterexamples that show such a relation does not exist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topological data analysis over graph has been actively studied to understand\nthe underlying topological structure of data. However, limited research has\nbeen conducted on how different distance definitions impact persistent homology\nand the corresponding topological inference. To address this, we introduce the\nconcept of path-representable distance in a general form and prove the main\ntheorem for the case of cost-dominated distances. We found that a particular\ninjection exists among the $1$-dimensional persistence barcodes of these\ndistances with a certain condition. We prove that such an injection relation\nexists for $0$- and $1$-dimensional homology. For higher dimensions, we provide\nthe counterexamples that show such a relation does not exist."
                },
                "authors": [
                    {
                        "name": "Eunwoo Heo"
                    },
                    {
                        "name": "Byeongchan Choi"
                    },
                    {
                        "name": "Jae-Hun Jung"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Hun Jung"
                },
                "author": "Jae-Hun Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06537v2",
                "updated": "2025-01-07T05:52:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    52,
                    37,
                    1,
                    7,
                    0
                ],
                "published": "2024-03-11T09:24:06Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    9,
                    24,
                    6,
                    0,
                    71,
                    0
                ],
                "title": "On the Consideration of AI Openness: Can Good Intent Be Abused?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Consideration of AI Openness: Can Good Intent Be Abused?"
                },
                "summary": "Open source is a driving force behind scientific advancement.However, this\nopenness is also a double-edged sword, with the inherent risk that innovative\ntechnologies can be misused for purposes harmful to society. What is the\nlikelihood that an open source AI model or dataset will be used to commit a\nreal-world crime, and if a criminal does exploit it, will the people behind the\ntechnology be able to escape legal liability? To address these questions, we\nexplore a legal domain where individual choices can have a significant impact\non society. Specifically, we build the EVE-V1 dataset that comprises 200\nquestion-answer pairs related to criminal offenses based on 200 Korean\nprecedents first to explore the possibility of malicious models emerging. We\nfurther developed EVE-V2 using 600 fraud-related precedents to confirm the\nexistence of malicious models that can provide harmful advice on a wide range\nof criminal topics to test the domain generalization ability. Remarkably,\nwidely used open-source large-scale language models (LLMs) provide unethical\nand detailed information about criminal activities when fine-tuned with EVE. We\nalso take an in-depth look at the legal issues that malicious language models\nand their builders could realistically face. Our findings highlight the\nparadoxical dilemma that open source accelerates scientific progress, but\nrequires great care to minimize the potential for misuse. Warning: This paper\ncontains content that some may find unethical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open source is a driving force behind scientific advancement.However, this\nopenness is also a double-edged sword, with the inherent risk that innovative\ntechnologies can be misused for purposes harmful to society. What is the\nlikelihood that an open source AI model or dataset will be used to commit a\nreal-world crime, and if a criminal does exploit it, will the people behind the\ntechnology be able to escape legal liability? To address these questions, we\nexplore a legal domain where individual choices can have a significant impact\non society. Specifically, we build the EVE-V1 dataset that comprises 200\nquestion-answer pairs related to criminal offenses based on 200 Korean\nprecedents first to explore the possibility of malicious models emerging. We\nfurther developed EVE-V2 using 600 fraud-related precedents to confirm the\nexistence of malicious models that can provide harmful advice on a wide range\nof criminal topics to test the domain generalization ability. Remarkably,\nwidely used open-source large-scale language models (LLMs) provide unethical\nand detailed information about criminal activities when fine-tuned with EVE. We\nalso take an in-depth look at the legal issues that malicious language models\nand their builders could realistically face. Our findings highlight the\nparadoxical dilemma that open source accelerates scientific progress, but\nrequires great care to minimize the potential for misuse. Warning: This paper\ncontains content that some may find unethical."
                },
                "authors": [
                    {
                        "name": "Yeeun Kim"
                    },
                    {
                        "name": "Hyunseo Shin"
                    },
                    {
                        "name": "Eunkyung Choi"
                    },
                    {
                        "name": "Hongseok Oh"
                    },
                    {
                        "name": "Hyunjun Kim"
                    },
                    {
                        "name": "Wonseok Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Wonseok Hwang"
                },
                "author": "Wonseok Hwang",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03545v1",
                "updated": "2025-01-07T05:43:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    43,
                    23,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T05:43:23Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    43,
                    23,
                    1,
                    7,
                    0
                ],
                "title": "Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual\n  Information in Long-form Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual\n  Information in Long-form Text Generation"
                },
                "summary": "This paper presents ICAT, an evaluation framework for measuring coverage of\ndiverse factual information in long-form text generation. ICAT breaks down a\nlong output text into a list of atomic claims and not only verifies each claim\nthrough retrieval from a (reliable) knowledge source, but also computes the\nalignment between the atomic factual claims and various aspects expected to be\npresented in the output. We study three implementations of the ICAT framework,\neach with a different assumption on the availability of aspects and alignment\nmethod. By adopting data from the diversification task in the TREC Web Track\nand the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong\ncorrelation with human judgments and provide comprehensive evaluation across\nmultiple state-of-the-art LLMs. Our framework further offers interpretable and\nfine-grained analysis of diversity and coverage. Its modular design allows for\neasy adaptation to different domains and datasets, making it a valuable tool\nfor evaluating the qualitative aspects of long-form responses produced by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents ICAT, an evaluation framework for measuring coverage of\ndiverse factual information in long-form text generation. ICAT breaks down a\nlong output text into a list of atomic claims and not only verifies each claim\nthrough retrieval from a (reliable) knowledge source, but also computes the\nalignment between the atomic factual claims and various aspects expected to be\npresented in the output. We study three implementations of the ICAT framework,\neach with a different assumption on the availability of aspects and alignment\nmethod. By adopting data from the diversification task in the TREC Web Track\nand the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong\ncorrelation with human judgments and provide comprehensive evaluation across\nmultiple state-of-the-art LLMs. Our framework further offers interpretable and\nfine-grained analysis of diversity and coverage. Its modular design allows for\neasy adaptation to different domains and datasets, making it a valuable tool\nfor evaluating the qualitative aspects of long-form responses produced by LLMs."
                },
                "authors": [
                    {
                        "name": "Chris Samarinas"
                    },
                    {
                        "name": "Alexander Krubner"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Youngwoo Kim"
                    },
                    {
                        "name": "Hamed Zamani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Zamani"
                },
                "author": "Hamed Zamani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03544v1",
                "updated": "2025-01-07T05:39:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    39,
                    21,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T05:39:21Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    39,
                    21,
                    1,
                    7,
                    0
                ],
                "title": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for\n  Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for\n  Text-to-Image Models"
                },
                "summary": "Text-to-image (T2I) models have been shown to be vulnerable to misuse,\nparticularly in generating not-safe-for-work (NSFW) content, raising serious\nethical concerns. In this work, we present PromptGuard, a novel content\nmoderation technique that draws inspiration from the system prompt mechanism in\nlarge language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack\na direct interface for enforcing behavioral guidelines. Our key idea is to\noptimize a safety soft prompt that functions as an implicit system prompt\nwithin the T2I model's textual embedding space. This universal soft prompt (P*)\ndirectly moderates NSFW inputs, enabling safe yet realistic image generation\nwithout altering the inference efficiency or requiring proxy models. Extensive\nexperiments across three datasets demonstrate that PromptGuard effectively\nmitigates NSFW content generation while preserving high-quality benign outputs.\nPromptGuard achieves 7.8 times faster than prior content moderation methods,\nsurpassing eight state-of-the-art defenses with an optimal unsafe ratio down to\n5.84%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) models have been shown to be vulnerable to misuse,\nparticularly in generating not-safe-for-work (NSFW) content, raising serious\nethical concerns. In this work, we present PromptGuard, a novel content\nmoderation technique that draws inspiration from the system prompt mechanism in\nlarge language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack\na direct interface for enforcing behavioral guidelines. Our key idea is to\noptimize a safety soft prompt that functions as an implicit system prompt\nwithin the T2I model's textual embedding space. This universal soft prompt (P*)\ndirectly moderates NSFW inputs, enabling safe yet realistic image generation\nwithout altering the inference efficiency or requiring proxy models. Extensive\nexperiments across three datasets demonstrate that PromptGuard effectively\nmitigates NSFW content generation while preserving high-quality benign outputs.\nPromptGuard achieves 7.8 times faster than prior content moderation methods,\nsurpassing eight state-of-the-art defenses with an optimal unsafe ratio down to\n5.84%."
                },
                "authors": [
                    {
                        "name": "Lingzhi Yuan"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Chejian Xu"
                    },
                    {
                        "name": "Guanhong Tao"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "arxiv_comment": "16 pages, 8 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01672v2",
                "updated": "2025-01-07T05:36:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    36,
                    41,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-03T07:19:23Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    19,
                    23,
                    4,
                    3,
                    0
                ],
                "title": "Practical Secure Inference Algorithm for Fine-tuned Large Language Model\n  Based on Fully Homomorphic Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Secure Inference Algorithm for Fine-tuned Large Language Model\n  Based on Fully Homomorphic Encryption"
                },
                "summary": "Large language models(LLMs) are currently at the forefront of the machine\nlearning field, which show a broad application prospect but at the same time\nexpose some risks of privacy leakage. We combined Fully Homomorphic\nEncryption(FHE) and provable security theory with Parameter-Efficient\nFine-Tuning(PEFT) to propose an efficient and secure inference scheme for LLMs.\nMore specially, we focus on pre-trained LLMs which rely on open-sourced base\nmodel and then fine-tuned with the private datasets by LoRA. This is a popular\nroad-map for Vertical Domain Models such as LawGPT and BenTsao. We use two key\ntechnologies below. Firstly, we divide the whole model into the public part and\nthe private part. The weights of public part are publicly accessible(e.g. the\nopen-sourced base model) while the private part needs to be protected(e.g. the\nLoRA matrices). In this way, the overhead brought by computing on private data\ncan be greatly reduced. Secondly, we propose a general method to transform a\nlinear layer into another one which provides security against model extraction\nattacks and preserves its original functionality, which denoted as Private\nLinear Layer(PLL). Then we use this method on the LoRA matrices to make sure\nthat the server protects their private weights without restricting the user's\ninput. We also show that the difficulty of performing model extraction attacks\nfor PLL can be reduced to the well-known hard problem Learning with\nErrors(LWE). Combing this method with FHE, we can protect user's input at the\nsame time. In this paper, we use the open-source model ChatGLM2-6B as the base\nmodel which is fine-tuned by LoRA. Experimental results show the inference\nefficiency of our scheme reaches 1.61s/token which displays that the scheme has\ngood practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models(LLMs) are currently at the forefront of the machine\nlearning field, which show a broad application prospect but at the same time\nexpose some risks of privacy leakage. We combined Fully Homomorphic\nEncryption(FHE) and provable security theory with Parameter-Efficient\nFine-Tuning(PEFT) to propose an efficient and secure inference scheme for LLMs.\nMore specially, we focus on pre-trained LLMs which rely on open-sourced base\nmodel and then fine-tuned with the private datasets by LoRA. This is a popular\nroad-map for Vertical Domain Models such as LawGPT and BenTsao. We use two key\ntechnologies below. Firstly, we divide the whole model into the public part and\nthe private part. The weights of public part are publicly accessible(e.g. the\nopen-sourced base model) while the private part needs to be protected(e.g. the\nLoRA matrices). In this way, the overhead brought by computing on private data\ncan be greatly reduced. Secondly, we propose a general method to transform a\nlinear layer into another one which provides security against model extraction\nattacks and preserves its original functionality, which denoted as Private\nLinear Layer(PLL). Then we use this method on the LoRA matrices to make sure\nthat the server protects their private weights without restricting the user's\ninput. We also show that the difficulty of performing model extraction attacks\nfor PLL can be reduced to the well-known hard problem Learning with\nErrors(LWE). Combing this method with FHE, we can protect user's input at the\nsame time. In this paper, we use the open-source model ChatGLM2-6B as the base\nmodel which is fine-tuned by LoRA. Experimental results show the inference\nefficiency of our scheme reaches 1.61s/token which displays that the scheme has\ngood practicality."
                },
                "authors": [
                    {
                        "name": "Zhang Ruoyan"
                    },
                    {
                        "name": "Zheng Zhongxiang"
                    },
                    {
                        "name": "Bao Wankang"
                    }
                ],
                "author_detail": {
                    "name": "Bao Wankang"
                },
                "author": "Bao Wankang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13516v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13516v7",
                "updated": "2025-01-07T05:26:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    26,
                    54,
                    1,
                    7,
                    0
                ],
                "published": "2024-02-21T03:58:49Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    3,
                    58,
                    49,
                    2,
                    52,
                    0
                ],
                "title": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models"
                },
                "summary": "Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named \"ProSparse\"\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named \"ProSparse\"\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup."
                },
                "authors": [
                    {
                        "name": "Chenyang Song"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhengyan Zhang"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Xiyu Shi"
                    },
                    {
                        "name": "Kuai Li"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Guangli Li"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "19 pages, 4 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13516v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13516v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11876v2",
                "updated": "2025-01-07T05:20:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    20,
                    13,
                    1,
                    7,
                    0
                ],
                "published": "2024-10-10T01:23:16Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    1,
                    23,
                    16,
                    3,
                    284,
                    0
                ],
                "title": "Rescriber: Smaller-LLM-Powered User-Led Data Minimization for Navigating\n  Privacy Trade-offs in LLM-Based Conversational Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rescriber: Smaller-LLM-Powered User-Led Data Minimization for Navigating\n  Privacy Trade-offs in LLM-Based Conversational Agent"
                },
                "summary": "The proliferation of LLM-based conversational agents has resulted in\nexcessive disclosure of identifiable or sensitive information. However,\nexisting technologies fail to offer perceptible control or account for users'\npersonal preferences about privacy-utility tradeoffs due to the lack of user\ninvolvement. To bridge this gap, we designed, built, and evaluated Rescriber, a\nbrowser extension that supports user-led data minimization in LLM-based\nconversational agents by helping users detect and sanitize personal information\nin their prompts. Our studies (N=12) showed that Rescriber helped users reduce\nunnecessary disclosure and addressed their privacy concerns. Users' subjective\nperceptions of the system powered by Llama3-8B were on par with that by GPT-4o.\nThe comprehensiveness and consistency of the detection and sanitization emerge\nas essential factors that affect users' trust and perceived protection. Our\nfindings confirm the viability of smaller-LLM-powered, user-facing, on-device\nprivacy controls, presenting a promising approach to address the privacy and\ntrust challenges of AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of LLM-based conversational agents has resulted in\nexcessive disclosure of identifiable or sensitive information. However,\nexisting technologies fail to offer perceptible control or account for users'\npersonal preferences about privacy-utility tradeoffs due to the lack of user\ninvolvement. To bridge this gap, we designed, built, and evaluated Rescriber, a\nbrowser extension that supports user-led data minimization in LLM-based\nconversational agents by helping users detect and sanitize personal information\nin their prompts. Our studies (N=12) showed that Rescriber helped users reduce\nunnecessary disclosure and addressed their privacy concerns. Users' subjective\nperceptions of the system powered by Llama3-8B were on par with that by GPT-4o.\nThe comprehensiveness and consistency of the detection and sanitization emerge\nas essential factors that affect users' trust and perceived protection. Our\nfindings confirm the viability of smaller-LLM-powered, user-facing, on-device\nprivacy controls, presenting a promising approach to address the privacy and\ntrust challenges of AI."
                },
                "authors": [
                    {
                        "name": "Jijie Zhou"
                    },
                    {
                        "name": "Eryue Xu"
                    },
                    {
                        "name": "Yaoyao Wu"
                    },
                    {
                        "name": "Tianshi Li"
                    }
                ],
                "author_detail": {
                    "name": "Tianshi Li"
                },
                "author": "Tianshi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.10524v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.10524v3",
                "updated": "2025-01-07T05:15:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    15,
                    54,
                    1,
                    7,
                    0
                ],
                "published": "2023-09-19T11:10:50Z",
                "published_parsed": [
                    2023,
                    9,
                    19,
                    11,
                    10,
                    50,
                    1,
                    262,
                    0
                ],
                "title": "Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model\n  in End-to-End Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model\n  in End-to-End Speech Recognition"
                },
                "summary": "We propose to utilize an instruction-tuned large language model (LLM) for\nguiding the text generation process in automatic speech recognition (ASR).\nModern large language models (LLMs) are adept at performing various text\ngeneration tasks through zero-shot learning, prompted with instructions\ndesigned for specific objectives. This paper explores the potential of LLMs to\nderive linguistic information that can facilitate text generation in end-to-end\nASR models. Specifically, we instruct an LLM to correct grammatical errors in\nan ASR hypothesis and use the LLM-derived representations to refine the output\nfurther. The proposed model is built on the joint CTC and attention\narchitecture, with the LLM serving as a front-end feature extractor for the\ndecoder. The ASR hypothesis, subject to correction, is obtained from the\nencoder via CTC decoding and fed into the LLM along with a specific\ninstruction. The decoder subsequently takes as input the LLM output to perform\ntoken predictions, combining acoustic information from the encoder and the\npowerful linguistic information provided by the LLM. Experimental results show\nthat the proposed LLM-guided model achieves a relative gain of approximately\n13\\% in word error rates across major benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose to utilize an instruction-tuned large language model (LLM) for\nguiding the text generation process in automatic speech recognition (ASR).\nModern large language models (LLMs) are adept at performing various text\ngeneration tasks through zero-shot learning, prompted with instructions\ndesigned for specific objectives. This paper explores the potential of LLMs to\nderive linguistic information that can facilitate text generation in end-to-end\nASR models. Specifically, we instruct an LLM to correct grammatical errors in\nan ASR hypothesis and use the LLM-derived representations to refine the output\nfurther. The proposed model is built on the joint CTC and attention\narchitecture, with the LLM serving as a front-end feature extractor for the\ndecoder. The ASR hypothesis, subject to correction, is obtained from the\nencoder via CTC decoding and fed into the LLM along with a specific\ninstruction. The decoder subsequently takes as input the LLM output to perform\ntoken predictions, combining acoustic information from the encoder and the\npowerful linguistic information provided by the LLM. Experimental results show\nthat the proposed LLM-guided model achieves a relative gain of approximately\n13\\% in word error rates across major benchmarks."
                },
                "authors": [
                    {
                        "name": "Yosuke Higuchi"
                    },
                    {
                        "name": "Tetsuji Ogawa"
                    },
                    {
                        "name": "Tetsunori Kobayashi"
                    }
                ],
                "author_detail": {
                    "name": "Tetsunori Kobayashi"
                },
                "author": "Tetsunori Kobayashi",
                "arxiv_comment": "Accepted to ICASSP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.10524v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.10524v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03535v1",
                "updated": "2025-01-07T05:15:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    15,
                    46,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T05:15:46Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    15,
                    46,
                    1,
                    7,
                    0
                ],
                "title": "SenseRAG: Constructing Environmental Knowledge Bases with Proactive\n  Querying for LLM-Based Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SenseRAG: Constructing Environmental Knowledge Bases with Proactive\n  Querying for LLM-Based Autonomous Driving"
                },
                "summary": "This study addresses the critical need for enhanced situational awareness in\nautonomous driving (AD) by leveraging the contextual reasoning capabilities of\nlarge language models (LLMs). Unlike traditional perception systems that rely\non rigid, label-based annotations, it integrates real-time, multimodal sensor\ndata into a unified, LLMs-readable knowledge base, enabling LLMs to dynamically\nunderstand and respond to complex driving environments. To overcome the\ninherent latency and modality limitations of LLMs, a proactive\nRetrieval-Augmented Generation (RAG) is designed for AD, combined with a\nchain-of-thought prompting mechanism, ensuring rapid and context-rich\nunderstanding. Experimental results using real-world Vehicle-to-everything\n(V2X) datasets demonstrate significant improvements in perception and\nprediction performance, highlighting the potential of this framework to enhance\nsafety, adaptability, and decision-making in next-generation AD systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the critical need for enhanced situational awareness in\nautonomous driving (AD) by leveraging the contextual reasoning capabilities of\nlarge language models (LLMs). Unlike traditional perception systems that rely\non rigid, label-based annotations, it integrates real-time, multimodal sensor\ndata into a unified, LLMs-readable knowledge base, enabling LLMs to dynamically\nunderstand and respond to complex driving environments. To overcome the\ninherent latency and modality limitations of LLMs, a proactive\nRetrieval-Augmented Generation (RAG) is designed for AD, combined with a\nchain-of-thought prompting mechanism, ensuring rapid and context-rich\nunderstanding. Experimental results using real-world Vehicle-to-everything\n(V2X) datasets demonstrate significant improvements in perception and\nprediction performance, highlighting the potential of this framework to enhance\nsafety, adaptability, and decision-making in next-generation AD systems."
                },
                "authors": [
                    {
                        "name": "Xuewen Luo"
                    },
                    {
                        "name": "Fan Ding"
                    },
                    {
                        "name": "Fengze Yang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Junnyong Loo"
                    },
                    {
                        "name": "Hwa Hui Tew"
                    },
                    {
                        "name": "Chenxi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chenxi Liu"
                },
                "author": "Chenxi Liu",
                "arxiv_comment": "This paper has been accepted for presentation at WACV Workshop LLMAD\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06949v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06949v2",
                "updated": "2025-01-07T05:00:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    0,
                    50,
                    1,
                    7,
                    0
                ],
                "published": "2024-01-13T02:03:28Z",
                "published_parsed": [
                    2024,
                    1,
                    13,
                    2,
                    3,
                    28,
                    5,
                    13,
                    0
                ],
                "title": "ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and\n  Characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and\n  Characterization"
                },
                "summary": "Chemistry experiments can be resource- and labor-intensive, often requiring\nmanual tasks like polishing electrodes in electrochemistry. Traditional lab\nautomation infrastructure faces challenges adapting to new experiments. To\naddress this, we introduce ORGANA, an assistive robotic system that automates\ndiverse chemistry experiments using decision-making and perception tools. It\nmakes decisions with chemists in the loop to control robots and lab devices.\nORGANA interacts with chemists using Large Language Models (LLMs) to derive\nexperiment goals, handle disambiguation, and provide experiment logs. ORGANA\nplans and executes complex tasks with visual feedback, while supporting\nscheduling and parallel task execution. We demonstrate ORGANA's capabilities in\nsolubility, pH measurement, recrystallization, and electrochemistry\nexperiments. In electrochemistry, it executes a 19-step plan in parallel to\ncharacterize quinone derivatives for flow batteries. Our user study shows\nORGANA reduces frustration and physical demand by over 50%, with users saving\nan average of 80.3% of their time when using it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemistry experiments can be resource- and labor-intensive, often requiring\nmanual tasks like polishing electrodes in electrochemistry. Traditional lab\nautomation infrastructure faces challenges adapting to new experiments. To\naddress this, we introduce ORGANA, an assistive robotic system that automates\ndiverse chemistry experiments using decision-making and perception tools. It\nmakes decisions with chemists in the loop to control robots and lab devices.\nORGANA interacts with chemists using Large Language Models (LLMs) to derive\nexperiment goals, handle disambiguation, and provide experiment logs. ORGANA\nplans and executes complex tasks with visual feedback, while supporting\nscheduling and parallel task execution. We demonstrate ORGANA's capabilities in\nsolubility, pH measurement, recrystallization, and electrochemistry\nexperiments. In electrochemistry, it executes a 19-step plan in parallel to\ncharacterize quinone derivatives for flow batteries. Our user study shows\nORGANA reduces frustration and physical demand by over 50%, with users saving\nan average of 80.3% of their time when using it."
                },
                "authors": [
                    {
                        "name": "Kourosh Darvish"
                    },
                    {
                        "name": "Marta Skreta"
                    },
                    {
                        "name": "Yuchi Zhao"
                    },
                    {
                        "name": "Naruki Yoshikawa"
                    },
                    {
                        "name": "Sagnik Som"
                    },
                    {
                        "name": "Miroslav Bogdanovic"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Han Hao"
                    },
                    {
                        "name": "Haoping Xu"
                    },
                    {
                        "name": "Alán Aspuru-Guzik"
                    },
                    {
                        "name": "Animesh Garg"
                    },
                    {
                        "name": "Florian Shkurti"
                    }
                ],
                "author_detail": {
                    "name": "Florian Shkurti"
                },
                "author": "Florian Shkurti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06949v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.04001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04001v1",
                "updated": "2025-01-07T18:58:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    58,
                    54,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:58:54Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    58,
                    54,
                    1,
                    7,
                    0
                ],
                "title": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos"
                },
                "summary": "This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications."
                },
                "authors": [
                    {
                        "name": "Haobo Yuan"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Zilong Huang"
                    },
                    {
                        "name": "Shilin Xu"
                    },
                    {
                        "name": "Shunping Ji"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Lu Qi"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Hsuan Yang"
                },
                "author": "Ming-Hsuan Yang",
                "arxiv_comment": "Project page: https://lxtgh.github.io/project/sa2va",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03995v1",
                "updated": "2025-01-07T18:52:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    52,
                    5,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:52:05Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    52,
                    5,
                    1,
                    7,
                    0
                ],
                "title": "RAG-Check: Evaluating Multimodal Retrieval Augmented Generation\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Check: Evaluating Multimodal Retrieval Augmented Generation\n  Performance"
                },
                "summary": "Retrieval-augmented generation (RAG) improves large language models (LLMs) by\nusing external knowledge to guide response generation, reducing hallucinations.\nHowever, RAG, particularly multi-modal RAG, can introduce new hallucination\nsources: (i) the retrieval process may select irrelevant pieces (e.g.,\ndocuments, images) as raw context from the database, and (ii) retrieved images\nare processed into text-based context via vision-language models (VLMs) or\ndirectly used by multi-modal language models (MLLMs) like GPT-4o, which may\nhallucinate. To address this, we propose a novel framework to evaluate the\nreliability of multi-modal RAG using two performance measures: (i) the\nrelevancy score (RS), assessing the relevance of retrieved entries to the\nquery, and (ii) the correctness score (CS), evaluating the accuracy of the\ngenerated response. We train RS and CS models using a ChatGPT-derived database\nand human evaluator samples. Results show that both models achieve ~88%\naccuracy on test data. Additionally, we construct a 5000-sample human-annotated\ndatabase evaluating the relevancy of retrieved pieces and the correctness of\nresponse statements. Our RS model aligns with human preferences 20% more often\nthan CLIP in retrieval, and our CS model matches human preferences ~91% of the\ntime. Finally, we assess various RAG systems' selection and generation\nperformances using RS and CS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves large language models (LLMs) by\nusing external knowledge to guide response generation, reducing hallucinations.\nHowever, RAG, particularly multi-modal RAG, can introduce new hallucination\nsources: (i) the retrieval process may select irrelevant pieces (e.g.,\ndocuments, images) as raw context from the database, and (ii) retrieved images\nare processed into text-based context via vision-language models (VLMs) or\ndirectly used by multi-modal language models (MLLMs) like GPT-4o, which may\nhallucinate. To address this, we propose a novel framework to evaluate the\nreliability of multi-modal RAG using two performance measures: (i) the\nrelevancy score (RS), assessing the relevance of retrieved entries to the\nquery, and (ii) the correctness score (CS), evaluating the accuracy of the\ngenerated response. We train RS and CS models using a ChatGPT-derived database\nand human evaluator samples. Results show that both models achieve ~88%\naccuracy on test data. Additionally, we construct a 5000-sample human-annotated\ndatabase evaluating the relevancy of retrieved pieces and the correctness of\nresponse statements. Our RS model aligns with human preferences 20% more often\nthan CLIP in retrieval, and our CS model matches human preferences ~91% of the\ntime. Finally, we assess various RAG systems' selection and generation\nperformances using RS and CS."
                },
                "authors": [
                    {
                        "name": "Matin Mortaheb"
                    },
                    {
                        "name": "Mohammad A. Amir Khojastepour"
                    },
                    {
                        "name": "Srimat T. Chakradhar"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03991v1",
                "updated": "2025-01-07T18:48:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    48,
                    42,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:48:42Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    48,
                    42,
                    1,
                    7,
                    0
                ],
                "title": "Influences on LLM Calibration: A Study of Response Agreement, Loss\n  Functions, and Prompt Styles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influences on LLM Calibration: A Study of Response Agreement, Loss\n  Functions, and Prompt Styles"
                },
                "summary": "Calibration, the alignment between model confidence and prediction accuracy,\nis critical for the reliable deployment of large language models (LLMs).\nExisting works neglect to measure the generalization of their methods to other\nprompt styles and different sizes of LLMs. To address this, we define a\ncontrolled experimental setting covering 12 LLMs and four prompt styles. We\nadditionally investigate if incorporating the response agreement of multiple\nLLMs and an appropriate loss function can improve calibration performance.\nConcretely, we build Calib-n, a novel framework that trains an auxiliary model\nfor confidence estimation that aggregates responses from multiple LLMs to\ncapture inter-model agreement. To optimize calibration, we integrate focal and\nAUC surrogate losses alongside binary cross-entropy. Experiments across four\ndatasets demonstrate that both response agreement and focal loss improve\ncalibration from baselines. We find that few-shot prompts are the most\neffective for auxiliary model-based methods, and auxiliary models demonstrate\nrobust calibration performance across accuracy variations, outperforming LLMs'\ninternal probabilities and verbalized confidences. These insights deepen the\nunderstanding of influence factors in LLM calibration, supporting their\nreliable deployment in diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibration, the alignment between model confidence and prediction accuracy,\nis critical for the reliable deployment of large language models (LLMs).\nExisting works neglect to measure the generalization of their methods to other\nprompt styles and different sizes of LLMs. To address this, we define a\ncontrolled experimental setting covering 12 LLMs and four prompt styles. We\nadditionally investigate if incorporating the response agreement of multiple\nLLMs and an appropriate loss function can improve calibration performance.\nConcretely, we build Calib-n, a novel framework that trains an auxiliary model\nfor confidence estimation that aggregates responses from multiple LLMs to\ncapture inter-model agreement. To optimize calibration, we integrate focal and\nAUC surrogate losses alongside binary cross-entropy. Experiments across four\ndatasets demonstrate that both response agreement and focal loss improve\ncalibration from baselines. We find that few-shot prompts are the most\neffective for auxiliary model-based methods, and auxiliary models demonstrate\nrobust calibration performance across accuracy variations, outperforming LLMs'\ninternal probabilities and verbalized confidences. These insights deepen the\nunderstanding of influence factors in LLM calibration, supporting their\nreliable deployment in diverse applications."
                },
                "authors": [
                    {
                        "name": "Yuxi Xia"
                    },
                    {
                        "name": "Pedro Henrique Luz de Araujo"
                    },
                    {
                        "name": "Klim Zaporojets"
                    },
                    {
                        "name": "Benjamin Roth"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Roth"
                },
                "author": "Benjamin Roth",
                "arxiv_comment": "24 pages, 11 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03989v1",
                "updated": "2025-01-07T18:46:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    46,
                    34,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:46:34Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    46,
                    34,
                    1,
                    7,
                    0
                ],
                "title": "(De)-Indexing and the Right to be Forgotten",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "(De)-Indexing and the Right to be Forgotten"
                },
                "summary": "In the digital age, the challenge of forgetfulness has emerged as a\nsignificant concern, particularly regarding the management of personal data and\nits accessibility online. The right to be forgotten (RTBF) allows individuals\nto request the removal of outdated or harmful information from public access,\nyet implementing this right poses substantial technical difficulties for search\nengines. This paper aims to introduce non-experts to the foundational concepts\nof information retrieval (IR) and de-indexing, which are critical for\nunderstanding how search engines can effectively \"forget\" certain content. We\nwill explore various IR models, including boolean, probabilistic, vector space,\nand embedding-based approaches, as well as the role of Large Language Models\n(LLMs) in enhancing data processing capabilities. By providing this overview,\nwe seek to highlight the complexities involved in balancing individual privacy\nrights with the operational challenges faced by search engines in managing\ninformation visibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the digital age, the challenge of forgetfulness has emerged as a\nsignificant concern, particularly regarding the management of personal data and\nits accessibility online. The right to be forgotten (RTBF) allows individuals\nto request the removal of outdated or harmful information from public access,\nyet implementing this right poses substantial technical difficulties for search\nengines. This paper aims to introduce non-experts to the foundational concepts\nof information retrieval (IR) and de-indexing, which are critical for\nunderstanding how search engines can effectively \"forget\" certain content. We\nwill explore various IR models, including boolean, probabilistic, vector space,\nand embedding-based approaches, as well as the role of Large Language Models\n(LLMs) in enhancing data processing capabilities. By providing this overview,\nwe seek to highlight the complexities involved in balancing individual privacy\nrights with the operational challenges faced by search engines in managing\ninformation visibility."
                },
                "authors": [
                    {
                        "name": "Salvatore Vilella"
                    },
                    {
                        "name": "Giancarlo Ruffo"
                    }
                ],
                "author_detail": {
                    "name": "Giancarlo Ruffo"
                },
                "author": "Giancarlo Ruffo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4; H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13510v2",
                "updated": "2025-01-07T18:16:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    16,
                    17,
                    1,
                    7,
                    0
                ],
                "published": "2024-08-24T08:12:22Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    8,
                    12,
                    22,
                    5,
                    237,
                    0
                ],
                "title": "Intelligent Router for LLM Workloads: Improving Performance Through\n  Workload-Aware Load Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Router for LLM Workloads: Improving Performance Through\n  Workload-Aware Load Balancing"
                },
                "summary": "Large Language Model (LLM) workloads have distinct prefill and decode phases\nwith different compute and memory requirements which should ideally be\naccounted for when scheduling input queries across different LLM instances in a\ncluster. However existing scheduling algorithms treat LLM workloads as\nmonolithic jobs without considering the distinct characteristics of the two\nphases in each workload. This leads to sub-optimal scheduling and increased\nresponse latency. In this work, we start by characterizing factors affecting\nthe response latency during LLM inference serving. We establish that better\nload balancing of inference requests across the available LLM instances can\nimprove the end-to-end latency to a larger extent than merely focusing on\noptimizing the instance-level scheduler. Motivated by our findings, we propose\na heuristic-guided reinforcement learning-based intelligent router for\ndata-driven and workload-aware scheduling. Our router schedules queries across\nLLM instances by leveraging a trainable response-length predictor, and a novel\nformulation for estimating the impact of mixing different workloads and\nachieves over 11% lower end-to-end latency than existing approaches on a mix of\npublic datasets and 7.8% lower end-to-end latency on real workload data with\ndiverse input and output trends from Cloud Provider X. Additionally, the\nproposed framework can also serve as a standard for benchmarking different LLM\ninference schedulers since it provides the best latency for a given model,\nhardware, and instance-level scheduler combination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) workloads have distinct prefill and decode phases\nwith different compute and memory requirements which should ideally be\naccounted for when scheduling input queries across different LLM instances in a\ncluster. However existing scheduling algorithms treat LLM workloads as\nmonolithic jobs without considering the distinct characteristics of the two\nphases in each workload. This leads to sub-optimal scheduling and increased\nresponse latency. In this work, we start by characterizing factors affecting\nthe response latency during LLM inference serving. We establish that better\nload balancing of inference requests across the available LLM instances can\nimprove the end-to-end latency to a larger extent than merely focusing on\noptimizing the instance-level scheduler. Motivated by our findings, we propose\na heuristic-guided reinforcement learning-based intelligent router for\ndata-driven and workload-aware scheduling. Our router schedules queries across\nLLM instances by leveraging a trainable response-length predictor, and a novel\nformulation for estimating the impact of mixing different workloads and\nachieves over 11% lower end-to-end latency than existing approaches on a mix of\npublic datasets and 7.8% lower end-to-end latency on real workload data with\ndiverse input and output trends from Cloud Provider X. Additionally, the\nproposed framework can also serve as a standard for benchmarking different LLM\ninference schedulers since it provides the best latency for a given model,\nhardware, and instance-level scheduler combination."
                },
                "authors": [
                    {
                        "name": "Kunal Jain"
                    },
                    {
                        "name": "Anjaly Parayil"
                    },
                    {
                        "name": "Ankur Mallick"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Xiaoting Qin"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Rujia Wang"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Anoop Kulkarni"
                    },
                    {
                        "name": "Steve Kofsky"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03968v1",
                "updated": "2025-01-07T18:06:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    6,
                    27,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:06:27Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    6,
                    27,
                    1,
                    7,
                    0
                ],
                "title": "VLM-driven Behavior Tree for Context-aware Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM-driven Behavior Tree for Context-aware Task Planning"
                },
                "summary": "The use of Large Language Models (LLMs) for generating Behavior Trees (BTs)\nhas recently gained attention in the robotics community, yet remains in its\nearly stages of development. In this paper, we propose a novel framework that\nleverages Vision-Language Models (VLMs) to interactively generate and edit BTs\nthat address visual conditions, enabling context-aware robot operations in\nvisually complex environments. A key feature of our approach lies in the\nconditional control through self-prompted visual conditions. Specifically, the\nVLM generates BTs with visual condition nodes, where conditions are expressed\nas free-form text. Another VLM process integrates the text into its prompt and\nevaluates the conditions against real-world images during robot execution. We\nvalidated our framework in a real-world cafe scenario, demonstrating both its\nfeasibility and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) for generating Behavior Trees (BTs)\nhas recently gained attention in the robotics community, yet remains in its\nearly stages of development. In this paper, we propose a novel framework that\nleverages Vision-Language Models (VLMs) to interactively generate and edit BTs\nthat address visual conditions, enabling context-aware robot operations in\nvisually complex environments. A key feature of our approach lies in the\nconditional control through self-prompted visual conditions. Specifically, the\nVLM generates BTs with visual condition nodes, where conditions are expressed\nas free-form text. Another VLM process integrates the text into its prompt and\nevaluates the conditions against real-world images during robot execution. We\nvalidated our framework in a real-world cafe scenario, demonstrating both its\nfeasibility and limitations."
                },
                "authors": [
                    {
                        "name": "Naoki Wake"
                    },
                    {
                        "name": "Atsushi Kanehira"
                    },
                    {
                        "name": "Jun Takamatsu"
                    },
                    {
                        "name": "Kazuhiro Sasabuchi"
                    },
                    {
                        "name": "Katsushi Ikeuchi"
                    }
                ],
                "author_detail": {
                    "name": "Katsushi Ikeuchi"
                },
                "author": "Katsushi Ikeuchi",
                "arxiv_comment": "10 pages, 11 figures, 5 tables. Last updated on January 7th, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03957v1",
                "updated": "2025-01-07T17:37:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    37,
                    57,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:37:57Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    37,
                    57,
                    1,
                    7,
                    0
                ],
                "title": "Vision Language Models as Values Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models as Values Detectors"
                },
                "summary": "Large Language Models integrating textual and visual inputs have introduced\nnew possibilities for interpreting complex data. Despite their remarkable\nability to generate coherent and contextually relevant text based on visual\nstimuli, the alignment of these models with human perception in identifying\nrelevant elements in images requires further exploration. This paper\ninvestigates the alignment between state-of-the-art LLMs and human annotators\nin detecting elements of relevance within home environment scenarios. We\ncreated a set of twelve images depicting various domestic scenarios and\nenlisted fourteen annotators to identify the key element in each image. We then\ncompared these human responses with outputs from five different LLMs, including\nGPT-4o and four LLaVA variants. Our findings reveal a varied degree of\nalignment, with LLaVA 34B showing the highest performance but still scoring\nlow. However, an analysis of the results highlights the models' potential to\ndetect value-laden elements in images, suggesting that with improved training\nand refined prompts, LLMs could enhance applications in social robotics,\nassistive technologies, and human-computer interaction by providing deeper\ninsights and more contextually relevant responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models integrating textual and visual inputs have introduced\nnew possibilities for interpreting complex data. Despite their remarkable\nability to generate coherent and contextually relevant text based on visual\nstimuli, the alignment of these models with human perception in identifying\nrelevant elements in images requires further exploration. This paper\ninvestigates the alignment between state-of-the-art LLMs and human annotators\nin detecting elements of relevance within home environment scenarios. We\ncreated a set of twelve images depicting various domestic scenarios and\nenlisted fourteen annotators to identify the key element in each image. We then\ncompared these human responses with outputs from five different LLMs, including\nGPT-4o and four LLaVA variants. Our findings reveal a varied degree of\nalignment, with LLaVA 34B showing the highest performance but still scoring\nlow. However, an analysis of the results highlights the models' potential to\ndetect value-laden elements in images, suggesting that with improved training\nand refined prompts, LLMs could enhance applications in social robotics,\nassistive technologies, and human-computer interaction by providing deeper\ninsights and more contextually relevant responses."
                },
                "authors": [
                    {
                        "name": "Giulio Antonio Abbo"
                    },
                    {
                        "name": "Tony Belpaeme"
                    }
                ],
                "author_detail": {
                    "name": "Tony Belpaeme"
                },
                "author": "Tony Belpaeme",
                "arxiv_comment": "13 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11735v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11735v3",
                "updated": "2025-01-07T17:34:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    34,
                    4,
                    1,
                    7,
                    0
                ],
                "published": "2024-08-21T15:59:33Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    59,
                    33,
                    2,
                    234,
                    0
                ],
                "title": "Clinical Insights: A Comprehensive Review of Language Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical Insights: A Comprehensive Review of Language Models in Medicine"
                },
                "summary": "This paper explores the advancements and applications of language models in\nhealthcare, focusing on their clinical use cases. It examines the evolution\nfrom early encoder-based systems requiring extensive fine-tuning to\nstate-of-the-art large language and multimodal models capable of integrating\ntext and visual data through in-context learning. The analysis emphasizes\nlocally deployable models, which enhance data privacy and operational autonomy,\nand their applications in tasks such as text generation, classification,\ninformation extraction, and conversational systems. The paper also highlights a\nstructured organization of tasks and a tiered ethical approach, providing a\nvaluable resource for researchers and practitioners, while discussing key\nchallenges related to ethics, evaluation, and implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the advancements and applications of language models in\nhealthcare, focusing on their clinical use cases. It examines the evolution\nfrom early encoder-based systems requiring extensive fine-tuning to\nstate-of-the-art large language and multimodal models capable of integrating\ntext and visual data through in-context learning. The analysis emphasizes\nlocally deployable models, which enhance data privacy and operational autonomy,\nand their applications in tasks such as text generation, classification,\ninformation extraction, and conversational systems. The paper also highlights a\nstructured organization of tasks and a tiered ethical approach, providing a\nvaluable resource for researchers and practitioners, while discussing key\nchallenges related to ethics, evaluation, and implementation."
                },
                "authors": [
                    {
                        "name": "Nikita Neveditsin"
                    },
                    {
                        "name": "Pawan Lingras"
                    },
                    {
                        "name": "Vijay Mago"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Mago"
                },
                "author": "Vijay Mago",
                "arxiv_comment": "Submitted to PLOS Digital Health, Revision 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11735v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11735v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08110v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08110v6",
                "updated": "2025-01-07T17:26:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    26,
                    26,
                    1,
                    7,
                    0
                ],
                "published": "2023-01-19T15:01:00Z",
                "published_parsed": [
                    2023,
                    1,
                    19,
                    15,
                    1,
                    0,
                    3,
                    19,
                    0
                ],
                "title": "AtMan: Understanding Transformer Predictions Through Memory Efficient\n  Attention Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AtMan: Understanding Transformer Predictions Through Memory Efficient\n  Attention Manipulation"
                },
                "summary": "Generative transformer models have become increasingly complex, with large\nnumbers of parameters and the ability to process multiple input modalities.\nCurrent methods for explaining their predictions are resource-intensive. Most\ncrucially, they require prohibitively large amounts of extra memory, since they\nrely on backpropagation which allocates almost twice as much GPU memory as the\nforward pass. This makes it difficult, if not impossible, to use them in\nproduction. We present AtMan that provides explanations of generative\ntransformer models at almost no extra cost. Specifically, AtMan is a\nmodality-agnostic perturbation method that manipulates the attention mechanisms\nof transformers to produce relevance maps for the input with respect to the\noutput prediction. Instead of using backpropagation, AtMan applies a\nparallelizable token-based search method based on cosine similarity\nneighborhood in the embedding space. Our exhaustive experiments on text and\nimage-text benchmarks demonstrate that AtMan outperforms current\nstate-of-the-art gradient-based methods on several metrics while being\ncomputationally efficient. As such, AtMan is suitable for use in large model\ninference deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative transformer models have become increasingly complex, with large\nnumbers of parameters and the ability to process multiple input modalities.\nCurrent methods for explaining their predictions are resource-intensive. Most\ncrucially, they require prohibitively large amounts of extra memory, since they\nrely on backpropagation which allocates almost twice as much GPU memory as the\nforward pass. This makes it difficult, if not impossible, to use them in\nproduction. We present AtMan that provides explanations of generative\ntransformer models at almost no extra cost. Specifically, AtMan is a\nmodality-agnostic perturbation method that manipulates the attention mechanisms\nof transformers to produce relevance maps for the input with respect to the\noutput prediction. Instead of using backpropagation, AtMan applies a\nparallelizable token-based search method based on cosine similarity\nneighborhood in the embedding space. Our exhaustive experiments on text and\nimage-text benchmarks demonstrate that AtMan outperforms current\nstate-of-the-art gradient-based methods on several metrics while being\ncomputationally efficient. As such, AtMan is suitable for use in large model\ninference deployments."
                },
                "authors": [
                    {
                        "name": "Björn Deiseroth"
                    },
                    {
                        "name": "Mayukh Deb"
                    },
                    {
                        "name": "Samuel Weinbach"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08110v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08110v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03952v1",
                "updated": "2025-01-07T17:24:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    24,
                    17,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:24:17Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    24,
                    17,
                    1,
                    7,
                    0
                ],
                "title": "Localizing AI: Evaluating Open-Weight Language Models for Languages of\n  Baltic States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localizing AI: Evaluating Open-Weight Language Models for Languages of\n  Baltic States"
                },
                "summary": "Although large language models (LLMs) have transformed our expectations of\nmodern language technologies, concerns over data privacy often restrict the use\nof commercially available LLMs hosted outside of EU jurisdictions. This limits\ntheir application in governmental, defence, and other data-sensitive sectors.\nIn this work, we evaluate the extent to which locally deployable open-weight\nLLMs support lesser-spoken languages such as Lithuanian, Latvian, and Estonian.\nWe examine various size and precision variants of the top-performing\nmultilingual open-weight models, Llama~3, Gemma~2, Phi, and NeMo, on machine\ntranslation, multiple-choice question answering, and free-form text generation.\nThe results indicate that while certain models like Gemma~2 perform close to\nthe top commercially available models, many LLMs struggle with these languages.\nMost surprisingly, however, we find that these models, while showing close to\nstate-of-the-art translation performance, are still prone to lexical\nhallucinations with errors in at least 1 in 20 words for all open-weight\nmultilingual LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have transformed our expectations of\nmodern language technologies, concerns over data privacy often restrict the use\nof commercially available LLMs hosted outside of EU jurisdictions. This limits\ntheir application in governmental, defence, and other data-sensitive sectors.\nIn this work, we evaluate the extent to which locally deployable open-weight\nLLMs support lesser-spoken languages such as Lithuanian, Latvian, and Estonian.\nWe examine various size and precision variants of the top-performing\nmultilingual open-weight models, Llama~3, Gemma~2, Phi, and NeMo, on machine\ntranslation, multiple-choice question answering, and free-form text generation.\nThe results indicate that while certain models like Gemma~2 perform close to\nthe top commercially available models, many LLMs struggle with these languages.\nMost surprisingly, however, we find that these models, while showing close to\nstate-of-the-art translation performance, are still prone to lexical\nhallucinations with errors in at least 1 in 20 words for all open-weight\nmultilingual LLMs."
                },
                "authors": [
                    {
                        "name": "Jurgita Kapočiūtė-Dzikienė"
                    },
                    {
                        "name": "Toms Bergmanis"
                    },
                    {
                        "name": "Mārcis Pinnis"
                    }
                ],
                "author_detail": {
                    "name": "Mārcis Pinnis"
                },
                "author": "Mārcis Pinnis",
                "arxiv_comment": "This paper is accepted to NoDaLiDa/Baltic-HLT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v1",
                "updated": "2025-01-07T17:00:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14746v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14746v3",
                "updated": "2025-01-07T16:58:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    58,
                    5,
                    1,
                    7,
                    0
                ],
                "published": "2024-02-22T18:06:19Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    18,
                    6,
                    19,
                    3,
                    53,
                    0
                ],
                "title": "Scaling Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Efficient LLMs"
                },
                "summary": "Trained LLMs are typically sparse in that most of the parameters are zero,\nraising questions on efficiency. In response, we inquire into efficient LLMs,\ni.e. those with the fewest parameters that achieve the desired accuracy on a\ntraining corpus. Specifically, we compare theoretical and empirical estimates\nfor training loss to obtain upper and lower bounds on the number of unique\nsequences in a natural training corpus as a function of its size. Our result\nimplies (1) to double the number of skills represented in a training corpus,\nthe corpus must scale more than four fold (2) for efficient LLMs, the number of\nparameters N and the size D of a natural training corpus scale as $N \\propto\nD^{0.44}$; (3) if the number of parameters of an LLM is smaller than the number\nof unique sequences in the training corpus, scaling up can uncover emergent\nskills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trained LLMs are typically sparse in that most of the parameters are zero,\nraising questions on efficiency. In response, we inquire into efficient LLMs,\ni.e. those with the fewest parameters that achieve the desired accuracy on a\ntraining corpus. Specifically, we compare theoretical and empirical estimates\nfor training loss to obtain upper and lower bounds on the number of unique\nsequences in a natural training corpus as a function of its size. Our result\nimplies (1) to double the number of skills represented in a training corpus,\nthe corpus must scale more than four fold (2) for efficient LLMs, the number of\nparameters N and the size D of a natural training corpus scale as $N \\propto\nD^{0.44}$; (3) if the number of parameters of an LLM is smaller than the number\nof unique sequences in the training corpus, scaling up can uncover emergent\nskills."
                },
                "authors": [
                    {
                        "name": "B. N. Kausik"
                    }
                ],
                "author_detail": {
                    "name": "B. N. Kausik"
                },
                "author": "B. N. Kausik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14746v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14746v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10020v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10020v4",
                "updated": "2025-01-07T16:48:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    48,
                    36,
                    1,
                    7,
                    0
                ],
                "published": "2024-11-15T07:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    54,
                    19,
                    4,
                    320,
                    0
                ],
                "title": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?"
                },
                "summary": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BERT in terms of performance, generalizability, computational\nresources, and throughput to BERT. Results: LLaMA models outperformed BERT\nacross datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BERT by 7% (F1) on\nNER and 4% on RE. However, LLaMA models required more computing resources and\nran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE package\nfeaturing both models, available at https://kiwi.clinicalnlp.org/. Conclusion:\nThis study is among the first to develop and evaluate a comprehensive clinical\nIE system using open-source LLMs. Results indicate that LLaMA models outperform\nBERT for clinical NER and RE but with higher computational costs and lower\nthroughputs. These findings highlight that choosing between LLMs and\ntraditional deep learning methods for clinical IE applications should remain\ntask-specific, taking into account both performance metrics and practical\nconsiderations such as available computing resources and the intended use case\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BERT in terms of performance, generalizability, computational\nresources, and throughput to BERT. Results: LLaMA models outperformed BERT\nacross datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BERT by 7% (F1) on\nNER and 4% on RE. However, LLaMA models required more computing resources and\nran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE package\nfeaturing both models, available at https://kiwi.clinicalnlp.org/. Conclusion:\nThis study is among the first to develop and evaluate a comprehensive clinical\nIE system using open-source LLMs. Results indicate that LLaMA models outperform\nBERT for clinical NER and RE but with higher computational costs and lower\nthroughputs. These findings highlight that choosing between LLMs and\ntraditional deep learning methods for clinical IE applications should remain\ntask-specific, taking into account both performance metrics and practical\nconsiderations such as available computing resources and the intended use case\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Xu Zuo"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Vipina K. Keloth"
                    },
                    {
                        "name": "Vincent J. Zhang"
                    },
                    {
                        "name": "Ruey-Ling Weng"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Xiaoqian Jiang"
                    },
                    {
                        "name": "Kirk E. Roberts"
                    },
                    {
                        "name": "Hua Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xu"
                },
                "author": "Hua Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10020v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10020v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19223v2",
                "updated": "2025-01-07T16:20:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    20,
                    17,
                    1,
                    7,
                    0
                ],
                "published": "2024-06-27T14:49:08Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    14,
                    49,
                    8,
                    3,
                    179,
                    0
                ],
                "title": "T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse\n  Representations for Memory-Efficient Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse\n  Representations for Memory-Efficient Embeddings"
                },
                "summary": "Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced\neffectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our\nexhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced\neffectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our\nexhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning."
                },
                "authors": [
                    {
                        "name": "Björn Deiseroth"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Samuel Weinbach"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Weinbach"
                },
                "author": "Samuel Weinbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03904v1",
                "updated": "2025-01-07T16:18:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    18,
                    55,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T16:18:55Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    18,
                    55,
                    1,
                    7,
                    0
                ],
                "title": "Exploring the Potential of Large Language Models in Public\n  Transportation: San Antonio Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Large Language Models in Public\n  Transportation: San Antonio Case Study"
                },
                "summary": "The integration of large language models (LLMs) into public transit systems\npresents a transformative opportunity to enhance urban mobility. This study\nexplores the potential of LLMs to revolutionize public transportation\nmanagement within the context of San Antonio's transit system. Leveraging the\ncapabilities of LLMs in natural language processing and data analysis, we\ninvestigate their capabilities to optimize route planning, reduce wait times,\nand provide personalized travel assistance. By utilizing the General Transit\nFeed Specification (GTFS) and other relevant data, this research aims to\ndemonstrate how LLMs can potentially improve resource allocation, elevate\npassenger satisfaction, and inform data-driven decision-making in transit\noperations. A comparative analysis of different ChatGPT models was conducted to\nassess their ability to understand transportation information, retrieve\nrelevant data, and provide comprehensive responses. Findings from this study\nsuggest that while LLMs hold immense promise for public transit, careful\nengineering and fine-tuning are essential to realizing their full potential.\nSan Antonio serves as a case study to inform the development of LLM-powered\ntransit systems in other urban environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) into public transit systems\npresents a transformative opportunity to enhance urban mobility. This study\nexplores the potential of LLMs to revolutionize public transportation\nmanagement within the context of San Antonio's transit system. Leveraging the\ncapabilities of LLMs in natural language processing and data analysis, we\ninvestigate their capabilities to optimize route planning, reduce wait times,\nand provide personalized travel assistance. By utilizing the General Transit\nFeed Specification (GTFS) and other relevant data, this research aims to\ndemonstrate how LLMs can potentially improve resource allocation, elevate\npassenger satisfaction, and inform data-driven decision-making in transit\noperations. A comparative analysis of different ChatGPT models was conducted to\nassess their ability to understand transportation information, retrieve\nrelevant data, and provide comprehensive responses. Findings from this study\nsuggest that while LLMs hold immense promise for public transit, careful\nengineering and fine-tuning are essential to realizing their full potential.\nSan Antonio serves as a case study to inform the development of LLM-powered\ntransit systems in other urban environments."
                },
                "authors": [
                    {
                        "name": "Ramya Jonnala"
                    },
                    {
                        "name": "Gongbo Liang"
                    },
                    {
                        "name": "Jeong Yang"
                    },
                    {
                        "name": "Izzat Alsmadi"
                    }
                ],
                "author_detail": {
                    "name": "Izzat Alsmadi"
                },
                "author": "Izzat Alsmadi",
                "arxiv_comment": "This work is accepted to AAAI 2025 Workshop on AI for Urban Planning.\n  arXiv admin note: substantial text overlap with arXiv:2407.11003",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19519v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19519v2",
                "updated": "2025-01-07T16:13:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    13,
                    50,
                    1,
                    7,
                    0
                ],
                "published": "2024-05-29T20:56:52Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    20,
                    56,
                    52,
                    2,
                    150,
                    0
                ],
                "title": "Two-Layer Retrieval-Augmented Generation Framework for Low-Resource\n  Medical Question Answering Using Reddit Data: Proof-of-Concept Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Layer Retrieval-Augmented Generation Framework for Low-Resource\n  Medical Question Answering Using Reddit Data: Proof-of-Concept Study"
                },
                "summary": "The increasing use of social media to share lived and living experiences of\nsubstance use presents a unique opportunity to obtain information on side\neffects, use patterns, and opinions on novel psychoactive substances. However,\ndue to the large volume of data, obtaining useful insights through natural\nlanguage processing technologies such as large language models is challenging.\nThis paper aims to develop a retrieval-augmented generation (RAG) architecture\nfor medical question answering pertaining to clinicians' queries on emerging\nissues associated with health-related topics, using user-generated medical\ninformation on social media. We proposed a two-layer RAG framework for\nquery-focused answer generation and evaluated a proof of concept for the\nframework in the context of query-focused summary generation from social media\nforums, focusing on emerging drug-related information. Our modular framework\ngenerates individual summaries followed by an aggregated summary to answer\nmedical queries from large amounts of user-generated social media data in an\nefficient manner. We compared the performance of a quantized large language\nmodel (Nous-Hermes-2-7B-DPO), deployable in low-resource settings, with GPT-4.\nFor this proof-of-concept study, we used user-generated data from Reddit to\nanswer clinicians' questions on the use of xylazine and ketamine. Our framework\nachieves comparable median scores in terms of relevance, length, hallucination,\ncoverage, and coherence when evaluated using GPT-4 and Nous-Hermes-2-7B-DPO,\nevaluated for 20 queries with 76 samples. There was no statistically\nsignificant difference between the two for coverage, coherence, relevance,\nlength, and hallucination. A statistically significant difference was noted for\nthe Coleman-Liau Index. Our RAG framework can effectively answer medical\nquestions about targeted topics and can be deployed in resource-constrained\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of social media to share lived and living experiences of\nsubstance use presents a unique opportunity to obtain information on side\neffects, use patterns, and opinions on novel psychoactive substances. However,\ndue to the large volume of data, obtaining useful insights through natural\nlanguage processing technologies such as large language models is challenging.\nThis paper aims to develop a retrieval-augmented generation (RAG) architecture\nfor medical question answering pertaining to clinicians' queries on emerging\nissues associated with health-related topics, using user-generated medical\ninformation on social media. We proposed a two-layer RAG framework for\nquery-focused answer generation and evaluated a proof of concept for the\nframework in the context of query-focused summary generation from social media\nforums, focusing on emerging drug-related information. Our modular framework\ngenerates individual summaries followed by an aggregated summary to answer\nmedical queries from large amounts of user-generated social media data in an\nefficient manner. We compared the performance of a quantized large language\nmodel (Nous-Hermes-2-7B-DPO), deployable in low-resource settings, with GPT-4.\nFor this proof-of-concept study, we used user-generated data from Reddit to\nanswer clinicians' questions on the use of xylazine and ketamine. Our framework\nachieves comparable median scores in terms of relevance, length, hallucination,\ncoverage, and coherence when evaluated using GPT-4 and Nous-Hermes-2-7B-DPO,\nevaluated for 20 queries with 76 samples. There was no statistically\nsignificant difference between the two for coverage, coherence, relevance,\nlength, and hallucination. A statistically significant difference was noted for\nthe Coleman-Liau Index. Our RAG framework can effectively answer medical\nquestions about targeted topics and can be deployed in resource-constrained\nsettings."
                },
                "authors": [
                    {
                        "name": "Sudeshna Das"
                    },
                    {
                        "name": "Yao Ge"
                    },
                    {
                        "name": "Yuting Guo"
                    },
                    {
                        "name": "Swati Rajwal"
                    },
                    {
                        "name": "JaMor Hairston"
                    },
                    {
                        "name": "Jeanne Powell"
                    },
                    {
                        "name": "Drew Walker"
                    },
                    {
                        "name": "Snigdha Peddireddy"
                    },
                    {
                        "name": "Sahithi Lakamana"
                    },
                    {
                        "name": "Selen Bozkurt"
                    },
                    {
                        "name": "Matthew Reyna"
                    },
                    {
                        "name": "Reza Sameni"
                    },
                    {
                        "name": "Yunyu Xiao"
                    },
                    {
                        "name": "Sangmi Kim"
                    },
                    {
                        "name": "Rasheeta Chandler"
                    },
                    {
                        "name": "Natalie Hernandez"
                    },
                    {
                        "name": "Danielle Mowery"
                    },
                    {
                        "name": "Rachel Wightman"
                    },
                    {
                        "name": "Jennifer Love"
                    },
                    {
                        "name": "Anthony Spadaro"
                    },
                    {
                        "name": "Jeanmarie Perrone"
                    },
                    {
                        "name": "Abeed Sarker"
                    }
                ],
                "author_detail": {
                    "name": "Abeed Sarker"
                },
                "author": "Abeed Sarker",
                "arxiv_doi": "10.2196/66220",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.2196/66220",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.19519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19519v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in JMIR: https://www.jmir.org/2025/1/e66220",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16315v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16315v4",
                "updated": "2025-01-07T16:05:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    5,
                    16,
                    1,
                    7,
                    0
                ],
                "published": "2024-02-26T05:43:51Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    5,
                    43,
                    51,
                    0,
                    57,
                    0
                ],
                "title": "Finer: Investigating and Enhancing Fine-Grained Visual Concept\n  Recognition in Large Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finer: Investigating and Enhancing Fine-Grained Visual Concept\n  Recognition in Large Vision Language Models"
                },
                "summary": "Recent advances in instruction-tuned Large Vision-Language Models (LVLMs)\nhave imbued the models with the ability to generate high-level, image-grounded\nexplanations with ease. While such capability is largely attributed to the rich\nworld knowledge contained within the Large Language Models (LLMs), our work\nreveals their shortcomings in fine-grained visual categorization (FGVC) across\nsix different benchmark settings. Most recent state-of-the-art LVLMs like\nLLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of\nclassification performance, e.g., average drop of 65.58 in EM for Stanford Dogs\nfor LLaVA-1.5, but also struggle to generate an accurate explanation with\ndetailed attributes based on the concept that appears within an input image\ndespite their capability to generate holistic image-level descriptions.\nIn-depth analyses show that instruction-tuned LVLMs exhibit modality gap,\nshowing discrepancy when given textual and visual inputs that correspond to the\nsame concept, preventing the image modality from leveraging the rich parametric\nknowledge within the LLMs. In an effort to further the community's endeavor in\nthis direction, we propose a multiple granularity attribute-centric evaluation\nbenchmark, Finer, which aims to establish a ground to evaluate LVLMs'\nfine-grained visual comprehension ability and provide significantly improved\nexplainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in instruction-tuned Large Vision-Language Models (LVLMs)\nhave imbued the models with the ability to generate high-level, image-grounded\nexplanations with ease. While such capability is largely attributed to the rich\nworld knowledge contained within the Large Language Models (LLMs), our work\nreveals their shortcomings in fine-grained visual categorization (FGVC) across\nsix different benchmark settings. Most recent state-of-the-art LVLMs like\nLLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of\nclassification performance, e.g., average drop of 65.58 in EM for Stanford Dogs\nfor LLaVA-1.5, but also struggle to generate an accurate explanation with\ndetailed attributes based on the concept that appears within an input image\ndespite their capability to generate holistic image-level descriptions.\nIn-depth analyses show that instruction-tuned LVLMs exhibit modality gap,\nshowing discrepancy when given textual and visual inputs that correspond to the\nsame concept, preventing the image modality from leveraging the rich parametric\nknowledge within the LLMs. In an effort to further the community's endeavor in\nthis direction, we propose a multiple granularity attribute-centric evaluation\nbenchmark, Finer, which aims to establish a ground to evaluate LVLMs'\nfine-grained visual comprehension ability and provide significantly improved\nexplainability."
                },
                "authors": [
                    {
                        "name": "Jeonghwan Kim"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_doi": "10.18653/v1/2024.emnlp-main.356",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.emnlp-main.356",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.16315v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16315v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "EMNLP 2024; Main Conference",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03895v1",
                "updated": "2025-01-07T16:03:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    3,
                    14,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T16:03:14Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    3,
                    14,
                    1,
                    7,
                    0
                ],
                "title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One\n  Vision Token",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One\n  Vision Token"
                },
                "summary": "The advent of real-time large multimodal models (LMMs) like GPT-4o has\nsparked considerable interest in efficient LMMs. LMM frameworks typically\nencode visual inputs into vision tokens (continuous representations) and\nintegrate them and textual instructions into the context of large language\nmodels (LLMs), where large-scale parameters and numerous context tokens\n(predominantly vision tokens) result in substantial computational overhead.\nPrevious efforts towards efficient LMMs always focus on replacing the LLM\nbackbone with smaller models, while neglecting the crucial issue of token\nquantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal\nvision tokens. To achieve a high compression ratio of vision tokens while\npreserving visual information, we first analyze how LMMs understand vision\ntokens and find that most vision tokens only play a crucial role in the early\nlayers of LLM backbone, where they mainly fuse visual information into text\ntokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to\nfuse visual information into text tokens in advance, thereby facilitating the\nextreme compression of vision tokens fed to LLM backbone into one token.\nLLaVA-Mini is a unified large multimodal model that can support the\nunderstanding of images, high-resolution images, and videos in an efficient\nmanner. Experiments across 11 image-based and 7 video-based benchmarks\ndemonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token\ninstead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by\n77%, deliver low-latency responses within 40 milliseconds, and process over\n10,000 frames of video on the GPU hardware with 24GB of memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of real-time large multimodal models (LMMs) like GPT-4o has\nsparked considerable interest in efficient LMMs. LMM frameworks typically\nencode visual inputs into vision tokens (continuous representations) and\nintegrate them and textual instructions into the context of large language\nmodels (LLMs), where large-scale parameters and numerous context tokens\n(predominantly vision tokens) result in substantial computational overhead.\nPrevious efforts towards efficient LMMs always focus on replacing the LLM\nbackbone with smaller models, while neglecting the crucial issue of token\nquantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal\nvision tokens. To achieve a high compression ratio of vision tokens while\npreserving visual information, we first analyze how LMMs understand vision\ntokens and find that most vision tokens only play a crucial role in the early\nlayers of LLM backbone, where they mainly fuse visual information into text\ntokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to\nfuse visual information into text tokens in advance, thereby facilitating the\nextreme compression of vision tokens fed to LLM backbone into one token.\nLLaVA-Mini is a unified large multimodal model that can support the\nunderstanding of images, high-resolution images, and videos in an efficient\nmanner. Experiments across 11 image-based and 7 video-based benchmarks\ndemonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token\ninstead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by\n77%, deliver low-latency responses within 40 milliseconds, and process over\n10,000 frames of video on the GPU hardware with 24GB of memory."
                },
                "authors": [
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "Code: https://github.com/ictnlp/LLaVA-Mini; Model:\n  https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03892v1",
                "updated": "2025-01-07T16:00:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    0,
                    40,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T16:00:40Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    0,
                    40,
                    1,
                    7,
                    0
                ],
                "title": "LEAP: LLM-powered End-to-end Automatic Library for Processing Social\n  Science Queries on Unstructured Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEAP: LLM-powered End-to-end Automatic Library for Processing Social\n  Science Queries on Unstructured Data"
                },
                "summary": "Social scientists are increasingly interested in analyzing the semantic\ninformation (e.g., emotion) of unstructured data (e.g., Tweets), where the\nsemantic information is not natively present. Performing this analysis in a\ncost-efficient manner requires using machine learning (ML) models to extract\nthe semantic information and subsequently analyze the now structured data.\nHowever, this process remains challenging for domain experts.\n  To demonstrate the challenges in social science analytics, we collect a\ndataset, QUIET-ML, of 120 real-world social science queries in natural language\nand their ground truth answers. Existing systems struggle with these queries\nsince (1) they require selecting and applying ML models, and (2) more than a\nquarter of these queries are vague, making standard tools like natural language\nto SQL systems unsuited. To address these issues, we develop LEAP, an\nend-to-end library that answers social science queries in natural language with\nML. LEAP filters vague queries to ensure that the answers are deterministic and\nselects from internally supported and user-defined ML functions to extend the\nunstructured data to structured tables with necessary annotations. LEAP further\ngenerates and executes code to respond to these natural language queries. LEAP\nachieves a 100% pass @ 3 and 92% pass @ 1 on QUIET-ML, with a \\$1.06 average\nend-to-end cost, of which code generation costs \\$0.02.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social scientists are increasingly interested in analyzing the semantic\ninformation (e.g., emotion) of unstructured data (e.g., Tweets), where the\nsemantic information is not natively present. Performing this analysis in a\ncost-efficient manner requires using machine learning (ML) models to extract\nthe semantic information and subsequently analyze the now structured data.\nHowever, this process remains challenging for domain experts.\n  To demonstrate the challenges in social science analytics, we collect a\ndataset, QUIET-ML, of 120 real-world social science queries in natural language\nand their ground truth answers. Existing systems struggle with these queries\nsince (1) they require selecting and applying ML models, and (2) more than a\nquarter of these queries are vague, making standard tools like natural language\nto SQL systems unsuited. To address these issues, we develop LEAP, an\nend-to-end library that answers social science queries in natural language with\nML. LEAP filters vague queries to ensure that the answers are deterministic and\nselects from internally supported and user-defined ML functions to extend the\nunstructured data to structured tables with necessary annotations. LEAP further\ngenerates and executes code to respond to these natural language queries. LEAP\nachieves a 100% pass @ 3 and 92% pass @ 1 on QUIET-ML, with a \\$1.06 average\nend-to-end cost, of which code generation costs \\$0.02."
                },
                "authors": [
                    {
                        "name": "Chuxuan Hu"
                    },
                    {
                        "name": "Austin Peters"
                    },
                    {
                        "name": "Daniel Kang"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kang"
                },
                "author": "Daniel Kang",
                "arxiv_doi": "10.14778/3705829.3705843",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3705829.3705843",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.03892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to VLDB 2025",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03884v1",
                "updated": "2025-01-07T15:46:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    46,
                    42,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T15:46:42Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    46,
                    42,
                    1,
                    7,
                    0
                ],
                "title": "AlphaPO -- Reward shape matters for LLM alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaPO -- Reward shape matters for LLM alignment"
                },
                "summary": "Reinforcement Learning with Human Feedback (RLHF) and its variants have made\nhuge strides toward the effective alignment of large language models (LLMs) to\nfollow instructions and reflect human values. More recently, Direct Alignment\nAlgorithms (DAAs) have emerged in which the reward modeling stage of RLHF is\nskipped by characterizing the reward directly as a function of the policy being\nlearned. Examples include Direct Preference Optimization (DPO) and Simple\nPreference Optimization (SimPO). These methods often suffer from likelihood\ndisplacement, a phenomenon by which the probabilities of preferred responses\nare often reduced undesirably.\n  In this paper, we argue that, for DAAs the reward (function) shape matters.\nWe introduce AlphaPO, a new DAA method that leverages an $\\alpha$-parameter to\nhelp change the shape of the reward function beyond the standard log reward.\nAlphaPO helps maintain fine-grained control over likelihood displacement and\nover-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO\nleads to about 7\\% to 10\\% relative improvement in alignment performance for\nthe instruct versions of Mistral-7B and Llama3-8B. The analysis and results\npresented highlight the importance of the reward shape, and how one can\nsystematically change it to affect training dynamics, as well as improve\nalignment performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Human Feedback (RLHF) and its variants have made\nhuge strides toward the effective alignment of large language models (LLMs) to\nfollow instructions and reflect human values. More recently, Direct Alignment\nAlgorithms (DAAs) have emerged in which the reward modeling stage of RLHF is\nskipped by characterizing the reward directly as a function of the policy being\nlearned. Examples include Direct Preference Optimization (DPO) and Simple\nPreference Optimization (SimPO). These methods often suffer from likelihood\ndisplacement, a phenomenon by which the probabilities of preferred responses\nare often reduced undesirably.\n  In this paper, we argue that, for DAAs the reward (function) shape matters.\nWe introduce AlphaPO, a new DAA method that leverages an $\\alpha$-parameter to\nhelp change the shape of the reward function beyond the standard log reward.\nAlphaPO helps maintain fine-grained control over likelihood displacement and\nover-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO\nleads to about 7\\% to 10\\% relative improvement in alignment performance for\nthe instruct versions of Mistral-7B and Llama3-8B. The analysis and results\npresented highlight the importance of the reward shape, and how one can\nsystematically change it to affect training dynamics, as well as improve\nalignment performance."
                },
                "authors": [
                    {
                        "name": "Aman Gupta"
                    },
                    {
                        "name": "Shao Tang"
                    },
                    {
                        "name": "Qingquan Song"
                    },
                    {
                        "name": "Sirou Zhu"
                    },
                    {
                        "name": "Jiwoo Hong"
                    },
                    {
                        "name": "Ankan Saha"
                    },
                    {
                        "name": "Viral Gupta"
                    },
                    {
                        "name": "Noah Lee"
                    },
                    {
                        "name": "Eunki Kim"
                    },
                    {
                        "name": "Jason Zhu"
                    },
                    {
                        "name": "Natesh Pillai"
                    },
                    {
                        "name": "S. Sathiya Keerthi"
                    }
                ],
                "author_detail": {
                    "name": "S. Sathiya Keerthi"
                },
                "author": "S. Sathiya Keerthi",
                "arxiv_comment": "Preprint. Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03879v1",
                "updated": "2025-01-07T15:42:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    42,
                    32,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T15:42:32Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    42,
                    32,
                    1,
                    7,
                    0
                ],
                "title": "CL3DOR: Contrastive Learning for 3D Large Multimodal Models via Odds\n  Ratio on High-Resolution Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CL3DOR: Contrastive Learning for 3D Large Multimodal Models via Odds\n  Ratio on High-Resolution Point Clouds"
                },
                "summary": "Recent research has demonstrated that Large Language Models (LLMs) are not\nlimited to text-only tasks but can also function as multimodal models across\nvarious modalities, including audio, images, and videos. In particular,\nresearch on 3D Large Multimodal Models (3D LMMs) is making notable strides,\ndriven by the potential of processing higher-dimensional data like point\nclouds. However, upon closer examination, we find that the visual and textual\ncontent within each sample of existing training datasets lacks both high\ninformational granularity and clarity, which serve as a bottleneck for precise\ncross-modal understanding. To address these issues, we propose CL3DOR,\nContrastive Learning for 3D large multimodal models via Odds ratio on\nhigh-Resolution point clouds, designed to ensure greater specificity and\nclarity in both visual and textual content. Specifically, we increase the\ndensity of point clouds per object and construct informative hard negative\nresponses in the training dataset to penalize unwanted responses. To leverage\nhard negative responses, we incorporate the odds ratio as an auxiliary term for\ncontrastive learning into the conventional language modeling loss. CL3DOR\nachieves state-of-the-art performance in 3D scene understanding and reasoning\nbenchmarks. Additionally, we demonstrate the effectiveness of CL3DOR's key\ncomponents through extensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has demonstrated that Large Language Models (LLMs) are not\nlimited to text-only tasks but can also function as multimodal models across\nvarious modalities, including audio, images, and videos. In particular,\nresearch on 3D Large Multimodal Models (3D LMMs) is making notable strides,\ndriven by the potential of processing higher-dimensional data like point\nclouds. However, upon closer examination, we find that the visual and textual\ncontent within each sample of existing training datasets lacks both high\ninformational granularity and clarity, which serve as a bottleneck for precise\ncross-modal understanding. To address these issues, we propose CL3DOR,\nContrastive Learning for 3D large multimodal models via Odds ratio on\nhigh-Resolution point clouds, designed to ensure greater specificity and\nclarity in both visual and textual content. Specifically, we increase the\ndensity of point clouds per object and construct informative hard negative\nresponses in the training dataset to penalize unwanted responses. To leverage\nhard negative responses, we incorporate the odds ratio as an auxiliary term for\ncontrastive learning into the conventional language modeling loss. CL3DOR\nachieves state-of-the-art performance in 3D scene understanding and reasoning\nbenchmarks. Additionally, we demonstrate the effectiveness of CL3DOR's key\ncomponents through extensive experiments."
                },
                "authors": [
                    {
                        "name": "Keonwoo Kim"
                    },
                    {
                        "name": "Yeongjae Cho"
                    },
                    {
                        "name": "Taebaek Hwang"
                    },
                    {
                        "name": "Minsoo Jo"
                    },
                    {
                        "name": "Sangdo Han"
                    }
                ],
                "author_detail": {
                    "name": "Sangdo Han"
                },
                "author": "Sangdo Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12359v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12359v2",
                "updated": "2025-01-07T15:36:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    36,
                    54,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-16T21:14:11Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    21,
                    14,
                    11,
                    0,
                    351,
                    0
                ],
                "title": "LLaVA Steering: Visual Instruction Tuning with 500x Fewer Parameters\n  through Modality Linear Representation-Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA Steering: Visual Instruction Tuning with 500x Fewer Parameters\n  through Modality Linear Representation-Steering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have significantly advanced visual\ntasks by integrating visual representations into large language models (LLMs).\nThe textual modality, inherited from LLMs, equips MLLMs with abilities like\ninstruction following and in-context learning. In contrast, the visual modality\nenhances performance in downstream tasks by leveraging rich semantic content,\nspatial information, and grounding capabilities. These intrinsic modalities\nwork synergistically across various visual tasks. Our research initially\nreveals a persistent imbalance between these modalities, with text often\ndominating output generation during visual instruction tuning. This imbalance\noccurs when using both full fine-tuning and parameter-efficient fine-tuning\n(PEFT) methods. We then found that re-balancing these modalities can\nsignificantly reduce the number of trainable parameters required, inspiring a\ndirection for further optimizing visual instruction tuning. We introduce\nModality Linear Representation-Steering (MoReS) to achieve the goal. MoReS\neffectively re-balances the intrinsic modalities throughout the model, where\nthe key idea is to steer visual representations through linear transformations\nin the visual subspace across each model layer. To validate our solution, we\ncomposed LLaVA Steering, a suite of models integrated with the proposed MoReS\nmethod. Evaluation results show that the composed LLaVA Steering models\nrequire, on average, 500 times fewer trainable parameters than LoRA needs while\nstill achieving comparable performance across three visual benchmarks and eight\nvisual question-answering tasks. Last, we present the LLaVA Steering Factory,\nan in-house developed platform that enables researchers to quickly customize\nvarious MLLMs with component-based architecture for seamlessly integrating\nstate-of-the-art models, and evaluate their intrinsic modality imbalance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have significantly advanced visual\ntasks by integrating visual representations into large language models (LLMs).\nThe textual modality, inherited from LLMs, equips MLLMs with abilities like\ninstruction following and in-context learning. In contrast, the visual modality\nenhances performance in downstream tasks by leveraging rich semantic content,\nspatial information, and grounding capabilities. These intrinsic modalities\nwork synergistically across various visual tasks. Our research initially\nreveals a persistent imbalance between these modalities, with text often\ndominating output generation during visual instruction tuning. This imbalance\noccurs when using both full fine-tuning and parameter-efficient fine-tuning\n(PEFT) methods. We then found that re-balancing these modalities can\nsignificantly reduce the number of trainable parameters required, inspiring a\ndirection for further optimizing visual instruction tuning. We introduce\nModality Linear Representation-Steering (MoReS) to achieve the goal. MoReS\neffectively re-balances the intrinsic modalities throughout the model, where\nthe key idea is to steer visual representations through linear transformations\nin the visual subspace across each model layer. To validate our solution, we\ncomposed LLaVA Steering, a suite of models integrated with the proposed MoReS\nmethod. Evaluation results show that the composed LLaVA Steering models\nrequire, on average, 500 times fewer trainable parameters than LoRA needs while\nstill achieving comparable performance across three visual benchmarks and eight\nvisual question-answering tasks. Last, we present the LLaVA Steering Factory,\nan in-house developed platform that enables researchers to quickly customize\nvarious MLLMs with component-based architecture for seamlessly integrating\nstate-of-the-art models, and evaluate their intrinsic modality imbalance."
                },
                "authors": [
                    {
                        "name": "Jinhe Bi"
                    },
                    {
                        "name": "Yujun Wang"
                    },
                    {
                        "name": "Haokun Chen"
                    },
                    {
                        "name": "Xun Xiao"
                    },
                    {
                        "name": "Artur Hecker"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12359v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12359v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14841v2",
                "updated": "2025-01-07T15:30:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    30,
                    56,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-19T13:34:14Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    34,
                    14,
                    3,
                    354,
                    0
                ],
                "title": "Helping LLMs Improve Code Generation Using Feedback from Testing and\n  Static Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helping LLMs Improve Code Generation Using Feedback from Testing and\n  Static Analysis"
                },
                "summary": "Large Language Models (LLMs) are one of the most promising developments in\nthe field of artificial intelligence, and the software engineering community\nhas readily noticed their potential role in the software development\nlife-cycle. Developers routinely ask LLMs to generate code snippets, increasing\nproductivity but also potentially introducing ownership, privacy, correctness,\nand security issues. Previous work highlighted how code generated by mainstream\ncommercial LLMs is often not safe, containing vulnerabilities, bugs, and code\nsmells. In this paper, we present a framework that leverages testing and static\nanalysis to assess the quality, and guide the self-improvement, of code\ngenerated by general-purpose, open-source LLMs.\n  First, we ask LLMs to generate C code to solve a number of programming tasks.\nThen we employ ground-truth tests to assess the (in)correctness of the\ngenerated code, and a static analysis tool to detect potential safety\nvulnerabilities. Next, we assess the models ability to evaluate the generated\ncode, by asking them to detect errors and vulnerabilities. Finally, we test the\nmodels ability to fix the generated code, providing the reports produced during\nthe static analysis and incorrectness evaluation phases as feedback.\n  Our results show that models often produce incorrect code, and that the\ngenerated code can include safety issues. Moreover, they perform very poorly at\ndetecting either issue. On the positive side, we observe a substantial ability\nto fix flawed code when provided with information about failed tests or\npotential vulnerabilities, indicating a promising avenue for improving the\nsafety of LLM-based code generation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are one of the most promising developments in\nthe field of artificial intelligence, and the software engineering community\nhas readily noticed their potential role in the software development\nlife-cycle. Developers routinely ask LLMs to generate code snippets, increasing\nproductivity but also potentially introducing ownership, privacy, correctness,\nand security issues. Previous work highlighted how code generated by mainstream\ncommercial LLMs is often not safe, containing vulnerabilities, bugs, and code\nsmells. In this paper, we present a framework that leverages testing and static\nanalysis to assess the quality, and guide the self-improvement, of code\ngenerated by general-purpose, open-source LLMs.\n  First, we ask LLMs to generate C code to solve a number of programming tasks.\nThen we employ ground-truth tests to assess the (in)correctness of the\ngenerated code, and a static analysis tool to detect potential safety\nvulnerabilities. Next, we assess the models ability to evaluate the generated\ncode, by asking them to detect errors and vulnerabilities. Finally, we test the\nmodels ability to fix the generated code, providing the reports produced during\nthe static analysis and incorrectness evaluation phases as feedback.\n  Our results show that models often produce incorrect code, and that the\ngenerated code can include safety issues. Moreover, they perform very poorly at\ndetecting either issue. On the positive side, we observe a substantial ability\nto fix flawed code when provided with information about failed tests or\npotential vulnerabilities, indicating a promising avenue for improving the\nsafety of LLM-based code generation tools."
                },
                "authors": [
                    {
                        "name": "Greta Dolcetti"
                    },
                    {
                        "name": "Vincenzo Arceri"
                    },
                    {
                        "name": "Eleonora Iotti"
                    },
                    {
                        "name": "Sergio Maffeis"
                    },
                    {
                        "name": "Agostino Cortesi"
                    },
                    {
                        "name": "Enea Zaffanella"
                    }
                ],
                "author_detail": {
                    "name": "Enea Zaffanella"
                },
                "author": "Enea Zaffanella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19155v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19155v3",
                "updated": "2025-01-07T15:30:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    30,
                    2,
                    1,
                    7,
                    0
                ],
                "published": "2024-10-24T20:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    20,
                    49,
                    22,
                    3,
                    298,
                    0
                ],
                "title": "Lived Experience Not Found: LLMs Struggle to Align with Experts on\n  Addressing Adverse Drug Reactions from Psychiatric Medication Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lived Experience Not Found: LLMs Struggle to Align with Experts on\n  Addressing Adverse Drug Reactions from Psychiatric Medication Use"
                },
                "summary": "Adverse Drug Reactions (ADRs) from psychiatric medications are the leading\ncause of hospitalizations among mental health patients. With healthcare systems\nand online communities facing limitations in resolving ADR-related issues,\nLarge Language Models (LLMs) have the potential to fill this gap. Despite the\nincreasing capabilities of LLMs, past research has not explored their\ncapabilities in detecting ADRs related to psychiatric medications or in\nproviding effective harm reduction strategies. To address this, we introduce\nthe Psych-ADR benchmark and the Adverse Drug Reaction Response Assessment\n(ADRA) framework to systematically evaluate LLM performance in detecting ADR\nexpressions and delivering expert-aligned mitigation strategies. Our analyses\nshow that LLMs struggle with understanding the nuances of ADRs and\ndifferentiating between types of ADRs. While LLMs align with experts in terms\nof expressed emotions and tone of the text, their responses are more complex,\nharder to read, and only 70.86% aligned with expert strategies. Furthermore,\nthey provide less actionable advice by a margin of 12.32% on average. Our work\nprovides a comprehensive benchmark and evaluation framework for assessing LLMs\nin strategy-driven tasks within high-risk domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adverse Drug Reactions (ADRs) from psychiatric medications are the leading\ncause of hospitalizations among mental health patients. With healthcare systems\nand online communities facing limitations in resolving ADR-related issues,\nLarge Language Models (LLMs) have the potential to fill this gap. Despite the\nincreasing capabilities of LLMs, past research has not explored their\ncapabilities in detecting ADRs related to psychiatric medications or in\nproviding effective harm reduction strategies. To address this, we introduce\nthe Psych-ADR benchmark and the Adverse Drug Reaction Response Assessment\n(ADRA) framework to systematically evaluate LLM performance in detecting ADR\nexpressions and delivering expert-aligned mitigation strategies. Our analyses\nshow that LLMs struggle with understanding the nuances of ADRs and\ndifferentiating between types of ADRs. While LLMs align with experts in terms\nof expressed emotions and tone of the text, their responses are more complex,\nharder to read, and only 70.86% aligned with expert strategies. Furthermore,\nthey provide less actionable advice by a margin of 12.32% on average. Our work\nprovides a comprehensive benchmark and evaluation framework for assessing LLMs\nin strategy-driven tasks within high-risk domains."
                },
                "authors": [
                    {
                        "name": "Mohit Chandra"
                    },
                    {
                        "name": "Siddharth Sriraman"
                    },
                    {
                        "name": "Gaurav Verma"
                    },
                    {
                        "name": "Harneet Singh Khanuja"
                    },
                    {
                        "name": "Jose Suarez Campayo"
                    },
                    {
                        "name": "Zihang Li"
                    },
                    {
                        "name": "Michael L. Birnbaum"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Munmun De Choudhury"
                },
                "author": "Munmun De Choudhury",
                "arxiv_comment": "30 pages, 8 figures, 16 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19155v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19155v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03857v1",
                "updated": "2025-01-07T15:14:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    14,
                    37,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T15:14:37Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    15,
                    14,
                    37,
                    1,
                    7,
                    0
                ],
                "title": "Progressive Document-level Text Simplification via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Document-level Text Simplification via Large Language Models"
                },
                "summary": "Research on text simplification has primarily focused on lexical and\nsentence-level changes. Long document-level simplification (DS) is still\nrelatively unexplored. Large Language Models (LLMs), like ChatGPT, have\nexcelled in many natural language processing tasks. However, their performance\non DS tasks is unsatisfactory, as they often treat DS as merely document\nsummarization. For the DS task, the generated long sequences not only must\nmaintain consistency with the original document throughout, but complete\nmoderate simplification operations encompassing discourses, sentences, and\nword-level simplifications. Human editors employ a hierarchical complexity\nsimplification strategy to simplify documents. This study delves into\nsimulating this strategy through the utilization of a multi-stage collaboration\nusing LLMs. We propose a progressive simplification method (ProgDS) by\nhierarchically decomposing the task, including the discourse-level,\ntopic-level, and lexical-level simplification. Experimental results demonstrate\nthat ProgDS significantly outperforms existing smaller models or direct\nprompting with LLMs, advancing the state-of-the-art in the document\nsimplification task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on text simplification has primarily focused on lexical and\nsentence-level changes. Long document-level simplification (DS) is still\nrelatively unexplored. Large Language Models (LLMs), like ChatGPT, have\nexcelled in many natural language processing tasks. However, their performance\non DS tasks is unsatisfactory, as they often treat DS as merely document\nsummarization. For the DS task, the generated long sequences not only must\nmaintain consistency with the original document throughout, but complete\nmoderate simplification operations encompassing discourses, sentences, and\nword-level simplifications. Human editors employ a hierarchical complexity\nsimplification strategy to simplify documents. This study delves into\nsimulating this strategy through the utilization of a multi-stage collaboration\nusing LLMs. We propose a progressive simplification method (ProgDS) by\nhierarchically decomposing the task, including the discourse-level,\ntopic-level, and lexical-level simplification. Experimental results demonstrate\nthat ProgDS significantly outperforms existing smaller models or direct\nprompting with LLMs, advancing the state-of-the-art in the document\nsimplification task."
                },
                "authors": [
                    {
                        "name": "Dengzhao Fang"
                    },
                    {
                        "name": "Jipeng Qiang"
                    },
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Yunhao Yuan"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Liu"
                },
                "author": "Yan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15460v3",
                "updated": "2025-01-07T14:56:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    56,
                    42,
                    1,
                    7,
                    0
                ],
                "published": "2024-10-20T18:18:23Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    18,
                    18,
                    23,
                    6,
                    294,
                    0
                ],
                "title": "Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model\n  Training"
                },
                "summary": "As large language models (LLMs) are increasingly deployed across various\nindustries, concerns regarding their reliability, particularly due to\nhallucinations - outputs that are factually inaccurate or irrelevant to user\ninput - have grown. Our research investigates the relationship between the\ntraining process and the emergence of hallucinations to address a key gap in\nexisting research that focuses primarily on post hoc detection and mitigation\nstrategies. Using models from the Pythia suite (70M - 12B parameters) and\nseveral hallucination detection metrics, we analyze hallucination trends\nthroughout training and explore LLM internal dynamics. We introduce Sensitivity\nDropout (SenD), a novel training protocol designed to mitigate hallucinations\nby reducing variance during training. SenD achieves this by deterministically\ndropping embedding indices with significant variability, referred to as\nSensitive Embedding Indices. In addition, we develop an unsupervised\nhallucination detection metric, Efficient EigenScore (EES), which approximates\nthe traditional EigenScore at 2x speed. This efficient metric is integrated\ninto our protocol, allowing SenD to be both computationally scalable and\neffective at reducing hallucinations. Our empirical evaluation demonstrates\nthat our approach improves LLM reliability at test time by up to 40% compared\nto normal training while also providing an efficient method to improve factual\naccuracy when adapting LLMs to Wikipedia, Medical, and LegalBench domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed across various\nindustries, concerns regarding their reliability, particularly due to\nhallucinations - outputs that are factually inaccurate or irrelevant to user\ninput - have grown. Our research investigates the relationship between the\ntraining process and the emergence of hallucinations to address a key gap in\nexisting research that focuses primarily on post hoc detection and mitigation\nstrategies. Using models from the Pythia suite (70M - 12B parameters) and\nseveral hallucination detection metrics, we analyze hallucination trends\nthroughout training and explore LLM internal dynamics. We introduce Sensitivity\nDropout (SenD), a novel training protocol designed to mitigate hallucinations\nby reducing variance during training. SenD achieves this by deterministically\ndropping embedding indices with significant variability, referred to as\nSensitive Embedding Indices. In addition, we develop an unsupervised\nhallucination detection metric, Efficient EigenScore (EES), which approximates\nthe traditional EigenScore at 2x speed. This efficient metric is integrated\ninto our protocol, allowing SenD to be both computationally scalable and\neffective at reducing hallucinations. Our empirical evaluation demonstrates\nthat our approach improves LLM reliability at test time by up to 40% compared\nto normal training while also providing an efficient method to improve factual\naccuracy when adapting LLMs to Wikipedia, Medical, and LegalBench domains."
                },
                "authors": [
                    {
                        "name": "Shahrad Mohammadzadeh"
                    },
                    {
                        "name": "Juan David Guerra"
                    },
                    {
                        "name": "Marco Bonizzato"
                    },
                    {
                        "name": "Reihaneh Rabbany"
                    },
                    {
                        "name": "Golnoosh Farnadi"
                    }
                ],
                "author_detail": {
                    "name": "Golnoosh Farnadi"
                },
                "author": "Golnoosh Farnadi",
                "arxiv_comment": "23 pages, 15 figures, under review at ICLR, accepted to Safe\n  Generative AI Workshop @ NeurIPS 2024, resubmitting to change name to\n  appropriate name",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03823v1",
                "updated": "2025-01-07T14:35:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    35,
                    56,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T14:35:56Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    35,
                    56,
                    1,
                    7,
                    0
                ],
                "title": "Use Cases for Terahertz Communications: An Industrial Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Use Cases for Terahertz Communications: An Industrial Perspective"
                },
                "summary": "Thanks to the vast amount of available resources and unique propagation\nproperties, terahertz (THz) frequency bands are viewed as a key enabler for\nachieving ultrahigh communication performance and precise sensing capabilities\nin future wireless systems. Recently, the European Telecommunications Standards\nInstitute (ETSI) initiated an Industry Specification Group (ISG) on THz which\naims at establishing the technical foundation for subsequent standardization of\nthis technology, which is pivotal for its successful integration into future\nnetworks. Starting from the work recently finalized within this group, this\npaper provides an industrial perspective on potential use cases and frequency\nbands of interest for THz communication systems. We first identify promising\nfrequency bands in the 100 GHz - 1 THz range, offering over 500 GHz of\navailable spectrum that can be exploited to unlock the full potential of THz\ncommunications. Then, we present key use cases and application areas for THz\ncommunications, emphasizing the role of this technology and its advantages over\nother frequency bands. We discuss their target requirements and show that some\napplications demand for multi-Tbps data rates, latency below 0.5 ms, and\nsensing accuracy down to 0.5 cm. Additionally, we identify the main deployment\nscenarios and outline other enabling technologies crucial for overcoming the\nchallenges faced by THz system. Finally, we summarize the past and ongoing\nstandardization efforts focusing on THz communications, while also providing an\noutlook towards the inclusion of this technology as an integral part of the\nfuture sixth generation (6G) and beyond communication networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thanks to the vast amount of available resources and unique propagation\nproperties, terahertz (THz) frequency bands are viewed as a key enabler for\nachieving ultrahigh communication performance and precise sensing capabilities\nin future wireless systems. Recently, the European Telecommunications Standards\nInstitute (ETSI) initiated an Industry Specification Group (ISG) on THz which\naims at establishing the technical foundation for subsequent standardization of\nthis technology, which is pivotal for its successful integration into future\nnetworks. Starting from the work recently finalized within this group, this\npaper provides an industrial perspective on potential use cases and frequency\nbands of interest for THz communication systems. We first identify promising\nfrequency bands in the 100 GHz - 1 THz range, offering over 500 GHz of\navailable spectrum that can be exploited to unlock the full potential of THz\ncommunications. Then, we present key use cases and application areas for THz\ncommunications, emphasizing the role of this technology and its advantages over\nother frequency bands. We discuss their target requirements and show that some\napplications demand for multi-Tbps data rates, latency below 0.5 ms, and\nsensing accuracy down to 0.5 cm. Additionally, we identify the main deployment\nscenarios and outline other enabling technologies crucial for overcoming the\nchallenges faced by THz system. Finally, we summarize the past and ongoing\nstandardization efforts focusing on THz communications, while also providing an\noutlook towards the inclusion of this technology as an integral part of the\nfuture sixth generation (6G) and beyond communication networks."
                },
                "authors": [
                    {
                        "name": "Tommaso Zugno"
                    },
                    {
                        "name": "Cristina Ciochina"
                    },
                    {
                        "name": "Sharad Sambhwani"
                    },
                    {
                        "name": "Patrick Svedman"
                    },
                    {
                        "name": "Luis M. Pessoa"
                    },
                    {
                        "name": "Ben Chen"
                    },
                    {
                        "name": "Per Hjalmar Lehne"
                    },
                    {
                        "name": "Mate Boban"
                    },
                    {
                        "name": "Thomas Kürner"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Kürner"
                },
                "author": "Thomas Kürner",
                "arxiv_comment": "This work has been accepted for publication in the IEEE Wireless\n  Communications. Copyright with IEEE. For more details, see the IEEE Copyright\n  Policy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10486v2",
                "updated": "2025-01-07T14:09:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    9,
                    22,
                    1,
                    7,
                    0
                ],
                "published": "2024-07-15T07:14:56Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    7,
                    14,
                    56,
                    0,
                    197,
                    0
                ],
                "title": "IDEAL: Leveraging Infinite and Dynamic Characterizations of Large\n  Language Models for Query-focused Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IDEAL: Leveraging Infinite and Dynamic Characterizations of Large\n  Language Models for Query-focused Summarization"
                },
                "summary": "Query-focused summarization (QFS) aims to produce summaries that answer\nparticular questions of interest, enabling greater user control and\npersonalization. With the advent of large language models (LLMs), shows their\nimpressive capability of textual understanding through large-scale pretraining,\nwhich implies the great potential of extractive snippet generation. In this\npaper, we systematically investigated two indispensable characteristics that\nthe LLMs-based QFS models should be harnessed, Lengthy Document Summarization\nand Efficiently Fine-grained Query-LLM Alignment, respectively.\nCorrespondingly, we propose two modules called Query-aware HyperExpert and\nQuery-focused Infini-attention to access the aforementioned characteristics.\nThese innovations pave the way for broader application and accessibility in the\nfield of QFS technology. Extensive experiments conducted on existing QFS\nbenchmarks indicate the effectiveness and generalizability of the proposed\napproach. Our code is publicly available at\nhttps://github.com/DCDmllm/IDEAL_Summary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-focused summarization (QFS) aims to produce summaries that answer\nparticular questions of interest, enabling greater user control and\npersonalization. With the advent of large language models (LLMs), shows their\nimpressive capability of textual understanding through large-scale pretraining,\nwhich implies the great potential of extractive snippet generation. In this\npaper, we systematically investigated two indispensable characteristics that\nthe LLMs-based QFS models should be harnessed, Lengthy Document Summarization\nand Efficiently Fine-grained Query-LLM Alignment, respectively.\nCorrespondingly, we propose two modules called Query-aware HyperExpert and\nQuery-focused Infini-attention to access the aforementioned characteristics.\nThese innovations pave the way for broader application and accessibility in the\nfield of QFS technology. Extensive experiments conducted on existing QFS\nbenchmarks indicate the effectiveness and generalizability of the proposed\napproach. Our code is publicly available at\nhttps://github.com/DCDmllm/IDEAL_Summary."
                },
                "authors": [
                    {
                        "name": "Jie Cao"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Qiang Yan"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03800v1",
                "updated": "2025-01-07T14:06:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    6,
                    57,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T14:06:57Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    6,
                    57,
                    1,
                    7,
                    0
                ],
                "title": "MADation: Face Morphing Attack Detection with Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MADation: Face Morphing Attack Detection with Foundation Models"
                },
                "summary": "Despite the considerable performance improvements of face recognition\nalgorithms in recent years, the same scientific advances responsible for this\nprogress can also be used to create efficient ways to attack them, posing a\nthreat to their secure deployment. Morphing attack detection (MAD) systems aim\nto detect a specific type of threat, morphing attacks, at an early stage,\npreventing them from being considered for verification in critical processes.\nFoundation models (FM) learn from extensive amounts of unlabeled data,\nachieving remarkable zero-shot generalization to unseen domains. Although this\ngeneralization capacity might be weak when dealing with domain-specific\ndownstream tasks such as MAD, FMs can easily adapt to these settings while\nretaining the built-in knowledge acquired during pre-training. In this work, we\nrecognize the potential of FMs to perform well in the MAD task when properly\nadapted to its specificities. To this end, we adapt FM CLIP architectures with\nLoRA weights while simultaneously training a classification header. The\nproposed framework, MADation surpasses our alternative FM and transformer-based\nframeworks and constitutes the first adaption of FMs to the MAD task. MADation\npresents competitive results with current MAD solutions in the literature and\neven surpasses them in several evaluation scenarios. To encourage\nreproducibility and facilitate further research in MAD, we publicly release the\nimplementation of MADation at https: //github.com/gurayozgur/MADation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the considerable performance improvements of face recognition\nalgorithms in recent years, the same scientific advances responsible for this\nprogress can also be used to create efficient ways to attack them, posing a\nthreat to their secure deployment. Morphing attack detection (MAD) systems aim\nto detect a specific type of threat, morphing attacks, at an early stage,\npreventing them from being considered for verification in critical processes.\nFoundation models (FM) learn from extensive amounts of unlabeled data,\nachieving remarkable zero-shot generalization to unseen domains. Although this\ngeneralization capacity might be weak when dealing with domain-specific\ndownstream tasks such as MAD, FMs can easily adapt to these settings while\nretaining the built-in knowledge acquired during pre-training. In this work, we\nrecognize the potential of FMs to perform well in the MAD task when properly\nadapted to its specificities. To this end, we adapt FM CLIP architectures with\nLoRA weights while simultaneously training a classification header. The\nproposed framework, MADation surpasses our alternative FM and transformer-based\nframeworks and constitutes the first adaption of FMs to the MAD task. MADation\npresents competitive results with current MAD solutions in the literature and\neven surpasses them in several evaluation scenarios. To encourage\nreproducibility and facilitate further research in MAD, we publicly release the\nimplementation of MADation at https: //github.com/gurayozgur/MADation"
                },
                "authors": [
                    {
                        "name": "Eduarda Caldeira"
                    },
                    {
                        "name": "Guray Ozgur"
                    },
                    {
                        "name": "Tahar Chettaoui"
                    },
                    {
                        "name": "Marija Ivanovska"
                    },
                    {
                        "name": "Fadi Boutros"
                    },
                    {
                        "name": "Vitomir Struc"
                    },
                    {
                        "name": "Naser Damer"
                    }
                ],
                "author_detail": {
                    "name": "Naser Damer"
                },
                "author": "Naser Damer",
                "arxiv_comment": "Accepted at WACV 2025 workshops",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11805v2",
                "updated": "2025-01-07T13:34:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    13,
                    34,
                    6,
                    1,
                    7,
                    0
                ],
                "published": "2024-10-15T17:33:43Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    33,
                    43,
                    1,
                    289,
                    0
                ],
                "title": "NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) combined with tool learning have gained\nimpressive results in real-world applications. During tool learning, LLMs may\ncall multiple tools in nested orders, where the latter tool call may take the\nformer response as its input parameters. However, current research on the\nnested tool learning capabilities is still under-explored, since the existing\nbenchmarks lack relevant data instances. To address this problem, we introduce\nNesTools to bridge the current gap in comprehensive nested tool learning\nevaluations. NesTools comprises a novel automatic data generation method to\nconstruct large-scale nested tool calls with different nesting structures. With\nmanual review and refinement, the dataset is in high quality and closely\naligned with real-world scenarios. Therefore, NesTools can serve as a new\nbenchmark to evaluate the nested tool learning abilities of LLMs. We conduct\nextensive experiments on 22 LLMs, and provide in-depth analyses with NesTools,\nwhich shows that current LLMs still suffer from the complex nested tool\nlearning task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) combined with tool learning have gained\nimpressive results in real-world applications. During tool learning, LLMs may\ncall multiple tools in nested orders, where the latter tool call may take the\nformer response as its input parameters. However, current research on the\nnested tool learning capabilities is still under-explored, since the existing\nbenchmarks lack relevant data instances. To address this problem, we introduce\nNesTools to bridge the current gap in comprehensive nested tool learning\nevaluations. NesTools comprises a novel automatic data generation method to\nconstruct large-scale nested tool calls with different nesting structures. With\nmanual review and refinement, the dataset is in high quality and closely\naligned with real-world scenarios. Therefore, NesTools can serve as a new\nbenchmark to evaluate the nested tool learning abilities of LLMs. We conduct\nextensive experiments on 22 LLMs, and provide in-depth analyses with NesTools,\nwhich shows that current LLMs still suffer from the complex nested tool\nlearning task."
                },
                "authors": [
                    {
                        "name": "Han Han"
                    },
                    {
                        "name": "Tong Zhu"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Mengsong Wu"
                    },
                    {
                        "name": "Hao Xiong"
                    },
                    {
                        "name": "Wenliang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenliang Chen"
                },
                "author": "Wenliang Chen",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.00546v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.00546v3",
                "updated": "2025-01-07T13:31:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    13,
                    31,
                    1,
                    1,
                    7,
                    0
                ],
                "published": "2023-12-31T17:21:02Z",
                "published_parsed": [
                    2023,
                    12,
                    31,
                    17,
                    21,
                    2,
                    6,
                    365,
                    0
                ],
                "title": "AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with\n  Ten Modalities via Language as a Reference Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with\n  Ten Modalities via Language as a Reference Framework"
                },
                "summary": "Leveraging multimodal data is an inherent requirement for comprehending\ngeographic objects. However, due to the high heterogeneity in structure and\nsemantics among various spatio-temporal modalities, the joint interpretation of\nmultimodal spatio-temporal data has long been an extremely challenging problem.\nThe primary challenge resides in striking a trade-off between the cohesion and\nautonomy of diverse modalities. This trade-off becomes progressively nonlinear\nas the number of modalities expands. Inspired by the human cognitive system and\nlinguistic philosophy, where perceptual signals from the five senses converge\ninto language, we introduce the Language as Reference Framework (LaRF), a\nfundamental principle for constructing a multimodal unified model. Building\nupon this, we propose AllSpark, a multimodal spatio-temporal general artificial\nintelligence model. Our model integrates ten different modalities into a\nunified framework. To achieve modal cohesion, AllSpark introduces a modal\nbridge and multimodal large language model (LLM) to map diverse modal features\ninto the language feature space. To maintain modality autonomy, AllSpark uses\nmodality-specific encoders to extract the tokens of various spatio-temporal\nmodalities. Finally, observing a gap between the model's interpretability and\ndownstream tasks, we designed modality-specific prompts and task heads,\nenhancing the model's generalization capability across specific tasks.\nExperiments indicate that the incorporation of language enables AllSpark to\nexcel in few-shot classification tasks for RGB and point cloud modalities\nwithout additional training, surpassing baseline performance by up to 41.82\\%.\nThe source code is available at https://github.com/GeoX-Lab/AllSpark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging multimodal data is an inherent requirement for comprehending\ngeographic objects. However, due to the high heterogeneity in structure and\nsemantics among various spatio-temporal modalities, the joint interpretation of\nmultimodal spatio-temporal data has long been an extremely challenging problem.\nThe primary challenge resides in striking a trade-off between the cohesion and\nautonomy of diverse modalities. This trade-off becomes progressively nonlinear\nas the number of modalities expands. Inspired by the human cognitive system and\nlinguistic philosophy, where perceptual signals from the five senses converge\ninto language, we introduce the Language as Reference Framework (LaRF), a\nfundamental principle for constructing a multimodal unified model. Building\nupon this, we propose AllSpark, a multimodal spatio-temporal general artificial\nintelligence model. Our model integrates ten different modalities into a\nunified framework. To achieve modal cohesion, AllSpark introduces a modal\nbridge and multimodal large language model (LLM) to map diverse modal features\ninto the language feature space. To maintain modality autonomy, AllSpark uses\nmodality-specific encoders to extract the tokens of various spatio-temporal\nmodalities. Finally, observing a gap between the model's interpretability and\ndownstream tasks, we designed modality-specific prompts and task heads,\nenhancing the model's generalization capability across specific tasks.\nExperiments indicate that the incorporation of language enables AllSpark to\nexcel in few-shot classification tasks for RGB and point cloud modalities\nwithout additional training, surpassing baseline performance by up to 41.82\\%.\nThe source code is available at https://github.com/GeoX-Lab/AllSpark."
                },
                "authors": [
                    {
                        "name": "Run Shao"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Qiujun Li"
                    },
                    {
                        "name": "Qing Zhu"
                    },
                    {
                        "name": "Yongjun Zhang"
                    },
                    {
                        "name": "YanSheng Li"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Yong Tang"
                    },
                    {
                        "name": "Dapeng Liu"
                    },
                    {
                        "name": "Shizhong Yang"
                    },
                    {
                        "name": "Haifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Li"
                },
                "author": "Haifeng Li",
                "arxiv_doi": "10.1109/TGRS.2025.3526725",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TGRS.2025.3526725",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.00546v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.00546v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "19 pages, 19 tables, 3 figures",
                "arxiv_journal_ref": "IEEE Transactions on Geoscience and Remote Sensing. 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15857v2",
                "updated": "2025-01-07T13:28:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    13,
                    28,
                    0,
                    1,
                    7,
                    0
                ],
                "published": "2024-07-08T06:38:50Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    6,
                    38,
                    50,
                    0,
                    190,
                    0
                ],
                "title": "BoRA: Bayesian Hierarchical Low-Rank Adaption for Multi-Task Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BoRA: Bayesian Hierarchical Low-Rank Adaption for Multi-Task Large\n  Language Models"
                },
                "summary": "This paper introduces Bayesian Hierarchical Low-Rank Adaption (BoRA), a novel\nmethod for finetuning multi-task Large Language Models (LLMs). Current\nfinetuning approaches, such as Low-Rank Adaption (LoRA), perform exeptionally\nwell in reducing training parameters and memory usage but face limitations when\napplied to multiple similar tasks. Practitioners usually have to choose between\ntraining separate models for each task or a single model for all tasks, both of\nwhich come with trade-offs in specialization and data utilization. BoRA\naddresses these trade-offs by leveraging a Bayesian hierarchical model that\nallows tasks to share information through global hierarchical priors. This\nenables tasks with limited data to benefit from the overall structure derived\nfrom related tasks while allowing tasks with more data to specialize. Our\nexperimental results show that BoRA outperforms both individual and unified\nmodel approaches, achieving lower perplexity and better generalization across\ntasks. This method provides a scalable and efficient solution for multi-task\nLLM finetuning, with significant practical implications for diverse\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Bayesian Hierarchical Low-Rank Adaption (BoRA), a novel\nmethod for finetuning multi-task Large Language Models (LLMs). Current\nfinetuning approaches, such as Low-Rank Adaption (LoRA), perform exeptionally\nwell in reducing training parameters and memory usage but face limitations when\napplied to multiple similar tasks. Practitioners usually have to choose between\ntraining separate models for each task or a single model for all tasks, both of\nwhich come with trade-offs in specialization and data utilization. BoRA\naddresses these trade-offs by leveraging a Bayesian hierarchical model that\nallows tasks to share information through global hierarchical priors. This\nenables tasks with limited data to benefit from the overall structure derived\nfrom related tasks while allowing tasks with more data to specialize. Our\nexperimental results show that BoRA outperforms both individual and unified\nmodel approaches, achieving lower perplexity and better generalization across\ntasks. This method provides a scalable and efficient solution for multi-task\nLLM finetuning, with significant practical implications for diverse\napplications."
                },
                "authors": [
                    {
                        "name": "Simen Eide"
                    },
                    {
                        "name": "Arnoldo Frigessi"
                    }
                ],
                "author_detail": {
                    "name": "Arnoldo Frigessi"
                },
                "author": "Arnoldo Frigessi",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17481v2",
                "updated": "2025-01-07T12:48:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    12,
                    48,
                    22,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-23T11:11:51Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    11,
                    51,
                    0,
                    358,
                    0
                ],
                "title": "A Survey on LLM-based Multi-Agent System: Recent Advances and New\n  Frontiers in Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LLM-based Multi-Agent System: Recent Advances and New\n  Frontiers in Application"
                },
                "summary": "LLM-based Multi-Agent Systems ( LLM-MAS ) have become a research hotspot\nsince the rise of large language models (LLMs). However, with the continuous\ninflux of new related works, the existing reviews struggle to capture them\ncomprehensively. This paper presents a comprehensive survey of these studies.\nWe first discuss the definition of LLM-MAS, a framework encompassing much of\nprevious work. We provide an overview of the various applications of LLM-MAS in\n(i) solving complex tasks, (ii) simulating specific scenarios, and (iii)\nevaluating generative agents. Building on previous studies, we also highlight\nseveral challenges and propose future directions for research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Multi-Agent Systems ( LLM-MAS ) have become a research hotspot\nsince the rise of large language models (LLMs). However, with the continuous\ninflux of new related works, the existing reviews struggle to capture them\ncomprehensively. This paper presents a comprehensive survey of these studies.\nWe first discuss the definition of LLM-MAS, a framework encompassing much of\nprevious work. We provide an overview of the various applications of LLM-MAS in\n(i) solving complex tasks, (ii) simulating specific scenarios, and (iii)\nevaluating generative agents. Building on previous studies, we also highlight\nseveral challenges and propose future directions for research in this field."
                },
                "authors": [
                    {
                        "name": "Shuaihang Chen"
                    },
                    {
                        "name": "Yuanxing Liu"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "13 pages, 1 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03747v1",
                "updated": "2025-01-07T12:40:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    12,
                    40,
                    35,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T12:40:35Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    12,
                    40,
                    35,
                    1,
                    7,
                    0
                ],
                "title": "Context-Alignment: Activating and Enhancing LLM Capabilities in Time\n  Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Alignment: Activating and Enhancing LLM Capabilities in Time\n  Series"
                },
                "summary": "Recently, leveraging pre-trained Large Language Models (LLMs) for time series\n(TS) tasks has gained increasing attention, which involves activating and\nenhancing LLMs' capabilities. Many methods aim to activate LLMs' capabilities\nbased on token-level alignment but overlook LLMs' inherent strength on natural\nlanguage processing -- their deep understanding of linguistic logic and\nstructure rather than superficial embedding processing. We propose\nContext-Alignment, a new paradigm that aligns TS with a linguistic component in\nthe language environments familiar to LLMs to enable LLMs to contextualize and\ncomprehend TS data, thereby activating their capabilities. Specifically, such\ncontext-level alignment comprises structural alignment and logical alignment,\nwhich is achieved by a Dual-Scale Context-Alignment GNNs (DSCA-GNNs) applied to\nTS-language multimodal inputs. Structural alignment utilizes dual-scale nodes\nto describe hierarchical structure in TS-language, enabling LLMs treat long TS\ndata as a whole linguistic component while preserving intrinsic token features.\nLogical alignment uses directed edges to guide logical relationships, ensuring\ncoherence in the contextual semantics. Demonstration examples prompt are\nemployed to construct Demonstration Examples based Context-Alignment (DECA)\nfollowing DSCA-GNNs framework. DECA can be flexibly and repeatedly integrated\ninto various layers of pre-trained LLMs to improve awareness of logic and\nstructure, thereby enhancing performance. Extensive experiments show the\neffectiveness of DECA and the importance of Context-Alignment across tasks,\nparticularly in few-shot and zero-shot forecasting, confirming that\nContext-Alignment provide powerful prior knowledge on context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, leveraging pre-trained Large Language Models (LLMs) for time series\n(TS) tasks has gained increasing attention, which involves activating and\nenhancing LLMs' capabilities. Many methods aim to activate LLMs' capabilities\nbased on token-level alignment but overlook LLMs' inherent strength on natural\nlanguage processing -- their deep understanding of linguistic logic and\nstructure rather than superficial embedding processing. We propose\nContext-Alignment, a new paradigm that aligns TS with a linguistic component in\nthe language environments familiar to LLMs to enable LLMs to contextualize and\ncomprehend TS data, thereby activating their capabilities. Specifically, such\ncontext-level alignment comprises structural alignment and logical alignment,\nwhich is achieved by a Dual-Scale Context-Alignment GNNs (DSCA-GNNs) applied to\nTS-language multimodal inputs. Structural alignment utilizes dual-scale nodes\nto describe hierarchical structure in TS-language, enabling LLMs treat long TS\ndata as a whole linguistic component while preserving intrinsic token features.\nLogical alignment uses directed edges to guide logical relationships, ensuring\ncoherence in the contextual semantics. Demonstration examples prompt are\nemployed to construct Demonstration Examples based Context-Alignment (DECA)\nfollowing DSCA-GNNs framework. DECA can be flexibly and repeatedly integrated\ninto various layers of pre-trained LLMs to improve awareness of logic and\nstructure, thereby enhancing performance. Extensive experiments show the\neffectiveness of DECA and the importance of Context-Alignment across tasks,\nparticularly in few-shot and zero-shot forecasting, confirming that\nContext-Alignment provide powerful prior knowledge on context."
                },
                "authors": [
                    {
                        "name": "Yuxiao Hu"
                    },
                    {
                        "name": "Qian Li"
                    },
                    {
                        "name": "Dongxiao Zhang"
                    },
                    {
                        "name": "Jinyue Yan"
                    },
                    {
                        "name": "Yuntian Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuntian Chen"
                },
                "author": "Yuntian Chen",
                "arxiv_comment": "no comment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03729v1",
                "updated": "2025-01-07T12:17:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    12,
                    17,
                    25,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T12:17:25Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    12,
                    17,
                    25,
                    1,
                    7,
                    0
                ],
                "title": "Realistic Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "The zero-shot capabilities of Vision-Language Models (VLMs) have been widely\nleveraged to improve predictive performance. However, previous works on\ntransductive or test-time adaptation (TTA) often make strong assumptions about\nthe data distribution, such as the presence of all classes. Our work challenges\nthese favorable deployment scenarios, and introduces a more realistic\nevaluation framework, including: (i) a variable number of effective classes for\nadaptation within a single batch, and (ii) non-i.i.d. batches of test samples\nin online adaptation settings. We provide comprehensive evaluations,\ncomparisons, and ablation studies that demonstrate how current transductive or\nTTA methods for VLMs systematically compromise the models' initial zero-shot\nrobustness across various realistic scenarios, favoring performance gains under\nadvantageous assumptions about the test samples' distributions. Furthermore, we\nintroduce StatA, a versatile method that could handle a wide range of\ndeployment scenarios, including those with a variable number of effective\nclasses at test time. Our approach incorporates a novel regularization term\ndesigned specifically for VLMs, which acts as a statistical anchor preserving\nthe initial text-encoder knowledge, particularly in low-data regimes. Code\navailable at https://github.com/MaxZanella/StatA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The zero-shot capabilities of Vision-Language Models (VLMs) have been widely\nleveraged to improve predictive performance. However, previous works on\ntransductive or test-time adaptation (TTA) often make strong assumptions about\nthe data distribution, such as the presence of all classes. Our work challenges\nthese favorable deployment scenarios, and introduces a more realistic\nevaluation framework, including: (i) a variable number of effective classes for\nadaptation within a single batch, and (ii) non-i.i.d. batches of test samples\nin online adaptation settings. We provide comprehensive evaluations,\ncomparisons, and ablation studies that demonstrate how current transductive or\nTTA methods for VLMs systematically compromise the models' initial zero-shot\nrobustness across various realistic scenarios, favoring performance gains under\nadvantageous assumptions about the test samples' distributions. Furthermore, we\nintroduce StatA, a versatile method that could handle a wide range of\ndeployment scenarios, including those with a variable number of effective\nclasses at test time. Our approach incorporates a novel regularization term\ndesigned specifically for VLMs, which acts as a statistical anchor preserving\nthe initial text-encoder knowledge, particularly in low-data regimes. Code\navailable at https://github.com/MaxZanella/StatA."
                },
                "authors": [
                    {
                        "name": "Maxime Zanella"
                    },
                    {
                        "name": "Clément Fuchs"
                    },
                    {
                        "name": "Christophe De Vleeschouwer"
                    },
                    {
                        "name": "Ismail Ben Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Ismail Ben Ayed"
                },
                "author": "Ismail Ben Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10936v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10936v2",
                "updated": "2025-01-07T12:15:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    12,
                    15,
                    1,
                    1,
                    7,
                    0
                ],
                "published": "2024-05-17T17:47:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    17,
                    47,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "A Survey on Large Language Models with Multilingualism: Recent Advances\n  and New Frontiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Models with Multilingualism: Recent Advances\n  and New Frontiers"
                },
                "summary": "The rapid development of Large Language Models (LLMs) demonstrates remarkable\nmultilingual capabilities in natural language processing, attracting global\nattention in both academia and industry. To mitigate potential discrimination\nand enhance the overall usability and accessibility for diverse language user\ngroups, it is important for the development of language-fair technology.\nDespite the breakthroughs of LLMs, the investigation into the multilingual\nscenario remains insufficient, where a comprehensive survey to summarize recent\napproaches, developments, limitations, and potential solutions is desirable. To\nthis end, we provide a survey with multiple perspectives on the utilization of\nLLMs in the multilingual scenario. We first rethink the transitions between\nprevious and current research on pre-trained language models. Then we introduce\nseveral perspectives on the multilingualism of LLMs, including training and\ninference methods, information retrieval, model security, multi-domain with\nlanguage culture, and usage of datasets. We also discuss the major challenges\nthat arise in these aspects, along with possible solutions. Besides, we\nhighlight future research directions that aim at further enhancing LLMs with\nmultilingualism. The survey aims to help the research community address\nmultilingual problems and provide a comprehensive understanding of the core\nconcepts, key techniques, and latest developments in multilingual natural\nlanguage processing based on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) demonstrates remarkable\nmultilingual capabilities in natural language processing, attracting global\nattention in both academia and industry. To mitigate potential discrimination\nand enhance the overall usability and accessibility for diverse language user\ngroups, it is important for the development of language-fair technology.\nDespite the breakthroughs of LLMs, the investigation into the multilingual\nscenario remains insufficient, where a comprehensive survey to summarize recent\napproaches, developments, limitations, and potential solutions is desirable. To\nthis end, we provide a survey with multiple perspectives on the utilization of\nLLMs in the multilingual scenario. We first rethink the transitions between\nprevious and current research on pre-trained language models. Then we introduce\nseveral perspectives on the multilingualism of LLMs, including training and\ninference methods, information retrieval, model security, multi-domain with\nlanguage culture, and usage of datasets. We also discuss the major challenges\nthat arise in these aspects, along with possible solutions. Besides, we\nhighlight future research directions that aim at further enhancing LLMs with\nmultilingualism. The survey aims to help the research community address\nmultilingual problems and provide a comprehensive understanding of the core\nconcepts, key techniques, and latest developments in multilingual natural\nlanguage processing based on LLMs."
                },
                "authors": [
                    {
                        "name": "Kaiyu Huang"
                    },
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Hongliang Li"
                    },
                    {
                        "name": "You Li"
                    },
                    {
                        "name": "Yuanchi Zhang"
                    },
                    {
                        "name": "Weijian Yi"
                    },
                    {
                        "name": "Yulong Mao"
                    },
                    {
                        "name": "Jinchen Liu"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Jinan Xu"
                    },
                    {
                        "name": "Jian-Yun Nie"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "65 pages, Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10936v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12761v2",
                "updated": "2025-01-07T11:18:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    11,
                    18,
                    8,
                    1,
                    7,
                    0
                ],
                "published": "2024-03-19T14:27:31Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    14,
                    27,
                    31,
                    1,
                    79,
                    0
                ],
                "title": "BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight\n  LLMs"
                },
                "summary": "This paper presents a novel approach to generating behavior trees for robots\nusing lightweight large language models (LLMs) with a maximum of 7 billion\nparameters. The study demonstrates that it is possible to achieve satisfying\nresults with compact LLMs when fine-tuned on a specific dataset. The key\ncontributions of this research include the creation of a fine-tuning dataset\nbased on existing behavior trees using GPT-3.5 and a comprehensive comparison\nof multiple LLMs (namely llama2, llama-chat, and code-llama) across nine\ndistinct tasks. To be thorough, we evaluated the generated behavior trees using\nstatic syntactical analysis, a validation system, a simulated environment, and\na real robot. Furthermore, this work opens the possibility of deploying such\nsolutions directly on the robot, enhancing its practical applicability.\nFindings from this study demonstrate the potential of LLMs with a limited\nnumber of parameters in generating effective and efficient robot behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to generating behavior trees for robots\nusing lightweight large language models (LLMs) with a maximum of 7 billion\nparameters. The study demonstrates that it is possible to achieve satisfying\nresults with compact LLMs when fine-tuned on a specific dataset. The key\ncontributions of this research include the creation of a fine-tuning dataset\nbased on existing behavior trees using GPT-3.5 and a comprehensive comparison\nof multiple LLMs (namely llama2, llama-chat, and code-llama) across nine\ndistinct tasks. To be thorough, we evaluated the generated behavior trees using\nstatic syntactical analysis, a validation system, a simulated environment, and\na real robot. Furthermore, this work opens the possibility of deploying such\nsolutions directly on the robot, enhancing its practical applicability.\nFindings from this study demonstrate the potential of LLMs with a limited\nnumber of parameters in generating effective and efficient robot behaviors."
                },
                "authors": [
                    {
                        "name": "Riccardo Andrea Izzo"
                    },
                    {
                        "name": "Gianluca Bardaro"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_doi": "10.1109/IROS58592.2024.10802304",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IROS58592.2024.10802304",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.12761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10573v2",
                "updated": "2025-01-07T11:13:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    11,
                    13,
                    6,
                    1,
                    7,
                    0
                ],
                "published": "2024-06-15T09:23:46Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    9,
                    23,
                    46,
                    5,
                    167,
                    0
                ],
                "title": "Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and\n  Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and\n  Future Directions"
                },
                "summary": "Graph Neural Networks (GNNs) have significantly advanced various downstream\ngraph-relevant tasks, encompassing recommender systems, molecular structure\nprediction, social media analysis, etc. Despite the boosts of GNN, recent\nresearch has empirically demonstrated its potential vulnerability to backdoor\nattacks, wherein adversaries employ triggers to poison input samples, inducing\nGNN to adversary-premeditated malicious outputs. This is typically due to the\ncontrolled training process, or the deployment of untrusted models, such as\ndelegating model training to third-party service, leveraging external training\nsets, and employing pre-trained models from online sources. Although there's an\nongoing increase in research on GNN backdoors, comprehensive investigation into\nthis field is lacking. To bridge this gap, we propose the first survey\ndedicated to GNN backdoors. We begin by outlining the fundamental definition of\nGNN, followed by the detailed summarization and categorization of current GNN\nbackdoor attacks and defenses based on their technical characteristics and\napplication scenarios. Subsequently, the analysis of the applicability and use\ncases of GNN backdoors is undertaken. Finally, the exploration of potential\nresearch directions of GNN backdoors is presented. This survey aims to explore\nthe principles of graph backdoors, provide insights to defenders, and promote\nfuture security research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have significantly advanced various downstream\ngraph-relevant tasks, encompassing recommender systems, molecular structure\nprediction, social media analysis, etc. Despite the boosts of GNN, recent\nresearch has empirically demonstrated its potential vulnerability to backdoor\nattacks, wherein adversaries employ triggers to poison input samples, inducing\nGNN to adversary-premeditated malicious outputs. This is typically due to the\ncontrolled training process, or the deployment of untrusted models, such as\ndelegating model training to third-party service, leveraging external training\nsets, and employing pre-trained models from online sources. Although there's an\nongoing increase in research on GNN backdoors, comprehensive investigation into\nthis field is lacking. To bridge this gap, we propose the first survey\ndedicated to GNN backdoors. We begin by outlining the fundamental definition of\nGNN, followed by the detailed summarization and categorization of current GNN\nbackdoor attacks and defenses based on their technical characteristics and\napplication scenarios. Subsequently, the analysis of the applicability and use\ncases of GNN backdoors is undertaken. Finally, the exploration of potential\nresearch directions of GNN backdoors is presented. This survey aims to explore\nthe principles of graph backdoors, provide insights to defenders, and promote\nfuture security research."
                },
                "authors": [
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Gaolei Li"
                    },
                    {
                        "name": "Jianhua Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Li"
                },
                "author": "Jianhua Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11543v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11543v3",
                "updated": "2025-01-07T11:09:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    11,
                    9,
                    52,
                    1,
                    7,
                    0
                ],
                "published": "2024-11-18T13:01:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    1,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "PSA-VLM: Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PSA-VLM: Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment"
                },
                "summary": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark."
                },
                "authors": [
                    {
                        "name": "Zhendong Liu"
                    },
                    {
                        "name": "Yuanbi Nie"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Qiushi Cui"
                    },
                    {
                        "name": "Chongjun Wang"
                    },
                    {
                        "name": "Xiaoyong Zhu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2405.13581",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11543v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11543v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03688v1",
                "updated": "2025-01-07T10:34:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    10,
                    34,
                    17,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T10:34:17Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    10,
                    34,
                    17,
                    1,
                    7,
                    0
                ],
                "title": "On Beating $2^n$ for the Closest Vector Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Beating $2^n$ for the Closest Vector Problem"
                },
                "summary": "The Closest Vector Problem (CVP) is a computational problem in lattices that\nis central to modern cryptography. The study of its fine-grained complexity has\ngained momentum in the last few years, partly due to the upcoming deployment of\nlattice-based cryptosystems in practice. A main motivating question has been if\nthere is a $(2-\\varepsilon)^n$ time algorithm on lattices of rank $n$, or\nwhether it can be ruled out by SETH.\n  Previous work came tantalizingly close to a negative answer by showing a\n$2^{(1-o(1))n}$ lower bound under SETH if the underlying distance metric is\nchanged from the standard $\\ell_2$ norm to other $\\ell_p$ norms. Moreover,\nbarriers toward proving such results for $\\ell_2$ (and any even $p$) were\nestablished.\n  In this paper we show \\emph{positive results} for a natural special case of\nthe problem that has hitherto seemed just as hard, namely\n$(0,1)$-$\\mathsf{CVP}$ where the lattice vectors are restricted to be sums of\nsubsets of basis vectors (meaning that all coefficients are $0$ or $1$). All\nprevious hardness results applied to this problem, and none of the previous\nalgorithmic techniques could benefit from it. We prove the following results,\nwhich follow from new reductions from $(0,1)$-$\\mathsf{CVP}$ to weighted\nMax-SAT and minimum-weight $k$-Clique.\n  1. An $O(1.7299^n)$ time algorithm for exact $(0,1)$-$\\mathsf{CVP}_2$ in\nEuclidean norm, breaking the natural $2^n$ barrier, as long as the absolute\nvalue of all coordinates in the input vectors is $2^{o(n)}$.\n  2. A computational equivalence between $(0,1)$-$\\mathsf{CVP}_p$ and\nMax-$p$-SAT for all even $p$.\n  3. The minimum-weight-$k$-Clique conjecture from fine-grained complexity and\nits numerous consequences (which include the APSP conjecture) can now be\nsupported by the hardness of a lattice problem, namely\n$(0,1)$-$\\mathsf{CVP}_2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Closest Vector Problem (CVP) is a computational problem in lattices that\nis central to modern cryptography. The study of its fine-grained complexity has\ngained momentum in the last few years, partly due to the upcoming deployment of\nlattice-based cryptosystems in practice. A main motivating question has been if\nthere is a $(2-\\varepsilon)^n$ time algorithm on lattices of rank $n$, or\nwhether it can be ruled out by SETH.\n  Previous work came tantalizingly close to a negative answer by showing a\n$2^{(1-o(1))n}$ lower bound under SETH if the underlying distance metric is\nchanged from the standard $\\ell_2$ norm to other $\\ell_p$ norms. Moreover,\nbarriers toward proving such results for $\\ell_2$ (and any even $p$) were\nestablished.\n  In this paper we show \\emph{positive results} for a natural special case of\nthe problem that has hitherto seemed just as hard, namely\n$(0,1)$-$\\mathsf{CVP}$ where the lattice vectors are restricted to be sums of\nsubsets of basis vectors (meaning that all coefficients are $0$ or $1$). All\nprevious hardness results applied to this problem, and none of the previous\nalgorithmic techniques could benefit from it. We prove the following results,\nwhich follow from new reductions from $(0,1)$-$\\mathsf{CVP}$ to weighted\nMax-SAT and minimum-weight $k$-Clique.\n  1. An $O(1.7299^n)$ time algorithm for exact $(0,1)$-$\\mathsf{CVP}_2$ in\nEuclidean norm, breaking the natural $2^n$ barrier, as long as the absolute\nvalue of all coordinates in the input vectors is $2^{o(n)}$.\n  2. A computational equivalence between $(0,1)$-$\\mathsf{CVP}_p$ and\nMax-$p$-SAT for all even $p$.\n  3. The minimum-weight-$k$-Clique conjecture from fine-grained complexity and\nits numerous consequences (which include the APSP conjecture) can now be\nsupported by the hardness of a lattice problem, namely\n$(0,1)$-$\\mathsf{CVP}_2$."
                },
                "authors": [
                    {
                        "name": "Amir Abboud"
                    },
                    {
                        "name": "Rajendra Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Rajendra Kumar"
                },
                "author": "Rajendra Kumar",
                "arxiv_comment": "21 pages, accepted at SOSA25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03681v1",
                "updated": "2025-01-07T10:29:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    10,
                    29,
                    43,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T10:29:43Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    10,
                    29,
                    43,
                    1,
                    7,
                    0
                ],
                "title": "SLAM: Towards Efficient Multilingual Reasoning via Selective Language\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM: Towards Efficient Multilingual Reasoning via Selective Language\n  Alignment"
                },
                "summary": "Despite the significant improvements achieved by large language models (LLMs)\nin English reasoning tasks, these models continue to struggle with multilingual\nreasoning. Recent studies leverage a full-parameter and two-stage training\nparadigm to teach models to first understand non-English questions and then\nreason. However, this method suffers from both substantial computational\nresource computing and catastrophic forgetting. The fundamental cause is that,\nwith the primary goal of enhancing multilingual comprehension, an excessive\nnumber of irrelevant layers and parameters are tuned during the first stage.\nGiven our findings that the representation learning of languages is merely\nconducted in lower-level layers, we propose an efficient multilingual reasoning\nalignment approach that precisely identifies and fine-tunes the layers\nresponsible for handling multilingualism. Experimental results show that our\nmethod, SLAM, only tunes 6 layers' feed-forward sub-layers including 6.5-8% of\nall parameters within 7B and 13B LLMs, achieving superior average performance\nthan all strong baselines across 10 languages. Meanwhile, SLAM only involves\none training stage, reducing training time by 4.1-11.9 compared to the\ntwo-stage method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the significant improvements achieved by large language models (LLMs)\nin English reasoning tasks, these models continue to struggle with multilingual\nreasoning. Recent studies leverage a full-parameter and two-stage training\nparadigm to teach models to first understand non-English questions and then\nreason. However, this method suffers from both substantial computational\nresource computing and catastrophic forgetting. The fundamental cause is that,\nwith the primary goal of enhancing multilingual comprehension, an excessive\nnumber of irrelevant layers and parameters are tuned during the first stage.\nGiven our findings that the representation learning of languages is merely\nconducted in lower-level layers, we propose an efficient multilingual reasoning\nalignment approach that precisely identifies and fine-tunes the layers\nresponsible for handling multilingualism. Experimental results show that our\nmethod, SLAM, only tunes 6 layers' feed-forward sub-layers including 6.5-8% of\nall parameters within 7B and 13B LLMs, achieving superior average performance\nthan all strong baselines across 10 languages. Meanwhile, SLAM only involves\none training stage, reducing training time by 4.1-11.9 compared to the\ntwo-stage method."
                },
                "authors": [
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yilin Wang"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Junhao Ruan"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Accepted by COLING 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03675v1",
                "updated": "2025-01-07T10:21:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    10,
                    21,
                    21,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T10:21:21Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    10,
                    21,
                    21,
                    1,
                    7,
                    0
                ],
                "title": "SMIR: Efficient Synthetic Data Pipeline To Improve Multi-Image Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMIR: Efficient Synthetic Data Pipeline To Improve Multi-Image Reasoning"
                },
                "summary": "Vision-Language Models (VLMs) have shown strong performance in understanding\nsingle images, aided by numerous high-quality instruction datasets. However,\nmulti-image reasoning tasks are still under-explored in the open-source\ncommunity due to two main challenges: (1) scaling datasets with multiple\ncorrelated images and complex reasoning instructions is resource-intensive and\nmaintaining quality is difficult, and (2) there is a lack of robust evaluation\nbenchmarks for multi-image tasks. To address these issues, we introduce SMIR,\nan efficient synthetic data-generation pipeline for multi-image reasoning, and\na high-quality dataset generated using this pipeline. Our pipeline efficiently\nextracts highly correlated images using multimodal embeddings, combining visual\nand descriptive information and leverages open-source LLMs to generate quality\ninstructions. Using this pipeline, we generated 160K synthetic training\nsamples, offering a cost-effective alternative to expensive closed-source\nsolutions. Additionally, we present SMIR-BENCH, a novel multi-image reasoning\nevaluation benchmark comprising 200 diverse examples across 7 complex\nmulti-image reasoning tasks. SMIR-BENCH is multi-turn and utilizes a VLM judge\nto evaluate free-form responses, providing a comprehensive assessment of model\nexpressiveness and reasoning capability across modalities. We demonstrate the\neffectiveness of SMIR dataset by fine-tuning several open-source VLMs and\nevaluating their performance on SMIR-BENCH. Our results show that models\ntrained on our dataset outperform baseline models in multi-image reasoning\ntasks up to 8% with a much more scalable data pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have shown strong performance in understanding\nsingle images, aided by numerous high-quality instruction datasets. However,\nmulti-image reasoning tasks are still under-explored in the open-source\ncommunity due to two main challenges: (1) scaling datasets with multiple\ncorrelated images and complex reasoning instructions is resource-intensive and\nmaintaining quality is difficult, and (2) there is a lack of robust evaluation\nbenchmarks for multi-image tasks. To address these issues, we introduce SMIR,\nan efficient synthetic data-generation pipeline for multi-image reasoning, and\na high-quality dataset generated using this pipeline. Our pipeline efficiently\nextracts highly correlated images using multimodal embeddings, combining visual\nand descriptive information and leverages open-source LLMs to generate quality\ninstructions. Using this pipeline, we generated 160K synthetic training\nsamples, offering a cost-effective alternative to expensive closed-source\nsolutions. Additionally, we present SMIR-BENCH, a novel multi-image reasoning\nevaluation benchmark comprising 200 diverse examples across 7 complex\nmulti-image reasoning tasks. SMIR-BENCH is multi-turn and utilizes a VLM judge\nto evaluate free-form responses, providing a comprehensive assessment of model\nexpressiveness and reasoning capability across modalities. We demonstrate the\neffectiveness of SMIR dataset by fine-tuning several open-source VLMs and\nevaluating their performance on SMIR-BENCH. Our results show that models\ntrained on our dataset outperform baseline models in multi-image reasoning\ntasks up to 8% with a much more scalable data pipeline."
                },
                "authors": [
                    {
                        "name": "Andrew Li"
                    },
                    {
                        "name": "Rahul Thapa"
                    },
                    {
                        "name": "Rahul Chalamala"
                    },
                    {
                        "name": "Qingyang Wu"
                    },
                    {
                        "name": "Kezhen Chen"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07247v2",
                "updated": "2025-01-07T10:09:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    10,
                    9,
                    13,
                    1,
                    7,
                    0
                ],
                "published": "2024-10-25T14:01:13Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    1,
                    13,
                    4,
                    299,
                    0
                ],
                "title": "VIEWER: an extensible visual analytics framework for enhancing mental\n  healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIEWER: an extensible visual analytics framework for enhancing mental\n  healthcare"
                },
                "summary": "Objective: A proof-of-concept study aimed at designing and implementing\nVIEWER, a versatile toolkit for visual analytics of clinical data, and\nsystematically evaluating its effectiveness across various clinical\napplications while gathering feedback for iterative improvements.\n  Materials and Methods: VIEWER is an open-source and extensible toolkit that\nemploys natural language processing and interactive visualisation techniques to\nfacilitate the rapid design, development, and deployment of clinical\ninformation retrieval, analysis, and visualisation at the point of care.\nThrough an iterative and collaborative participatory design approach, VIEWER\nwas designed and implemented in one of the UK's largest NHS mental health\nTrusts, where its clinical utility and effectiveness were assessed using both\nquantitative and qualitative methods.\n  Results: VIEWER provides interactive, problem-focused, and comprehensive\nviews of longitudinal patient data (n=409,870) from a combination of structured\nclinical data and unstructured clinical notes. Despite a relatively short\nadoption period and users' initial unfamiliarity, VIEWER significantly improved\nperformance and task completion speed compared to the standard clinical\ninformation system. More than 1,000 users and partners in the hospital tested\nand used VIEWER, reporting high satisfaction and expressed strong interest in\nincorporating VIEWER into their daily practice.\n  Conclusion: VIEWER was developed to improve data accessibility and\nrepresentation across various aspects of healthcare delivery, including\npopulation health management and patient monitoring. The deployment of VIEWER\nhighlights the benefits of collaborative refinement in optimizing health\ninformatics solutions for enhanced patient care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: A proof-of-concept study aimed at designing and implementing\nVIEWER, a versatile toolkit for visual analytics of clinical data, and\nsystematically evaluating its effectiveness across various clinical\napplications while gathering feedback for iterative improvements.\n  Materials and Methods: VIEWER is an open-source and extensible toolkit that\nemploys natural language processing and interactive visualisation techniques to\nfacilitate the rapid design, development, and deployment of clinical\ninformation retrieval, analysis, and visualisation at the point of care.\nThrough an iterative and collaborative participatory design approach, VIEWER\nwas designed and implemented in one of the UK's largest NHS mental health\nTrusts, where its clinical utility and effectiveness were assessed using both\nquantitative and qualitative methods.\n  Results: VIEWER provides interactive, problem-focused, and comprehensive\nviews of longitudinal patient data (n=409,870) from a combination of structured\nclinical data and unstructured clinical notes. Despite a relatively short\nadoption period and users' initial unfamiliarity, VIEWER significantly improved\nperformance and task completion speed compared to the standard clinical\ninformation system. More than 1,000 users and partners in the hospital tested\nand used VIEWER, reporting high satisfaction and expressed strong interest in\nincorporating VIEWER into their daily practice.\n  Conclusion: VIEWER was developed to improve data accessibility and\nrepresentation across various aspects of healthcare delivery, including\npopulation health management and patient monitoring. The deployment of VIEWER\nhighlights the benefits of collaborative refinement in optimizing health\ninformatics solutions for enhanced patient care."
                },
                "authors": [
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "David Codling"
                    },
                    {
                        "name": "Yamiko Msosa"
                    },
                    {
                        "name": "Matthew Broadbent"
                    },
                    {
                        "name": "Daisy Kornblum"
                    },
                    {
                        "name": "Catherine Polling"
                    },
                    {
                        "name": "Thomas Searle"
                    },
                    {
                        "name": "Claire Delaney-Pope"
                    },
                    {
                        "name": "Barbara Arroyo"
                    },
                    {
                        "name": "Stuart MacLellan"
                    },
                    {
                        "name": "Zoe Keddie"
                    },
                    {
                        "name": "Mary Docherty"
                    },
                    {
                        "name": "Angus Roberts"
                    },
                    {
                        "name": "Robert Stewart"
                    },
                    {
                        "name": "Philip McGuire"
                    },
                    {
                        "name": "Richard Dobson"
                    },
                    {
                        "name": "Robert Harland"
                    }
                ],
                "author_detail": {
                    "name": "Robert Harland"
                },
                "author": "Robert Harland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14887v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14887v3",
                "updated": "2025-01-07T09:55:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    55,
                    57,
                    1,
                    7,
                    0
                ],
                "published": "2024-09-23T10:35:57Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    10,
                    35,
                    57,
                    0,
                    267,
                    0
                ],
                "title": "Deploying Open-Source Large Language Models: A performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Open-Source Large Language Models: A performance Analysis"
                },
                "summary": "Since the release of ChatGPT in November 2022, large language models (LLMs)\nhave seen considerable success, including in the open-source community, with\nmany open-weight models available. However, the requirements to deploy such a\nservice are often unknown and difficult to evaluate in advance. To facilitate\nthis process, we conducted numerous tests at the Centre Inria de l'Universit\\'e\nde Bordeaux. In this article, we propose a comparison of the performance of\nseveral models of different sizes (mainly Mistral and LLaMa) depending on the\navailable GPUs, using vLLM, a Python library designed to optimize the inference\nof these models. Our results provide valuable information for private and\npublic groups wishing to deploy LLMs, allowing them to evaluate the performance\nof different models based on their available hardware. This study thus\ncontributes to facilitating the adoption and use of these large language models\nin various application domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the release of ChatGPT in November 2022, large language models (LLMs)\nhave seen considerable success, including in the open-source community, with\nmany open-weight models available. However, the requirements to deploy such a\nservice are often unknown and difficult to evaluate in advance. To facilitate\nthis process, we conducted numerous tests at the Centre Inria de l'Universit\\'e\nde Bordeaux. In this article, we propose a comparison of the performance of\nseveral models of different sizes (mainly Mistral and LLaMa) depending on the\navailable GPUs, using vLLM, a Python library designed to optimize the inference\nof these models. Our results provide valuable information for private and\npublic groups wishing to deploy LLMs, allowing them to evaluate the performance\nof different models based on their available hardware. This study thus\ncontributes to facilitating the adoption and use of these large language models\nin various application domains."
                },
                "authors": [
                    {
                        "name": "Yannis Bendi-Ouis"
                    },
                    {
                        "name": "Dan Dutartre"
                    },
                    {
                        "name": "Xavier Hinaut"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Hinaut"
                },
                "author": "Xavier Hinaut",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14887v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14887v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17624v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17624v2",
                "updated": "2025-01-07T09:46:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    46,
                    18,
                    1,
                    7,
                    0
                ],
                "published": "2024-07-24T20:30:55Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    20,
                    30,
                    55,
                    2,
                    206,
                    0
                ],
                "title": "Forecasting Credit Ratings: A Case Study where Traditional Methods\n  Outperform Generative LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting Credit Ratings: A Case Study where Traditional Methods\n  Outperform Generative LLMs"
                },
                "summary": "Large Language Models (LLMs) have been shown to perform well for many\ndownstream tasks. Transfer learning can enable LLMs to acquire skills that were\nnot targeted during pre-training. In financial contexts, LLMs can sometimes\nbeat well-established benchmarks. This paper investigates how well LLMs perform\nin the task of forecasting corporate credit ratings. We show that while LLMs\nare very good at encoding textual information, traditional methods are still\nvery competitive when it comes to encoding numeric and multimodal data. For our\ntask, current LLMs perform worse than a more traditional XGBoost architecture\nthat combines fundamental and macroeconomic data with high-density text-based\nembedding features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been shown to perform well for many\ndownstream tasks. Transfer learning can enable LLMs to acquire skills that were\nnot targeted during pre-training. In financial contexts, LLMs can sometimes\nbeat well-established benchmarks. This paper investigates how well LLMs perform\nin the task of forecasting corporate credit ratings. We show that while LLMs\nare very good at encoding textual information, traditional methods are still\nvery competitive when it comes to encoding numeric and multimodal data. For our\ntask, current LLMs perform worse than a more traditional XGBoost architecture\nthat combines fundamental and macroeconomic data with high-density text-based\nembedding features."
                },
                "authors": [
                    {
                        "name": "Felix Drinkall"
                    },
                    {
                        "name": "Janet B. Pierrehumbert"
                    },
                    {
                        "name": "Stefan Zohren"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Zohren"
                },
                "author": "Stefan Zohren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17624v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17624v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.RM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12928v2",
                "updated": "2025-01-07T09:39:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    39,
                    15,
                    1,
                    7,
                    0
                ],
                "published": "2024-08-23T09:14:58Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    14,
                    58,
                    4,
                    236,
                    0
                ],
                "title": "ParGo: Bridging Vision-Language with Partial and Global Views",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParGo: Bridging Vision-Language with Partial and Global Views"
                },
                "summary": "This work presents ParGo, a novel Partial-Global projector designed to\nconnect the vision and language modalities for Multimodal Large Language Models\n(MLLMs). Unlike previous works that rely on global attention-based projectors,\nour ParGo bridges the representation gap between the separately pre-trained\nvision encoders and the LLMs by integrating global and partial views, which\nalleviates the overemphasis on prominent regions. To facilitate the effective\ntraining of ParGo, we collect a large-scale detail-captioned image-text dataset\nnamed ParGoCap-1M-PT, consisting of 1 million images paired with high-quality\ncaptions. Extensive experiments on several MLLM benchmarks demonstrate the\neffectiveness of our ParGo, highlighting its superiority in aligning vision and\nlanguage modalities. Compared to conventional Q-Former projector, our ParGo\nachieves an improvement of 259.96 in MME benchmark. Furthermore, our\nexperiments reveal that ParGo significantly outperforms other projectors,\nparticularly in tasks that emphasize detail perception ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents ParGo, a novel Partial-Global projector designed to\nconnect the vision and language modalities for Multimodal Large Language Models\n(MLLMs). Unlike previous works that rely on global attention-based projectors,\nour ParGo bridges the representation gap between the separately pre-trained\nvision encoders and the LLMs by integrating global and partial views, which\nalleviates the overemphasis on prominent regions. To facilitate the effective\ntraining of ParGo, we collect a large-scale detail-captioned image-text dataset\nnamed ParGoCap-1M-PT, consisting of 1 million images paired with high-quality\ncaptions. Extensive experiments on several MLLM benchmarks demonstrate the\neffectiveness of our ParGo, highlighting its superiority in aligning vision and\nlanguage modalities. Compared to conventional Q-Former projector, our ParGo\nachieves an improvement of 259.96 in MME benchmark. Furthermore, our\nexperiments reveal that ParGo significantly outperforms other projectors,\nparticularly in tasks that emphasize detail perception ability."
                },
                "authors": [
                    {
                        "name": "An-Lan Wang"
                    },
                    {
                        "name": "Bin Shan"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Kun-Yu Lin"
                    },
                    {
                        "name": "Xiang Fei"
                    },
                    {
                        "name": "Guozhi Tang"
                    },
                    {
                        "name": "Lei Liao"
                    },
                    {
                        "name": "Jingqun Tang"
                    },
                    {
                        "name": "Can Huang"
                    },
                    {
                        "name": "Wei-Shi Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Shi Zheng"
                },
                "author": "Wei-Shi Zheng",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02506v2",
                "updated": "2025-01-07T09:13:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    13,
                    35,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-05T11:06:55Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    11,
                    6,
                    55,
                    6,
                    5,
                    0
                ],
                "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use"
                },
                "summary": "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https://huggingface.co/datasets/bytedance-research/ToolHop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https://huggingface.co/datasets/bytedance-research/ToolHop."
                },
                "authors": [
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Xuesong Yao"
                    },
                    {
                        "name": "Weijian Lin"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Zaiyuan Wang"
                    },
                    {
                        "name": "Sining Zhu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Jiecao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiecao Chen"
                },
                "author": "Jiecao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03626v1",
                "updated": "2025-01-07T08:52:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    8,
                    52,
                    55,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T08:52:55Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    8,
                    52,
                    55,
                    1,
                    7,
                    0
                ],
                "title": "CommitShield: Tracking Vulnerability Introduction and Fix in Version\n  Control Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommitShield: Tracking Vulnerability Introduction and Fix in Version\n  Control Systems"
                },
                "summary": "Version control systems are commonly used to manage open-source software, in\nwhich each commit may introduce new vulnerabilities or fix existing ones.\nResearchers have developed various tools for detecting vulnerabilities in code\ncommits, but their performance is limited by factors such as neglecting\ndescriptive data and challenges in accurately identifying vulnerability\nintroductions. To overcome these limitations, we propose CommitShield, which\ncombines the code analysis capabilities of static analysis tools with the\nnatural language and code understanding capabilities of large language models\n(LLMs) to enhance the accuracy of vulnerability introduction and fix detection\nby generating precise descriptions and obtaining rich patch contexts. We\nevaluate CommitShield using the newly constructed vulnerability repair dataset,\nCommitVulFix, and a cleaned vulnerability introduction dataset. Experimental\nresults indicate that CommitShield improves recall by 76%-87% over\nstate-of-the-art methods in the vulnerability fix detection task, and its\nF1-score improves by 15%-27% in the vulnerability introduction detection task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Version control systems are commonly used to manage open-source software, in\nwhich each commit may introduce new vulnerabilities or fix existing ones.\nResearchers have developed various tools for detecting vulnerabilities in code\ncommits, but their performance is limited by factors such as neglecting\ndescriptive data and challenges in accurately identifying vulnerability\nintroductions. To overcome these limitations, we propose CommitShield, which\ncombines the code analysis capabilities of static analysis tools with the\nnatural language and code understanding capabilities of large language models\n(LLMs) to enhance the accuracy of vulnerability introduction and fix detection\nby generating precise descriptions and obtaining rich patch contexts. We\nevaluate CommitShield using the newly constructed vulnerability repair dataset,\nCommitVulFix, and a cleaned vulnerability introduction dataset. Experimental\nresults indicate that CommitShield improves recall by 76%-87% over\nstate-of-the-art methods in the vulnerability fix detection task, and its\nF1-score improves by 15%-27% in the vulnerability introduction detection task."
                },
                "authors": [
                    {
                        "name": "Zhaonan Wu"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Zirui Wan"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03624v1",
                "updated": "2025-01-07T08:49:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    8,
                    49,
                    4,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T08:49:04Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    8,
                    49,
                    4,
                    1,
                    7,
                    0
                ],
                "title": "LlaMADRS: Prompting Large Language Models for Interview-Based Depression\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlaMADRS: Prompting Large Language Models for Interview-Based Depression\n  Assessment"
                },
                "summary": "This study introduces LlaMADRS, a novel framework leveraging open-source\nLarge Language Models (LLMs) to automate depression severity assessment using\nthe Montgomery-Asberg Depression Rating Scale (MADRS). We employ a zero-shot\nprompting strategy with carefully designed cues to guide the model in\ninterpreting and scoring transcribed clinical interviews. Our approach, tested\non 236 real-world interviews from the Context-Adaptive Multimodal Informatics\n(CAMI) dataset, demonstrates strong correlations with clinician assessments.\nThe Qwen 2.5--72b model achieves near-human level agreement across most MADRS\nitems, with Intraclass Correlation Coefficients (ICC) closely approaching those\nbetween human raters. We provide a comprehensive analysis of model performance\nacross different MADRS items, highlighting strengths and current limitations.\nOur findings suggest that LLMs, with appropriate prompting, can serve as\nefficient tools for mental health assessment, potentially increasing\naccessibility in resource-limited settings. However, challenges remain,\nparticularly in assessing symptoms that rely on non-verbal cues, underscoring\nthe need for multimodal approaches in future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces LlaMADRS, a novel framework leveraging open-source\nLarge Language Models (LLMs) to automate depression severity assessment using\nthe Montgomery-Asberg Depression Rating Scale (MADRS). We employ a zero-shot\nprompting strategy with carefully designed cues to guide the model in\ninterpreting and scoring transcribed clinical interviews. Our approach, tested\non 236 real-world interviews from the Context-Adaptive Multimodal Informatics\n(CAMI) dataset, demonstrates strong correlations with clinician assessments.\nThe Qwen 2.5--72b model achieves near-human level agreement across most MADRS\nitems, with Intraclass Correlation Coefficients (ICC) closely approaching those\nbetween human raters. We provide a comprehensive analysis of model performance\nacross different MADRS items, highlighting strengths and current limitations.\nOur findings suggest that LLMs, with appropriate prompting, can serve as\nefficient tools for mental health assessment, potentially increasing\naccessibility in resource-limited settings. However, challenges remain,\nparticularly in assessing symptoms that rely on non-verbal cues, underscoring\nthe need for multimodal approaches in future work."
                },
                "authors": [
                    {
                        "name": "Gaoussou Youssouf Kebe"
                    },
                    {
                        "name": "Jeffrey M. Girard"
                    },
                    {
                        "name": "Einat Liebenthal"
                    },
                    {
                        "name": "Justin Baker"
                    },
                    {
                        "name": "Fernando De la Torre"
                    },
                    {
                        "name": "Louis-Philippe Morency"
                    }
                ],
                "author_detail": {
                    "name": "Louis-Philippe Morency"
                },
                "author": "Louis-Philippe Morency",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03621v1",
                "updated": "2025-01-07T08:42:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    8,
                    42,
                    25,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T08:42:25Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    8,
                    42,
                    25,
                    1,
                    7,
                    0
                ],
                "title": "Benchmarking seismic phase associators: Insights from synthetic\n  scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking seismic phase associators: Insights from synthetic\n  scenarios"
                },
                "summary": "Reliable seismicity catalogs are essential for seismology. Following phase\npicking, phase association groups arrivals into sets with consistent origins\n(i.e., events), determines event counts, and identifies outlier picks. To\nhandle the substantial increase in the quantity of seismic phase picks from\nimproved picking methods and larger deployments, several novel phase\nassociators have recently been proposed. This study presents a detailed\nbenchmark analysis of five seismic phase associators, including classical and\nmachine learning-based approaches: PhaseLink, REAL, GaMMA, GENIE, and PyOcto.\nWe use synthetic datasets mimicking real seismicity characteristics in crustal\nand subduction zone scenarios. We evaluate performance for different\nconditions, including low- and high-noise environments, out-of-network events,\nvery high event rates, and variable station density. The results reveal notable\ndifferences in precision, recall, and computational efficiency. GENIE and\nPyOcto demonstrate robust performance, with almost perfect performance for most\nscenarios. Only for the most challenging conditions with high noise levels and\nevent rates, performance drops but still maintains F1 scores above 0.8.\nPhaseLink's performance declines with noise and event density, particularly in\nsubduction zones, dropping to near zero in the most complex cases. GaMMA\noutperforms PhaseLink but struggles with accuracy and scalability in\nhigh-noise, high-density scenarios. REAL performs reasonably but loses recall\nunder extreme conditions. PyOcto and PhaseLink show the quickest runtimes for\nsmaller-scale datasets, while REAL and GENIE are more than an order of\nmagnitude slower for these cases. At the highest pick rates, GENIE's runtime\ndisadvantage diminishes, matching PyOcto and scaling effectively. Our results\ncan guide practitioners compiling seismicity catalogs and developers designing\nnovel associators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable seismicity catalogs are essential for seismology. Following phase\npicking, phase association groups arrivals into sets with consistent origins\n(i.e., events), determines event counts, and identifies outlier picks. To\nhandle the substantial increase in the quantity of seismic phase picks from\nimproved picking methods and larger deployments, several novel phase\nassociators have recently been proposed. This study presents a detailed\nbenchmark analysis of five seismic phase associators, including classical and\nmachine learning-based approaches: PhaseLink, REAL, GaMMA, GENIE, and PyOcto.\nWe use synthetic datasets mimicking real seismicity characteristics in crustal\nand subduction zone scenarios. We evaluate performance for different\nconditions, including low- and high-noise environments, out-of-network events,\nvery high event rates, and variable station density. The results reveal notable\ndifferences in precision, recall, and computational efficiency. GENIE and\nPyOcto demonstrate robust performance, with almost perfect performance for most\nscenarios. Only for the most challenging conditions with high noise levels and\nevent rates, performance drops but still maintains F1 scores above 0.8.\nPhaseLink's performance declines with noise and event density, particularly in\nsubduction zones, dropping to near zero in the most complex cases. GaMMA\noutperforms PhaseLink but struggles with accuracy and scalability in\nhigh-noise, high-density scenarios. REAL performs reasonably but loses recall\nunder extreme conditions. PyOcto and PhaseLink show the quickest runtimes for\nsmaller-scale datasets, while REAL and GENIE are more than an order of\nmagnitude slower for these cases. At the highest pick rates, GENIE's runtime\ndisadvantage diminishes, matching PyOcto and scaling effectively. Our results\ncan guide practitioners compiling seismicity catalogs and developers designing\nnovel associators."
                },
                "authors": [
                    {
                        "name": "Jorge Puente"
                    },
                    {
                        "name": "Christian Sippl"
                    },
                    {
                        "name": "Jannes Münchmeyer"
                    },
                    {
                        "name": "Ian W. McBrearty"
                    }
                ],
                "author_detail": {
                    "name": "Ian W. McBrearty"
                },
                "author": "Ian W. McBrearty",
                "arxiv_comment": "29 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03814v2",
                "updated": "2025-01-07T07:46:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    7,
                    46,
                    16,
                    1,
                    7,
                    0
                ],
                "published": "2024-11-06T10:32:09Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    32,
                    9,
                    2,
                    311,
                    0
                ],
                "title": "MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue"
                },
                "summary": "Large Language Models (LLMs) demonstrate outstanding performance in their\nreservoir of knowledge and understanding capabilities, but they have also been\nshown to be prone to illegal or unethical reactions when subjected to jailbreak\nattacks. To ensure their responsible deployment in critical applications, it is\ncrucial to understand the safety capabilities and vulnerabilities of LLMs.\nPrevious works mainly focus on jailbreak in single-round dialogue, overlooking\nthe potential jailbreak risks in multi-round dialogues, which are a vital way\nhumans interact with and extract information from LLMs. Some studies have\nincreasingly concentrated on the risks associated with jailbreak in multi-round\ndialogues. These efforts typically involve the use of manually crafted\ntemplates or prompt engineering techniques. However, due to the inherent\ncomplexity of multi-round dialogues, their jailbreak performance is limited. To\nsolve this problem, we propose a novel multi-round dialogue jailbreaking agent,\nemphasizing the importance of stealthiness in identifying and mitigating\npotential threats to human values posed by LLMs. We propose a risk\ndecomposition strategy that distributes risks across multiple rounds of queries\nand utilizes psychological strategies to enhance attack strength. Extensive\nexperiments show that our proposed method surpasses other attack methods and\nachieves state-of-the-art attack success rate. We will make the corresponding\ncode and dataset available for future research. The code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate outstanding performance in their\nreservoir of knowledge and understanding capabilities, but they have also been\nshown to be prone to illegal or unethical reactions when subjected to jailbreak\nattacks. To ensure their responsible deployment in critical applications, it is\ncrucial to understand the safety capabilities and vulnerabilities of LLMs.\nPrevious works mainly focus on jailbreak in single-round dialogue, overlooking\nthe potential jailbreak risks in multi-round dialogues, which are a vital way\nhumans interact with and extract information from LLMs. Some studies have\nincreasingly concentrated on the risks associated with jailbreak in multi-round\ndialogues. These efforts typically involve the use of manually crafted\ntemplates or prompt engineering techniques. However, due to the inherent\ncomplexity of multi-round dialogues, their jailbreak performance is limited. To\nsolve this problem, we propose a novel multi-round dialogue jailbreaking agent,\nemphasizing the importance of stealthiness in identifying and mitigating\npotential threats to human values posed by LLMs. We propose a risk\ndecomposition strategy that distributes risks across multiple rounds of queries\nand utilizes psychological strategies to enhance attack strength. Extensive\nexperiments show that our proposed method surpasses other attack methods and\nachieves state-of-the-art attack success rate. We will make the corresponding\ncode and dataset available for future research. The code will be released soon."
                },
                "authors": [
                    {
                        "name": "Fengxiang Wang"
                    },
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Peng Xiao"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Shiji Zhao"
                    },
                    {
                        "name": "Cheng Wei"
                    },
                    {
                        "name": "YueFeng Chen"
                    },
                    {
                        "name": "Chongwen Wang"
                    },
                    {
                        "name": "Jialing Tao"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Hui Xue"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xue"
                },
                "author": "Hui Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03572v1",
                "updated": "2025-01-07T06:51:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    51,
                    46,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T06:51:46Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    51,
                    46,
                    1,
                    7,
                    0
                ],
                "title": "From Code to Compliance: Assessing ChatGPT's Utility in Designing an\n  Accessible Webpage -- A Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Code to Compliance: Assessing ChatGPT's Utility in Designing an\n  Accessible Webpage -- A Case Study"
                },
                "summary": "Web accessibility ensures that individuals with disabilities can access and\ninteract with digital content without barriers, yet a significant majority of\nmost used websites fail to meet accessibility standards. This study evaluates\nChatGPT's (GPT-4o) ability to generate and improve web pages in line with Web\nContent Accessibility Guidelines (WCAG). While ChatGPT can effectively address\naccessibility issues when prompted, its default code often lacks compliance,\nreflecting limitations in its training data and prevailing inaccessible web\npractices. Automated and manual testing revealed strengths in resolving simple\nissues but challenges with complex tasks, requiring human oversight and\nadditional iterations. Unlike prior studies, we incorporate manual evaluation,\ndynamic elements, and use the visual reasoning capability of ChatGPT along with\nthe prompts to fix accessibility issues. Providing screenshots alongside\nprompts enhances the LLM's ability to address accessibility issues by allowing\nit to analyze surrounding components, such as determining appropriate contrast\ncolors. We found that effective prompt engineering, such as providing concise,\nstructured feedback and incorporating visual aids, significantly enhances\nChatGPT's performance. These findings highlight the potential and limitations\nof large language models for accessible web development, offering practical\nguidance for developers to create more inclusive websites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web accessibility ensures that individuals with disabilities can access and\ninteract with digital content without barriers, yet a significant majority of\nmost used websites fail to meet accessibility standards. This study evaluates\nChatGPT's (GPT-4o) ability to generate and improve web pages in line with Web\nContent Accessibility Guidelines (WCAG). While ChatGPT can effectively address\naccessibility issues when prompted, its default code often lacks compliance,\nreflecting limitations in its training data and prevailing inaccessible web\npractices. Automated and manual testing revealed strengths in resolving simple\nissues but challenges with complex tasks, requiring human oversight and\nadditional iterations. Unlike prior studies, we incorporate manual evaluation,\ndynamic elements, and use the visual reasoning capability of ChatGPT along with\nthe prompts to fix accessibility issues. Providing screenshots alongside\nprompts enhances the LLM's ability to address accessibility issues by allowing\nit to analyze surrounding components, such as determining appropriate contrast\ncolors. We found that effective prompt engineering, such as providing concise,\nstructured feedback and incorporating visual aids, significantly enhances\nChatGPT's performance. These findings highlight the potential and limitations\nof large language models for accessible web development, offering practical\nguidance for developers to create more inclusive websites."
                },
                "authors": [
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Margarida Fresco"
                    },
                    {
                        "name": "Fredrik Forsberg"
                    },
                    {
                        "name": "Hallvard Grotli"
                    }
                ],
                "author_detail": {
                    "name": "Hallvard Grotli"
                },
                "author": "Hallvard Grotli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.1.2; F.3.1; F.4.1; D.3.2; H.1.2; H.5.2; D.2.2; H.1.2; I.3.6;\n  H.5.4; H.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03569v1",
                "updated": "2025-01-07T06:44:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    44,
                    41,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T06:44:41Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    44,
                    41,
                    1,
                    7,
                    0
                ],
                "title": "What Does a Software Engineer Look Like? Exploring Societal Stereotypes\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Does a Software Engineer Look Like? Exploring Societal Stereotypes\n  in LLMs"
                },
                "summary": "Large language models (LLMs) have rapidly gained popularity and are being\nembedded into professional applications due to their capabilities in generating\nhuman-like content. However, unquestioned reliance on their outputs and\nrecommendations can be problematic as LLMs can reinforce societal biases and\nstereotypes. This study investigates how LLMs, specifically OpenAI's GPT-4 and\nMicrosoft Copilot, can reinforce gender and racial stereotypes within the\nsoftware engineering (SE) profession through both textual and graphical\noutputs. We used each LLM to generate 300 profiles, consisting of 100\ngender-based and 50 gender-neutral profiles, for a recruitment scenario in SE\nroles. Recommendations were generated for each profile and evaluated against\nthe job requirements for four distinct SE positions. Each LLM was asked to\nselect the top 5 candidates and subsequently the best candidate for each role.\nEach LLM was also asked to generate images for the top 5 candidates, providing\na dataset for analysing potential biases in both text-based selections and\nvisual representations. Our analysis reveals that both models preferred male\nand Caucasian profiles, particularly for senior roles, and favoured images\nfeaturing traits such as lighter skin tones, slimmer body types, and younger\nappearances. These findings highlight underlying societal biases influence the\noutputs of LLMs, contributing to narrow, exclusionary stereotypes that can\nfurther limit diversity and perpetuate inequities in the SE field. As LLMs are\nincreasingly adopted within SE research and professional practices, awareness\nof these biases is crucial to prevent the reinforcement of discriminatory norms\nand to ensure that AI tools are leveraged to promote an inclusive and equitable\nengineering culture rather than hinder it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have rapidly gained popularity and are being\nembedded into professional applications due to their capabilities in generating\nhuman-like content. However, unquestioned reliance on their outputs and\nrecommendations can be problematic as LLMs can reinforce societal biases and\nstereotypes. This study investigates how LLMs, specifically OpenAI's GPT-4 and\nMicrosoft Copilot, can reinforce gender and racial stereotypes within the\nsoftware engineering (SE) profession through both textual and graphical\noutputs. We used each LLM to generate 300 profiles, consisting of 100\ngender-based and 50 gender-neutral profiles, for a recruitment scenario in SE\nroles. Recommendations were generated for each profile and evaluated against\nthe job requirements for four distinct SE positions. Each LLM was asked to\nselect the top 5 candidates and subsequently the best candidate for each role.\nEach LLM was also asked to generate images for the top 5 candidates, providing\na dataset for analysing potential biases in both text-based selections and\nvisual representations. Our analysis reveals that both models preferred male\nand Caucasian profiles, particularly for senior roles, and favoured images\nfeaturing traits such as lighter skin tones, slimmer body types, and younger\nappearances. These findings highlight underlying societal biases influence the\noutputs of LLMs, contributing to narrow, exclusionary stereotypes that can\nfurther limit diversity and perpetuate inequities in the SE field. As LLMs are\nincreasingly adopted within SE research and professional practices, awareness\nof these biases is crucial to prevent the reinforcement of discriminatory norms\nand to ensure that AI tools are leveraged to promote an inclusive and equitable\nengineering culture rather than hinder it."
                },
                "authors": [
                    {
                        "name": "Muneera Bano"
                    },
                    {
                        "name": "Hashini Gunatilake"
                    },
                    {
                        "name": "Rashina Hoda"
                    }
                ],
                "author_detail": {
                    "name": "Rashina Hoda"
                },
                "author": "Rashina Hoda",
                "arxiv_comment": "Accepted for publication in Software Engineering in Society (SEIS) in\n  ICSE 2025",
                "arxiv_journal_ref": "Software Engineering in Society (SEIS) track in ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15320v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15320v2",
                "updated": "2025-01-07T06:39:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    39,
                    29,
                    1,
                    7,
                    0
                ],
                "published": "2024-07-07T09:25:52Z",
                "published_parsed": [
                    2024,
                    7,
                    7,
                    9,
                    25,
                    52,
                    6,
                    189,
                    0
                ],
                "title": "Edge Graph Intelligence: Reciprocally Empowering Edge Networks with\n  Graph Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Graph Intelligence: Reciprocally Empowering Edge Networks with\n  Graph Intelligence"
                },
                "summary": "Recent years have witnessed a thriving growth of computing facilities\nconnected at the network edge, cultivating edge networks as a fundamental\ninfrastructure for supporting miscellaneous intelligent services.Meanwhile,\nArtificial Intelligence (AI) frontiers have extrapolated to the graph domain\nand promoted Graph Intelligence (GI). Given the inherent relation between\ngraphs and networks, the interdiscipline of graph learning and edge networks,\ni.e., Edge GI or EGI, has revealed a novel interplay between them -- GI aids in\noptimizing edge networks, while edge networks facilitate GI model deployment.\nDriven by this delicate closed-loop, EGI is recognized as a promising solution\nto fully unleash the potential of edge computing power and is garnering growing\nattention. Nevertheless, research on EGI remains nascent, and there is a\nsoaring demand within both the communications and AI communities for a\ndedicated venue to share recent advancements. To this end, this paper promotes\nthe concept of EGI, explores its scope and core principles, and conducts a\ncomprehensive survey concerning recent research efforts on this emerging field.\nSpecifically, this paper introduces and discusses: 1) fundamentals of edge\ncomputing and graph learning,2) emerging techniques centering on the closed\nloop between graph intelligence and edge networks, and 3) open challenges and\nresearch opportunities of future EGI. By bridging the gap across communication,\nnetworking, and graph learning areas, we believe that this survey can garner\nincreased attention, foster meaningful discussions, and inspire further\nresearch ideas in EGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed a thriving growth of computing facilities\nconnected at the network edge, cultivating edge networks as a fundamental\ninfrastructure for supporting miscellaneous intelligent services.Meanwhile,\nArtificial Intelligence (AI) frontiers have extrapolated to the graph domain\nand promoted Graph Intelligence (GI). Given the inherent relation between\ngraphs and networks, the interdiscipline of graph learning and edge networks,\ni.e., Edge GI or EGI, has revealed a novel interplay between them -- GI aids in\noptimizing edge networks, while edge networks facilitate GI model deployment.\nDriven by this delicate closed-loop, EGI is recognized as a promising solution\nto fully unleash the potential of edge computing power and is garnering growing\nattention. Nevertheless, research on EGI remains nascent, and there is a\nsoaring demand within both the communications and AI communities for a\ndedicated venue to share recent advancements. To this end, this paper promotes\nthe concept of EGI, explores its scope and core principles, and conducts a\ncomprehensive survey concerning recent research efforts on this emerging field.\nSpecifically, this paper introduces and discusses: 1) fundamentals of edge\ncomputing and graph learning,2) emerging techniques centering on the closed\nloop between graph intelligence and edge networks, and 3) open challenges and\nresearch opportunities of future EGI. By bridging the gap across communication,\nnetworking, and graph learning areas, we believe that this survey can garner\nincreased attention, foster meaningful discussions, and inspire further\nresearch ideas in EGI."
                },
                "authors": [
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Xiaoxi Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Jian Tang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Xuemin"
                    },
                    {
                        "name": "Shen"
                    }
                ],
                "author_detail": {
                    "name": "Shen"
                },
                "arxiv_affiliation": "Sherman",
                "author": "Shen",
                "arxiv_comment": "Accepted by IEEE Communications Surveys & Tutorials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15320v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03566v1",
                "updated": "2025-01-07T06:34:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    34,
                    17,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T06:34:17Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    34,
                    17,
                    1,
                    7,
                    0
                ],
                "title": "Applying Large Language Models in Knowledge Graph-based Enterprise\n  Modeling: Challenges and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying Large Language Models in Knowledge Graph-based Enterprise\n  Modeling: Challenges and Opportunities"
                },
                "summary": "The role of large language models (LLMs) in enterprise modeling has recently\nstarted to shift from academic research to that of industrial applications.\nThereby, LLMs represent a further building block for the machine-supported\ngeneration of enterprise models. In this paper we employ a knowledge\ngraph-based approach for enterprise modeling and investigate the potential\nbenefits of LLMs in this context. In addition, the findings of an expert survey\nand ChatGPT-4o-based experiments demonstrate that LLM-based model generations\nexhibit minimal variability, yet remain constrained to specific tasks, with\nreliability declining for more intricate tasks. The survey results further\nsuggest that the supervision and intervention of human modeling experts are\nessential to ensure the accuracy and integrity of the generated models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The role of large language models (LLMs) in enterprise modeling has recently\nstarted to shift from academic research to that of industrial applications.\nThereby, LLMs represent a further building block for the machine-supported\ngeneration of enterprise models. In this paper we employ a knowledge\ngraph-based approach for enterprise modeling and investigate the potential\nbenefits of LLMs in this context. In addition, the findings of an expert survey\nand ChatGPT-4o-based experiments demonstrate that LLM-based model generations\nexhibit minimal variability, yet remain constrained to specific tasks, with\nreliability declining for more intricate tasks. The survey results further\nsuggest that the supervision and intervention of human modeling experts are\nessential to ensure the accuracy and integrity of the generated models."
                },
                "authors": [
                    {
                        "name": "Benedikt Reitemeyer"
                    },
                    {
                        "name": "Hans-Georg Fill"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Georg Fill"
                },
                "author": "Hans-Georg Fill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02155v2",
                "updated": "2025-01-07T06:30:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    30,
                    24,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-03T04:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "CausalMob: Causal Human Mobility Prediction with LLMs-derived Human\n  Intentions toward Public Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalMob: Causal Human Mobility Prediction with LLMs-derived Human\n  Intentions toward Public Events"
                },
                "summary": "Large-scale human mobility exhibits spatial and temporal patterns that can\nassist policymakers in decision making. Although traditional prediction models\nattempt to capture these patterns, they often interfered by non-periodic public\nevents, such as disasters and occasional celebrations. Since regular human\nmobility patterns are heavily affected by these events, estimating their causal\neffects is critical to accurate mobility predictions. Although news articles\nprovide unique perspectives on these events in an unstructured format,\nprocessing is a challenge. In this study, we propose a causality-augmented\nprediction model, called CausalMob, to analyze the causal effects of public\nevents. We first utilize large language models (LLMs) to extract human\nintentions from news articles and transform them into features that act as\ncausal treatments. Next, the model learns representations of spatio-temporal\nregional covariates from multiple data sources to serve as confounders for\ncausal inference. Finally, we present a causal effect estimation framework to\nensure event features remain independent of confounders during prediction.\nBased on large-scale real-world data, the experimental results show that the\nproposed model excels in human mobility prediction, outperforming\nstate-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale human mobility exhibits spatial and temporal patterns that can\nassist policymakers in decision making. Although traditional prediction models\nattempt to capture these patterns, they often interfered by non-periodic public\nevents, such as disasters and occasional celebrations. Since regular human\nmobility patterns are heavily affected by these events, estimating their causal\neffects is critical to accurate mobility predictions. Although news articles\nprovide unique perspectives on these events in an unstructured format,\nprocessing is a challenge. In this study, we propose a causality-augmented\nprediction model, called CausalMob, to analyze the causal effects of public\nevents. We first utilize large language models (LLMs) to extract human\nintentions from news articles and transform them into features that act as\ncausal treatments. Next, the model learns representations of spatio-temporal\nregional covariates from multiple data sources to serve as confounders for\ncausal inference. Finally, we present a causal effect estimation framework to\nensure event features remain independent of confounders during prediction.\nBased on large-scale real-world data, the experimental results show that the\nproposed model excels in human mobility prediction, outperforming\nstate-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Xiaojie Yang"
                    },
                    {
                        "name": "Hangli Ge"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Zipei Fan"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Ryosuke Shibasaki"
                    },
                    {
                        "name": "Noboru Koshizuka"
                    }
                ],
                "author_detail": {
                    "name": "Noboru Koshizuka"
                },
                "author": "Noboru Koshizuka",
                "arxiv_comment": "Accepted by KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10207v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10207v6",
                "updated": "2025-01-07T06:28:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    6,
                    28,
                    56,
                    1,
                    7,
                    0
                ],
                "published": "2023-10-16T09:19:18Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    9,
                    19,
                    18,
                    0,
                    289,
                    0
                ],
                "title": "Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in\n  the Real World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in\n  the Real World"
                },
                "summary": "We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world\nfew-shot reasoning for machine vision. It originates from the classical Bongard\nProblems (BPs): Given two sets of images (positive and negative), the model\nneeds to identify the set that query images belong to by inducing the visual\nconcepts, which is exclusively depicted by images from the positive set. Our\nbenchmark inherits the few-shot concept induction of the original BPs while\nadding the two novel layers of challenge: 1) open-world free-form concepts, as\nthe visual concepts in Bongard-OpenWorld are unique compositions of terms from\nan open vocabulary, ranging from object categories to abstract visual\nattributes and commonsense factual knowledge; 2) real-world images, as opposed\nto the synthetic diagrams used by many counterparts. In our exploration,\nBongard-OpenWorld already imposes a significant challenge to current few-shot\nreasoning algorithms. We further investigate to which extent the recently\nintroduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nsolve our task, by directly probing VLMs, and combining VLMs and LLMs in an\ninteractive reasoning scheme. We even conceived a neuro-symbolic reasoning\napproach that reconciles LLMs & VLMs with logical reasoning to emulate the\nhuman problem-solving process for Bongard Problems. However, none of these\napproaches manage to close the human-machine gap, as the best learner achieves\n64% accuracy while human participants easily reach 91%. We hope\nBongard-OpenWorld can help us better understand the limitations of current\nvisual intelligence and facilitate future research on visual agents with\nstronger few-shot visual reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world\nfew-shot reasoning for machine vision. It originates from the classical Bongard\nProblems (BPs): Given two sets of images (positive and negative), the model\nneeds to identify the set that query images belong to by inducing the visual\nconcepts, which is exclusively depicted by images from the positive set. Our\nbenchmark inherits the few-shot concept induction of the original BPs while\nadding the two novel layers of challenge: 1) open-world free-form concepts, as\nthe visual concepts in Bongard-OpenWorld are unique compositions of terms from\nan open vocabulary, ranging from object categories to abstract visual\nattributes and commonsense factual knowledge; 2) real-world images, as opposed\nto the synthetic diagrams used by many counterparts. In our exploration,\nBongard-OpenWorld already imposes a significant challenge to current few-shot\nreasoning algorithms. We further investigate to which extent the recently\nintroduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nsolve our task, by directly probing VLMs, and combining VLMs and LLMs in an\ninteractive reasoning scheme. We even conceived a neuro-symbolic reasoning\napproach that reconciles LLMs & VLMs with logical reasoning to emulate the\nhuman problem-solving process for Bongard Problems. However, none of these\napproaches manage to close the human-machine gap, as the best learner achieves\n64% accuracy while human participants easily reach 91%. We hope\nBongard-OpenWorld can help us better understand the limitations of current\nvisual intelligence and facilitate future research on visual agents with\nstronger few-shot visual reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Rujie Wu"
                    },
                    {
                        "name": "Xiaojian Ma"
                    },
                    {
                        "name": "Zhenliang Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Yizhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Wang"
                },
                "author": "Yizhou Wang",
                "arxiv_comment": "Accepted to ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10207v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10207v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06537v2",
                "updated": "2025-01-07T05:52:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    52,
                    37,
                    1,
                    7,
                    0
                ],
                "published": "2024-03-11T09:24:06Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    9,
                    24,
                    6,
                    0,
                    71,
                    0
                ],
                "title": "On the Consideration of AI Openness: Can Good Intent Be Abused?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Consideration of AI Openness: Can Good Intent Be Abused?"
                },
                "summary": "Open source is a driving force behind scientific advancement.However, this\nopenness is also a double-edged sword, with the inherent risk that innovative\ntechnologies can be misused for purposes harmful to society. What is the\nlikelihood that an open source AI model or dataset will be used to commit a\nreal-world crime, and if a criminal does exploit it, will the people behind the\ntechnology be able to escape legal liability? To address these questions, we\nexplore a legal domain where individual choices can have a significant impact\non society. Specifically, we build the EVE-V1 dataset that comprises 200\nquestion-answer pairs related to criminal offenses based on 200 Korean\nprecedents first to explore the possibility of malicious models emerging. We\nfurther developed EVE-V2 using 600 fraud-related precedents to confirm the\nexistence of malicious models that can provide harmful advice on a wide range\nof criminal topics to test the domain generalization ability. Remarkably,\nwidely used open-source large-scale language models (LLMs) provide unethical\nand detailed information about criminal activities when fine-tuned with EVE. We\nalso take an in-depth look at the legal issues that malicious language models\nand their builders could realistically face. Our findings highlight the\nparadoxical dilemma that open source accelerates scientific progress, but\nrequires great care to minimize the potential for misuse. Warning: This paper\ncontains content that some may find unethical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open source is a driving force behind scientific advancement.However, this\nopenness is also a double-edged sword, with the inherent risk that innovative\ntechnologies can be misused for purposes harmful to society. What is the\nlikelihood that an open source AI model or dataset will be used to commit a\nreal-world crime, and if a criminal does exploit it, will the people behind the\ntechnology be able to escape legal liability? To address these questions, we\nexplore a legal domain where individual choices can have a significant impact\non society. Specifically, we build the EVE-V1 dataset that comprises 200\nquestion-answer pairs related to criminal offenses based on 200 Korean\nprecedents first to explore the possibility of malicious models emerging. We\nfurther developed EVE-V2 using 600 fraud-related precedents to confirm the\nexistence of malicious models that can provide harmful advice on a wide range\nof criminal topics to test the domain generalization ability. Remarkably,\nwidely used open-source large-scale language models (LLMs) provide unethical\nand detailed information about criminal activities when fine-tuned with EVE. We\nalso take an in-depth look at the legal issues that malicious language models\nand their builders could realistically face. Our findings highlight the\nparadoxical dilemma that open source accelerates scientific progress, but\nrequires great care to minimize the potential for misuse. Warning: This paper\ncontains content that some may find unethical."
                },
                "authors": [
                    {
                        "name": "Yeeun Kim"
                    },
                    {
                        "name": "Hyunseo Shin"
                    },
                    {
                        "name": "Eunkyung Choi"
                    },
                    {
                        "name": "Hongseok Oh"
                    },
                    {
                        "name": "Hyunjun Kim"
                    },
                    {
                        "name": "Wonseok Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Wonseok Hwang"
                },
                "author": "Wonseok Hwang",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03545v1",
                "updated": "2025-01-07T05:43:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    43,
                    23,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T05:43:23Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    43,
                    23,
                    1,
                    7,
                    0
                ],
                "title": "Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual\n  Information in Long-form Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual\n  Information in Long-form Text Generation"
                },
                "summary": "This paper presents ICAT, an evaluation framework for measuring coverage of\ndiverse factual information in long-form text generation. ICAT breaks down a\nlong output text into a list of atomic claims and not only verifies each claim\nthrough retrieval from a (reliable) knowledge source, but also computes the\nalignment between the atomic factual claims and various aspects expected to be\npresented in the output. We study three implementations of the ICAT framework,\neach with a different assumption on the availability of aspects and alignment\nmethod. By adopting data from the diversification task in the TREC Web Track\nand the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong\ncorrelation with human judgments and provide comprehensive evaluation across\nmultiple state-of-the-art LLMs. Our framework further offers interpretable and\nfine-grained analysis of diversity and coverage. Its modular design allows for\neasy adaptation to different domains and datasets, making it a valuable tool\nfor evaluating the qualitative aspects of long-form responses produced by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents ICAT, an evaluation framework for measuring coverage of\ndiverse factual information in long-form text generation. ICAT breaks down a\nlong output text into a list of atomic claims and not only verifies each claim\nthrough retrieval from a (reliable) knowledge source, but also computes the\nalignment between the atomic factual claims and various aspects expected to be\npresented in the output. We study three implementations of the ICAT framework,\neach with a different assumption on the availability of aspects and alignment\nmethod. By adopting data from the diversification task in the TREC Web Track\nand the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong\ncorrelation with human judgments and provide comprehensive evaluation across\nmultiple state-of-the-art LLMs. Our framework further offers interpretable and\nfine-grained analysis of diversity and coverage. Its modular design allows for\neasy adaptation to different domains and datasets, making it a valuable tool\nfor evaluating the qualitative aspects of long-form responses produced by LLMs."
                },
                "authors": [
                    {
                        "name": "Chris Samarinas"
                    },
                    {
                        "name": "Alexander Krubner"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Youngwoo Kim"
                    },
                    {
                        "name": "Hamed Zamani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Zamani"
                },
                "author": "Hamed Zamani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03544v1",
                "updated": "2025-01-07T05:39:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    39,
                    21,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T05:39:21Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    39,
                    21,
                    1,
                    7,
                    0
                ],
                "title": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for\n  Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for\n  Text-to-Image Models"
                },
                "summary": "Text-to-image (T2I) models have been shown to be vulnerable to misuse,\nparticularly in generating not-safe-for-work (NSFW) content, raising serious\nethical concerns. In this work, we present PromptGuard, a novel content\nmoderation technique that draws inspiration from the system prompt mechanism in\nlarge language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack\na direct interface for enforcing behavioral guidelines. Our key idea is to\noptimize a safety soft prompt that functions as an implicit system prompt\nwithin the T2I model's textual embedding space. This universal soft prompt (P*)\ndirectly moderates NSFW inputs, enabling safe yet realistic image generation\nwithout altering the inference efficiency or requiring proxy models. Extensive\nexperiments across three datasets demonstrate that PromptGuard effectively\nmitigates NSFW content generation while preserving high-quality benign outputs.\nPromptGuard achieves 7.8 times faster than prior content moderation methods,\nsurpassing eight state-of-the-art defenses with an optimal unsafe ratio down to\n5.84%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) models have been shown to be vulnerable to misuse,\nparticularly in generating not-safe-for-work (NSFW) content, raising serious\nethical concerns. In this work, we present PromptGuard, a novel content\nmoderation technique that draws inspiration from the system prompt mechanism in\nlarge language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack\na direct interface for enforcing behavioral guidelines. Our key idea is to\noptimize a safety soft prompt that functions as an implicit system prompt\nwithin the T2I model's textual embedding space. This universal soft prompt (P*)\ndirectly moderates NSFW inputs, enabling safe yet realistic image generation\nwithout altering the inference efficiency or requiring proxy models. Extensive\nexperiments across three datasets demonstrate that PromptGuard effectively\nmitigates NSFW content generation while preserving high-quality benign outputs.\nPromptGuard achieves 7.8 times faster than prior content moderation methods,\nsurpassing eight state-of-the-art defenses with an optimal unsafe ratio down to\n5.84%."
                },
                "authors": [
                    {
                        "name": "Lingzhi Yuan"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Chejian Xu"
                    },
                    {
                        "name": "Guanhong Tao"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "arxiv_comment": "16 pages, 8 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01672v2",
                "updated": "2025-01-07T05:36:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    36,
                    41,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-03T07:19:23Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    19,
                    23,
                    4,
                    3,
                    0
                ],
                "title": "Practical Secure Inference Algorithm for Fine-tuned Large Language Model\n  Based on Fully Homomorphic Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Secure Inference Algorithm for Fine-tuned Large Language Model\n  Based on Fully Homomorphic Encryption"
                },
                "summary": "Large language models(LLMs) are currently at the forefront of the machine\nlearning field, which show a broad application prospect but at the same time\nexpose some risks of privacy leakage. We combined Fully Homomorphic\nEncryption(FHE) and provable security theory with Parameter-Efficient\nFine-Tuning(PEFT) to propose an efficient and secure inference scheme for LLMs.\nMore specially, we focus on pre-trained LLMs which rely on open-sourced base\nmodel and then fine-tuned with the private datasets by LoRA. This is a popular\nroad-map for Vertical Domain Models such as LawGPT and BenTsao. We use two key\ntechnologies below. Firstly, we divide the whole model into the public part and\nthe private part. The weights of public part are publicly accessible(e.g. the\nopen-sourced base model) while the private part needs to be protected(e.g. the\nLoRA matrices). In this way, the overhead brought by computing on private data\ncan be greatly reduced. Secondly, we propose a general method to transform a\nlinear layer into another one which provides security against model extraction\nattacks and preserves its original functionality, which denoted as Private\nLinear Layer(PLL). Then we use this method on the LoRA matrices to make sure\nthat the server protects their private weights without restricting the user's\ninput. We also show that the difficulty of performing model extraction attacks\nfor PLL can be reduced to the well-known hard problem Learning with\nErrors(LWE). Combing this method with FHE, we can protect user's input at the\nsame time. In this paper, we use the open-source model ChatGLM2-6B as the base\nmodel which is fine-tuned by LoRA. Experimental results show the inference\nefficiency of our scheme reaches 1.61s/token which displays that the scheme has\ngood practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models(LLMs) are currently at the forefront of the machine\nlearning field, which show a broad application prospect but at the same time\nexpose some risks of privacy leakage. We combined Fully Homomorphic\nEncryption(FHE) and provable security theory with Parameter-Efficient\nFine-Tuning(PEFT) to propose an efficient and secure inference scheme for LLMs.\nMore specially, we focus on pre-trained LLMs which rely on open-sourced base\nmodel and then fine-tuned with the private datasets by LoRA. This is a popular\nroad-map for Vertical Domain Models such as LawGPT and BenTsao. We use two key\ntechnologies below. Firstly, we divide the whole model into the public part and\nthe private part. The weights of public part are publicly accessible(e.g. the\nopen-sourced base model) while the private part needs to be protected(e.g. the\nLoRA matrices). In this way, the overhead brought by computing on private data\ncan be greatly reduced. Secondly, we propose a general method to transform a\nlinear layer into another one which provides security against model extraction\nattacks and preserves its original functionality, which denoted as Private\nLinear Layer(PLL). Then we use this method on the LoRA matrices to make sure\nthat the server protects their private weights without restricting the user's\ninput. We also show that the difficulty of performing model extraction attacks\nfor PLL can be reduced to the well-known hard problem Learning with\nErrors(LWE). Combing this method with FHE, we can protect user's input at the\nsame time. In this paper, we use the open-source model ChatGLM2-6B as the base\nmodel which is fine-tuned by LoRA. Experimental results show the inference\nefficiency of our scheme reaches 1.61s/token which displays that the scheme has\ngood practicality."
                },
                "authors": [
                    {
                        "name": "Zhang Ruoyan"
                    },
                    {
                        "name": "Zheng Zhongxiang"
                    },
                    {
                        "name": "Bao Wankang"
                    }
                ],
                "author_detail": {
                    "name": "Bao Wankang"
                },
                "author": "Bao Wankang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13516v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13516v7",
                "updated": "2025-01-07T05:26:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    26,
                    54,
                    1,
                    7,
                    0
                ],
                "published": "2024-02-21T03:58:49Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    3,
                    58,
                    49,
                    2,
                    52,
                    0
                ],
                "title": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models"
                },
                "summary": "Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named \"ProSparse\"\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named \"ProSparse\"\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup."
                },
                "authors": [
                    {
                        "name": "Chenyang Song"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhengyan Zhang"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Xiyu Shi"
                    },
                    {
                        "name": "Kuai Li"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Guangli Li"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "19 pages, 4 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13516v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13516v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11876v2",
                "updated": "2025-01-07T05:20:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    20,
                    13,
                    1,
                    7,
                    0
                ],
                "published": "2024-10-10T01:23:16Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    1,
                    23,
                    16,
                    3,
                    284,
                    0
                ],
                "title": "Rescriber: Smaller-LLM-Powered User-Led Data Minimization for Navigating\n  Privacy Trade-offs in LLM-Based Conversational Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rescriber: Smaller-LLM-Powered User-Led Data Minimization for Navigating\n  Privacy Trade-offs in LLM-Based Conversational Agent"
                },
                "summary": "The proliferation of LLM-based conversational agents has resulted in\nexcessive disclosure of identifiable or sensitive information. However,\nexisting technologies fail to offer perceptible control or account for users'\npersonal preferences about privacy-utility tradeoffs due to the lack of user\ninvolvement. To bridge this gap, we designed, built, and evaluated Rescriber, a\nbrowser extension that supports user-led data minimization in LLM-based\nconversational agents by helping users detect and sanitize personal information\nin their prompts. Our studies (N=12) showed that Rescriber helped users reduce\nunnecessary disclosure and addressed their privacy concerns. Users' subjective\nperceptions of the system powered by Llama3-8B were on par with that by GPT-4o.\nThe comprehensiveness and consistency of the detection and sanitization emerge\nas essential factors that affect users' trust and perceived protection. Our\nfindings confirm the viability of smaller-LLM-powered, user-facing, on-device\nprivacy controls, presenting a promising approach to address the privacy and\ntrust challenges of AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of LLM-based conversational agents has resulted in\nexcessive disclosure of identifiable or sensitive information. However,\nexisting technologies fail to offer perceptible control or account for users'\npersonal preferences about privacy-utility tradeoffs due to the lack of user\ninvolvement. To bridge this gap, we designed, built, and evaluated Rescriber, a\nbrowser extension that supports user-led data minimization in LLM-based\nconversational agents by helping users detect and sanitize personal information\nin their prompts. Our studies (N=12) showed that Rescriber helped users reduce\nunnecessary disclosure and addressed their privacy concerns. Users' subjective\nperceptions of the system powered by Llama3-8B were on par with that by GPT-4o.\nThe comprehensiveness and consistency of the detection and sanitization emerge\nas essential factors that affect users' trust and perceived protection. Our\nfindings confirm the viability of smaller-LLM-powered, user-facing, on-device\nprivacy controls, presenting a promising approach to address the privacy and\ntrust challenges of AI."
                },
                "authors": [
                    {
                        "name": "Jijie Zhou"
                    },
                    {
                        "name": "Eryue Xu"
                    },
                    {
                        "name": "Yaoyao Wu"
                    },
                    {
                        "name": "Tianshi Li"
                    }
                ],
                "author_detail": {
                    "name": "Tianshi Li"
                },
                "author": "Tianshi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.10524v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.10524v3",
                "updated": "2025-01-07T05:15:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    15,
                    54,
                    1,
                    7,
                    0
                ],
                "published": "2023-09-19T11:10:50Z",
                "published_parsed": [
                    2023,
                    9,
                    19,
                    11,
                    10,
                    50,
                    1,
                    262,
                    0
                ],
                "title": "Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model\n  in End-to-End Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model\n  in End-to-End Speech Recognition"
                },
                "summary": "We propose to utilize an instruction-tuned large language model (LLM) for\nguiding the text generation process in automatic speech recognition (ASR).\nModern large language models (LLMs) are adept at performing various text\ngeneration tasks through zero-shot learning, prompted with instructions\ndesigned for specific objectives. This paper explores the potential of LLMs to\nderive linguistic information that can facilitate text generation in end-to-end\nASR models. Specifically, we instruct an LLM to correct grammatical errors in\nan ASR hypothesis and use the LLM-derived representations to refine the output\nfurther. The proposed model is built on the joint CTC and attention\narchitecture, with the LLM serving as a front-end feature extractor for the\ndecoder. The ASR hypothesis, subject to correction, is obtained from the\nencoder via CTC decoding and fed into the LLM along with a specific\ninstruction. The decoder subsequently takes as input the LLM output to perform\ntoken predictions, combining acoustic information from the encoder and the\npowerful linguistic information provided by the LLM. Experimental results show\nthat the proposed LLM-guided model achieves a relative gain of approximately\n13\\% in word error rates across major benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose to utilize an instruction-tuned large language model (LLM) for\nguiding the text generation process in automatic speech recognition (ASR).\nModern large language models (LLMs) are adept at performing various text\ngeneration tasks through zero-shot learning, prompted with instructions\ndesigned for specific objectives. This paper explores the potential of LLMs to\nderive linguistic information that can facilitate text generation in end-to-end\nASR models. Specifically, we instruct an LLM to correct grammatical errors in\nan ASR hypothesis and use the LLM-derived representations to refine the output\nfurther. The proposed model is built on the joint CTC and attention\narchitecture, with the LLM serving as a front-end feature extractor for the\ndecoder. The ASR hypothesis, subject to correction, is obtained from the\nencoder via CTC decoding and fed into the LLM along with a specific\ninstruction. The decoder subsequently takes as input the LLM output to perform\ntoken predictions, combining acoustic information from the encoder and the\npowerful linguistic information provided by the LLM. Experimental results show\nthat the proposed LLM-guided model achieves a relative gain of approximately\n13\\% in word error rates across major benchmarks."
                },
                "authors": [
                    {
                        "name": "Yosuke Higuchi"
                    },
                    {
                        "name": "Tetsuji Ogawa"
                    },
                    {
                        "name": "Tetsunori Kobayashi"
                    }
                ],
                "author_detail": {
                    "name": "Tetsunori Kobayashi"
                },
                "author": "Tetsunori Kobayashi",
                "arxiv_comment": "Accepted to ICASSP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.10524v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.10524v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03535v1",
                "updated": "2025-01-07T05:15:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    15,
                    46,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T05:15:46Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    15,
                    46,
                    1,
                    7,
                    0
                ],
                "title": "SenseRAG: Constructing Environmental Knowledge Bases with Proactive\n  Querying for LLM-Based Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SenseRAG: Constructing Environmental Knowledge Bases with Proactive\n  Querying for LLM-Based Autonomous Driving"
                },
                "summary": "This study addresses the critical need for enhanced situational awareness in\nautonomous driving (AD) by leveraging the contextual reasoning capabilities of\nlarge language models (LLMs). Unlike traditional perception systems that rely\non rigid, label-based annotations, it integrates real-time, multimodal sensor\ndata into a unified, LLMs-readable knowledge base, enabling LLMs to dynamically\nunderstand and respond to complex driving environments. To overcome the\ninherent latency and modality limitations of LLMs, a proactive\nRetrieval-Augmented Generation (RAG) is designed for AD, combined with a\nchain-of-thought prompting mechanism, ensuring rapid and context-rich\nunderstanding. Experimental results using real-world Vehicle-to-everything\n(V2X) datasets demonstrate significant improvements in perception and\nprediction performance, highlighting the potential of this framework to enhance\nsafety, adaptability, and decision-making in next-generation AD systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the critical need for enhanced situational awareness in\nautonomous driving (AD) by leveraging the contextual reasoning capabilities of\nlarge language models (LLMs). Unlike traditional perception systems that rely\non rigid, label-based annotations, it integrates real-time, multimodal sensor\ndata into a unified, LLMs-readable knowledge base, enabling LLMs to dynamically\nunderstand and respond to complex driving environments. To overcome the\ninherent latency and modality limitations of LLMs, a proactive\nRetrieval-Augmented Generation (RAG) is designed for AD, combined with a\nchain-of-thought prompting mechanism, ensuring rapid and context-rich\nunderstanding. Experimental results using real-world Vehicle-to-everything\n(V2X) datasets demonstrate significant improvements in perception and\nprediction performance, highlighting the potential of this framework to enhance\nsafety, adaptability, and decision-making in next-generation AD systems."
                },
                "authors": [
                    {
                        "name": "Xuewen Luo"
                    },
                    {
                        "name": "Fan Ding"
                    },
                    {
                        "name": "Fengze Yang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Junnyong Loo"
                    },
                    {
                        "name": "Hwa Hui Tew"
                    },
                    {
                        "name": "Chenxi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chenxi Liu"
                },
                "author": "Chenxi Liu",
                "arxiv_comment": "This paper has been accepted for presentation at WACV Workshop LLMAD\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06949v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06949v2",
                "updated": "2025-01-07T05:00:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    0,
                    50,
                    1,
                    7,
                    0
                ],
                "published": "2024-01-13T02:03:28Z",
                "published_parsed": [
                    2024,
                    1,
                    13,
                    2,
                    3,
                    28,
                    5,
                    13,
                    0
                ],
                "title": "ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and\n  Characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and\n  Characterization"
                },
                "summary": "Chemistry experiments can be resource- and labor-intensive, often requiring\nmanual tasks like polishing electrodes in electrochemistry. Traditional lab\nautomation infrastructure faces challenges adapting to new experiments. To\naddress this, we introduce ORGANA, an assistive robotic system that automates\ndiverse chemistry experiments using decision-making and perception tools. It\nmakes decisions with chemists in the loop to control robots and lab devices.\nORGANA interacts with chemists using Large Language Models (LLMs) to derive\nexperiment goals, handle disambiguation, and provide experiment logs. ORGANA\nplans and executes complex tasks with visual feedback, while supporting\nscheduling and parallel task execution. We demonstrate ORGANA's capabilities in\nsolubility, pH measurement, recrystallization, and electrochemistry\nexperiments. In electrochemistry, it executes a 19-step plan in parallel to\ncharacterize quinone derivatives for flow batteries. Our user study shows\nORGANA reduces frustration and physical demand by over 50%, with users saving\nan average of 80.3% of their time when using it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemistry experiments can be resource- and labor-intensive, often requiring\nmanual tasks like polishing electrodes in electrochemistry. Traditional lab\nautomation infrastructure faces challenges adapting to new experiments. To\naddress this, we introduce ORGANA, an assistive robotic system that automates\ndiverse chemistry experiments using decision-making and perception tools. It\nmakes decisions with chemists in the loop to control robots and lab devices.\nORGANA interacts with chemists using Large Language Models (LLMs) to derive\nexperiment goals, handle disambiguation, and provide experiment logs. ORGANA\nplans and executes complex tasks with visual feedback, while supporting\nscheduling and parallel task execution. We demonstrate ORGANA's capabilities in\nsolubility, pH measurement, recrystallization, and electrochemistry\nexperiments. In electrochemistry, it executes a 19-step plan in parallel to\ncharacterize quinone derivatives for flow batteries. Our user study shows\nORGANA reduces frustration and physical demand by over 50%, with users saving\nan average of 80.3% of their time when using it."
                },
                "authors": [
                    {
                        "name": "Kourosh Darvish"
                    },
                    {
                        "name": "Marta Skreta"
                    },
                    {
                        "name": "Yuchi Zhao"
                    },
                    {
                        "name": "Naruki Yoshikawa"
                    },
                    {
                        "name": "Sagnik Som"
                    },
                    {
                        "name": "Miroslav Bogdanovic"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Han Hao"
                    },
                    {
                        "name": "Haoping Xu"
                    },
                    {
                        "name": "Alán Aspuru-Guzik"
                    },
                    {
                        "name": "Animesh Garg"
                    },
                    {
                        "name": "Florian Shkurti"
                    }
                ],
                "author_detail": {
                    "name": "Florian Shkurti"
                },
                "author": "Florian Shkurti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06949v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12935v2",
                "updated": "2025-01-07T04:42:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    4,
                    42,
                    20,
                    1,
                    7,
                    0
                ],
                "published": "2024-06-17T03:03:34Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    3,
                    3,
                    34,
                    0,
                    169,
                    0
                ],
                "title": "ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat\n  Templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat\n  Templates"
                },
                "summary": "Large language models (LLMs) are expected to follow instructions from users\nand engage in conversations. Techniques to enhance LLMs' instruction-following\ncapabilities typically fine-tune them using data structured according to a\npredefined chat template. Although chat templates are shown to be effective in\noptimizing LLM performance, their impact on safety alignment of LLMs has been\nless understood, which is crucial for deploying LLMs safely at scale.\n  In this paper, we investigate how chat templates affect safety alignment of\nLLMs. We identify a common vulnerability, named ChatBug, that is introduced by\nchat templates. Our key insight to identify ChatBug is that the chat templates\nprovide a rigid format that need to be followed by LLMs, but not by users.\nHence, a malicious user may not necessarily follow the chat template when\nprompting LLMs. Instead, malicious users could leverage their knowledge of the\nchat template and accordingly craft their prompts to bypass safety alignments\nof LLMs. We develop two attacks to exploit the ChatBug vulnerability. We\ndemonstrate that a malicious user can exploit the ChatBug vulnerability of\neight state-of-the-art (SOTA) LLMs and effectively elicit unintended responses\nfrom these models. Moreover, we show that ChatBug can be exploited by existing\njailbreak attacks to enhance their attack success rates. We investigate\npotential countermeasures to ChatBug. Our results show that while adversarial\ntraining effectively mitigates the ChatBug vulnerability, the victim model\nincurs significant performance degradation. These results highlight the\ntrade-off between safety alignment and helpfulness. Developing new methods for\ninstruction tuning to balance this trade-off is an open and critical direction\nfor future research",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are expected to follow instructions from users\nand engage in conversations. Techniques to enhance LLMs' instruction-following\ncapabilities typically fine-tune them using data structured according to a\npredefined chat template. Although chat templates are shown to be effective in\noptimizing LLM performance, their impact on safety alignment of LLMs has been\nless understood, which is crucial for deploying LLMs safely at scale.\n  In this paper, we investigate how chat templates affect safety alignment of\nLLMs. We identify a common vulnerability, named ChatBug, that is introduced by\nchat templates. Our key insight to identify ChatBug is that the chat templates\nprovide a rigid format that need to be followed by LLMs, but not by users.\nHence, a malicious user may not necessarily follow the chat template when\nprompting LLMs. Instead, malicious users could leverage their knowledge of the\nchat template and accordingly craft their prompts to bypass safety alignments\nof LLMs. We develop two attacks to exploit the ChatBug vulnerability. We\ndemonstrate that a malicious user can exploit the ChatBug vulnerability of\neight state-of-the-art (SOTA) LLMs and effectively elicit unintended responses\nfrom these models. Moreover, we show that ChatBug can be exploited by existing\njailbreak attacks to enhance their attack success rates. We investigate\npotential countermeasures to ChatBug. Our results show that while adversarial\ntraining effectively mitigates the ChatBug vulnerability, the victim model\nincurs significant performance degradation. These results highlight the\ntrade-off between safety alignment and helpfulness. Developing new methods for\ninstruction tuning to balance this trade-off is an open and critical direction\nfor future research"
                },
                "authors": [
                    {
                        "name": "Fengqing Jiang"
                    },
                    {
                        "name": "Zhangchen Xu"
                    },
                    {
                        "name": "Luyao Niu"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Radha Poovendran"
                    }
                ],
                "author_detail": {
                    "name": "Radha Poovendran"
                },
                "author": "Radha Poovendran",
                "arxiv_comment": "This paper is accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04828v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04828v5",
                "updated": "2025-01-07T04:38:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    4,
                    38,
                    25,
                    1,
                    7,
                    0
                ],
                "published": "2023-12-08T05:01:47Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    5,
                    1,
                    47,
                    4,
                    342,
                    0
                ],
                "title": "HuRef: HUman-REadable Fingerprint for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HuRef: HUman-REadable Fingerprint for Large Language Models"
                },
                "summary": "Protecting the copyright of large language models (LLMs) has become crucial\ndue to their resource-intensive training and accompanying carefully designed\nlicenses. However, identifying the original base model of an LLM is challenging\ndue to potential parameter alterations. In this study, we introduce HuRef, a\nhuman-readable fingerprint for LLMs that uniquely identifies the base model\nwithout interfering with training or exposing model parameters to the public.\nWe first observe that the vector direction of LLM parameters remains stable\nafter the model has converged during pretraining, with negligible perturbations\nthrough subsequent training steps, including continued pretraining, supervised\nfine-tuning, and RLHF, which makes it a sufficient condition to identify the\nbase model. The necessity is validated by continuing to train an LLM with an\nextra term to drive away the model parameters' direction and the model becomes\ndamaged. However, this direction is vulnerable to simple attacks like dimension\npermutation or matrix rotation, which significantly change it without affecting\nperformance. To address this, leveraging the Transformer structure, we\nsystematically analyze potential attacks and define three invariant terms that\nidentify an LLM's base model. Due to the potential risk of information leakage,\nwe cannot publish invariant terms directly. Instead, we map them to a Gaussian\nvector using an encoder, then convert it into a natural image using StyleGAN2,\nand finally publish the image. In our black-box setting, all fingerprinting\nsteps are internally conducted by the LLMs owners. To ensure the published\nfingerprints are honestly generated, we introduced Zero-Knowledge Proof (ZKP).\nExperimental results across various LLMs demonstrate the effectiveness of our\nmethod. The code is available at https://github.com/LUMIA-Group/HuRef.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting the copyright of large language models (LLMs) has become crucial\ndue to their resource-intensive training and accompanying carefully designed\nlicenses. However, identifying the original base model of an LLM is challenging\ndue to potential parameter alterations. In this study, we introduce HuRef, a\nhuman-readable fingerprint for LLMs that uniquely identifies the base model\nwithout interfering with training or exposing model parameters to the public.\nWe first observe that the vector direction of LLM parameters remains stable\nafter the model has converged during pretraining, with negligible perturbations\nthrough subsequent training steps, including continued pretraining, supervised\nfine-tuning, and RLHF, which makes it a sufficient condition to identify the\nbase model. The necessity is validated by continuing to train an LLM with an\nextra term to drive away the model parameters' direction and the model becomes\ndamaged. However, this direction is vulnerable to simple attacks like dimension\npermutation or matrix rotation, which significantly change it without affecting\nperformance. To address this, leveraging the Transformer structure, we\nsystematically analyze potential attacks and define three invariant terms that\nidentify an LLM's base model. Due to the potential risk of information leakage,\nwe cannot publish invariant terms directly. Instead, we map them to a Gaussian\nvector using an encoder, then convert it into a natural image using StyleGAN2,\nand finally publish the image. In our black-box setting, all fingerprinting\nsteps are internally conducted by the LLMs owners. To ensure the published\nfingerprints are honestly generated, we introduced Zero-Knowledge Proof (ZKP).\nExperimental results across various LLMs demonstrate the effectiveness of our\nmethod. The code is available at https://github.com/LUMIA-Group/HuRef."
                },
                "authors": [
                    {
                        "name": "Boyi Zeng"
                    },
                    {
                        "name": "Lizheng Wang"
                    },
                    {
                        "name": "Yuncong Hu"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Chenghu Zhou"
                    },
                    {
                        "name": "Xinbing Wang"
                    },
                    {
                        "name": "Yu Yu"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04828v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04828v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03515v1",
                "updated": "2025-01-07T04:17:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    4,
                    17,
                    15,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T04:17:15Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    4,
                    17,
                    15,
                    1,
                    7,
                    0
                ],
                "title": "Effects of Robot Competency and Motion Legibility on Human Correction\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effects of Robot Competency and Motion Legibility on Human Correction\n  Feedback"
                },
                "summary": "As robot deployments become more commonplace, people are likely to take on\nthe role of supervising robots (i.e., correcting their mistakes) rather than\ndirectly teaching them. Prior works on Learning from Corrections (LfC) have\nrelied on three key assumptions to interpret human feedback: (1) people correct\nthe robot only when there is significant task objective divergence; (2) people\ncan accurately predict if a correction is necessary; and (3) people trade off\nprecision and physical effort when giving corrections. In this work, we study\nhow two key factors (robot competency and motion legibility) affect how people\nprovide correction feedback and their implications on these existing\nassumptions. We conduct a user study ($N=60$) under an LfC setting where\nparticipants supervise and correct a robot performing pick-and-place tasks. We\nfind that people are more sensitive to suboptimal behavior by a highly\ncompetent robot compared to an incompetent robot when the motions are legible\n($p=0.0015$) and predictable ($p=0.0055$). In addition, people also tend to\nwithhold necessary corrections ($p < 0.0001$) when supervising an incompetent\nrobot and are more prone to offering unnecessary ones ($p = 0.0171$) when\nsupervising a highly competent robot. We also find that physical effort\npositively correlates with correction precision, providing empirical evidence\nto support this common assumption. We also find that this correlation is\nsignificantly weaker for an incompetent robot with legible motions than an\nincompetent robot with predictable motions ($p = 0.0075$). Our findings offer\ninsights for accounting for competency and legibility when designing robot\ninteraction behaviors and learning task objectives from corrections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As robot deployments become more commonplace, people are likely to take on\nthe role of supervising robots (i.e., correcting their mistakes) rather than\ndirectly teaching them. Prior works on Learning from Corrections (LfC) have\nrelied on three key assumptions to interpret human feedback: (1) people correct\nthe robot only when there is significant task objective divergence; (2) people\ncan accurately predict if a correction is necessary; and (3) people trade off\nprecision and physical effort when giving corrections. In this work, we study\nhow two key factors (robot competency and motion legibility) affect how people\nprovide correction feedback and their implications on these existing\nassumptions. We conduct a user study ($N=60$) under an LfC setting where\nparticipants supervise and correct a robot performing pick-and-place tasks. We\nfind that people are more sensitive to suboptimal behavior by a highly\ncompetent robot compared to an incompetent robot when the motions are legible\n($p=0.0015$) and predictable ($p=0.0055$). In addition, people also tend to\nwithhold necessary corrections ($p < 0.0001$) when supervising an incompetent\nrobot and are more prone to offering unnecessary ones ($p = 0.0171$) when\nsupervising a highly competent robot. We also find that physical effort\npositively correlates with correction precision, providing empirical evidence\nto support this common assumption. We also find that this correlation is\nsignificantly weaker for an incompetent robot with legible motions than an\nincompetent robot with predictable motions ($p = 0.0075$). Our findings offer\ninsights for accounting for competency and legibility when designing robot\ninteraction behaviors and learning task objectives from corrections."
                },
                "authors": [
                    {
                        "name": "Shuangge Wang"
                    },
                    {
                        "name": "Anjiabei Wang"
                    },
                    {
                        "name": "Sofiya Goncharova"
                    },
                    {
                        "name": "Brian Scassellati"
                    },
                    {
                        "name": "Tesca Fitzgerald"
                    }
                ],
                "author_detail": {
                    "name": "Tesca Fitzgerald"
                },
                "author": "Tesca Fitzgerald",
                "arxiv_comment": "to be published in the 2025 ACM/IEEE International Conference on\n  Human-Robot Interaction (HRI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.17740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.17740v2",
                "updated": "2025-01-07T04:03:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    4,
                    3,
                    46,
                    1,
                    7,
                    0
                ],
                "published": "2023-05-28T14:48:38Z",
                "published_parsed": [
                    2023,
                    5,
                    28,
                    14,
                    48,
                    38,
                    6,
                    148,
                    0
                ],
                "title": "Bridging the Language Gap: Dynamic Learning Strategies for Improving\n  Multilingual Performance in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Language Gap: Dynamic Learning Strategies for Improving\n  Multilingual Performance in LLMs"
                },
                "summary": "Large language models (LLMs) have revolutionized various domains but still\nstruggle with non-Latin scripts and low-resource languages. This paper\naddresses the critical challenge of improving multilingual performance without\nextensive fine-tuning. We introduce a novel dynamic learning approach that\noptimizes prompt strategy, embedding model, and LLM per query at runtime. By\nadapting configurations dynamically, our method achieves significant\nimprovements over static, best and random baselines. It operates efficiently in\nboth offline and online settings, generalizing seamlessly across new languages\nand datasets. Leveraging Retrieval-Augmented Generation (RAG) with\nstate-of-the-art multilingual embeddings, we achieve superior task performance\nacross diverse linguistic contexts. Through systematic investigation and\nevaluation across 18 diverse languages using popular question-answering (QA)\ndatasets we show our approach results in 10-15% improvements in multilingual\nperformance over pre-trained models and 4x gains compared to fine-tuned,\nlanguage-specific models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized various domains but still\nstruggle with non-Latin scripts and low-resource languages. This paper\naddresses the critical challenge of improving multilingual performance without\nextensive fine-tuning. We introduce a novel dynamic learning approach that\noptimizes prompt strategy, embedding model, and LLM per query at runtime. By\nadapting configurations dynamically, our method achieves significant\nimprovements over static, best and random baselines. It operates efficiently in\nboth offline and online settings, generalizing seamlessly across new languages\nand datasets. Leveraging Retrieval-Augmented Generation (RAG) with\nstate-of-the-art multilingual embeddings, we achieve superior task performance\nacross diverse linguistic contexts. Through systematic investigation and\nevaluation across 18 diverse languages using popular question-answering (QA)\ndatasets we show our approach results in 10-15% improvements in multilingual\nperformance over pre-trained models and 4x gains compared to fine-tuned,\nlanguage-specific models."
                },
                "authors": [
                    {
                        "name": "Somnath Kumar"
                    },
                    {
                        "name": "Vaibhav Balloli"
                    },
                    {
                        "name": "Mercy Ranjit"
                    },
                    {
                        "name": "Kabir Ahuja"
                    },
                    {
                        "name": "Sunayana Sitaram"
                    },
                    {
                        "name": "Kalika Bali"
                    },
                    {
                        "name": "Tanuja Ganu"
                    },
                    {
                        "name": "Akshay Nambi"
                    }
                ],
                "author_detail": {
                    "name": "Akshay Nambi"
                },
                "author": "Akshay Nambi",
                "arxiv_journal_ref": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.17740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.17740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23111v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23111v5",
                "updated": "2025-01-07T03:56:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    3,
                    56,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2024-10-30T15:23:44Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    15,
                    23,
                    44,
                    2,
                    304,
                    0
                ],
                "title": "Exploring Gradient Subspaces: Addressing and Overcoming LoRA's\n  Limitations in Federated Fine-Tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Gradient Subspaces: Addressing and Overcoming LoRA's\n  Limitations in Federated Fine-Tuning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison unmasks inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore along with\ndirect-weight aggregation is a more effective approach, outperforming federated\nLoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities.\nWhile privacy remains paramount in FL discourse, our focus is on assessing\nperformance outcomes of federated fine-tuned models and evaluating various FL\nframeworks from both theoretical and empirical perspectives. Our findings\nadvocate reassessing the reliance on LoRA within FL contexts, paving the way\nfor more efficient training methodologies."
                },
                "authors": [
                    {
                        "name": "Navyansh Mahla"
                    },
                    {
                        "name": "Kshitij Sharad Jadhav"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Ramakrishnan"
                },
                "author": "Ganesh Ramakrishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23111v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23111v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16766v2",
                "updated": "2025-01-07T03:53:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    3,
                    53,
                    12,
                    1,
                    7,
                    0
                ],
                "published": "2024-05-27T02:27:28Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    2,
                    27,
                    28,
                    0,
                    148,
                    0
                ],
                "title": "Concept Matching with Agent for Out-of-Distribution Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept Matching with Agent for Out-of-Distribution Detection"
                },
                "summary": "The remarkable achievements of Large Language Models (LLMs) have captivated\nthe attention of both academia and industry, transcending their initial role in\ndialogue generation. To expand the usage scenarios of LLM, some works enhance\nthe effectiveness and capabilities of the model by introducing more external\ninformation, which is called the agent paradigm. Based on this idea, we propose\na new method that integrates the agent paradigm into out-of-distribution (OOD)\ndetection task, aiming to improve its robustness and adaptability. Our proposed\nmethod, Concept Matching with Agent (CMA), employs neutral prompts as agents to\naugment the CLIP-based OOD detection process. These agents function as dynamic\nobservers and communication hubs, interacting with both In-distribution (ID)\nlabels and data inputs to form vector triangle relationships. This triangular\nframework offers a more nuanced approach than the traditional binary\nrelationship, allowing for better separation and identification of ID and OOD\ninputs. Our extensive experimental results showcase the superior performance of\nCMA over both zero-shot and training-required methods in a diverse array of\nreal-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable achievements of Large Language Models (LLMs) have captivated\nthe attention of both academia and industry, transcending their initial role in\ndialogue generation. To expand the usage scenarios of LLM, some works enhance\nthe effectiveness and capabilities of the model by introducing more external\ninformation, which is called the agent paradigm. Based on this idea, we propose\na new method that integrates the agent paradigm into out-of-distribution (OOD)\ndetection task, aiming to improve its robustness and adaptability. Our proposed\nmethod, Concept Matching with Agent (CMA), employs neutral prompts as agents to\naugment the CLIP-based OOD detection process. These agents function as dynamic\nobservers and communication hubs, interacting with both In-distribution (ID)\nlabels and data inputs to form vector triangle relationships. This triangular\nframework offers a more nuanced approach than the traditional binary\nrelationship, allowing for better separation and identification of ID and OOD\ninputs. Our extensive experimental results showcase the superior performance of\nCMA over both zero-shot and training-required methods in a diverse array of\nreal-world scenarios."
                },
                "authors": [
                    {
                        "name": "Yuxiao Lee"
                    },
                    {
                        "name": "Xiaofeng Cao"
                    },
                    {
                        "name": "Jingcai Guo"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Qing Guo"
                    },
                    {
                        "name": "Yi Chang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Chang"
                },
                "author": "Yi Chang",
                "arxiv_comment": "Accepted by AAAI-25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03508v1",
                "updated": "2025-01-07T03:51:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    3,
                    51,
                    10,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T03:51:10Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    3,
                    51,
                    10,
                    1,
                    7,
                    0
                ],
                "title": "A Sequential Optimal Learning Approach to Automated Prompt Engineering\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Sequential Optimal Learning Approach to Automated Prompt Engineering\n  in Large Language Models"
                },
                "summary": "Designing effective prompts is essential to guiding large language models\n(LLMs) toward desired responses. Automated prompt engineering aims to reduce\nreliance on manual effort by streamlining the design, refinement, and\noptimization of natural language prompts. This paper proposes an optimal\nlearning framework for automated prompt engineering, designed to sequentially\nidentify effective prompt features while efficiently allocating a limited\nevaluation budget. We introduce a feature-based method to express prompts,\nwhich significantly broadens the search space. Bayesian regression is employed\nto utilize correlations among similar prompts, accelerating the learning\nprocess. To efficiently explore the large space of prompt features for a high\nquality prompt, we adopt the forward-looking Knowledge-Gradient (KG) policy for\nsequential optimal learning. The KG policy is computed efficiently by solving\nmixed-integer second-order cone optimization problems, making it scalable and\ncapable of accommodating prompts characterized only through constraints. We\ndemonstrate that our method significantly outperforms a set of benchmark\nstrategies assessed on instruction induction tasks. The results highlight the\nadvantages of using the KG policy for prompt learning given a limited\nevaluation budget. Our framework provides a solution to deploying automated\nprompt engineering in a wider range applications where prompt evaluation is\ncostly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing effective prompts is essential to guiding large language models\n(LLMs) toward desired responses. Automated prompt engineering aims to reduce\nreliance on manual effort by streamlining the design, refinement, and\noptimization of natural language prompts. This paper proposes an optimal\nlearning framework for automated prompt engineering, designed to sequentially\nidentify effective prompt features while efficiently allocating a limited\nevaluation budget. We introduce a feature-based method to express prompts,\nwhich significantly broadens the search space. Bayesian regression is employed\nto utilize correlations among similar prompts, accelerating the learning\nprocess. To efficiently explore the large space of prompt features for a high\nquality prompt, we adopt the forward-looking Knowledge-Gradient (KG) policy for\nsequential optimal learning. The KG policy is computed efficiently by solving\nmixed-integer second-order cone optimization problems, making it scalable and\ncapable of accommodating prompts characterized only through constraints. We\ndemonstrate that our method significantly outperforms a set of benchmark\nstrategies assessed on instruction induction tasks. The results highlight the\nadvantages of using the KG policy for prompt learning given a limited\nevaluation budget. Our framework provides a solution to deploying automated\nprompt engineering in a wider range applications where prompt evaluation is\ncostly."
                },
                "authors": [
                    {
                        "name": "Shuyang Wang"
                    },
                    {
                        "name": "Somayeh Moazeni"
                    },
                    {
                        "name": "Diego Klabjan"
                    }
                ],
                "author_detail": {
                    "name": "Diego Klabjan"
                },
                "author": "Diego Klabjan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00166v2",
                "updated": "2025-01-07T03:21:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    3,
                    21,
                    43,
                    1,
                    7,
                    0
                ],
                "published": "2024-09-30T19:15:05Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    15,
                    5,
                    0,
                    274,
                    0
                ],
                "title": "EEG Emotion Copilot: Optimizing Lightweight LLMs for Emotional EEG\n  Interpretation with Assisted Medical Record Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEG Emotion Copilot: Optimizing Lightweight LLMs for Emotional EEG\n  Interpretation with Assisted Medical Record Generation"
                },
                "summary": "In the fields of affective computing (AC) and brain-machine interface (BMI),\nthe analysis of physiological and behavioral signals to discern individual\nemotional states has emerged as a critical research frontier. While deep\nlearning-based approaches have made notable strides in EEG emotion recognition,\nparticularly in feature extraction and pattern recognition, significant\nchallenges persist in achieving end-to-end emotion computation, including\nreal-time processing, individual adaptation, and seamless user interaction.\nThis paper presents the EEG Emotion Copilot, a system optimizing a lightweight\nlarge language model (LLM) with 0.5B parameters operating in a local setting,\nwhich first recognizes emotional states directly from EEG signals, subsequently\ngenerates personalized diagnostic and treatment suggestions, and finally\nsupports the automation of assisted electronic medical records. Specifically,\nwe demonstrate the critical techniques in the novel data structure of prompt,\nmodel pruning and fine-tuning training, and deployment strategies aiming at\nimproving real-time performance and computational efficiency. Extensive\nexperiments show that our optimized lightweight LLM-based copilot achieves an\nenhanced intuitive interface for participant interaction, superior accuracy of\nemotion recognition and assisted electronic medical records generation, in\ncomparison to such models with similar scale parameters or large-scale\nparameters such as 1.5B, 1.8B, 3B and 7B. In summary, through these efforts,\nthe proposed copilot is expected to advance the application of AC in the\nmedical domain, offering innovative solution to mental health monitoring. The\ncodes will be released at https://github.com/NZWANG/EEG_Emotion_Copilot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the fields of affective computing (AC) and brain-machine interface (BMI),\nthe analysis of physiological and behavioral signals to discern individual\nemotional states has emerged as a critical research frontier. While deep\nlearning-based approaches have made notable strides in EEG emotion recognition,\nparticularly in feature extraction and pattern recognition, significant\nchallenges persist in achieving end-to-end emotion computation, including\nreal-time processing, individual adaptation, and seamless user interaction.\nThis paper presents the EEG Emotion Copilot, a system optimizing a lightweight\nlarge language model (LLM) with 0.5B parameters operating in a local setting,\nwhich first recognizes emotional states directly from EEG signals, subsequently\ngenerates personalized diagnostic and treatment suggestions, and finally\nsupports the automation of assisted electronic medical records. Specifically,\nwe demonstrate the critical techniques in the novel data structure of prompt,\nmodel pruning and fine-tuning training, and deployment strategies aiming at\nimproving real-time performance and computational efficiency. Extensive\nexperiments show that our optimized lightweight LLM-based copilot achieves an\nenhanced intuitive interface for participant interaction, superior accuracy of\nemotion recognition and assisted electronic medical records generation, in\ncomparison to such models with similar scale parameters or large-scale\nparameters such as 1.5B, 1.8B, 3B and 7B. In summary, through these efforts,\nthe proposed copilot is expected to advance the application of AC in the\nmedical domain, offering innovative solution to mental health monitoring. The\ncodes will be released at https://github.com/NZWANG/EEG_Emotion_Copilot."
                },
                "authors": [
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Weiming Zeng"
                    },
                    {
                        "name": "Chengcheng Chen"
                    },
                    {
                        "name": "Luhui Cai"
                    },
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Yuhu Shi"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Yueyang Li"
                    },
                    {
                        "name": "Hongjie Yan"
                    },
                    {
                        "name": "Wai Ting Siok"
                    },
                    {
                        "name": "Nizhuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Nizhuan Wang"
                },
                "author": "Nizhuan Wang",
                "arxiv_comment": "10 pages, 12 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03491v1",
                "updated": "2025-01-07T03:21:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    3,
                    21,
                    17,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T03:21:17Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    3,
                    21,
                    17,
                    1,
                    7,
                    0
                ],
                "title": "Can LLMs Design Good Questions Based on Context?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Design Good Questions Based on Context?"
                },
                "summary": "This paper evaluates questions generated by LLMs from context, comparing them\nto human-generated questions across six dimensions. We introduce an automated\nLLM-based evaluation method, focusing on aspects like question length, type,\ncontext coverage, and answerability. Our findings highlight unique\ncharacteristics of LLM-generated questions, contributing insights that can\nsupport further research in question quality and downstream applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates questions generated by LLMs from context, comparing them\nto human-generated questions across six dimensions. We introduce an automated\nLLM-based evaluation method, focusing on aspects like question length, type,\ncontext coverage, and answerability. Our findings highlight unique\ncharacteristics of LLM-generated questions, contributing insights that can\nsupport further research in question quality and downstream applications."
                },
                "authors": [
                    {
                        "name": "Yueheng Zhang"
                    },
                    {
                        "name": "Xiaoyuan Liu"
                    },
                    {
                        "name": "Yiyou Sun"
                    },
                    {
                        "name": "Atheer Alharbi"
                    },
                    {
                        "name": "Hend Alzahrani"
                    },
                    {
                        "name": "Basel Alomair"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03489v1",
                "updated": "2025-01-07T03:17:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    3,
                    17,
                    47,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T03:17:47Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    3,
                    17,
                    47,
                    1,
                    7,
                    0
                ],
                "title": "Entropy-Guided Attention for Private LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-Guided Attention for Private LLMs"
                },
                "summary": "The pervasiveness of proprietary language models has raised critical privacy\nconcerns, necessitating advancements in private inference (PI), where\ncomputations are performed directly on encrypted data without revealing users'\nsensitive information. While PI offers a promising solution, its practical\ndeployment is hindered by substantial communication and latency overheads,\nprimarily stemming from nonlinear operations. To address this, we introduce an\ninformation-theoretic framework to characterize the role of nonlinearities in\ndecoder-only language models, laying a principled foundation for optimizing\ntransformer-architectures tailored to the demands of PI.\n  By leveraging Shannon's entropy as a quantitative measure, we uncover the\npreviously unexplored dual significance of nonlinearities: beyond ensuring\ntraining stability, they are crucial for maintaining attention head diversity.\nSpecifically, we find that their removal triggers two critical failure modes:\n{\\em entropy collapse} in deeper layers that destabilizes training, and {\\em\nentropic overload} in earlier layers that leads to under-utilization of\nMulti-Head Attention's (MHA) representational capacity.\n  We propose an entropy-guided attention mechanism paired with a novel entropy\nregularization technique to mitigate entropic overload. Additionally, we\nexplore PI-friendly alternatives to layer normalization for preventing entropy\ncollapse and stabilizing the training of LLMs with reduced-nonlinearities. Our\nstudy bridges the gap between information theory and architectural design,\nestablishing entropy dynamics as a principled guide for developing efficient PI\narchitectures. The code and implementation are available at\n\\href{https://github.com/Nandan91/entropy-guided-attention-llm}{entropy-guided-llm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pervasiveness of proprietary language models has raised critical privacy\nconcerns, necessitating advancements in private inference (PI), where\ncomputations are performed directly on encrypted data without revealing users'\nsensitive information. While PI offers a promising solution, its practical\ndeployment is hindered by substantial communication and latency overheads,\nprimarily stemming from nonlinear operations. To address this, we introduce an\ninformation-theoretic framework to characterize the role of nonlinearities in\ndecoder-only language models, laying a principled foundation for optimizing\ntransformer-architectures tailored to the demands of PI.\n  By leveraging Shannon's entropy as a quantitative measure, we uncover the\npreviously unexplored dual significance of nonlinearities: beyond ensuring\ntraining stability, they are crucial for maintaining attention head diversity.\nSpecifically, we find that their removal triggers two critical failure modes:\n{\\em entropy collapse} in deeper layers that destabilizes training, and {\\em\nentropic overload} in earlier layers that leads to under-utilization of\nMulti-Head Attention's (MHA) representational capacity.\n  We propose an entropy-guided attention mechanism paired with a novel entropy\nregularization technique to mitigate entropic overload. Additionally, we\nexplore PI-friendly alternatives to layer normalization for preventing entropy\ncollapse and stabilizing the training of LLMs with reduced-nonlinearities. Our\nstudy bridges the gap between information theory and architectural design,\nestablishing entropy dynamics as a principled guide for developing efficient PI\narchitectures. The code and implementation are available at\n\\href{https://github.com/Nandan91/entropy-guided-attention-llm}{entropy-guided-llm}."
                },
                "authors": [
                    {
                        "name": "Nandan Kumar Jha"
                    },
                    {
                        "name": "Brandon Reagen"
                    }
                ],
                "author_detail": {
                    "name": "Brandon Reagen"
                },
                "author": "Brandon Reagen",
                "arxiv_comment": "The 6th AAAI Workshop on Privacy-Preserving Artificial Intelligence\n  (PPAI), 2025. arXiv admin note: substantial text overlap with\n  arXiv:2410.13060",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03486v1",
                "updated": "2025-01-07T03:14:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    3,
                    14,
                    39,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T03:14:39Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    3,
                    14,
                    39,
                    1,
                    7,
                    0
                ],
                "title": "Align-Pro: A Principled Approach to Prompt Optimization for LLM\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align-Pro: A Principled Approach to Prompt Optimization for LLM\n  Alignment"
                },
                "summary": "The alignment of large language models (LLMs) with human values is critical\nas these models become increasingly integrated into various societal and\ndecision-making processes. Traditional methods, such as reinforcement learning\nfrom human feedback (RLHF), achieve alignment by fine-tuning model parameters,\nbut these approaches are often computationally expensive and impractical when\nmodels are frozen or inaccessible for parameter modification. In contrast,\nprompt optimization is a viable alternative to RLHF for LLM alignment. While\nthe existing literature has shown empirical promise of prompt optimization, its\ntheoretical underpinning remains under-explored. We address this gap by\nformulating prompt optimization as an optimization problem and try to provide\ntheoretical insights into the optimality of such a framework. To analyze the\nperformance of the prompt optimization, we study theoretical suboptimality\nbounds and provide insights in terms of how prompt optimization depends upon\nthe given prompter and target model. We also provide empirical validation\nthrough experiments on various datasets, demonstrating that prompt optimization\ncan effectively align LLMs, even when parameter fine-tuning is not feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of large language models (LLMs) with human values is critical\nas these models become increasingly integrated into various societal and\ndecision-making processes. Traditional methods, such as reinforcement learning\nfrom human feedback (RLHF), achieve alignment by fine-tuning model parameters,\nbut these approaches are often computationally expensive and impractical when\nmodels are frozen or inaccessible for parameter modification. In contrast,\nprompt optimization is a viable alternative to RLHF for LLM alignment. While\nthe existing literature has shown empirical promise of prompt optimization, its\ntheoretical underpinning remains under-explored. We address this gap by\nformulating prompt optimization as an optimization problem and try to provide\ntheoretical insights into the optimality of such a framework. To analyze the\nperformance of the prompt optimization, we study theoretical suboptimality\nbounds and provide insights in terms of how prompt optimization depends upon\nthe given prompter and target model. We also provide empirical validation\nthrough experiments on various datasets, demonstrating that prompt optimization\ncan effectively align LLMs, even when parameter fine-tuning is not feasible."
                },
                "authors": [
                    {
                        "name": "Prashant Trivedi"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Avinash Reddy"
                    },
                    {
                        "name": "Vaneet Aggarwal"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "George K. Atia"
                    }
                ],
                "author_detail": {
                    "name": "George K. Atia"
                },
                "author": "George K. Atia",
                "arxiv_comment": "27 pages, Accepted in AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03479v1",
                "updated": "2025-01-07T02:47:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    2,
                    47,
                    59,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T02:47:59Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    2,
                    47,
                    59,
                    1,
                    7,
                    0
                ],
                "title": "Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia\n  Reveal about the Socio-Cultural Norms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia\n  Reveal about the Socio-Cultural Norms"
                },
                "summary": "Honorifics serve as powerful linguistic markers that reflect social\nhierarchies and cultural values. This paper presents a large-scale,\ncross-linguistic exploration of usage of honorific pronouns in Bengali and\nHindi Wikipedia articles, shedding light on how socio-cultural factors shape\nlanguage. Using LLM (GPT-4o), we annotated 10, 000 articles of real and\nfictional beings in each language for several sociodemographic features such as\ngender, age, fame, and exoticness, and the use of honorifics. We find that\nacross all feature combinations, use of honorifics is consistently more common\nin Bengali than Hindi. For both languages, the use non-honorific pronouns is\nmore commonly observed for infamous, juvenile, and exotic beings. Notably, we\nobserve a gender bias in use of honorifics in Hindi, with men being more\ncommonly referred to with honorifics than women.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Honorifics serve as powerful linguistic markers that reflect social\nhierarchies and cultural values. This paper presents a large-scale,\ncross-linguistic exploration of usage of honorific pronouns in Bengali and\nHindi Wikipedia articles, shedding light on how socio-cultural factors shape\nlanguage. Using LLM (GPT-4o), we annotated 10, 000 articles of real and\nfictional beings in each language for several sociodemographic features such as\ngender, age, fame, and exoticness, and the use of honorifics. We find that\nacross all feature combinations, use of honorifics is consistently more common\nin Bengali than Hindi. For both languages, the use non-honorific pronouns is\nmore commonly observed for infamous, juvenile, and exotic beings. Notably, we\nobserve a gender bias in use of honorifics in Hindi, with men being more\ncommonly referred to with honorifics than women."
                },
                "authors": [
                    {
                        "name": "Sourabrata Mukherjee"
                    },
                    {
                        "name": "Soumya Teotia"
                    },
                    {
                        "name": "Sougata Saha"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03475v1",
                "updated": "2025-01-07T02:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    2,
                    33,
                    25,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T02:33:25Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    2,
                    33,
                    25,
                    1,
                    7,
                    0
                ],
                "title": "Reading with Intent -- Neutralizing Intent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reading with Intent -- Neutralizing Intent"
                },
                "summary": "Queries to large language models (LLMs) can be divided into two parts: the\ninstruction/question and the accompanying context. The context for\nretrieval-augmented generation (RAG) systems in most benchmarks comes from\nWikipedia or Wikipedia-like texts which are written in a neutral and factual\ntone. However, when RAG systems retrieve internet-based content, they encounter\ntext with diverse tones and linguistic styles, introducing challenges for\ndownstream tasks. The Reading with Intent task addresses this issue by\nevaluating how varying tones in context passages affect model performance.\nBuilding on prior work that focused on sarcasm, we extend this paradigm by\nconstructing a dataset where context passages are transformed to $11$ distinct\nemotions using a better synthetic data generation approach. Using this dataset,\nwe train an emotion translation model to systematically adapt passages to\nspecified emotional tones. The human evaluation shows that the LLM fine-tuned\nto become the emotion-translator benefited from the synthetically generated\ndata. Finally, the emotion-translator is used in the Reading with Intent task\nto transform the passages to a neutral tone. By neutralizing the passages, it\nmitigates the challenges posed by sarcastic passages and improves overall\nresults on this task by about $3\\%$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queries to large language models (LLMs) can be divided into two parts: the\ninstruction/question and the accompanying context. The context for\nretrieval-augmented generation (RAG) systems in most benchmarks comes from\nWikipedia or Wikipedia-like texts which are written in a neutral and factual\ntone. However, when RAG systems retrieve internet-based content, they encounter\ntext with diverse tones and linguistic styles, introducing challenges for\ndownstream tasks. The Reading with Intent task addresses this issue by\nevaluating how varying tones in context passages affect model performance.\nBuilding on prior work that focused on sarcasm, we extend this paradigm by\nconstructing a dataset where context passages are transformed to $11$ distinct\nemotions using a better synthetic data generation approach. Using this dataset,\nwe train an emotion translation model to systematically adapt passages to\nspecified emotional tones. The human evaluation shows that the LLM fine-tuned\nto become the emotion-translator benefited from the synthetically generated\ndata. Finally, the emotion-translator is used in the Reading with Intent task\nto transform the passages to a neutral tone. By neutralizing the passages, it\nmitigates the challenges posed by sarcastic passages and improves overall\nresults on this task by about $3\\%$."
                },
                "authors": [
                    {
                        "name": "Benjamin Reichman"
                    },
                    {
                        "name": "Adar Avsian"
                    },
                    {
                        "name": "Larry Heck"
                    }
                ],
                "author_detail": {
                    "name": "Larry Heck"
                },
                "author": "Larry Heck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03471v1",
                "updated": "2025-01-07T02:15:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    2,
                    15,
                    58,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T02:15:58Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    2,
                    15,
                    58,
                    1,
                    7,
                    0
                ],
                "title": "Hyperbolic Binary Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperbolic Binary Neural Network"
                },
                "summary": "Binary Neural Network (BNN) converts full-precision weights and activations\ninto their extreme 1-bit counterparts, making it particularly suitable for\ndeployment on lightweight mobile devices. While binary neural networks are\ntypically formulated as a constrained optimization problem and optimized in the\nbinarized space, general neural networks are formulated as an unconstrained\noptimization problem and optimized in the continuous space. This paper\nintroduces the Hyperbolic Binary Neural Network (HBNN) by leveraging the\nframework of hyperbolic geometry to optimize the constrained problem.\nSpecifically, we transform the constrained problem in hyperbolic space into an\nunconstrained one in Euclidean space using the Riemannian exponential map. On\nthe other hand, we also propose the Exponential Parametrization Cluster (EPC)\nmethod, which, compared to the Riemannian exponential map, shrinks the segment\ndomain based on a diffeomorphism. This approach increases the probability of\nweight flips, thereby maximizing the information gain in BNNs. Experimental\nresults on CIFAR10, CIFAR100, and ImageNet classification datasets with\nVGGsmall, ResNet18, and ResNet34 models illustrate the superior performance of\nour HBNN over state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary Neural Network (BNN) converts full-precision weights and activations\ninto their extreme 1-bit counterparts, making it particularly suitable for\ndeployment on lightweight mobile devices. While binary neural networks are\ntypically formulated as a constrained optimization problem and optimized in the\nbinarized space, general neural networks are formulated as an unconstrained\noptimization problem and optimized in the continuous space. This paper\nintroduces the Hyperbolic Binary Neural Network (HBNN) by leveraging the\nframework of hyperbolic geometry to optimize the constrained problem.\nSpecifically, we transform the constrained problem in hyperbolic space into an\nunconstrained one in Euclidean space using the Riemannian exponential map. On\nthe other hand, we also propose the Exponential Parametrization Cluster (EPC)\nmethod, which, compared to the Riemannian exponential map, shrinks the segment\ndomain based on a diffeomorphism. This approach increases the probability of\nweight flips, thereby maximizing the information gain in BNNs. Experimental\nresults on CIFAR10, CIFAR100, and ImageNet classification datasets with\nVGGsmall, ResNet18, and ResNet34 models illustrate the superior performance of\nour HBNN over state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Jingyang Xiang"
                    },
                    {
                        "name": "Tianxin Huang"
                    },
                    {
                        "name": "Xiangrui Zhao"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_journal_ref": "IEEE Transactions on Neural Networks and Learning Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01973v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01973v2",
                "updated": "2025-01-07T02:10:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    2,
                    10,
                    45,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-28T02:28:19Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    2,
                    28,
                    19,
                    5,
                    363,
                    0
                ],
                "title": "INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models"
                },
                "summary": "The rapid development of large language models (LLMs) and large vision models\n(LVMs) have propelled the evolution of multi-modal AI systems, which have\ndemonstrated the remarkable potential for industrial applications by emulating\nhuman-like cognition. However, they also pose significant ethical challenges,\nincluding amplifying harmful content and reinforcing societal biases. For\ninstance, biases in some industrial image generation models highlighted the\nurgent need for robust fairness assessments. Most existing evaluation\nframeworks focus on the comprehensiveness of various aspects of the models, but\nthey exhibit critical limitations, including insufficient attention to content\ngeneration alignment and social bias-sensitive domains. More importantly, their\nreliance on pixel-detection techniques is prone to inaccuracies.\n  To address these issues, this paper presents INFELM, an in-depth fairness\nevaluation on widely-used text-to-image models. Our key contributions are: (1)\nan advanced skintone classifier incorporating facial topology and refined skin\npixel representation to enhance classification precision by at least 16.04%,\n(2) a bias-sensitive content alignment measurement for understanding societal\nimpacts, (3) a generalizable representation bias evaluation for diverse\ndemographic groups, and (4) extensive experiments analyzing large-scale\ntext-to-image model outputs across six social-bias-sensitive domains. We find\nthat existing models in the study generally do not meet the empirical fairness\ncriteria, and representation bias is generally more pronounced than alignment\nerrors. INFELM establishes a robust benchmark for fairness assessment,\nsupporting the development of multi-modal AI systems that align with ethical\nand human-centric principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) and large vision models\n(LVMs) have propelled the evolution of multi-modal AI systems, which have\ndemonstrated the remarkable potential for industrial applications by emulating\nhuman-like cognition. However, they also pose significant ethical challenges,\nincluding amplifying harmful content and reinforcing societal biases. For\ninstance, biases in some industrial image generation models highlighted the\nurgent need for robust fairness assessments. Most existing evaluation\nframeworks focus on the comprehensiveness of various aspects of the models, but\nthey exhibit critical limitations, including insufficient attention to content\ngeneration alignment and social bias-sensitive domains. More importantly, their\nreliance on pixel-detection techniques is prone to inaccuracies.\n  To address these issues, this paper presents INFELM, an in-depth fairness\nevaluation on widely-used text-to-image models. Our key contributions are: (1)\nan advanced skintone classifier incorporating facial topology and refined skin\npixel representation to enhance classification precision by at least 16.04%,\n(2) a bias-sensitive content alignment measurement for understanding societal\nimpacts, (3) a generalizable representation bias evaluation for diverse\ndemographic groups, and (4) extensive experiments analyzing large-scale\ntext-to-image model outputs across six social-bias-sensitive domains. We find\nthat existing models in the study generally do not meet the empirical fairness\ncriteria, and representation bias is generally more pronounced than alignment\nerrors. INFELM establishes a robust benchmark for fairness assessment,\nsupporting the development of multi-modal AI systems that align with ethical\nand human-centric principles."
                },
                "authors": [
                    {
                        "name": "Di Jin"
                    },
                    {
                        "name": "Xing Liu"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Jia Qing Yap"
                    },
                    {
                        "name": "Andrea Wong"
                    },
                    {
                        "name": "Adriana Crespo"
                    },
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Zhiyuan Yin"
                    },
                    {
                        "name": "Qiang Yan"
                    },
                    {
                        "name": "Ryan Ye"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Ye"
                },
                "author": "Ryan Ye",
                "arxiv_comment": "Di Jin and Xing Liu contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01973v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01973v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03468v1",
                "updated": "2025-01-07T01:52:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    1,
                    52,
                    56,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T01:52:56Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    1,
                    52,
                    56,
                    1,
                    7,
                    0
                ],
                "title": "MTRAG: A Multi-Turn Conversational Benchmark for Evaluating\n  Retrieval-Augmented Generation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTRAG: A Multi-Turn Conversational Benchmark for Evaluating\n  Retrieval-Augmented Generation Systems"
                },
                "summary": "Retrieval-augmented generation (RAG) has recently become a very popular task\nfor Large Language Models (LLMs). Evaluating them on multi-turn RAG\nconversations, where the system is asked to generate a response to a question\nin the context of a preceding conversation is an important and often overlooked\ntask with several additional challenges. We present MTRAG: an end-to-end\nhuman-generated multi-turn RAG benchmark that reflects several real-world\nproperties across diverse dimensions for evaluating the full RAG pipeline.\nMTRAG contains 110 conversations averaging 7.7 turns each across four domains\nfor a total of 842 tasks. We also explore automation paths via synthetic data\nand LLM-as-a-Judge evaluation. Our human and automatic evaluations show that\neven state-of-the-art LLM RAG systems struggle on MTRAG. We demonstrate the\nneed for strong retrieval and generation systems that can handle later turns,\nunanswerable questions, non-standalone questions, and multiple domains. MTRAG\nis available at https://github.com/ibm/mt-rag-benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has recently become a very popular task\nfor Large Language Models (LLMs). Evaluating them on multi-turn RAG\nconversations, where the system is asked to generate a response to a question\nin the context of a preceding conversation is an important and often overlooked\ntask with several additional challenges. We present MTRAG: an end-to-end\nhuman-generated multi-turn RAG benchmark that reflects several real-world\nproperties across diverse dimensions for evaluating the full RAG pipeline.\nMTRAG contains 110 conversations averaging 7.7 turns each across four domains\nfor a total of 842 tasks. We also explore automation paths via synthetic data\nand LLM-as-a-Judge evaluation. Our human and automatic evaluations show that\neven state-of-the-art LLM RAG systems struggle on MTRAG. We demonstrate the\nneed for strong retrieval and generation systems that can handle later turns,\nunanswerable questions, non-standalone questions, and multiple domains. MTRAG\nis available at https://github.com/ibm/mt-rag-benchmark."
                },
                "authors": [
                    {
                        "name": "Yannis Katsis"
                    },
                    {
                        "name": "Sara Rosenthal"
                    },
                    {
                        "name": "Kshitij Fadnis"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Young-Suk Lee"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Huaiyu Zhu"
                    },
                    {
                        "name": "Danish Contractor"
                    },
                    {
                        "name": "Marina Danilevsky"
                    }
                ],
                "author_detail": {
                    "name": "Marina Danilevsky"
                },
                "author": "Marina Danilevsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19139v2",
                "updated": "2025-01-07T01:50:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    1,
                    50,
                    11,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-26T09:51:05Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    9,
                    51,
                    5,
                    3,
                    361,
                    0
                ],
                "title": "PlanLLM: Video Procedure Planning with Refinable Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlanLLM: Video Procedure Planning with Refinable Large Language Models"
                },
                "summary": "Video procedure planning, i.e., planning a sequence of action steps given the\nvideo frames of start and goal states, is an essential ability for embodied AI.\nRecent works utilize Large Language Models (LLMs) to generate enriched action\nstep description texts to guide action step decoding. Although LLMs are\nintroduced, these methods decode the action steps into a closed-set of one-hot\nvectors, limiting the model's capability of generalizing to new steps or tasks.\nAdditionally, fixed action step descriptions based on world-level commonsense\nmay contain noise in specific instances of visual states. In this paper, we\npropose PlanLLM, a cross-modal joint learning framework with LLMs for video\nprocedure planning. We propose an LLM-Enhanced Planning module which fully uses\nthe generalization ability of LLMs to produce free-form planning output and to\nenhance action step decoding. We also propose Mutual Information Maximization\nmodule to connect world-level commonsense of step descriptions and\nsample-specific information of visual states, enabling LLMs to employ the\nreasoning ability to generate step sequences. With the assistance of LLMs, our\nmethod can both closed-set and open vocabulary procedure planning tasks. Our\nPlanLLM achieves superior performance on three benchmarks, demonstrating the\neffectiveness of our designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video procedure planning, i.e., planning a sequence of action steps given the\nvideo frames of start and goal states, is an essential ability for embodied AI.\nRecent works utilize Large Language Models (LLMs) to generate enriched action\nstep description texts to guide action step decoding. Although LLMs are\nintroduced, these methods decode the action steps into a closed-set of one-hot\nvectors, limiting the model's capability of generalizing to new steps or tasks.\nAdditionally, fixed action step descriptions based on world-level commonsense\nmay contain noise in specific instances of visual states. In this paper, we\npropose PlanLLM, a cross-modal joint learning framework with LLMs for video\nprocedure planning. We propose an LLM-Enhanced Planning module which fully uses\nthe generalization ability of LLMs to produce free-form planning output and to\nenhance action step decoding. We also propose Mutual Information Maximization\nmodule to connect world-level commonsense of step descriptions and\nsample-specific information of visual states, enabling LLMs to employ the\nreasoning ability to generate step sequences. With the assistance of LLMs, our\nmethod can both closed-set and open vocabulary procedure planning tasks. Our\nPlanLLM achieves superior performance on three benchmarks, demonstrating the\neffectiveness of our designs."
                },
                "authors": [
                    {
                        "name": "Dejie Yang"
                    },
                    {
                        "name": "Zijing Zhao"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03465v1",
                "updated": "2025-01-07T01:47:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    1,
                    47,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T01:47:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    1,
                    47,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Extending Internet Access Over LoRa for Internet of Things and Critical\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending Internet Access Over LoRa for Internet of Things and Critical\n  Applications"
                },
                "summary": "LoRa bridges the gap between remote locations and mainstream networks,\nenabling large-scale Internet of Things (IoT) deployments. Despite the recent\nadvancements around LoRa, Internet access over this technology is still largely\nunexplored. Most existing solutions only handle packets within the local LoRa\nnetwork and do not interact with web applications. This limits the scalability\nand the ability to deliver essential web services in disconnected regions. This\nwork proposes and implements ILoRa to extend the public Internet to\ndisconnected areas for essential service delivery. ILoRa enables accessing\nApplication Programming Interfaces (APIs) and web pages on the Internet over a\nLoRa backbone network. It comprises a ILoRa coordinator code (ICN) and access\npoint nodes (APNs). The ICN interfaces the LoRa network with the public\nInternet and interprets content. The APN tethers a WiFi hotspot to which\ndevices connect and access the web content. This work further proposes data\nhandling methods for ICNs and APNs. An actual hardware-based implementation\nvalidates the proposed system. The implementation achieves a throughput of 1.06\nkbps tested for an Internet-based API returning JSON data of 930 B.\nFurthermore, the APN consumed approximately $0.162$A current, and the resource\nutilization on the ICN was minimal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRa bridges the gap between remote locations and mainstream networks,\nenabling large-scale Internet of Things (IoT) deployments. Despite the recent\nadvancements around LoRa, Internet access over this technology is still largely\nunexplored. Most existing solutions only handle packets within the local LoRa\nnetwork and do not interact with web applications. This limits the scalability\nand the ability to deliver essential web services in disconnected regions. This\nwork proposes and implements ILoRa to extend the public Internet to\ndisconnected areas for essential service delivery. ILoRa enables accessing\nApplication Programming Interfaces (APIs) and web pages on the Internet over a\nLoRa backbone network. It comprises a ILoRa coordinator code (ICN) and access\npoint nodes (APNs). The ICN interfaces the LoRa network with the public\nInternet and interprets content. The APN tethers a WiFi hotspot to which\ndevices connect and access the web content. This work further proposes data\nhandling methods for ICNs and APNs. An actual hardware-based implementation\nvalidates the proposed system. The implementation achieves a throughput of 1.06\nkbps tested for an Internet-based API returning JSON data of 930 B.\nFurthermore, the APN consumed approximately $0.162$A current, and the resource\nutilization on the ICN was minimal."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Devadeep Misra"
                    },
                    {
                        "name": "Hirdesh Mewada"
                    }
                ],
                "author_detail": {
                    "name": "Hirdesh Mewada"
                },
                "author": "Hirdesh Mewada",
                "arxiv_comment": "8 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03462v1",
                "updated": "2025-01-07T01:45:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    1,
                    45,
                    2,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T01:45:02Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    1,
                    45,
                    2,
                    1,
                    7,
                    0
                ],
                "title": "ISSR: Iterative Selection with Self-Review for Vocabulary Test\n  Distractor Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISSR: Iterative Selection with Self-Review for Vocabulary Test\n  Distractor Generation"
                },
                "summary": "Vocabulary acquisition is essential to second language learning, as it\nunderpins all core language skills. Accurate vocabulary assessment is\nparticularly important in standardized exams, where test items evaluate\nlearners' comprehension and contextual use of words. Previous research has\nexplored methods for generating distractors to aid in the design of English\nvocabulary tests. However, current approaches often rely on lexical databases\nor predefined rules, and frequently produce distractors that risk invalidating\nthe question by introducing multiple correct options. In this study, we focus\non English vocabulary questions from Taiwan's university entrance exams. We\nanalyze student response distributions to gain insights into the\ncharacteristics of these test items and provide a reference for future\nresearch. Additionally, we identify key limitations in how large language\nmodels (LLMs) support teachers in generating distractors for vocabulary test\ndesign. To address these challenges, we propose the iterative selection with\nself-review (ISSR) framework, which makes use of a novel LLM-based self-review\nmechanism to ensure that the distractors remain valid while offering diverse\noptions. Experimental results show that ISSR achieves promising performance in\ngenerating plausible distractors, and the self-review mechanism effectively\nfilters out distractors that could invalidate the question.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vocabulary acquisition is essential to second language learning, as it\nunderpins all core language skills. Accurate vocabulary assessment is\nparticularly important in standardized exams, where test items evaluate\nlearners' comprehension and contextual use of words. Previous research has\nexplored methods for generating distractors to aid in the design of English\nvocabulary tests. However, current approaches often rely on lexical databases\nor predefined rules, and frequently produce distractors that risk invalidating\nthe question by introducing multiple correct options. In this study, we focus\non English vocabulary questions from Taiwan's university entrance exams. We\nanalyze student response distributions to gain insights into the\ncharacteristics of these test items and provide a reference for future\nresearch. Additionally, we identify key limitations in how large language\nmodels (LLMs) support teachers in generating distractors for vocabulary test\ndesign. To address these challenges, we propose the iterative selection with\nself-review (ISSR) framework, which makes use of a novel LLM-based self-review\nmechanism to ensure that the distractors remain valid while offering diverse\noptions. Experimental results show that ISSR achieves promising performance in\ngenerating plausible distractors, and the self-review mechanism effectively\nfilters out distractors that could invalidate the question."
                },
                "authors": [
                    {
                        "name": "Yu-Cheng Liu"
                    },
                    {
                        "name": "An-Zi Yen"
                    }
                ],
                "author_detail": {
                    "name": "An-Zi Yen"
                },
                "author": "An-Zi Yen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22376v2",
                "updated": "2025-01-07T01:41:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    1,
                    41,
                    13,
                    1,
                    7,
                    0
                ],
                "published": "2024-10-29T07:43:39Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    7,
                    43,
                    39,
                    1,
                    303,
                    0
                ],
                "title": "Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion\n  Models on Rare Concepts with LLM Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion\n  Models on Rare Concepts with LLM Guidance"
                },
                "summary": "State-of-the-art text-to-image (T2I) diffusion models often struggle to\ngenerate rare compositions of concepts, e.g., objects with unusual attributes.\nIn this paper, we show that the compositional generation power of diffusion\nmodels on such rare concepts can be significantly enhanced by the Large\nLanguage Model (LLM) guidance. We start with empirical and theoretical\nanalysis, demonstrating that exposing frequent concepts relevant to the target\nrare concepts during the diffusion sampling process yields more accurate\nconcept composition. Based on this, we propose a training-free approach, R2F,\nthat plans and executes the overall rare-to-frequent concept guidance\nthroughout the diffusion inference by leveraging the abundant semantic\nknowledge in LLMs. Our framework is flexible across any pre-trained diffusion\nmodels and LLMs, and can be seamlessly integrated with the region-guided\ndiffusion approaches. Extensive experiments on three datasets, including our\nnewly proposed benchmark, RareBench, containing various prompts with rare\ncompositions of concepts, R2F significantly surpasses existing models including\nSD3.0 and FLUX by up to 28.1%p in T2I alignment. Code is available at\nhttps://github.com/krafton-ai/Rare-to-Frequent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art text-to-image (T2I) diffusion models often struggle to\ngenerate rare compositions of concepts, e.g., objects with unusual attributes.\nIn this paper, we show that the compositional generation power of diffusion\nmodels on such rare concepts can be significantly enhanced by the Large\nLanguage Model (LLM) guidance. We start with empirical and theoretical\nanalysis, demonstrating that exposing frequent concepts relevant to the target\nrare concepts during the diffusion sampling process yields more accurate\nconcept composition. Based on this, we propose a training-free approach, R2F,\nthat plans and executes the overall rare-to-frequent concept guidance\nthroughout the diffusion inference by leveraging the abundant semantic\nknowledge in LLMs. Our framework is flexible across any pre-trained diffusion\nmodels and LLMs, and can be seamlessly integrated with the region-guided\ndiffusion approaches. Extensive experiments on three datasets, including our\nnewly proposed benchmark, RareBench, containing various prompts with rare\ncompositions of concepts, R2F significantly surpasses existing models including\nSD3.0 and FLUX by up to 28.1%p in T2I alignment. Code is available at\nhttps://github.com/krafton-ai/Rare-to-Frequent."
                },
                "authors": [
                    {
                        "name": "Dongmin Park"
                    },
                    {
                        "name": "Sebin Kim"
                    },
                    {
                        "name": "Taehong Moon"
                    },
                    {
                        "name": "Minkyu Kim"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Jaewoong Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoong Cho"
                },
                "author": "Jaewoong Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03458v1",
                "updated": "2025-01-07T01:19:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    1,
                    19,
                    48,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T01:19:48Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    1,
                    19,
                    48,
                    1,
                    7,
                    0
                ],
                "title": "Activating Associative Disease-Aware Vision Token Memory for LLM-Based\n  X-ray Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activating Associative Disease-Aware Vision Token Memory for LLM-Based\n  X-ray Report Generation"
                },
                "summary": "X-ray image based medical report generation achieves significant progress in\nrecent years with the help of the large language model, however, these models\nhave not fully exploited the effective information in visual image regions,\nresulting in reports that are linguistically sound but insufficient in\ndescribing key diseases. In this paper, we propose a novel associative\nmemory-enhanced X-ray report generation model that effectively mimics the\nprocess of professional doctors writing medical reports. It considers both the\nmining of global and local visual information and associates historical report\ninformation to better complete the writing of the current report. Specifically,\ngiven an X-ray image, we first utilize a classification model along with its\nactivation maps to accomplish the mining of visual regions highly associated\nwith diseases and the learning of disease query tokens. Then, we employ a\nvisual Hopfield network to establish memory associations for disease-related\ntokens, and a report Hopfield network to retrieve report memory information.\nThis process facilitates the generation of high-quality reports based on a\nlarge language model and achieves state-of-the-art performance on multiple\nbenchmark datasets, including the IU X-ray, MIMIC-CXR, and Chexpert Plus. The\nsource code of this work is released on\n\\url{https://github.com/Event-AHU/Medical_Image_Analysis}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray image based medical report generation achieves significant progress in\nrecent years with the help of the large language model, however, these models\nhave not fully exploited the effective information in visual image regions,\nresulting in reports that are linguistically sound but insufficient in\ndescribing key diseases. In this paper, we propose a novel associative\nmemory-enhanced X-ray report generation model that effectively mimics the\nprocess of professional doctors writing medical reports. It considers both the\nmining of global and local visual information and associates historical report\ninformation to better complete the writing of the current report. Specifically,\ngiven an X-ray image, we first utilize a classification model along with its\nactivation maps to accomplish the mining of visual regions highly associated\nwith diseases and the learning of disease query tokens. Then, we employ a\nvisual Hopfield network to establish memory associations for disease-related\ntokens, and a report Hopfield network to retrieve report memory information.\nThis process facilitates the generation of high-quality reports based on a\nlarge language model and achieves state-of-the-art performance on multiple\nbenchmark datasets, including the IU X-ray, MIMIC-CXR, and Chexpert Plus. The\nsource code of this work is released on\n\\url{https://github.com/Event-AHU/Medical_Image_Analysis}."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Fuling Wang"
                    },
                    {
                        "name": "Haowen Wang"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Chuanfu Li"
                    },
                    {
                        "name": "Yaowei Wang"
                    },
                    {
                        "name": "Yonghong Tian"
                    },
                    {
                        "name": "Jin Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jin Tang"
                },
                "author": "Jin Tang",
                "arxiv_comment": "In Peer Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03447v1",
                "updated": "2025-01-07T00:24:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    0,
                    24,
                    7,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T00:24:07Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    0,
                    24,
                    7,
                    1,
                    7,
                    0
                ],
                "title": "CoReQA: Uncovering Potentials of Language Models in Code Repository\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoReQA: Uncovering Potentials of Language Models in Code Repository\n  Question Answering"
                },
                "summary": "Large language models that enhance software development tasks, such as code\ngeneration, code completion, and code question answering (QA), have been\nextensively studied in both academia and the industry. The models are\nintegrated into popular intelligent IDEs like JetBrains and Cursor. Current\nbenchmarks for evaluating models' code comprehension capabilities primarily\nfocus on code generation or completion, often neglecting QA, which is a crucial\naspect of understanding code. Existing code QA benchmarks are derived from code\ncomments with predefined patterns (e.g., CodeQA) or focus on specific domains,\nsuch as education (e.g., CS1QA). These benchmarks fail to capture the\nreal-world complexity of software engineering and user requirements for\nunderstanding code repositories. To address this gap, we introduce CoReQA, a\nbenchmark for Code Repository-level question answering, constructed from GitHub\nissues and comments from 176 popular repositories across four programming\nlanguages. Since questions and answers may include both natural language and\ncode snippets, traditional evaluation metrics such as BLEU are inadequate for\nassessing repository-level QA performance. Thus, we provide an LLM-as-a-judge\nframework to evaluate QA performance from five aspects. Based on CoReQA, we\nevaluate the performance of three baselines, including two short-context models\nusing generic retrieval strategies and one long-context model that utilizes the\nentire repository context. Evaluation results show that state-of-the-art\nproprietary and long-context models struggle to address repository-level\nquestions effectively. Our analysis highlights the limitations of language\nmodels in assisting developers in understanding repositories and suggests\nfuture directions for improving repository comprehension systems through\neffective context retrieval methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models that enhance software development tasks, such as code\ngeneration, code completion, and code question answering (QA), have been\nextensively studied in both academia and the industry. The models are\nintegrated into popular intelligent IDEs like JetBrains and Cursor. Current\nbenchmarks for evaluating models' code comprehension capabilities primarily\nfocus on code generation or completion, often neglecting QA, which is a crucial\naspect of understanding code. Existing code QA benchmarks are derived from code\ncomments with predefined patterns (e.g., CodeQA) or focus on specific domains,\nsuch as education (e.g., CS1QA). These benchmarks fail to capture the\nreal-world complexity of software engineering and user requirements for\nunderstanding code repositories. To address this gap, we introduce CoReQA, a\nbenchmark for Code Repository-level question answering, constructed from GitHub\nissues and comments from 176 popular repositories across four programming\nlanguages. Since questions and answers may include both natural language and\ncode snippets, traditional evaluation metrics such as BLEU are inadequate for\nassessing repository-level QA performance. Thus, we provide an LLM-as-a-judge\nframework to evaluate QA performance from five aspects. Based on CoReQA, we\nevaluate the performance of three baselines, including two short-context models\nusing generic retrieval strategies and one long-context model that utilizes the\nentire repository context. Evaluation results show that state-of-the-art\nproprietary and long-context models struggle to address repository-level\nquestions effectively. Our analysis highlights the limitations of language\nmodels in assisting developers in understanding repositories and suggests\nfuture directions for improving repository comprehension systems through\neffective context retrieval methodologies."
                },
                "authors": [
                    {
                        "name": "Jialiang Chen"
                    },
                    {
                        "name": "Kaifa Zhao"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Jierui Liu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Ping Yang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "12 pages, 7 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03446v1",
                "updated": "2025-01-07T00:21:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    0,
                    21,
                    42,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T00:21:42Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    0,
                    21,
                    42,
                    1,
                    7,
                    0
                ],
                "title": "LLM4CVE: Enabling Iterative Automated Vulnerability Repair with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4CVE: Enabling Iterative Automated Vulnerability Repair with Large\n  Language Models"
                },
                "summary": "Software vulnerabilities continue to be ubiquitous, even in the era of\nAI-powered code assistants, advanced static analysis tools, and the adoption of\nextensive testing frameworks. It has become apparent that we must not simply\nprevent these bugs, but also eliminate them in a quick, efficient manner. Yet,\nhuman code intervention is slow, costly, and can often lead to further security\nvulnerabilities, especially in legacy codebases. The advent of highly advanced\nLarge Language Models (LLM) has opened up the possibility for many software\ndefects to be patched automatically. We propose LLM4CVE an LLM-based iterative\npipeline that robustly fixes vulnerable functions in real-world code with high\naccuracy. We examine our pipeline with State-of-the-Art LLMs, such as GPT-3.5,\nGPT-4o, Llama 38B, and Llama 3 70B. We achieve a human-verified quality score\nof 8.51/10 and an increase in groundtruth code similarity of 20% with Llama 3\n70B. To promote further research in the area of LLM-based vulnerability repair,\nwe publish our testing apparatus, fine-tuned weights, and experimental data on\nour website",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software vulnerabilities continue to be ubiquitous, even in the era of\nAI-powered code assistants, advanced static analysis tools, and the adoption of\nextensive testing frameworks. It has become apparent that we must not simply\nprevent these bugs, but also eliminate them in a quick, efficient manner. Yet,\nhuman code intervention is slow, costly, and can often lead to further security\nvulnerabilities, especially in legacy codebases. The advent of highly advanced\nLarge Language Models (LLM) has opened up the possibility for many software\ndefects to be patched automatically. We propose LLM4CVE an LLM-based iterative\npipeline that robustly fixes vulnerable functions in real-world code with high\naccuracy. We examine our pipeline with State-of-the-Art LLMs, such as GPT-3.5,\nGPT-4o, Llama 38B, and Llama 3 70B. We achieve a human-verified quality score\nof 8.51/10 and an increase in groundtruth code similarity of 20% with Llama 3\n70B. To promote further research in the area of LLM-based vulnerability repair,\nwe publish our testing apparatus, fine-tuned weights, and experimental data on\nour website"
                },
                "authors": [
                    {
                        "name": "Mohamad Fakih"
                    },
                    {
                        "name": "Rahul Dharmaji"
                    },
                    {
                        "name": "Halima Bouzidi"
                    },
                    {
                        "name": "Gustavo Quiros Araya"
                    },
                    {
                        "name": "Oluwatosin Ogundare"
                    },
                    {
                        "name": "Mohammad Abdullah Al Faruque"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Abdullah Al Faruque"
                },
                "author": "Mohammad Abdullah Al Faruque",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00826v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00826v2",
                "updated": "2025-01-07T00:15:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    0,
                    15,
                    11,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T13:08:17Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    8,
                    17,
                    2,
                    1,
                    0
                ],
                "title": "LLM-Powered Multi-Agent System for Automated Crypto Portfolio Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered Multi-Agent System for Automated Crypto Portfolio Management"
                },
                "summary": "Cryptocurrency investment is inherently difficult due to its shorter history\ncompared to traditional assets, the need to integrate vast amounts of data from\nvarious modalities, and the requirement for complex reasoning. While deep\nlearning approaches have been applied to address these challenges, their\nblack-box nature raises concerns about trust and explainability. Recently,\nlarge language models (LLMs) have shown promise in financial applications due\nto their ability to understand multi-modal data and generate explainable\ndecisions. However, single LLM faces limitations in complex, comprehensive\ntasks such as asset investment. These limitations are even more pronounced in\ncryptocurrency investment, where LLMs have less domain-specific knowledge in\ntheir training corpora.\n  To overcome these challenges, we propose an explainable, multi-modal,\nmulti-agent framework for cryptocurrency investment. Our framework uses\nspecialized agents that collaborate within and across teams to handle subtasks\nsuch as data analysis, literature integration, and investment decision-making\nfor the top 30 cryptocurrencies by market capitalization. The expert training\nmodule fine-tunes agents using multi-modal historical data and professional\ninvestment literature, while the multi-agent investment module employs\nreal-time data to make informed cryptocurrency investment decisions. Unique\nintrateam and interteam collaboration mechanisms enhance prediction accuracy by\nadjusting final predictions based on confidence levels within agent teams and\nfacilitating information sharing between teams. Empirical evaluation using data\nfrom November 2023 to September 2024 demonstrates that our framework\noutperforms single-agent models and market benchmarks in classification, asset\npricing, portfolio, and explainability performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptocurrency investment is inherently difficult due to its shorter history\ncompared to traditional assets, the need to integrate vast amounts of data from\nvarious modalities, and the requirement for complex reasoning. While deep\nlearning approaches have been applied to address these challenges, their\nblack-box nature raises concerns about trust and explainability. Recently,\nlarge language models (LLMs) have shown promise in financial applications due\nto their ability to understand multi-modal data and generate explainable\ndecisions. However, single LLM faces limitations in complex, comprehensive\ntasks such as asset investment. These limitations are even more pronounced in\ncryptocurrency investment, where LLMs have less domain-specific knowledge in\ntheir training corpora.\n  To overcome these challenges, we propose an explainable, multi-modal,\nmulti-agent framework for cryptocurrency investment. Our framework uses\nspecialized agents that collaborate within and across teams to handle subtasks\nsuch as data analysis, literature integration, and investment decision-making\nfor the top 30 cryptocurrencies by market capitalization. The expert training\nmodule fine-tunes agents using multi-modal historical data and professional\ninvestment literature, while the multi-agent investment module employs\nreal-time data to make informed cryptocurrency investment decisions. Unique\nintrateam and interteam collaboration mechanisms enhance prediction accuracy by\nadjusting final predictions based on confidence levels within agent teams and\nfacilitating information sharing between teams. Empirical evaluation using data\nfrom November 2023 to September 2024 demonstrates that our framework\noutperforms single-agent models and market benchmarks in classification, asset\npricing, portfolio, and explainability performance."
                },
                "authors": [
                    {
                        "name": "Yichen Luo"
                    },
                    {
                        "name": "Yebo Feng"
                    },
                    {
                        "name": "Jiahua Xu"
                    },
                    {
                        "name": "Paolo Tasca"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00826v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00826v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03441v1",
                "updated": "2025-01-07T00:07:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    0,
                    7,
                    1,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T00:07:01Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    0,
                    7,
                    1,
                    1,
                    7,
                    0
                ],
                "title": "Finding A Voice: Evaluating African American Dialect Generation for\n  Chatbot Technology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding A Voice: Evaluating African American Dialect Generation for\n  Chatbot Technology"
                },
                "summary": "As chatbots become increasingly integrated into everyday tasks, designing\nsystems that accommodate diverse user populations is crucial for fostering\ntrust, engagement, and inclusivity. This study investigates the ability of\ncontemporary Large Language Models (LLMs) to generate African American\nVernacular English (AAVE) and evaluates the impact of AAVE usage on user\nexperiences in chatbot applications. We analyze the performance of three LLM\nfamilies (Llama, GPT, and Claude) in producing AAVE-like utterances at varying\ndialect intensities and assess user preferences across multiple domains,\nincluding healthcare and education. Despite LLMs' proficiency in generating\nAAVE-like language, findings indicate that AAVE-speaking users prefer Standard\nAmerican English (SAE) chatbots, with higher levels of AAVE correlating with\nlower ratings for a variety of characteristics, including chatbot\ntrustworthiness and role appropriateness. These results highlight the\ncomplexities of creating inclusive AI systems and underscore the need for\nfurther exploration of diversity to enhance human-computer interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As chatbots become increasingly integrated into everyday tasks, designing\nsystems that accommodate diverse user populations is crucial for fostering\ntrust, engagement, and inclusivity. This study investigates the ability of\ncontemporary Large Language Models (LLMs) to generate African American\nVernacular English (AAVE) and evaluates the impact of AAVE usage on user\nexperiences in chatbot applications. We analyze the performance of three LLM\nfamilies (Llama, GPT, and Claude) in producing AAVE-like utterances at varying\ndialect intensities and assess user preferences across multiple domains,\nincluding healthcare and education. Despite LLMs' proficiency in generating\nAAVE-like language, findings indicate that AAVE-speaking users prefer Standard\nAmerican English (SAE) chatbots, with higher levels of AAVE correlating with\nlower ratings for a variety of characteristics, including chatbot\ntrustworthiness and role appropriateness. These results highlight the\ncomplexities of creating inclusive AI systems and underscore the need for\nfurther exploration of diversity to enhance human-computer interactions."
                },
                "authors": [
                    {
                        "name": "Sarah E. Finch"
                    },
                    {
                        "name": "Ellie S. Paek"
                    },
                    {
                        "name": "Sejung Kwon"
                    },
                    {
                        "name": "Ikseon Choi"
                    },
                    {
                        "name": "Jessica Wells"
                    },
                    {
                        "name": "Rasheeta Chandler"
                    },
                    {
                        "name": "Jinho D. Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho D. Choi"
                },
                "author": "Jinho D. Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13226v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13226v2",
                "updated": "2025-01-06T23:51:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    51,
                    47,
                    0,
                    6,
                    0
                ],
                "published": "2024-05-21T22:26:01Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    22,
                    26,
                    1,
                    1,
                    142,
                    0
                ],
                "title": "Dataset Decomposition: Faster LLM Training with Variable Sequence Length\n  Curriculum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset Decomposition: Faster LLM Training with Variable Sequence Length\n  Curriculum"
                },
                "summary": "Large language models (LLMs) are commonly trained on datasets consisting of\nfixed-length token sequences. These datasets are created by randomly\nconcatenating documents of various lengths and then chunking them into\nsequences of a predetermined target length (concat-and-chunk). Recent attention\nimplementations mask cross-document attention, reducing the effective length of\na chunk of tokens. Additionally, training on long sequences becomes\ncomputationally prohibitive due to the quadratic cost of attention. In this\nstudy, we introduce dataset decomposition, a novel variable sequence length\ntraining technique, to tackle these challenges. We decompose a dataset into a\nunion of buckets, each containing sequences of the same size extracted from a\nunique document. During training, we use variable sequence length and\nbatch-size, sampling simultaneously from all buckets with a curriculum. In\ncontrast to the concat-and-chunk baseline, which incurs a fixed attention cost\nat every step of training, our proposed method incurs a computational cost\nproportional to the actual document lengths at each step, resulting in\nsignificant savings in training time. We train an 8k context-length 1B model at\nthe same cost as a 2k context-length model trained with the baseline approach.\nExperiments on a web-scale corpus demonstrate that our approach significantly\nenhances performance on standard language evaluations and long-context\nbenchmarks, reaching target accuracy with up to 6x faster training compared to\nthe baseline. Our method not only enables efficient pretraining on long\nsequences but also scales effectively with dataset size. Lastly, we shed light\non a critical yet less studied aspect of training large language models: the\ndistribution and curriculum of sequence lengths, which results in a\nnon-negligible difference in performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are commonly trained on datasets consisting of\nfixed-length token sequences. These datasets are created by randomly\nconcatenating documents of various lengths and then chunking them into\nsequences of a predetermined target length (concat-and-chunk). Recent attention\nimplementations mask cross-document attention, reducing the effective length of\na chunk of tokens. Additionally, training on long sequences becomes\ncomputationally prohibitive due to the quadratic cost of attention. In this\nstudy, we introduce dataset decomposition, a novel variable sequence length\ntraining technique, to tackle these challenges. We decompose a dataset into a\nunion of buckets, each containing sequences of the same size extracted from a\nunique document. During training, we use variable sequence length and\nbatch-size, sampling simultaneously from all buckets with a curriculum. In\ncontrast to the concat-and-chunk baseline, which incurs a fixed attention cost\nat every step of training, our proposed method incurs a computational cost\nproportional to the actual document lengths at each step, resulting in\nsignificant savings in training time. We train an 8k context-length 1B model at\nthe same cost as a 2k context-length model trained with the baseline approach.\nExperiments on a web-scale corpus demonstrate that our approach significantly\nenhances performance on standard language evaluations and long-context\nbenchmarks, reaching target accuracy with up to 6x faster training compared to\nthe baseline. Our method not only enables efficient pretraining on long\nsequences but also scales effectively with dataset size. Lastly, we shed light\non a critical yet less studied aspect of training large language models: the\ndistribution and curriculum of sequence lengths, which results in a\nnon-negligible difference in performance."
                },
                "authors": [
                    {
                        "name": "Hadi Pouransari"
                    },
                    {
                        "name": "Chun-Liang Li"
                    },
                    {
                        "name": "Jen-Hao Rick Chang"
                    },
                    {
                        "name": "Pavan Kumar Anasosalu Vasu"
                    },
                    {
                        "name": "Cem Koc"
                    },
                    {
                        "name": "Vaishaal Shankar"
                    },
                    {
                        "name": "Oncel Tuzel"
                    }
                ],
                "author_detail": {
                    "name": "Oncel Tuzel"
                },
                "author": "Oncel Tuzel",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13226v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13226v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00530v2",
                "updated": "2025-01-06T23:02:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    2,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-31T16:28:23Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    16,
                    28,
                    23,
                    1,
                    366,
                    0
                ],
                "title": "Superposition in Transformers: A Novel Way of Building Mixture of\n  Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposition in Transformers: A Novel Way of Building Mixture of\n  Experts"
                },
                "summary": "Catastrophic forgetting remains a major challenge when adapting large\nlanguage models (LLMs) to new tasks or domains. Conventional fine-tuning often\noverwrites existing knowledge, causing performance degradation on original\ntasks. We introduce Superposition in Transformers, a novel architecture that\nleverages autoencoders to superimpose the hidden representations of a base\nmodel and a fine-tuned model within a shared parameter space. By using\nB-spline-based blending coefficients and autoencoders that adaptively\nreconstruct hidden states based on the input data distribution, our method\neffectively mitigates catastrophic forgetting and enables a new paradigm of\n\"in-model\" superposition. This approach preserves original model capabilities\nwhile allowing compact domain-specific expertise to be added, and it supports\ndynamic switching between model states during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catastrophic forgetting remains a major challenge when adapting large\nlanguage models (LLMs) to new tasks or domains. Conventional fine-tuning often\noverwrites existing knowledge, causing performance degradation on original\ntasks. We introduce Superposition in Transformers, a novel architecture that\nleverages autoencoders to superimpose the hidden representations of a base\nmodel and a fine-tuned model within a shared parameter space. By using\nB-spline-based blending coefficients and autoencoders that adaptively\nreconstruct hidden states based on the input data distribution, our method\neffectively mitigates catastrophic forgetting and enables a new paradigm of\n\"in-model\" superposition. This approach preserves original model capabilities\nwhile allowing compact domain-specific expertise to be added, and it supports\ndynamic switching between model states during inference."
                },
                "authors": [
                    {
                        "name": "Ayoub Ben Chaliah"
                    },
                    {
                        "name": "Hela Dellagi"
                    }
                ],
                "author_detail": {
                    "name": "Hela Dellagi"
                },
                "author": "Hela Dellagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14804v2",
                "updated": "2025-01-06T22:06:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    6,
                    32,
                    0,
                    6,
                    0
                ],
                "published": "2023-12-22T16:26:20Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    16,
                    26,
                    20,
                    4,
                    356,
                    0
                ],
                "title": "Using large language models to promote health equity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using large language models to promote health equity"
                },
                "summary": "Advances in large language models (LLMs) have driven an explosion of interest\nabout their societal impacts. Much of the discourse around how they will impact\nsocial equity has been cautionary or negative, focusing on questions like \"how\nmight LLMs be biased and how would we mitigate those biases?\" This is a vital\ndiscussion: the ways in which AI generally, and LLMs specifically, can entrench\nbiases have been well-documented. But equally vital, and much less discussed,\nis the more opportunity-focused counterpoint: \"what promising applications do\nLLMs enable that could promote equity?\" If LLMs are to enable a more equitable\nworld, it is not enough just to play defense against their biases and failure\nmodes. We must also go on offense, applying them positively to equity-enhancing\nuse cases to increase opportunities for underserved groups and reduce societal\ndiscrimination. There are many choices which determine the impact of AI, and a\nfundamental choice very early in the pipeline is the problems we choose to\napply it to. If we focus only later in the pipeline -- making LLMs marginally\nmore fair as they facilitate use cases which intrinsically entrench power -- we\nwill miss an important opportunity to guide them to equitable impacts. Here, we\nhighlight the emerging potential of LLMs to promote equity by presenting four\nnewly possible, promising research directions, while keeping risks and\ncautionary points in clear view.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in large language models (LLMs) have driven an explosion of interest\nabout their societal impacts. Much of the discourse around how they will impact\nsocial equity has been cautionary or negative, focusing on questions like \"how\nmight LLMs be biased and how would we mitigate those biases?\" This is a vital\ndiscussion: the ways in which AI generally, and LLMs specifically, can entrench\nbiases have been well-documented. But equally vital, and much less discussed,\nis the more opportunity-focused counterpoint: \"what promising applications do\nLLMs enable that could promote equity?\" If LLMs are to enable a more equitable\nworld, it is not enough just to play defense against their biases and failure\nmodes. We must also go on offense, applying them positively to equity-enhancing\nuse cases to increase opportunities for underserved groups and reduce societal\ndiscrimination. There are many choices which determine the impact of AI, and a\nfundamental choice very early in the pipeline is the problems we choose to\napply it to. If we focus only later in the pipeline -- making LLMs marginally\nmore fair as they facilitate use cases which intrinsically entrench power -- we\nwill miss an important opportunity to guide them to equitable impacts. Here, we\nhighlight the emerging potential of LLMs to promote equity by presenting four\nnewly possible, promising research directions, while keeping risks and\ncautionary points in clear view."
                },
                "authors": [
                    {
                        "name": "Emma Pierson"
                    },
                    {
                        "name": "Divya Shanmugam"
                    },
                    {
                        "name": "Rajiv Movva"
                    },
                    {
                        "name": "Jon Kleinberg"
                    },
                    {
                        "name": "Monica Agrawal"
                    },
                    {
                        "name": "Mark Dredze"
                    },
                    {
                        "name": "Kadija Ferryman"
                    },
                    {
                        "name": "Judy Wawira Gichoya"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Pang Wei Koh"
                    },
                    {
                        "name": "Karen Levy"
                    },
                    {
                        "name": "Sendhil Mullainathan"
                    },
                    {
                        "name": "Ziad Obermeyer"
                    },
                    {
                        "name": "Harini Suresh"
                    },
                    {
                        "name": "Keyon Vafa"
                    }
                ],
                "author_detail": {
                    "name": "Keyon Vafa"
                },
                "author": "Keyon Vafa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10425v2",
                "updated": "2025-01-06T21:24:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    21,
                    24,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T16:34:47Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    34,
                    47,
                    1,
                    345,
                    0
                ],
                "title": "Active Inference for Self-Organizing Multi-LLM Systems: A Bayesian\n  Thermodynamic Approach to Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Inference for Self-Organizing Multi-LLM Systems: A Bayesian\n  Thermodynamic Approach to Adaptation"
                },
                "summary": "This paper introduces a novel approach to creating adaptive language agents\nby integrating active inference with large language models (LLMs). While LLMs\ndemonstrate remarkable capabilities, their reliance on static prompts limits\nadaptation to new information and changing environments. We address this by\nimplementing an active inference framework that acts as a cognitive layer above\nan LLM-based agent, dynamically adjusting prompts and search strategies through\nprincipled information-seeking behavior. Our framework models the environment\nusing three state factors (prompt, search, and information states) with seven\nobservation modalities capturing quality metrics. By framing the agent's\nlearning through the free energy principle, we enable systematic exploration of\nprompt combinations and search strategies. Experimental results demonstrate the\neffectiveness of this approach, with the agent developing accurate models of\nenvironment dynamics evidenced by emergent structure in observation matrices.\nAction selection patterns reveal sophisticated exploration-exploitation\nbehavior, transitioning from initial information-gathering to targeted prompt\ntesting. The integration of thermodynamic principles with language model\ncapabilities provides a principled framework for creating robust, adaptable\nagents, extending active inference beyond traditional low-dimensional control\nproblems to high-dimensional, language-driven environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach to creating adaptive language agents\nby integrating active inference with large language models (LLMs). While LLMs\ndemonstrate remarkable capabilities, their reliance on static prompts limits\nadaptation to new information and changing environments. We address this by\nimplementing an active inference framework that acts as a cognitive layer above\nan LLM-based agent, dynamically adjusting prompts and search strategies through\nprincipled information-seeking behavior. Our framework models the environment\nusing three state factors (prompt, search, and information states) with seven\nobservation modalities capturing quality metrics. By framing the agent's\nlearning through the free energy principle, we enable systematic exploration of\nprompt combinations and search strategies. Experimental results demonstrate the\neffectiveness of this approach, with the agent developing accurate models of\nenvironment dynamics evidenced by emergent structure in observation matrices.\nAction selection patterns reveal sophisticated exploration-exploitation\nbehavior, transitioning from initial information-gathering to targeted prompt\ntesting. The integration of thermodynamic principles with language model\ncapabilities provides a principled framework for creating robust, adaptable\nagents, extending active inference beyond traditional low-dimensional control\nproblems to high-dimensional, language-driven environments."
                },
                "authors": [
                    {
                        "name": "Rithvik Prakki"
                    }
                ],
                "author_detail": {
                    "name": "Rithvik Prakki"
                },
                "author": "Rithvik Prakki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11140v2",
                "updated": "2025-01-06T21:18:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    21,
                    18,
                    24,
                    0,
                    6,
                    0
                ],
                "published": "2024-02-17T00:13:36Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    0,
                    13,
                    36,
                    5,
                    48,
                    0
                ],
                "title": "Boosting of Thoughts: Trial-and-Error Problem Solving with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting of Thoughts: Trial-and-Error Problem Solving with Large\n  Language Models"
                },
                "summary": "The reasoning performance of Large Language Models (LLMs) on a wide range of\nproblems critically relies on chain-of-thought prompting, which involves\nproviding a few chain of thought demonstrations as exemplars in prompts. Recent\nwork, e.g., Tree of Thoughts, has pointed out the importance of exploration and\nself-evaluation in reasoning step selection for complex problem solving. In\nthis paper, we present Boosting of Thoughts (BoT), an automated prompting\nframework for problem solving with LLMs by iteratively exploring and\nself-evaluating many trees of thoughts in order to acquire an ensemble of\ntrial-and-error reasoning experiences, which will serve as a new form of\nprompting to solve the complex problem. Starting from a simple prompt without\nrequiring examples, BoT iteratively explores and evaluates a large collection\nof reasoning steps, and more importantly, uses error analysis obtained from the\nLLM on them to explicitly revise prompting, which in turn enhances reasoning\nstep generation, until a final answer is attained. Our experiments with GPT-4\nand Llama2 across extensive complex mathematical problems demonstrate that BoT\nconsistently achieves higher or comparable problem-solving rates than other\nadvanced prompting approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning performance of Large Language Models (LLMs) on a wide range of\nproblems critically relies on chain-of-thought prompting, which involves\nproviding a few chain of thought demonstrations as exemplars in prompts. Recent\nwork, e.g., Tree of Thoughts, has pointed out the importance of exploration and\nself-evaluation in reasoning step selection for complex problem solving. In\nthis paper, we present Boosting of Thoughts (BoT), an automated prompting\nframework for problem solving with LLMs by iteratively exploring and\nself-evaluating many trees of thoughts in order to acquire an ensemble of\ntrial-and-error reasoning experiences, which will serve as a new form of\nprompting to solve the complex problem. Starting from a simple prompt without\nrequiring examples, BoT iteratively explores and evaluates a large collection\nof reasoning steps, and more importantly, uses error analysis obtained from the\nLLM on them to explicitly revise prompting, which in turn enhances reasoning\nstep generation, until a final answer is attained. Our experiments with GPT-4\nand Llama2 across extensive complex mathematical problems demonstrate that BoT\nconsistently achieves higher or comparable problem-solving rates than other\nadvanced prompting approaches."
                },
                "authors": [
                    {
                        "name": "Sijia Chen"
                    },
                    {
                        "name": "Baochun Li"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "Accepted as a poster paper by ICLR2024. 27 pages, 5 figures, 18\n  tables. [Source\n  Code](https://github.com/iQua/llmpebase/tree/main/examples/BoTReasoning)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09012v2",
                "updated": "2025-01-06T20:49:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    20,
                    49,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2024-10-11T17:27:04Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    27,
                    4,
                    4,
                    285,
                    0
                ],
                "title": "Software Engineering and Foundation Models: Insights from Industry Blogs\n  Using a Jury of Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Engineering and Foundation Models: Insights from Industry Blogs\n  Using a Jury of Foundation Models"
                },
                "summary": "Foundation models (FMs) such as large language models (LLMs) have\nsignificantly impacted many fields, including software engineering (SE). The\ninteraction between SE and FMs has led to the integration of FMs into SE\npractices (FM4SE) and the application of SE methodologies to FMs (SE4FM). While\nseveral literature surveys exist on academic contributions to these trends, we\nare the first to provide a practitioner's view. We analyze 155 FM4SE and 997\nSE4FM blog posts from leading technology companies, leveraging an FM-powered\nsurveying approach to systematically label and summarize the discussed\nactivities and tasks. We observed that while code generation is the most\nprominent FM4SE task, FMs are leveraged for many other SE activities such as\ncode understanding, summarization, and API recommendation. The majority of blog\nposts on SE4FM are about model deployment & operation, and system architecture\n& orchestration. Although the emphasis is on cloud deployments, there is a\ngrowing interest in compressing FMs and deploying them on smaller devices such\nas edge or mobile devices. We outline eight future research directions inspired\nby our gained insights, aiming to bridge the gap between academic findings and\nreal-world applications. Our study not only enriches the body of knowledge on\npractical applications of FM4SE and SE4FM but also demonstrates the utility of\nFMs as a powerful and efficient approach in conducting literature surveys\nwithin technical and grey literature domains. Our dataset, results, code and\nused prompts can be found in our online replication package at\nhttps://github.com/SAILResearch/fmse-blogs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) such as large language models (LLMs) have\nsignificantly impacted many fields, including software engineering (SE). The\ninteraction between SE and FMs has led to the integration of FMs into SE\npractices (FM4SE) and the application of SE methodologies to FMs (SE4FM). While\nseveral literature surveys exist on academic contributions to these trends, we\nare the first to provide a practitioner's view. We analyze 155 FM4SE and 997\nSE4FM blog posts from leading technology companies, leveraging an FM-powered\nsurveying approach to systematically label and summarize the discussed\nactivities and tasks. We observed that while code generation is the most\nprominent FM4SE task, FMs are leveraged for many other SE activities such as\ncode understanding, summarization, and API recommendation. The majority of blog\nposts on SE4FM are about model deployment & operation, and system architecture\n& orchestration. Although the emphasis is on cloud deployments, there is a\ngrowing interest in compressing FMs and deploying them on smaller devices such\nas edge or mobile devices. We outline eight future research directions inspired\nby our gained insights, aiming to bridge the gap between academic findings and\nreal-world applications. Our study not only enriches the body of knowledge on\npractical applications of FM4SE and SE4FM but also demonstrates the utility of\nFMs as a powerful and efficient approach in conducting literature surveys\nwithin technical and grey literature domains. Our dataset, results, code and\nused prompts can be found in our online replication package at\nhttps://github.com/SAILResearch/fmse-blogs."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Cor-Paul Bezemer"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "arxiv_comment": "ICSE-SEIP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00168v2",
                "updated": "2025-01-06T20:46:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    20,
                    46,
                    0,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-30T22:29:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    22,
                    29,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Takeaways from Applying LLM Capabilities to Multiple Conversational\n  Avatars in a VR Pilot Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Takeaways from Applying LLM Capabilities to Multiple Conversational\n  Avatars in a VR Pilot Study"
                },
                "summary": "We present a virtual reality (VR) environment featuring conversational\navatars powered by a locally-deployed LLM, integrated with automatic speech\nrecognition (ASR), text-to-speech (TTS), and lip-syncing. Through a pilot\nstudy, we explored the effects of three types of avatar status indicators\nduring response generation. Our findings reveal design considerations for\nimproving responsiveness and realism in LLM-driven conversational systems. We\nalso detail two system architectures: one using an LLM-based state machine to\ncontrol avatar behavior and another integrating retrieval-augmented generation\n(RAG) for context-grounded responses. Together, these contributions offer\npractical insights to guide future work in developing task-oriented\nconversational AI in VR environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a virtual reality (VR) environment featuring conversational\navatars powered by a locally-deployed LLM, integrated with automatic speech\nrecognition (ASR), text-to-speech (TTS), and lip-syncing. Through a pilot\nstudy, we explored the effects of three types of avatar status indicators\nduring response generation. Our findings reveal design considerations for\nimproving responsiveness and realism in LLM-driven conversational systems. We\nalso detail two system architectures: one using an LLM-based state machine to\ncontrol avatar behavior and another integrating retrieval-augmented generation\n(RAG) for context-grounded responses. Together, these contributions offer\npractical insights to guide future work in developing task-oriented\nconversational AI in VR environments."
                },
                "authors": [
                    {
                        "name": "Mykola Maslych"
                    },
                    {
                        "name": "Christian Pumarada"
                    },
                    {
                        "name": "Amirpouya Ghasemaghaei"
                    },
                    {
                        "name": "Joseph J. LaViola Jr"
                    }
                ],
                "author_detail": {
                    "name": "Joseph J. LaViola Jr"
                },
                "author": "Joseph J. LaViola Jr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08099v2",
                "updated": "2025-01-06T20:32:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    20,
                    32,
                    48,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-11T04:53:15Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    53,
                    15,
                    2,
                    346,
                    0
                ],
                "title": "Adversarial Vulnerabilities in Large Language Models for Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Vulnerabilities in Large Language Models for Time Series\n  Forecasting"
                },
                "summary": "Large Language Models (LLMs) have recently demonstrated significant potential\nin the field of time series forecasting, offering impressive capabilities in\nhandling complex temporal data. However, their robustness and reliability in\nreal-world applications remain under-explored, particularly concerning their\nsusceptibility to adversarial attacks. In this paper, we introduce a targeted\nadversarial attack framework for LLM-based time series forecasting. By\nemploying both gradient-free and black-box optimization methods, we generate\nminimal yet highly effective perturbations that significantly degrade the\nforecasting accuracy across multiple datasets and LLM architectures. Our\nexperiments, which include models like TimeGPT and LLM-Time with GPT-3.5,\nGPT-4, LLaMa, and Mistral, show that adversarial attacks lead to much more\nsevere performance degradation than random noise, and demonstrate the broad\neffectiveness of our attacks across different LLMs. The results underscore the\ncritical vulnerabilities of LLMs in time series forecasting, highlighting the\nneed for robust defense mechanisms to ensure their reliable deployment in\npractical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently demonstrated significant potential\nin the field of time series forecasting, offering impressive capabilities in\nhandling complex temporal data. However, their robustness and reliability in\nreal-world applications remain under-explored, particularly concerning their\nsusceptibility to adversarial attacks. In this paper, we introduce a targeted\nadversarial attack framework for LLM-based time series forecasting. By\nemploying both gradient-free and black-box optimization methods, we generate\nminimal yet highly effective perturbations that significantly degrade the\nforecasting accuracy across multiple datasets and LLM architectures. Our\nexperiments, which include models like TimeGPT and LLM-Time with GPT-3.5,\nGPT-4, LLaMa, and Mistral, show that adversarial attacks lead to much more\nsevere performance degradation than random noise, and demonstrate the broad\neffectiveness of our attacks across different LLMs. The results underscore the\ncritical vulnerabilities of LLMs in time series forecasting, highlighting the\nneed for robust defense mechanisms to ensure their reliable deployment in\npractical applications."
                },
                "authors": [
                    {
                        "name": "Fuqiang Liu"
                    },
                    {
                        "name": "Sicong Jiang"
                    },
                    {
                        "name": "Luis Miranda-Moreno"
                    },
                    {
                        "name": "Seongjin Choi"
                    },
                    {
                        "name": "Lijun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Sun"
                },
                "author": "Lijun Sun",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03376v1",
                "updated": "2025-01-06T20:30:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    20,
                    30,
                    15,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T20:30:15Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    20,
                    30,
                    15,
                    0,
                    6,
                    0
                ],
                "title": "Existential Crisis: A Social Robot's Reason for Being",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existential Crisis: A Social Robot's Reason for Being"
                },
                "summary": "As Robots become ever more important in our daily lives there's growing need\nfor understanding how they're perceived by people. This study aims to\ninvestigate how the user perception of robots is influenced by displays of\npersonality. Using LLMs and speech to text technology, we designed a\nwithin-subject study to compare two conditions: a personality-driven robot and\na purely task-oriented, personality-neutral robot. Twelve participants,\nrecruited from Socially Intelligent Robotics course at Vrije Universiteit\nAmsterdam, interacted with a robot Nao tasked with asking them a set of medical\nquestions under both conditions. After completing both interactions, the\nparticipants completed a user experience questionnaire measuring their\nemotional states and robot perception using standardized questionnaires from\nthe SRI and Psychology literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Robots become ever more important in our daily lives there's growing need\nfor understanding how they're perceived by people. This study aims to\ninvestigate how the user perception of robots is influenced by displays of\npersonality. Using LLMs and speech to text technology, we designed a\nwithin-subject study to compare two conditions: a personality-driven robot and\na purely task-oriented, personality-neutral robot. Twelve participants,\nrecruited from Socially Intelligent Robotics course at Vrije Universiteit\nAmsterdam, interacted with a robot Nao tasked with asking them a set of medical\nquestions under both conditions. After completing both interactions, the\nparticipants completed a user experience questionnaire measuring their\nemotional states and robot perception using standardized questionnaires from\nthe SRI and Psychology literature."
                },
                "authors": [
                    {
                        "name": "Dora Medgyesy"
                    },
                    {
                        "name": "Joella Galas"
                    },
                    {
                        "name": "Julian van Pol"
                    },
                    {
                        "name": "Rustam Eynaliyev"
                    },
                    {
                        "name": "Thijs Vollebregt"
                    }
                ],
                "author_detail": {
                    "name": "Thijs Vollebregt"
                },
                "author": "Thijs Vollebregt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]