[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.11495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11495v1",
                "updated": "2025-08-15T14:17:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T14:17:24Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "title": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation"
                },
                "summary": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators."
                },
                "authors": [
                    {
                        "name": "Jingnan Xu"
                    },
                    {
                        "name": "Leixia Wang"
                    },
                    {
                        "name": "Xiaofeng Meng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Meng"
                },
                "author": "Xiaofeng Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11291v1",
                "updated": "2025-08-15T07:55:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:55:05Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "title": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks"
                },
                "summary": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks."
                },
                "authors": [
                    {
                        "name": "Rui Bao"
                    },
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Chen"
                },
                "author": "Zhiyong Chen",
                "arxiv_comment": "accepted by IEEE/CIC ICCC workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11260v1",
                "updated": "2025-08-15T06:53:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:53:28Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?"
                },
                "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages."
                },
                "authors": [
                    {
                        "name": "Mukund Choudhary"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Gaurja Aeron"
                    },
                    {
                        "name": "Antara Raaghavi Bhattacharya"
                    },
                    {
                        "name": "Dang Khoa Dang Dinh"
                    },
                    {
                        "name": "Ikhlasul Akmal Hanif"
                    },
                    {
                        "name": "Daria Kotova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v2",
                "updated": "2025-08-15T04:27:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    27,
                    30,
                    4,
                    227,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10875v1",
                "updated": "2025-08-14T17:47:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:47:22Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "title": "A Survey on Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs."
                },
                "authors": [
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Mingda Chen"
                    },
                    {
                        "name": "Bowei Guo"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10824v2",
                "updated": "2025-08-16T03:17:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    3,
                    17,
                    35,
                    5,
                    228,
                    0
                ],
                "published": "2025-08-14T16:48:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    48,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures"
                },
                "summary": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures."
                },
                "authors": [
                    {
                        "name": "Parsa Omidi"
                    },
                    {
                        "name": "Xingshuai Huang"
                    },
                    {
                        "name": "Axel Laborieux"
                    },
                    {
                        "name": "Bahareh Nikpour"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Armaghan Eshaghi"
                    }
                ],
                "author_detail": {
                    "name": "Armaghan Eshaghi"
                },
                "author": "Armaghan Eshaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v2",
                "updated": "2025-08-14T16:12:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    12,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v2",
                "updated": "2025-08-14T15:37:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    37,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache"
                },
                "summary": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10963v1",
                "updated": "2025-08-14T14:11:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    48,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:11:48Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    48,
                    3,
                    226,
                    0
                ],
                "title": "EVCtrl: Efficient Control Adapter for Visual Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVCtrl: Efficient Control Adapter for Visual Generation"
                },
                "summary": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Zixiang Yang"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Yinhan Zhang"
                    },
                    {
                        "name": "Shanhui Mo"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10613v1",
                "updated": "2025-08-14T13:10:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    10,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T13:10:43Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    10,
                    43,
                    3,
                    226,
                    0
                ],
                "title": "Routing and Wavelength Assignment with Minimal Attack Radius for QKD\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routing and Wavelength Assignment with Minimal Attack Radius for QKD\n  Networks"
                },
                "summary": "Quantum Key Distribution (QKD) can distribute keys with guaranteed security\nbut remains susceptible to key exchange interruption due to physical-layer\nthreats, such as high-power jamming attacks. To address this challenge, we\nfirst introduce a novel metric, namely Maximum Number of Affected Requests\n(maxNAR), to quantify the worst-case impact of a single physical-layer attack,\nand then we investigate a new problem of Routing and Wavelength Assignment with\nMinimal Attack Radius (RWA-MAR). We formulate the problem using an Integer\nLinear Programming (ILP) model and propose a scalable heuristic to efficiently\nminimize maxNAR. Our approach incorporates key caching through Quantum Key\nPools (QKPs) to enhance resilience and optimize resource utilization. Moreover,\nwe model the impact of different QKD network architectures, employing Optical\nBypass (OB) for optical switching of quantum channels and Trusted Relay (TR)\nfor secure key forwarding. Moreover, a tunable parameter is designed in the\nheuristic to guide the preference for OB or TR, offering enhanced adaptability\nand dynamic control in diverse network scenarios. Simulation results confirm\nthat our method significantly outperforms the baseline in terms of security and\nscalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution (QKD) can distribute keys with guaranteed security\nbut remains susceptible to key exchange interruption due to physical-layer\nthreats, such as high-power jamming attacks. To address this challenge, we\nfirst introduce a novel metric, namely Maximum Number of Affected Requests\n(maxNAR), to quantify the worst-case impact of a single physical-layer attack,\nand then we investigate a new problem of Routing and Wavelength Assignment with\nMinimal Attack Radius (RWA-MAR). We formulate the problem using an Integer\nLinear Programming (ILP) model and propose a scalable heuristic to efficiently\nminimize maxNAR. Our approach incorporates key caching through Quantum Key\nPools (QKPs) to enhance resilience and optimize resource utilization. Moreover,\nwe model the impact of different QKD network architectures, employing Optical\nBypass (OB) for optical switching of quantum channels and Trusted Relay (TR)\nfor secure key forwarding. Moreover, a tunable parameter is designed in the\nheuristic to guide the preference for OB or TR, offering enhanced adaptability\nand dynamic control in diverse network scenarios. Simulation results confirm\nthat our method significantly outperforms the baseline in terms of security and\nscalability."
                },
                "authors": [
                    {
                        "name": "Mengyao Li"
                    },
                    {
                        "name": "Qiaolun Zhang"
                    },
                    {
                        "name": "Zongshuai Yang"
                    },
                    {
                        "name": "Stefano Bregni"
                    },
                    {
                        "name": "Alberto Gatto"
                    },
                    {
                        "name": "Raouf Boutaba"
                    },
                    {
                        "name": "Massimo Tornatore"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Tornatore"
                },
                "author": "Massimo Tornatore",
                "arxiv_comment": "6 pages, this paper has been successfully accepted by GLOBECOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08601v3",
                "updated": "2025-08-14T10:26:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    26,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T03:34:21Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    34,
                    21,
                    1,
                    224,
                    0
                ],
                "title": "Yan: Foundational Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yan: Foundational Interactive Video Generation"
                },
                "summary": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/."
                },
                "authors": [
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Fangyun Zhou"
                    },
                    {
                        "name": "Jiacheng Lv"
                    },
                    {
                        "name": "Jianqi Ma"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Junyan Lv"
                    },
                    {
                        "name": "Junyou Li"
                    },
                    {
                        "name": "Minwen Deng"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Wenkai Lv"
                    },
                    {
                        "name": "Yangbin Yu"
                    },
                    {
                        "name": "Yewen Wang"
                    },
                    {
                        "name": "Yonghang Guan"
                    },
                    {
                        "name": "Zhihao Hu"
                    },
                    {
                        "name": "Zhongbin Fang"
                    },
                    {
                        "name": "Zhongqian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhongqian Sun"
                },
                "author": "Zhongqian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08895v2",
                "updated": "2025-08-14T09:04:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    4,
                    56,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T12:35:55Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs"
                },
                "summary": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Zhifeng Shen"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Haoqian Wu"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v1",
                "updated": "2025-08-14T08:04:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 claims that occupancy-based\nattacks can recover AES keys from the MIRAGE randomized cache. In this paper,\nwe examine these claims and find that they arise from fundamental modeling\nflaws. Most critically, the authors' simulation of MIRAGE uses a constant seed\nto initialize the random number generator used for global evictions in MIRAGE,\ncausing every AES encryption they trace to evict the same deterministic\nsequence of cache lines. This artificially creates a highly repeatable timing\npattern that is not representative of a realistic implementation of MIRAGE,\nwhere eviction sequences vary randomly between encryptions. When we instead\nrandomize the eviction seed for each run, reflecting realistic operation, the\ncorrelation between AES T-table accesses and attacker runtimes disappears, and\nthe attack fails. These findings show that the reported leakage is an artifact\nof incorrect modeling, and not an actual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 claims that occupancy-based\nattacks can recover AES keys from the MIRAGE randomized cache. In this paper,\nwe examine these claims and find that they arise from fundamental modeling\nflaws. Most critically, the authors' simulation of MIRAGE uses a constant seed\nto initialize the random number generator used for global evictions in MIRAGE,\ncausing every AES encryption they trace to evict the same deterministic\nsequence of cache lines. This artificially creates a highly repeatable timing\npattern that is not representative of a realistic implementation of MIRAGE,\nwhere eviction sequences vary randomly between encryptions. When we instead\nrandomize the eviction seed for each run, reflecting realistic operation, the\ncorrelation between AES T-table accesses and attacker runtimes disappears, and\nthe attack fails. These findings show that the reported leakage is an artifact\nof incorrect modeling, and not an actual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10424v1",
                "updated": "2025-08-14T07:54:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    54,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:54:44Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    54,
                    44,
                    3,
                    226,
                    0
                ],
                "title": "NanoControl: A Lightweight Framework for Precise and Efficient Control\n  in Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoControl: A Lightweight Framework for Precise and Efficient Control\n  in Diffusion Transformer"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability."
                },
                "authors": [
                    {
                        "name": "Shanyuan Liu"
                    },
                    {
                        "name": "Jian Zhu"
                    },
                    {
                        "name": "Junda Lu"
                    },
                    {
                        "name": "Yue Gong"
                    },
                    {
                        "name": "Liuzhuozheng Li"
                    },
                    {
                        "name": "Bo Cheng"
                    },
                    {
                        "name": "Yuhang Ma"
                    },
                    {
                        "name": "Liebucha Wu"
                    },
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Dawei Leng"
                    },
                    {
                        "name": "Yuhui Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yuhui Yin"
                },
                "author": "Yuhui Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10395v1",
                "updated": "2025-08-14T06:52:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T06:52:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization"
                },
                "summary": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models."
                },
                "authors": [
                    {
                        "name": "Aditya Tomar"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Rishabh Tiwari"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v3",
                "updated": "2025-08-13T17:55:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    55,
                    58,
                    2,
                    225,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v1",
                "updated": "2025-08-13T13:54:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining."
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v1",
                "updated": "2025-08-13T07:40:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v4",
                "updated": "2025-08-13T06:13:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    6,
                    13,
                    36,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v2",
                "updated": "2025-08-13T04:24:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    24,
                    56,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v4",
                "updated": "2025-08-13T04:03:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    3,
                    10,
                    2,
                    225,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09442v1",
                "updated": "2025-08-13T02:48:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T02:48:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference"
                },
                "summary": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment."
                },
                "authors": [
                    {
                        "name": "Zhifan Luo"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Su Zhang"
                    },
                    {
                        "name": "Lijing Zhou"
                    },
                    {
                        "name": "Yuke Hu"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Zhihao Liu"
                    },
                    {
                        "name": "Zhan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Qin"
                },
                "author": "Zhan Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09419v1",
                "updated": "2025-08-13T01:39:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T01:39:09Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "title": "Design and Simulation of 6T SRAM Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Simulation of 6T SRAM Array"
                },
                "summary": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given."
                },
                "authors": [
                    {
                        "name": "Justin London"
                    }
                ],
                "author_detail": {
                    "name": "Justin London"
                },
                "author": "Justin London",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08744v2",
                "updated": "2025-08-13T01:39:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    3,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T08:39:32Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    39,
                    32,
                    1,
                    224,
                    0
                ],
                "title": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality."
                },
                "authors": [
                    {
                        "name": "Zhonggen Li"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Bocheng Yu"
                    },
                    {
                        "name": "Baihua Zheng"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09262v1",
                "updated": "2025-08-12T18:05:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    18,
                    5,
                    33,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T18:05:33Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    18,
                    5,
                    33,
                    1,
                    224,
                    0
                ],
                "title": "Harnessing Input-Adaptive Inference for Efficient VLN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Input-Adaptive Inference for Efficient VLN"
                },
                "summary": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation."
                },
                "authors": [
                    {
                        "name": "Dongwoo Kang"
                    },
                    {
                        "name": "Akhil Perincherry"
                    },
                    {
                        "name": "Zachary Coalson"
                    },
                    {
                        "name": "Aiden Gabriel"
                    },
                    {
                        "name": "Stefan Lee"
                    },
                    {
                        "name": "Sanghyun Hong"
                    }
                ],
                "author_detail": {
                    "name": "Sanghyun Hong"
                },
                "author": "Sanghyun Hong",
                "arxiv_comment": "Accepted to ICCV 2025 [Poster]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v1",
                "updated": "2025-08-12T16:47:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Yi Fei"
                    },
                    {
                        "name": "Zeng Weidi"
                    }
                ],
                "author_detail": {
                    "name": "Zeng Weidi"
                },
                "author": "Zeng Weidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09001v1",
                "updated": "2025-08-12T15:11:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:11:47Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "title": "Retrospective Sparse Attention for Efficient Long-Context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrospective Sparse Attention for Efficient Long-Context Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%."
                },
                "authors": [
                    {
                        "name": "Seonghwan Choi"
                    },
                    {
                        "name": "Beomseok Kang"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08978v1",
                "updated": "2025-08-12T14:40:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:40:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "TaoCache: Structure-Maintained Video Generation Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaoCache: Structure-Maintained Video Generation Acceleration"
                },
                "summary": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups."
                },
                "authors": [
                    {
                        "name": "Zhentao Fan"
                    },
                    {
                        "name": "Zongzuo Wang"
                    },
                    {
                        "name": "Weiwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weiwei Zhang"
                },
                "author": "Weiwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11488v2",
                "updated": "2025-08-12T10:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    43,
                    55,
                    1,
                    224,
                    0
                ],
                "published": "2023-11-30T16:02:04Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    16,
                    2,
                    4,
                    3,
                    334,
                    0
                ],
                "title": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows"
                },
                "summary": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes."
                },
                "authors": [
                    {
                        "name": "Thiago Garrett"
                    },
                    {
                        "name": "Weijia Song"
                    },
                    {
                        "name": "Roman Vitenberg"
                    },
                    {
                        "name": "Ken Birman"
                    }
                ],
                "author_detail": {
                    "name": "Ken Birman"
                },
                "author": "Ken Birman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v3",
                "updated": "2025-08-12T05:51:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    5,
                    51,
                    37,
                    1,
                    224,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08600v1",
                "updated": "2025-08-12T03:33:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T03:33:15Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "title": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes"
                },
                "summary": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased."
                },
                "authors": [
                    {
                        "name": "Timur V. Tscherbul"
                    },
                    {
                        "name": "Roman V. Krems"
                    }
                ],
                "author_detail": {
                    "name": "Roman V. Krems"
                },
                "author": "Roman V. Krems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07675v2",
                "updated": "2025-08-12T02:51:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    51,
                    12,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T06:53:27Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    6,
                    53,
                    27,
                    0,
                    223,
                    0
                ],
                "title": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Baran Atalar"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08134v2",
                "updated": "2025-08-12T02:27:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    27,
                    5,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T16:10:00Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    0,
                    0,
                    223,
                    0
                ],
                "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control"
                },
                "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement."
                },
                "authors": [
                    {
                        "name": "Zeqian Long"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Xinhua Zhang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma",
                "arxiv_comment": "Project webpage is available at https://follow-your-shape.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08531v1",
                "updated": "2025-08-12T00:06:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    0,
                    6,
                    34,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T00:06:34Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    0,
                    6,
                    34,
                    1,
                    224,
                    0
                ],
                "title": "Profiling Large Language Model Inference on Apple Silicon: A\n  Quantization Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profiling Large Language Model Inference on Apple Silicon: A\n  Quantization Perspective"
                },
                "summary": "A systematic understanding of Apple Silicon is lacking in the current\nlandscape of hardware efficiency; research focus is largely centered on\naccelerating GPUs for large-scale training or inference on CUDA devices. This\npaper investigates Apple Silicon's unique memory architecture that offers a\nunified memory integrating CPU and GPU memory and its implications for\non-device LLM inference.\n  We decipher myths about whether Apple Silicon is efficient for on-device\ninference compared to competitors such as NVIDIA GPUs by directly conducting\nlatency and throughput comparison benchmarks. We explain the performance gap\nbetween them through profiling low level hardware metrics - ALU utilization,\nmemory bandwidth, buffer usage, cache residency etc. at runtime. We draw\nseveral insights regarding performance bottlenecks such as dequantization\noverhead, compute throughput and memory bandwidth. We debunk existing false\nclaims regarding large language model inference such as compressing models to\nlower bit precision is a defacto promise for faster inference across all\nhardware platforms. We find that the large unified memory enables Apple Silicon\nto be both cost effective and efficient against NVIDIA GPUs for ultra large\nlanguage models.\n  Our large scale evaluation on 5 hardware testbeds incorporating three Apple\nM-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX\nA6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from\n8B to 405B parameters and 14 quantization schemes gives an understanding of how\nApple Silicon fits within the paradigm of on-device LLM inference. Our analysis\nreveals multiple resource interdependencies and unexpected findings, while also\nquantifying established insights. To the best of our knowledge, this study\nmakes the first attempt to present a thorough characterization and analysis of\nApple Silicon for on-device inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A systematic understanding of Apple Silicon is lacking in the current\nlandscape of hardware efficiency; research focus is largely centered on\naccelerating GPUs for large-scale training or inference on CUDA devices. This\npaper investigates Apple Silicon's unique memory architecture that offers a\nunified memory integrating CPU and GPU memory and its implications for\non-device LLM inference.\n  We decipher myths about whether Apple Silicon is efficient for on-device\ninference compared to competitors such as NVIDIA GPUs by directly conducting\nlatency and throughput comparison benchmarks. We explain the performance gap\nbetween them through profiling low level hardware metrics - ALU utilization,\nmemory bandwidth, buffer usage, cache residency etc. at runtime. We draw\nseveral insights regarding performance bottlenecks such as dequantization\noverhead, compute throughput and memory bandwidth. We debunk existing false\nclaims regarding large language model inference such as compressing models to\nlower bit precision is a defacto promise for faster inference across all\nhardware platforms. We find that the large unified memory enables Apple Silicon\nto be both cost effective and efficient against NVIDIA GPUs for ultra large\nlanguage models.\n  Our large scale evaluation on 5 hardware testbeds incorporating three Apple\nM-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX\nA6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from\n8B to 405B parameters and 14 quantization schemes gives an understanding of how\nApple Silicon fits within the paradigm of on-device LLM inference. Our analysis\nreveals multiple resource interdependencies and unexpected findings, while also\nquantifying established insights. To the best of our knowledge, this study\nmakes the first attempt to present a thorough characterization and analysis of\nApple Silicon for on-device inference."
                },
                "authors": [
                    {
                        "name": "Afsara Benazir"
                    },
                    {
                        "name": "Felix Xiaozhu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Felix Xiaozhu Lin"
                },
                "author": "Felix Xiaozhu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08457v1",
                "updated": "2025-08-11T20:30:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    20,
                    30,
                    31,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T20:30:31Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    20,
                    30,
                    31,
                    0,
                    223,
                    0
                ],
                "title": "Architecting Long-Context LLM Acceleration with Packing-Prefetch\n  Scheduler and Ultra-Large Capacity On-Chip Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecting Long-Context LLM Acceleration with Packing-Prefetch\n  Scheduler and Ultra-Large Capacity On-Chip Memories"
                },
                "summary": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Hanchen Yang"
                    },
                    {
                        "name": "Muhammed Ahosan Ul Karim"
                    },
                    {
                        "name": "Harsono Simka"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "7 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.3; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08438v1",
                "updated": "2025-08-11T19:55:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    19,
                    55,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T19:55:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    19,
                    55,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM\n  Inference"
                },
                "summary": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Zecheng Lin"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Jianchang Su"
                    },
                    {
                        "name": "Cheng Chu"
                    },
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Wenhui Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "17 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08081v1",
                "updated": "2025-08-11T15:28:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:28:28Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "title": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures"
                },
                "summary": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29."
                },
                "authors": [
                    {
                        "name": "Florian Naef"
                    },
                    {
                        "name": "Thomas Willwacher"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Willwacher"
                },
                "author": "Thomas Willwacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.QA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v2",
                "updated": "2025-08-11T14:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    15,
                    27,
                    0,
                    223,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "15 pages, 14 figures; Accepted by ICCV2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v1",
                "updated": "2025-08-11T10:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical\n  Approach for Multi-Tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical\n  Approach for Multi-Tenant LLM Serving"
                },
                "summary": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Under review for a computer science conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07811v1",
                "updated": "2025-08-11T09:54:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T09:54:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration"
                },
                "summary": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions."
                },
                "authors": [
                    {
                        "name": "Sicheng Gao"
                    },
                    {
                        "name": "Nancy Mehta"
                    },
                    {
                        "name": "Zongwei Wu"
                    },
                    {
                        "name": "Radu Timofte"
                    }
                ],
                "author_detail": {
                    "name": "Radu Timofte"
                },
                "author": "Radu Timofte",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v2",
                "updated": "2025-08-11T08:10:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    10,
                    21,
                    0,
                    223,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v5",
                "updated": "2025-08-16T23:41:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    23,
                    41,
                    48,
                    5,
                    228,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "15 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07570v1",
                "updated": "2025-08-11T03:03:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T03:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios."
                },
                "authors": [
                    {
                        "name": "Khanh-Binh Nguyen"
                    },
                    {
                        "name": "Phuoc-Nguyen Bui"
                    },
                    {
                        "name": "Hyunseung Choo"
                    },
                    {
                        "name": "Duc Thanh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Duc Thanh Nguyen"
                },
                "author": "Duc Thanh Nguyen",
                "arxiv_comment": "12 pages, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09208v1",
                "updated": "2025-08-10T14:05:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    10,
                    14,
                    5,
                    36,
                    6,
                    222,
                    0
                ],
                "published": "2025-08-10T14:05:36Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    14,
                    5,
                    36,
                    6,
                    222,
                    0
                ],
                "title": "CoMoE: Collaborative Optimization of Expert Aggregation and Offloading\n  for MoE-based LLMs at Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMoE: Collaborative Optimization of Expert Aggregation and Offloading\n  for MoE-based LLMs at Edge"
                },
                "summary": "The proliferation of large language models (LLMs) has driven the adoption of\nMixture-of-Experts (MoE) architectures as a promising solution to scale model\ncapacity while controlling computational costs. However, deploying MoE models\nin resource-constrained mobile edge computing environments presents significant\nchallenges due to their large memory footprint and dynamic expert activation\npatterns. To address these challenges, we propose a novel dynamic\nresource-aware collaborative optimization framework that jointly optimizes\nexpert aggregation granularity and offloading strategies based on real-time\ndevice resource states, network conditions, and input characteristics in mobile\nedge environments, denoted as CoMoE. In CoMoE, we first systematically analyze\nexisting expert aggregation techniques, including expert parameter\nmerging,knowledge distillation,and parameter sharing decomposition, identifying\ntheir limitations in dynamic mobile environments.We then investigate expert\noffloading strategies encompassing expert prediction and prefetching, expert\ncaching and scheduling, and multi-tier storage architectures, revealing the\ninterdependencies between routing decisions and offloading performance.The\nCoMoE incorporates adaptive scheduling mechanisms that respond to user mobility\nand varying network conditions, enabling efficient MoE deployment across\nheterogeneous edge devices. Extensive experiments on real mobile edge testbeds\ndemonstrate that CoMoE achieves approximately 70% reduction in memory usage\ncompared to baseline methods, 10.5% lower inference latency than existing\nexpert offloading techniques, while maintaining model performance stability.\nFor large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE\nreduces memory requirements from 15.6GB to 4.7GB, enabling deployment on\nresource-constrained mobile edge devices that previously could only support\nmuch smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has driven the adoption of\nMixture-of-Experts (MoE) architectures as a promising solution to scale model\ncapacity while controlling computational costs. However, deploying MoE models\nin resource-constrained mobile edge computing environments presents significant\nchallenges due to their large memory footprint and dynamic expert activation\npatterns. To address these challenges, we propose a novel dynamic\nresource-aware collaborative optimization framework that jointly optimizes\nexpert aggregation granularity and offloading strategies based on real-time\ndevice resource states, network conditions, and input characteristics in mobile\nedge environments, denoted as CoMoE. In CoMoE, we first systematically analyze\nexisting expert aggregation techniques, including expert parameter\nmerging,knowledge distillation,and parameter sharing decomposition, identifying\ntheir limitations in dynamic mobile environments.We then investigate expert\noffloading strategies encompassing expert prediction and prefetching, expert\ncaching and scheduling, and multi-tier storage architectures, revealing the\ninterdependencies between routing decisions and offloading performance.The\nCoMoE incorporates adaptive scheduling mechanisms that respond to user mobility\nand varying network conditions, enabling efficient MoE deployment across\nheterogeneous edge devices. Extensive experiments on real mobile edge testbeds\ndemonstrate that CoMoE achieves approximately 70% reduction in memory usage\ncompared to baseline methods, 10.5% lower inference latency than existing\nexpert offloading techniques, while maintaining model performance stability.\nFor large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE\nreduces memory requirements from 15.6GB to 4.7GB, enabling deployment on\nresource-constrained mobile edge devices that previously could only support\nmuch smaller models."
                },
                "authors": [
                    {
                        "name": "Muqing Li"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Wenchao Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Haijun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haijun Zhang"
                },
                "author": "Haijun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v2",
                "updated": "2025-08-09T11:31:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    31,
                    44,
                    5,
                    221,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06937v1",
                "updated": "2025-08-09T11:06:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-09T11:06:58Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing"
                },
                "summary": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods."
                },
                "authors": [
                    {
                        "name": "Weiyan Xie"
                    },
                    {
                        "name": "Han Gao"
                    },
                    {
                        "name": "Didan Deng"
                    },
                    {
                        "name": "Kaican Li"
                    },
                    {
                        "name": "April Hua Liu"
                    },
                    {
                        "name": "Yongxiang Huang"
                    },
                    {
                        "name": "Nevin L. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Nevin L. Zhang"
                },
                "author": "Nevin L. Zhang",
                "arxiv_comment": "Project Page: vaynexie.github.io/CannyEdit/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03974v2",
                "updated": "2025-08-09T02:33:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    2,
                    33,
                    21,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-05T23:47:34Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    23,
                    47,
                    34,
                    1,
                    217,
                    0
                ],
                "title": "Managing Data for Scalable and Interactive Event Sequence Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing Data for Scalable and Interactive Event Sequence Visualization"
                },
                "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization."
                },
                "authors": [
                    {
                        "name": "Sayef Azad Sakin"
                    },
                    {
                        "name": "Katherine E. Isaacs"
                    }
                ],
                "author_detail": {
                    "name": "Katherine E. Isaacs"
                },
                "author": "Katherine E. Isaacs",
                "arxiv_comment": "The 15th IEEE Workshop on Large Data Analysis and Visualization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v2",
                "updated": "2025-08-09T00:12:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    0,
                    12,
                    1,
                    5,
                    221,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v3",
                "updated": "2025-08-08T18:16:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    18,
                    16,
                    33,
                    4,
                    220,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06447v1",
                "updated": "2025-08-08T16:42:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:42:38Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning"
                },
                "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Rubing Yang"
                    },
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Desheng Hui"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Jianlei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianlei Yang"
                },
                "author": "Jianlei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v3",
                "updated": "2025-08-08T14:25:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    25,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06297v1",
                "updated": "2025-08-08T13:19:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:19:30Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "title": "KV Cache Compression for Inference Efficiency in LLMs: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression for Inference Efficiency in LLMs: A Review"
                },
                "summary": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanyu Liu"
                    },
                    {
                        "name": "Jingying Fu"
                    },
                    {
                        "name": "Sixiang Liu"
                    },
                    {
                        "name": "Yitian Zou"
                    },
                    {
                        "name": "You Fu"
                    },
                    {
                        "name": "Jiehan Zhou"
                    },
                    {
                        "name": "Shouhua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shouhua Zhang"
                },
                "arxiv_affiliation": "University of Oulu",
                "author": "Shouhua Zhang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06160v1",
                "updated": "2025-08-08T09:29:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:29:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment"
                },
                "summary": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff."
                },
                "authors": [
                    {
                        "name": "Zhenbang Du"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Lifu Wang"
                    },
                    {
                        "name": "Jiayi Qian"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v1",
                "updated": "2025-08-08T08:54:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06064v1",
                "updated": "2025-08-08T06:53:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T06:53:50Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "title": "A Generic Complete Anytime Beam Search for Optimal Decision Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generic Complete Anytime Beam Search for Optimal Decision Tree"
                },
                "summary": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees."
                },
                "authors": [
                    {
                        "name": "Harold Silvère Kiossou"
                    },
                    {
                        "name": "Siegfried Nijssen"
                    },
                    {
                        "name": "Pierre Schaus"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Schaus"
                },
                "author": "Pierre Schaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2104.13123v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2104.13123v3",
                "updated": "2025-08-08T06:38:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    38,
                    1,
                    4,
                    220,
                    0
                ],
                "published": "2021-04-27T11:55:54Z",
                "published_parsed": [
                    2021,
                    4,
                    27,
                    11,
                    55,
                    54,
                    1,
                    117,
                    0
                ],
                "title": "Affine Springer fibers and depth zero L-packets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affine Springer fibers and depth zero L-packets"
                },
                "summary": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable."
                },
                "authors": [
                    {
                        "name": "Roman Bezrukavnikov"
                    },
                    {
                        "name": "Yakov Varshavsky"
                    }
                ],
                "author_detail": {
                    "name": "Yakov Varshavsky"
                },
                "author": "Yakov Varshavsky",
                "arxiv_comment": "v.3, 96 pages, minor changes in abstract and introduction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2104.13123v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2104.13123v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "22E50, 22E57",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09192v1",
                "updated": "2025-08-08T04:51:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    4,
                    51,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T04:51:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    4,
                    51,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing"
                },
                "summary": "Diffusion Large Language Models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs for text generation, with the potential\nto decode multiple tokens in a single iteration. However, none of the existing\nopen-source dLLMs have achieved superior inference speed over AR LLMs of\nsimilar size. This paper breaks this barrier based on a simple and effective\nstrategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key\ncapabilities: (1) block-wise autoregressive generation to enable KV cache\nutilization; (2) prediction of following tokens without requiring completion of\nprior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs\nare refurbished into an AR-diffusion hybrid paradigm for efficient inference.\nD2F can be implemented with an asymmetric distillation process based on\npre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,\nwhich enables a trade-off between efficiency and efficacy. Empirically, D2F\ndLLMs achieve more than $\\mathbf{2.5\\times}$ inference speed than LLaMA3 and\nQwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the\nacceleration can be more than $\\mathbf{50\\times}$ while maintaining comparable\noutput quality. The code is available at\nhttps://github.com/zhijie-group/Discrete-Diffusion-Forcing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs for text generation, with the potential\nto decode multiple tokens in a single iteration. However, none of the existing\nopen-source dLLMs have achieved superior inference speed over AR LLMs of\nsimilar size. This paper breaks this barrier based on a simple and effective\nstrategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key\ncapabilities: (1) block-wise autoregressive generation to enable KV cache\nutilization; (2) prediction of following tokens without requiring completion of\nprior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs\nare refurbished into an AR-diffusion hybrid paradigm for efficient inference.\nD2F can be implemented with an asymmetric distillation process based on\npre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,\nwhich enables a trade-off between efficiency and efficacy. Empirically, D2F\ndLLMs achieve more than $\\mathbf{2.5\\times}$ inference speed than LLaMA3 and\nQwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the\nacceleration can be more than $\\mathbf{50\\times}$ while maintaining comparable\noutput quality. The code is available at\nhttps://github.com/zhijie-group/Discrete-Diffusion-Forcing."
                },
                "authors": [
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Chenkai Xu"
                    },
                    {
                        "name": "Yijie Jin"
                    },
                    {
                        "name": "Jiachun Jin"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05904v1",
                "updated": "2025-08-07T23:53:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:53:31Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "title": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data"
                },
                "summary": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks."
                },
                "authors": [
                    {
                        "name": "Brandon Baker"
                    },
                    {
                        "name": "Elliott Brossard"
                    },
                    {
                        "name": "Chenwei Xie"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Deen Liu"
                    },
                    {
                        "name": "Yijun Xie"
                    },
                    {
                        "name": "Arthur Zwiegincew"
                    },
                    {
                        "name": "Nitya Kumar Sharma"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Eugene Retunsky"
                    },
                    {
                        "name": "Mike Halcrow"
                    },
                    {
                        "name": "Derek Denny-Brown"
                    },
                    {
                        "name": "Istvan Cseri"
                    },
                    {
                        "name": "Tyler Akidau"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "arxiv_comment": "12 pages, 6 figures, accepted in ICDCS 2025",
                "arxiv_journal_ref": "Proc. 45th IEEE International Conference on Distributed Computing\n  Systems (ICDCS), Glasgow, UK, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05898v1",
                "updated": "2025-08-07T23:11:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:11:33Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "title": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates"
                },
                "summary": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA."
                },
                "authors": [
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Aijun An"
                    },
                    {
                        "name": "Ali cheraghian"
                    }
                ],
                "author_detail": {
                    "name": "Ali cheraghian"
                },
                "author": "Ali cheraghian",
                "arxiv_comment": "BMVC2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10024v1",
                "updated": "2025-08-07T21:18:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    21,
                    18,
                    52,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T21:18:52Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    21,
                    18,
                    52,
                    3,
                    219,
                    0
                ],
                "title": "RTTC: Reward-Guided Collaborative Test-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTTC: Reward-Guided Collaborative Test-Time Compute"
                },
                "summary": "Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the\nperformance of Large Language Models (LLMs) at inference, leveraging strategies\nsuch as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).\nHowever, the optimal adaptation strategy varies across queries, and\nindiscriminate application of TTC strategy incurs substantial computational\noverhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a\nnovel framework that adaptively selects the most effective TTC strategy for\neach query via a pretrained reward model, maximizing downstream accuracy across\ndiverse domains and tasks. RTTC operates in a distributed server-client\narchitecture, retrieving relevant samples from a remote knowledge base and\napplying RAG or lightweight fine-tuning on client devices only when necessary.\nTo further mitigate redundant computation, we propose Query-State Caching,\nwhich enables the efficient reuse of historical query states at both retrieval\nand adaptation levels. Extensive experiments across multiple LLMs and\nbenchmarks demonstrate that RTTC consistently achieves superior accuracy\ncompared to vanilla RAG or TTT, validating the necessity of adaptive,\nreward-guided TTC selection and the potential of RTTC for scalable,\nhigh-performance language model adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the\nperformance of Large Language Models (LLMs) at inference, leveraging strategies\nsuch as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).\nHowever, the optimal adaptation strategy varies across queries, and\nindiscriminate application of TTC strategy incurs substantial computational\noverhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a\nnovel framework that adaptively selects the most effective TTC strategy for\neach query via a pretrained reward model, maximizing downstream accuracy across\ndiverse domains and tasks. RTTC operates in a distributed server-client\narchitecture, retrieving relevant samples from a remote knowledge base and\napplying RAG or lightweight fine-tuning on client devices only when necessary.\nTo further mitigate redundant computation, we propose Query-State Caching,\nwhich enables the efficient reuse of historical query states at both retrieval\nand adaptation levels. Extensive experiments across multiple LLMs and\nbenchmarks demonstrate that RTTC consistently achieves superior accuracy\ncompared to vanilla RAG or TTT, validating the necessity of adaptive,\nreward-guided TTC selection and the potential of RTTC for scalable,\nhigh-performance language model adaptation."
                },
                "authors": [
                    {
                        "name": "J. Pablo Muñoz"
                    },
                    {
                        "name": "Jinjie Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Yuan"
                },
                "author": "Jinjie Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v1",
                "updated": "2025-08-07T09:47:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05091v1",
                "updated": "2025-08-07T07:19:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T07:19:02Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "title": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation"
                },
                "summary": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration."
                },
                "authors": [
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Busheng Su"
                    },
                    {
                        "name": "Finn Wong"
                    }
                ],
                "author_detail": {
                    "name": "Finn Wong"
                },
                "author": "Finn Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05012v1",
                "updated": "2025-08-07T03:49:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T03:49:56Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "title": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines"
                },
                "summary": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion."
                },
                "authors": [
                    {
                        "name": "Ugur Cetintemel"
                    },
                    {
                        "name": "Shu Chen"
                    },
                    {
                        "name": "Alexander W. Lee"
                    },
                    {
                        "name": "Deepti Raghavan"
                    }
                ],
                "author_detail": {
                    "name": "Deepti Raghavan"
                },
                "author": "Deepti Raghavan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v2",
                "updated": "2025-08-06T16:57:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    57,
                    39,
                    2,
                    218,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Zhengming Zhang"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Accepted by ICCV 2025; Code released at\n  https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04581v1",
                "updated": "2025-08-06T16:06:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:06:43Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning"
                },
                "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance."
                },
                "authors": [
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    }
                ],
                "author_detail": {
                    "name": "Stamatios Lefkimmiatis"
                },
                "author": "Stamatios Lefkimmiatis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v3",
                "updated": "2025-08-06T15:38:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    38,
                    6,
                    2,
                    218,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads"
                },
                "summary": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Ted Hart"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "Proceedings of the VLDB Endowment 18 (VLDB'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v1",
                "updated": "2025-08-06T14:02:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference"
                },
                "summary": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v3",
                "updated": "2025-08-06T11:46:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    46,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "arxiv_comment": "Submission under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04257v1",
                "updated": "2025-08-06T09:40:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T09:40:09Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs"
                },
                "summary": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "arxiv_comment": "Published as a conference paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00370v2",
                "updated": "2025-08-06T08:32:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    8,
                    32,
                    53,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-01T07:03:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices"
                },
                "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Poh Seng Lim"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "JungHau Foo"
                    },
                    {
                        "name": "Yap Deep"
                    },
                    {
                        "name": "Timothy Lee Jun Jie"
                    },
                    {
                        "name": "Kelvin Teh Kae Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Danyu Feng"
                    },
                    {
                        "name": "Hao-Yun Chen"
                    },
                    {
                        "name": "Peng-Wen Chen"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wong Wai Mun"
                    }
                ],
                "author_detail": {
                    "name": "Wong Wai Mun"
                },
                "author": "Wong Wai Mun",
                "arxiv_comment": "The data and method in the paper need to be re-audited",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03837v1",
                "updated": "2025-08-05T18:34:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T18:34:48Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "title": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems"
                },
                "summary": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs."
                },
                "authors": [
                    {
                        "name": "Davide Zoni"
                    },
                    {
                        "name": "Andrea Galimberti"
                    },
                    {
                        "name": "Adriano Guarisco"
                    }
                ],
                "author_detail": {
                    "name": "Adriano Guarisco"
                },
                "author": "Adriano Guarisco",
                "arxiv_comment": "9 pages, 13 figures, 1 table, accepted for presentation at 2025\n  International Conference on Computer-Aided Design (ICCAD), Munich, Germany,\n  October 26-30, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v2",
                "updated": "2025-08-05T16:17:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    16,
                    17,
                    1,
                    1,
                    217,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03321v1",
                "updated": "2025-08-05T11:00:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T11:00:41Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "title": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios"
                },
                "summary": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS."
                },
                "authors": [
                    {
                        "name": "Jörn Bodenhausen"
                    },
                    {
                        "name": "Simon Mangel"
                    },
                    {
                        "name": "Thomas Vogt"
                    },
                    {
                        "name": "Martin Henze"
                    }
                ],
                "author_detail": {
                    "name": "Martin Henze"
                },
                "author": "Martin Henze",
                "arxiv_comment": "Accepted for publication in Proceedings of the 2025 IEEE 50th\n  Conference on Local Computer Networks (LCN)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03258v1",
                "updated": "2025-08-05T09:35:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T09:35:52Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "title": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%."
                },
                "authors": [
                    {
                        "name": "Yueyue Liu"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Yuantian Miao"
                    }
                ],
                "author_detail": {
                    "name": "Yuantian Miao"
                },
                "author": "Yuantian Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02240v2",
                "updated": "2025-08-05T02:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    2,
                    13,
                    39,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T09:39:31Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    39,
                    31,
                    0,
                    216,
                    0
                ],
                "title": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}"
                },
                "authors": [
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Jiaxing Yan"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Zetao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v5",
                "updated": "2025-08-05T00:25:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    0,
                    25,
                    53,
                    1,
                    217,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition"
                },
                "summary": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "Philip Levis"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v1",
                "updated": "2025-08-04T16:14:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02401v1",
                "updated": "2025-08-04T13:26:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:26:16Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git."
                },
                "authors": [
                    {
                        "name": "Xiaolin Lin"
                    },
                    {
                        "name": "Jingcun Wang"
                    },
                    {
                        "name": "Olga Kondrateva"
                    },
                    {
                        "name": "Yiyu Shi"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Grace Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Grace Li Zhang"
                },
                "author": "Grace Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02280v1",
                "updated": "2025-08-04T10:51:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T10:51:20Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "title": "OnPair: Short Strings Compression for Fast Random Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnPair: Short Strings Compression for Fast Random Access"
                },
                "summary": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage."
                },
                "authors": [
                    {
                        "name": "Francesco Gargiulo"
                    },
                    {
                        "name": "Rossano Venturini"
                    }
                ],
                "author_detail": {
                    "name": "Rossano Venturini"
                },
                "author": "Rossano Venturini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; E.4; H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02215v1",
                "updated": "2025-08-04T09:08:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:08:43Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"
                },
                "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK."
                },
                "authors": [
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Zhiyuan He"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19906v2",
                "updated": "2025-08-04T08:19:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    8,
                    19,
                    26,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-26T10:34:53Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "title": "CaliDrop: KV Cache Compression with Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaliDrop: KV Cache Compression with Calibration"
                },
                "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v2",
                "updated": "2025-08-04T04:48:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    4,
                    48,
                    41,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02930v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02930v4",
                "updated": "2025-08-04T02:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    47,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2024-07-03T09:02:05Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    2,
                    5,
                    2,
                    185,
                    0
                ],
                "title": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs"
                },
                "summary": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Kun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yang"
                },
                "author": "Kun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02930v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02930v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v2",
                "updated": "2025-08-04T02:17:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    17,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "14 pages, 7 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01898v1",
                "updated": "2025-08-03T19:16:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T19:16:40Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "title": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution"
                },
                "summary": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Md-Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in the IEEE Transactions on\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v1",
                "updated": "2025-08-03T18:15:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Linxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Boqian Wang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16607v2",
                "updated": "2025-08-03T10:27:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    10,
                    27,
                    19,
                    6,
                    215,
                    0
                ],
                "published": "2025-01-28T00:52:23Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    52,
                    23,
                    1,
                    28,
                    0
                ],
                "title": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search"
                },
                "summary": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy."
                },
                "authors": [
                    {
                        "name": "Shuozhi Yuan"
                    },
                    {
                        "name": "Limin Chen"
                    },
                    {
                        "name": "Miaomiao Yuan"
                    },
                    {
                        "name": "Jin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Zhao"
                },
                "author": "Jin Zhao",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02751v1",
                "updated": "2025-08-03T09:15:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T09:15:36Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference"
                },
                "summary": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Yajuan Peng"
                    },
                    {
                        "name": "Cam-Tu Nguyen"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Xiaoming Fu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Fu"
                },
                "author": "Xiaoming Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19718v2",
                "updated": "2025-08-02T23:59:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    23,
                    59,
                    11,
                    5,
                    214,
                    0
                ],
                "published": "2025-07-25T23:55:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting"
                },
                "summary": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency."
                },
                "authors": [
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Hamid Gadirov"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v1",
                "updated": "2025-08-02T21:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gaëtan Hadjeres"
                    },
                    {
                        "name": "Gaël Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_comment": "Accepted to the Transactions of the International Society for Music\n  Information Retrieval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01298v1",
                "updated": "2025-08-02T10:12:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T10:12:45Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "title": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access"
                },
                "summary": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations."
                },
                "authors": [
                    {
                        "name": "Azadeh Sadat Miraftab"
                    },
                    {
                        "name": "Ahmadreza Montazerolghaem"
                    },
                    {
                        "name": "Behrad Mahboobi"
                    }
                ],
                "author_detail": {
                    "name": "Behrad Mahboobi"
                },
                "author": "Behrad Mahboobi",
                "arxiv_doi": "10.1007/s10586-025-05256-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-025-05256-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01261v1",
                "updated": "2025-08-02T08:33:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T08:33:30Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "title": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models"
                },
                "summary": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v2",
                "updated": "2025-08-02T06:50:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    50,
                    59,
                    5,
                    214,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "To appear in ASPLOS'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v1",
                "updated": "2025-08-02T06:43:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance."
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06526v1",
                "updated": "2025-08-02T03:50:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    3,
                    50,
                    14,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T03:50:14Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    3,
                    50,
                    14,
                    5,
                    214,
                    0
                ],
                "title": "PiKV: KV Cache Management System for Mixture of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiKV: KV Cache Management System for Mixture of Experts"
                },
                "summary": "As large language models continue to scale up in both size and context\nlength, the memory and communication cost of key-value (KV) cache storage has\nbecome a major bottleneck in multi-GPU and multi-node inference. While\nMoE-based architectures sparsify computation across experts, the corresponding\nKV caches remain dense and globally synchronized, resulting in significant\noverhead.\n  We introduce \\textbf{PiKV}, a parallel and distributed KV cache serving\nframework tailored for MoE architecture. PiKV leverages \\textit{expert-sharded\nKV storage} to partition caches across GPUs, \\textit{PiKV routing} to reduce\ntoken-to-KV access, and a \\textit{PiKV Scheduling} to adaptively retain\nquery-relevant entries. To further reduce memory usage, PiKV integrates\n\\textit{PiKV Compression} modules the caching pipeline for acceleration.\n  PiKV is recently publicly available as an open-source software library:\n\\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.\nExperiments details is recorded at:\n\\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\\_Results}.\nWe also have PiKV integrated with Nvidia kvpress for acceleration, details see\n\\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.\nPiKV is still a living project, aiming to become a comprehesive KV Cache\nmanagement system for MoE Architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models continue to scale up in both size and context\nlength, the memory and communication cost of key-value (KV) cache storage has\nbecome a major bottleneck in multi-GPU and multi-node inference. While\nMoE-based architectures sparsify computation across experts, the corresponding\nKV caches remain dense and globally synchronized, resulting in significant\noverhead.\n  We introduce \\textbf{PiKV}, a parallel and distributed KV cache serving\nframework tailored for MoE architecture. PiKV leverages \\textit{expert-sharded\nKV storage} to partition caches across GPUs, \\textit{PiKV routing} to reduce\ntoken-to-KV access, and a \\textit{PiKV Scheduling} to adaptively retain\nquery-relevant entries. To further reduce memory usage, PiKV integrates\n\\textit{PiKV Compression} modules the caching pipeline for acceleration.\n  PiKV is recently publicly available as an open-source software library:\n\\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.\nExperiments details is recorded at:\n\\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\\_Results}.\nWe also have PiKV integrated with Nvidia kvpress for acceleration, details see\n\\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.\nPiKV is still a living project, aiming to become a comprehesive KV Cache\nmanagement system for MoE Architectures."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    },
                    {
                        "name": "Xuhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xuhong Wang"
                },
                "author": "Xuhong Wang",
                "arxiv_comment": "Accepted to ICML ES-MoFo III WorkShop Paper Link:\n  https://openreview.net/pdf?id=hHoK1kBPd9 Github Link:\n  https://github.com/NoakLiu/PiKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v2",
                "updated": "2025-08-02T00:31:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    0,
                    31,
                    18,
                    5,
                    214,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01051v1",
                "updated": "2025-08-01T20:08:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T20:08:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "QPP-RNG: A Conceptual Quantum System for True Randomness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPP-RNG: A Conceptual Quantum System for True Randomness"
                },
                "summary": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems."
                },
                "authors": [
                    {
                        "name": "Randy Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Randy Kuang"
                },
                "author": "Randy Kuang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00647v1",
                "updated": "2025-08-01T14:05:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:05:44Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "title": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)"
                },
                "summary": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions."
                },
                "authors": [
                    {
                        "name": "Alessandro Lacerenza"
                    },
                    {
                        "name": "Alda Rubini"
                    },
                    {
                        "name": "Andrea Alimenti"
                    },
                    {
                        "name": "Sergio Fabiani"
                    },
                    {
                        "name": "Ettore Del Monte"
                    },
                    {
                        "name": "Riccardo Campana"
                    },
                    {
                        "name": "Mauro Centrone"
                    },
                    {
                        "name": "Enrico Costa"
                    },
                    {
                        "name": "Nicolas De Angelis"
                    },
                    {
                        "name": "Giovanni De Cesare"
                    },
                    {
                        "name": "Sergio Di Cosimo"
                    },
                    {
                        "name": "Giuseppe Di Persio"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Pasqualino Loffredo"
                    },
                    {
                        "name": "Giovanni Lombardi"
                    },
                    {
                        "name": "Gabriele Minervini"
                    },
                    {
                        "name": "Fabio Muleri"
                    },
                    {
                        "name": "Paolo Romano"
                    },
                    {
                        "name": "Emanuele Scalise"
                    },
                    {
                        "name": "Enrico Silva"
                    },
                    {
                        "name": "Paolo Soffitta"
                    },
                    {
                        "name": "Davide Albanesi"
                    },
                    {
                        "name": "Ilaria Baffo"
                    },
                    {
                        "name": "Daniele Brienza"
                    },
                    {
                        "name": "Valerio Campamaggiore"
                    },
                    {
                        "name": "Giovanni Cucinella"
                    },
                    {
                        "name": "Andrea Curatolo"
                    },
                    {
                        "name": "Giulia de Iulis"
                    },
                    {
                        "name": "Andrea Del Re"
                    },
                    {
                        "name": "Vito Di Bari"
                    },
                    {
                        "name": "Simone Di Filippo"
                    },
                    {
                        "name": "Immacolata Donnarumma"
                    },
                    {
                        "name": "Pierluigi Fanelli"
                    },
                    {
                        "name": "Nicolas Gagliardi"
                    },
                    {
                        "name": "Paolo Leonetti"
                    },
                    {
                        "name": "Matteo Merge"
                    },
                    {
                        "name": "Dario Modenini"
                    },
                    {
                        "name": "Andrea Negri"
                    },
                    {
                        "name": "Daniele Pecorella"
                    },
                    {
                        "name": "Massimo Perelli"
                    },
                    {
                        "name": "Alice Ponti"
                    },
                    {
                        "name": "Francesca Sbop"
                    },
                    {
                        "name": "Paolo Tortora"
                    },
                    {
                        "name": "Alessandro Turchi"
                    },
                    {
                        "name": "Valerio Vagelli"
                    },
                    {
                        "name": "Emanuele Zaccagnino"
                    },
                    {
                        "name": "Alessandro Zambardi"
                    },
                    {
                        "name": "Costantino Zazza"
                    }
                ],
                "author_detail": {
                    "name": "Costantino Zazza"
                },
                "author": "Costantino Zazza",
                "arxiv_comment": "6 pages, 2 figures, SPIE Optics+Photonics 2025 proceeding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00629v1",
                "updated": "2025-08-01T13:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:40:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach"
                },
                "summary": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks"
                },
                "authors": [
                    {
                        "name": "Francisco Crespo"
                    },
                    {
                        "name": "Javier Villegas"
                    },
                    {
                        "name": "Carlos Baena"
                    },
                    {
                        "name": "Eduardo Baena"
                    },
                    {
                        "name": "Sergio Fortes"
                    },
                    {
                        "name": "Raquel Barco"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Barco"
                },
                "author": "Raquel Barco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00616v1",
                "updated": "2025-08-01T13:25:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:25:28Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "title": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications"
                },
                "summary": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups."
                },
                "authors": [
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Jiancheng An"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This papar has been submitted to the IEEE Global Communications\n  Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.11628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11628v1",
                "updated": "2025-08-15T17:56:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    56,
                    24,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T17:56:24Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    56,
                    24,
                    4,
                    227,
                    0
                ],
                "title": "Is ChatGPT-5 Ready for Mammogram VQA?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is ChatGPT-5 Ready for Mammogram VQA?"
                },
                "summary": "Mammogram visual question answering (VQA) integrates image interpretation\nwith clinical reasoning and has potential to support breast cancer screening.\nWe systematically evaluated the GPT-5 family and GPT-4o model on four public\nmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,\nabnormality detection, and malignancy classification tasks. GPT-5 consistently\nwas the best performing model but lagged behind both human experts and\ndomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores\namong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),\ncalcification (63.5%), and malignancy (52.8%) classification. On InBreast, it\nattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%\nmalignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection\nand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS\naccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared\nwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and\nspecificity (52.3%). While GPT-5 exhibits promising capabilities for screening\ntasks, its performance remains insufficient for high-stakes clinical imaging\napplications without targeted domain adaptation and optimization. However, the\ntremendous improvements in performance from GPT-4o to GPT-5 show a promising\ntrend in the potential for general large language models (LLMs) to assist with\nmammography VQA tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mammogram visual question answering (VQA) integrates image interpretation\nwith clinical reasoning and has potential to support breast cancer screening.\nWe systematically evaluated the GPT-5 family and GPT-4o model on four public\nmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,\nabnormality detection, and malignancy classification tasks. GPT-5 consistently\nwas the best performing model but lagged behind both human experts and\ndomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores\namong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),\ncalcification (63.5%), and malignancy (52.8%) classification. On InBreast, it\nattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%\nmalignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection\nand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS\naccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared\nwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and\nspecificity (52.3%). While GPT-5 exhibits promising capabilities for screening\ntasks, its performance remains insufficient for high-stakes clinical imaging\napplications without targeted domain adaptation and optimization. However, the\ntremendous improvements in performance from GPT-4o to GPT-5 show a promising\ntrend in the potential for general large language models (LLMs) to assist with\nmammography VQA tasks."
                },
                "authors": [
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Shansong Wang"
                    },
                    {
                        "name": "Mingzhe Hu"
                    },
                    {
                        "name": "Mojtaba Safari"
                    },
                    {
                        "name": "Zachary Eidex"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Yang"
                },
                "author": "Xiaofeng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11624v1",
                "updated": "2025-08-15T17:52:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    52,
                    56,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T17:52:56Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    52,
                    56,
                    4,
                    227,
                    0
                ],
                "title": "LoRAtorio: An intrinsic approach to LoRA Skill Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRAtorio: An intrinsic approach to LoRA Skill Composition"
                },
                "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted technique in\ntext-to-image diffusion models, enabling the personalisation of visual concepts\nsuch as characters, styles, and objects. However, existing approaches struggle\nto effectively compose multiple LoRA adapters, particularly in open-ended\nsettings where the number and nature of required skills are not known in\nadvance. In this work, we present LoRAtorio, a novel train-free framework for\nmulti-LoRA composition that leverages intrinsic model behaviour. Our method is\nmotivated by two key observations: (1) LoRA adapters trained on narrow domains\nproduce denoised outputs that diverge from the base model, and (2) when\noperating out-of-distribution, LoRA outputs show behaviour closer to the base\nmodel than when conditioned in distribution. The balance between these two\nobservations allows for exceptional performance in the single LoRA scenario,\nwhich nevertheless deteriorates when multiple LoRAs are loaded. Our method\noperates in the latent space by dividing it into spatial patches and computing\ncosine similarity between each patch's predicted noise and that of the base\nmodel. These similarities are used to construct a spatially-aware weight\nmatrix, which guides a weighted aggregation of LoRA outputs. To address domain\ndrift, we further propose a modification to classifier-free guidance that\nincorporates the base model's unconditional score into the composition. We\nextend this formulation to a dynamic module selection setting, enabling\ninference-time selection of relevant LoRA adapters from a large pool. LoRAtorio\nachieves state-of-the-art performance, showing up to a 1.3% improvement in\nClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises\neffectively to multiple latent diffusion models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has become a widely adopted technique in\ntext-to-image diffusion models, enabling the personalisation of visual concepts\nsuch as characters, styles, and objects. However, existing approaches struggle\nto effectively compose multiple LoRA adapters, particularly in open-ended\nsettings where the number and nature of required skills are not known in\nadvance. In this work, we present LoRAtorio, a novel train-free framework for\nmulti-LoRA composition that leverages intrinsic model behaviour. Our method is\nmotivated by two key observations: (1) LoRA adapters trained on narrow domains\nproduce denoised outputs that diverge from the base model, and (2) when\noperating out-of-distribution, LoRA outputs show behaviour closer to the base\nmodel than when conditioned in distribution. The balance between these two\nobservations allows for exceptional performance in the single LoRA scenario,\nwhich nevertheless deteriorates when multiple LoRAs are loaded. Our method\noperates in the latent space by dividing it into spatial patches and computing\ncosine similarity between each patch's predicted noise and that of the base\nmodel. These similarities are used to construct a spatially-aware weight\nmatrix, which guides a weighted aggregation of LoRA outputs. To address domain\ndrift, we further propose a modification to classifier-free guidance that\nincorporates the base model's unconditional score into the composition. We\nextend this formulation to a dynamic module selection setting, enabling\ninference-time selection of relevant LoRA adapters from a large pool. LoRAtorio\nachieves state-of-the-art performance, showing up to a 1.3% improvement in\nClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises\neffectively to multiple latent diffusion models."
                },
                "authors": [
                    {
                        "name": "Niki Foteinopoulou"
                    },
                    {
                        "name": "Ignas Budvytis"
                    },
                    {
                        "name": "Stephan Liwicki"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Liwicki"
                },
                "author": "Stephan Liwicki",
                "arxiv_comment": "32 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11622v1",
                "updated": "2025-08-15T17:48:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    48,
                    36,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T17:48:36Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    48,
                    36,
                    4,
                    227,
                    0
                ],
                "title": "Deconfounding via Profiled Transfer Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deconfounding via Profiled Transfer Learning"
                },
                "summary": "Unmeasured confounders are a major source of bias in regression-based effect\nestimation and causal inference. In this paper, we advocate a new profiled\ntransfer learning framework, ProTrans, to address confounding effects in the\ntarget dataset, when additional source datasets that possess similar\nconfounding structures are available. We introduce the concept of profiled\nresiduals to characterize the shared confounding patterns between source and\ntarget datasets. By incorporating these profiled residuals into the target\ndebiasing step, we effectively mitigates the latent confounding effects. We\nalso propose a source selection strategy to enhance robustness of ProTrans\nagainst noninformative sources. As a byproduct, ProTrans can also be utilized\nto estimate treatment effects when potential confounders exist, without the use\nof auxiliary features such as instrumental or proxy variables, which are often\nchallenging to select in practice. Theoretically, we prove that the resulting\nestimated model shift from sources to target is confounding-free without any\nassumptions imposed on the true confounding structure, and that the target\nparameter estimation achieves the minimax optimal rate under mild conditions.\nSimulated and real-world experiments validate the effectiveness of ProTrans and\nsupport the theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmeasured confounders are a major source of bias in regression-based effect\nestimation and causal inference. In this paper, we advocate a new profiled\ntransfer learning framework, ProTrans, to address confounding effects in the\ntarget dataset, when additional source datasets that possess similar\nconfounding structures are available. We introduce the concept of profiled\nresiduals to characterize the shared confounding patterns between source and\ntarget datasets. By incorporating these profiled residuals into the target\ndebiasing step, we effectively mitigates the latent confounding effects. We\nalso propose a source selection strategy to enhance robustness of ProTrans\nagainst noninformative sources. As a byproduct, ProTrans can also be utilized\nto estimate treatment effects when potential confounders exist, without the use\nof auxiliary features such as instrumental or proxy variables, which are often\nchallenging to select in practice. Theoretically, we prove that the resulting\nestimated model shift from sources to target is confounding-free without any\nassumptions imposed on the true confounding structure, and that the target\nparameter estimation achieves the minimax optimal rate under mild conditions.\nSimulated and real-world experiments validate the effectiveness of ProTrans and\nsupport the theoretical findings."
                },
                "authors": [
                    {
                        "name": "Ziyuan Chen"
                    },
                    {
                        "name": "Yifan Jiang"
                    },
                    {
                        "name": "Jingyuan Liu"
                    },
                    {
                        "name": "Fang Yao"
                    }
                ],
                "author_detail": {
                    "name": "Fang Yao"
                },
                "author": "Fang Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11616v1",
                "updated": "2025-08-15T17:29:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    29,
                    6,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T17:29:06Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    29,
                    6,
                    4,
                    227,
                    0
                ],
                "title": "Controlling Multimodal LLMs via Reward-guided Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Multimodal LLMs via Reward-guided Decoding"
                },
                "summary": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods."
                },
                "authors": [
                    {
                        "name": "Oscar Mañas"
                    },
                    {
                        "name": "Pierluca D'Oro"
                    },
                    {
                        "name": "Koustuv Sinha"
                    },
                    {
                        "name": "Adriana Romero-Soriano"
                    },
                    {
                        "name": "Michal Drozdzal"
                    },
                    {
                        "name": "Aishwarya Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Aishwarya Agrawal"
                },
                "author": "Aishwarya Agrawal",
                "arxiv_comment": "Published at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11614v1",
                "updated": "2025-08-15T17:24:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    24,
                    36,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T17:24:36Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    24,
                    36,
                    4,
                    227,
                    0
                ],
                "title": "Bulk viscous cosmological models with cosmological constant:\n  Observational constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bulk viscous cosmological models with cosmological constant:\n  Observational constraints"
                },
                "summary": "We investigate whether viscous cold dark matter (vCDM) in a\n$\\Lambda$-dominated FLRW universe can alleviate the Hubble tension while\nsatisfying thermodynamic constraints, examining both flat and curved\ngeometries. We model vCDM with bulk viscosity $\\zeta =\n\\zeta_0\\,(\\Omega_{vc}/\\Omega_{vc0})^m$, where $m$ determines the viscosity\nevolution and $\\Omega_{vc}$ is the density parameter of vCDM. We explore two\nparticular scenarios: constant viscosity ($m=0$), and variable viscosity ($m$\nfree). Using Bayesian inference, we constrain these models with the latest\ndatasets: the Pantheon+ SN Ia sample (both with SH0ES calibration, PPS, and\nwithout it, PP), $H(z)$ measurements from CC and BAO as separate datasets, and\na Gaussian prior on $H_0$ from 2022 SH0ES baseline, $H_0=73.04 \\pm 1.04$\nkm/s/Mpc (R22 prior). We compare the models via information criteria such as\nAIC, BIC, DIC, and Bayesian evidence. Our results reveal that the Hubble\ntension persists, although it shows partial alleviation ($\\sim 1\\sigma$\ntension) in all investigated scenarios when local measurements are included.\nFor the flat $m=0$ case, the joint analysis yields $H_0 =\n71.05^{+0.62}_{-0.60}$ km/s/Mpc. Curved model initially favors $\\Omega_{K0} >\n0$ (at more than $2\\sigma$), but this preference shifts toward flatness once\nthe PPS+R22 prior are included. Notably, the current viscosity is constrained\nto $\\zeta_0 \\sim 10^6$ Pa s in all scenarios, in agreement with the\nthermodynamic requirements. Although model selection via BIC and Bayesian\nevidence favors $\\Lambda$CDM, AIC and DIC show mild support for viscous models\nin some datasets. Bulk viscous models moderately improve fits but neither\nresolve the Hubble tension nor outperform the $\\Lambda$CDM model. To achieve\nmore robust constraints, future analyses should incorporate CMB observations,\nwhich are expected to break parameter degeneracies involving $m$ and\n$\\tilde{\\zeta}_0$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether viscous cold dark matter (vCDM) in a\n$\\Lambda$-dominated FLRW universe can alleviate the Hubble tension while\nsatisfying thermodynamic constraints, examining both flat and curved\ngeometries. We model vCDM with bulk viscosity $\\zeta =\n\\zeta_0\\,(\\Omega_{vc}/\\Omega_{vc0})^m$, where $m$ determines the viscosity\nevolution and $\\Omega_{vc}$ is the density parameter of vCDM. We explore two\nparticular scenarios: constant viscosity ($m=0$), and variable viscosity ($m$\nfree). Using Bayesian inference, we constrain these models with the latest\ndatasets: the Pantheon+ SN Ia sample (both with SH0ES calibration, PPS, and\nwithout it, PP), $H(z)$ measurements from CC and BAO as separate datasets, and\na Gaussian prior on $H_0$ from 2022 SH0ES baseline, $H_0=73.04 \\pm 1.04$\nkm/s/Mpc (R22 prior). We compare the models via information criteria such as\nAIC, BIC, DIC, and Bayesian evidence. Our results reveal that the Hubble\ntension persists, although it shows partial alleviation ($\\sim 1\\sigma$\ntension) in all investigated scenarios when local measurements are included.\nFor the flat $m=0$ case, the joint analysis yields $H_0 =\n71.05^{+0.62}_{-0.60}$ km/s/Mpc. Curved model initially favors $\\Omega_{K0} >\n0$ (at more than $2\\sigma$), but this preference shifts toward flatness once\nthe PPS+R22 prior are included. Notably, the current viscosity is constrained\nto $\\zeta_0 \\sim 10^6$ Pa s in all scenarios, in agreement with the\nthermodynamic requirements. Although model selection via BIC and Bayesian\nevidence favors $\\Lambda$CDM, AIC and DIC show mild support for viscous models\nin some datasets. Bulk viscous models moderately improve fits but neither\nresolve the Hubble tension nor outperform the $\\Lambda$CDM model. To achieve\nmore robust constraints, future analyses should incorporate CMB observations,\nwhich are expected to break parameter degeneracies involving $m$ and\n$\\tilde{\\zeta}_0$."
                },
                "authors": [
                    {
                        "name": "R. Noemí Villalobos"
                    },
                    {
                        "name": "Yerko Vásquez"
                    },
                    {
                        "name": "Norman Cruz"
                    },
                    {
                        "name": "Carlos H. López-Caraballo"
                    }
                ],
                "author_detail": {
                    "name": "Carlos H. López-Caraballo"
                },
                "author": "Carlos H. López-Caraballo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12052v2",
                "updated": "2025-08-15T17:10:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    10,
                    35,
                    4,
                    227,
                    0
                ],
                "published": "2025-02-17T17:22:49Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    17,
                    22,
                    49,
                    0,
                    48,
                    0
                ],
                "title": "A Dual-Perspective NLG Meta-Evaluation Framework with Automatic\n  Benchmark and Better Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dual-Perspective NLG Meta-Evaluation Framework with Automatic\n  Benchmark and Better Interpretability"
                },
                "summary": "In NLG meta-evaluation, evaluation metrics are typically assessed based on\ntheir consistency with humans. However, we identify some limitations in\ntraditional NLG meta-evaluation approaches, such as issues in handling human\nratings and ambiguous selections of correlation measures, which undermine the\neffectiveness of meta-evaluation. In this work, we propose a dual-perspective\nNLG meta-evaluation framework that focuses on different evaluation\ncapabilities, thereby providing better interpretability. In addition, we\nintroduce a method of automatically constructing the corresponding benchmarks\nwithout requiring new human annotations. Furthermore, we conduct experiments\nwith 16 representative LLMs as the evaluators based on our proposed framework,\ncomprehensively analyzing their evaluation performance from different\nperspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In NLG meta-evaluation, evaluation metrics are typically assessed based on\ntheir consistency with humans. However, we identify some limitations in\ntraditional NLG meta-evaluation approaches, such as issues in handling human\nratings and ambiguous selections of correlation measures, which undermine the\neffectiveness of meta-evaluation. In this work, we propose a dual-perspective\nNLG meta-evaluation framework that focuses on different evaluation\ncapabilities, thereby providing better interpretability. In addition, we\nintroduce a method of automatically constructing the corresponding benchmarks\nwithout requiring new human annotations. Furthermore, we conduct experiments\nwith 16 representative LLMs as the evaluators based on our proposed framework,\ncomprehensively analyzing their evaluation performance from different\nperspectives."
                },
                "authors": [
                    {
                        "name": "Xinyu Hu"
                    },
                    {
                        "name": "Mingqi Gao"
                    },
                    {
                        "name": "Li Lin"
                    },
                    {
                        "name": "Zhenghan Yu"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11599v1",
                "updated": "2025-08-15T17:07:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    7,
                    54,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T17:07:54Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    7,
                    54,
                    4,
                    227,
                    0
                ],
                "title": "CryptoScope: Utilizing Large Language Models for Automated Cryptographic\n  Logic Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CryptoScope: Utilizing Large Language Models for Automated Cryptographic\n  Logic Vulnerability Detection"
                },
                "summary": "Cryptographic algorithms are fundamental to modern security, yet their\nimplementations frequently harbor subtle logic flaws that are hard to detect.\nWe introduce CryptoScope, a novel framework for automated cryptographic\nvulnerability detection powered by Large Language Models (LLMs). CryptoScope\ncombines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation\n(RAG), guided by a curated cryptographic knowledge base containing over 12,000\nentries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily\nderived from real-world CVE vulnerabilities, complemented by cryptographic\nchallenges from major Capture The Flag (CTF) competitions and synthetic\nexamples across 11 programming languages. CryptoScope consistently improves\nperformance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,\nGPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9\npreviously undisclosed flaws in widely used open-source cryptographic projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptographic algorithms are fundamental to modern security, yet their\nimplementations frequently harbor subtle logic flaws that are hard to detect.\nWe introduce CryptoScope, a novel framework for automated cryptographic\nvulnerability detection powered by Large Language Models (LLMs). CryptoScope\ncombines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation\n(RAG), guided by a curated cryptographic knowledge base containing over 12,000\nentries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily\nderived from real-world CVE vulnerabilities, complemented by cryptographic\nchallenges from major Capture The Flag (CTF) competitions and synthetic\nexamples across 11 programming languages. CryptoScope consistently improves\nperformance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,\nGPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9\npreviously undisclosed flaws in widely used open-source cryptographic projects."
                },
                "authors": [
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Zimo Ji"
                    },
                    {
                        "name": "Tao Zheng"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Xiao Lan"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Lan"
                },
                "author": "Xiao Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05147v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05147v2",
                "updated": "2025-08-15T16:46:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    46,
                    42,
                    4,
                    227,
                    0
                ],
                "published": "2025-04-07T14:52:40Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    52,
                    40,
                    0,
                    97,
                    0
                ],
                "title": "Pr$εε$mpt: Sanitizing Sensitive Prompts for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pr$εε$mpt: Sanitizing Sensitive Prompts for LLMs"
                },
                "summary": "The rise of large language models (LLMs) has introduced new privacy\nchallenges, particularly during inference where sensitive information in\nprompts may be exposed to proprietary LLM APIs. In this paper, we address the\nproblem of formally protecting the sensitive information contained in a prompt\nwhile maintaining response quality. To this end, first, we introduce a\ncryptographically inspired notion of a prompt sanitizer which transforms an\ninput prompt to protect its sensitive tokens. Second, we propose\nPr$\\epsilon\\epsilon$mpt, a novel system that implements a prompt sanitizer.\nPr$\\epsilon\\epsilon$mpt categorizes sensitive tokens into two types: (1) those\nwhere the LLM's response depends solely on the format (such as SSNs, credit\ncard numbers), for which we use format-preserving encryption (FPE); and (2)\nthose where the response depends on specific values, (such as age, salary) for\nwhich we apply metric differential privacy (mDP). Our evaluation demonstrates\nthat Pr$\\epsilon\\epsilon$mpt is a practical method to achieve meaningful\nprivacy guarantees, while maintaining high utility compared to unsanitized\nprompts, and outperforming prior methods",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has introduced new privacy\nchallenges, particularly during inference where sensitive information in\nprompts may be exposed to proprietary LLM APIs. In this paper, we address the\nproblem of formally protecting the sensitive information contained in a prompt\nwhile maintaining response quality. To this end, first, we introduce a\ncryptographically inspired notion of a prompt sanitizer which transforms an\ninput prompt to protect its sensitive tokens. Second, we propose\nPr$\\epsilon\\epsilon$mpt, a novel system that implements a prompt sanitizer.\nPr$\\epsilon\\epsilon$mpt categorizes sensitive tokens into two types: (1) those\nwhere the LLM's response depends solely on the format (such as SSNs, credit\ncard numbers), for which we use format-preserving encryption (FPE); and (2)\nthose where the response depends on specific values, (such as age, salary) for\nwhich we apply metric differential privacy (mDP). Our evaluation demonstrates\nthat Pr$\\epsilon\\epsilon$mpt is a practical method to achieve meaningful\nprivacy guarantees, while maintaining high utility compared to unsanitized\nprompts, and outperforming prior methods"
                },
                "authors": [
                    {
                        "name": "Amrita Roy Chowdhury"
                    },
                    {
                        "name": "David Glukhov"
                    },
                    {
                        "name": "Divyam Anshumaan"
                    },
                    {
                        "name": "Prasad Chalasani"
                    },
                    {
                        "name": "Nicolas Papernot"
                    },
                    {
                        "name": "Somesh Jha"
                    },
                    {
                        "name": "Mihir Bellare"
                    }
                ],
                "author_detail": {
                    "name": "Mihir Bellare"
                },
                "author": "Mihir Bellare",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05147v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05147v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11584v1",
                "updated": "2025-08-15T16:42:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    42,
                    23,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T16:42:23Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    42,
                    23,
                    4,
                    227,
                    0
                ],
                "title": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for\n  Robotic Vision Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for\n  Robotic Vision Tasks"
                },
                "summary": "Deploying multiple machine learning models on resource-constrained robotic\nplatforms for different perception tasks often results in redundant\ncomputations, large memory footprints, and complex integration challenges. In\nresponse, this work presents Visual Perception Engine (VPEngine), a modular\nframework designed to enable efficient GPU usage for visual multitasking while\nmaintaining extensibility and developer accessibility. Our framework\narchitecture leverages a shared foundation model backbone that extracts image\nrepresentations, which are efficiently shared, without any unnecessary GPU-CPU\nmemory transfers, across multiple specialized task-specific model heads running\nin parallel. This design eliminates the computational redundancy inherent in\nfeature extraction component when deploying traditional sequential models while\nenabling dynamic task prioritization based on application demands. We\ndemonstrate our framework's capabilities through an example implementation\nusing DINOv2 as the foundation model with multiple task (depth, object\ndetection and semantic segmentation) heads, achieving up to 3x speedup compared\nto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine\noffers efficient GPU utilization and maintains a constant memory footprint\nwhile allowing per-task inference frequencies to be adjusted dynamically during\nruntime. The framework is written in Python and is open source with ROS2 C++\n(Humble) bindings for ease of use by the robotics community across diverse\nrobotic platforms. Our example implementation demonstrates end-to-end real-time\nperformance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying multiple machine learning models on resource-constrained robotic\nplatforms for different perception tasks often results in redundant\ncomputations, large memory footprints, and complex integration challenges. In\nresponse, this work presents Visual Perception Engine (VPEngine), a modular\nframework designed to enable efficient GPU usage for visual multitasking while\nmaintaining extensibility and developer accessibility. Our framework\narchitecture leverages a shared foundation model backbone that extracts image\nrepresentations, which are efficiently shared, without any unnecessary GPU-CPU\nmemory transfers, across multiple specialized task-specific model heads running\nin parallel. This design eliminates the computational redundancy inherent in\nfeature extraction component when deploying traditional sequential models while\nenabling dynamic task prioritization based on application demands. We\ndemonstrate our framework's capabilities through an example implementation\nusing DINOv2 as the foundation model with multiple task (depth, object\ndetection and semantic segmentation) heads, achieving up to 3x speedup compared\nto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine\noffers efficient GPU utilization and maintains a constant memory footprint\nwhile allowing per-task inference frequencies to be adjusted dynamically during\nruntime. The framework is written in Python and is open source with ROS2 C++\n(Humble) bindings for ease of use by the robotics community across diverse\nrobotic platforms. Our example implementation demonstrates end-to-end real-time\nperformance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized\nmodels."
                },
                "authors": [
                    {
                        "name": "Jakub Łucki"
                    },
                    {
                        "name": "Jonathan Becktor"
                    },
                    {
                        "name": "Georgios Georgakis"
                    },
                    {
                        "name": "Robert Royce"
                    },
                    {
                        "name": "Shehryar Khattak"
                    }
                ],
                "author_detail": {
                    "name": "Shehryar Khattak"
                },
                "author": "Shehryar Khattak",
                "arxiv_comment": "6 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11582v1",
                "updated": "2025-08-15T16:40:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    40,
                    29,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T16:40:29Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    40,
                    29,
                    4,
                    227,
                    0
                ],
                "title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme\n  Reasoning Efficiency in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme\n  Reasoning Efficiency in Large Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "HuiKang Su"
                    },
                    {
                        "name": "Jiannan Guan"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18561v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18561v2",
                "updated": "2025-08-15T16:33:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    33,
                    32,
                    4,
                    227,
                    0
                ],
                "published": "2025-02-25T19:00:01Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    19,
                    0,
                    1,
                    1,
                    56,
                    0
                ],
                "title": "Testing a New Star Formation History Model from Principal Component\n  Analysis to Facilitate Spectral Synthesis Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing a New Star Formation History Model from Principal Component\n  Analysis to Facilitate Spectral Synthesis Modeling"
                },
                "summary": "The spectrum of a galaxy is a complicated convolution of many properties of\nthe galaxy, such as the star formation history (SFH), initial mass function,\nand metallicity. Inferring galaxy properties from the observed spectrum via\nspectral synthesis modeling is thus challenging. In particular, a simple yet\nflexible model for the SFH is required to obtain unbiased inferences. In this\npaper, we use SFHs from the IllustrisTNG and EAGLE simulations to test SFH\nmodels in terms of their capability of describing the simulated SFHs and the\nspectra generated from them. In addition to some commonly used SFH models\n($\\Gamma$, $\\tau$, and nonparametric), we also examine a model developed from\nprincipal component analysis (PCA), trained by a set of SFHs from IllustrisTNG.\nWe find that when using the first five principal components (eigenhistories),\nthe PCA-based models can achieve a good balance between simplicity and\naccuracy. Among the models tested, the PCA-based model provides high\nflexibility, by capturing diverse and complex simulated SFHs. To accurately\nreproduce spectra generated from the simulated SFHs, it is necessary to have a\ndegree of freedom to describe the most recent SFH (e.g., a step function\ncovering the age of 0 - 0.3 Gyr). Overall, the PCA+step model performs well in\ncapturing the diversity of SFHs and reproducing the associated spectra,\nsuggesting it is a promising and reliable approach for spectral synthesis\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spectrum of a galaxy is a complicated convolution of many properties of\nthe galaxy, such as the star formation history (SFH), initial mass function,\nand metallicity. Inferring galaxy properties from the observed spectrum via\nspectral synthesis modeling is thus challenging. In particular, a simple yet\nflexible model for the SFH is required to obtain unbiased inferences. In this\npaper, we use SFHs from the IllustrisTNG and EAGLE simulations to test SFH\nmodels in terms of their capability of describing the simulated SFHs and the\nspectra generated from them. In addition to some commonly used SFH models\n($\\Gamma$, $\\tau$, and nonparametric), we also examine a model developed from\nprincipal component analysis (PCA), trained by a set of SFHs from IllustrisTNG.\nWe find that when using the first five principal components (eigenhistories),\nthe PCA-based models can achieve a good balance between simplicity and\naccuracy. Among the models tested, the PCA-based model provides high\nflexibility, by capturing diverse and complex simulated SFHs. To accurately\nreproduce spectra generated from the simulated SFHs, it is necessary to have a\ndegree of freedom to describe the most recent SFH (e.g., a step function\ncovering the age of 0 - 0.3 Gyr). Overall, the PCA+step model performs well in\ncapturing the diversity of SFHs and reproducing the associated spectra,\nsuggesting it is a promising and reliable approach for spectral synthesis\nmodeling."
                },
                "authors": [
                    {
                        "name": "Yanzhe Zhang"
                    },
                    {
                        "name": "H. J. Mo"
                    },
                    {
                        "name": "Katherine E. Whitaker"
                    },
                    {
                        "name": "Shuang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shuang Zhou"
                },
                "author": "Shuang Zhou",
                "arxiv_doi": "10.3847/1538-4357/adeb70",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adeb70",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.18561v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18561v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 12 figures, 2 tables, accepted for publication in ApJ",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11575v1",
                "updated": "2025-08-15T16:31:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    31,
                    12,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T16:31:12Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    31,
                    12,
                    4,
                    227,
                    0
                ],
                "title": "Activate Me!: Designing Efficient Activation Functions for\n  Privacy-Preserving Machine Learning with Fully Homomorphic Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activate Me!: Designing Efficient Activation Functions for\n  Privacy-Preserving Machine Learning with Fully Homomorphic Encryption"
                },
                "summary": "The growing adoption of machine learning in sensitive areas such as\nhealthcare and defense introduces significant privacy and security challenges.\nThese domains demand robust data protection, as models depend on large volumes\nof sensitive information for both training and inference. Fully Homomorphic\nEncryption (FHE) presents a compelling solution by enabling computations\ndirectly on encrypted data, maintaining confidentiality across the entire\nmachine learning workflow. However, FHE inherently supports only linear\noperations, making it difficult to implement non-linear activation functions,\nessential components of modern neural networks. This work focuses on designing,\nimplementing, and evaluating activation functions tailored for FHE-based\nmachine learning. We investigate two commonly used functions: the Square\nfunction and Rectified Linear Unit (ReLU), using LeNet-5 and ResNet-20\narchitectures with the CKKS scheme from the OpenFHE library. For ReLU, we\nassess two methods: a conventional low-degree polynomial approximation and a\nnovel scheme-switching technique that securely evaluates ReLU under FHE\nconstraints. Our findings show that the Square function performs well in\nshallow networks like LeNet-5, achieving 99.4% accuracy with 128 seconds per\nimage. In contrast, deeper models like ResNet-20 benefit more from ReLU. The\npolynomial approximation yields 83.8% accuracy with 1,145 seconds per image,\nwhile our scheme-switching method improves accuracy to 89.8%, albeit with a\nlonger inference time of 1,697 seconds. These results underscore a critical\ntrade-off in FHE-based ML: faster activation functions often reduce accuracy,\nwhereas those preserving accuracy demand greater computational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of machine learning in sensitive areas such as\nhealthcare and defense introduces significant privacy and security challenges.\nThese domains demand robust data protection, as models depend on large volumes\nof sensitive information for both training and inference. Fully Homomorphic\nEncryption (FHE) presents a compelling solution by enabling computations\ndirectly on encrypted data, maintaining confidentiality across the entire\nmachine learning workflow. However, FHE inherently supports only linear\noperations, making it difficult to implement non-linear activation functions,\nessential components of modern neural networks. This work focuses on designing,\nimplementing, and evaluating activation functions tailored for FHE-based\nmachine learning. We investigate two commonly used functions: the Square\nfunction and Rectified Linear Unit (ReLU), using LeNet-5 and ResNet-20\narchitectures with the CKKS scheme from the OpenFHE library. For ReLU, we\nassess two methods: a conventional low-degree polynomial approximation and a\nnovel scheme-switching technique that securely evaluates ReLU under FHE\nconstraints. Our findings show that the Square function performs well in\nshallow networks like LeNet-5, achieving 99.4% accuracy with 128 seconds per\nimage. In contrast, deeper models like ResNet-20 benefit more from ReLU. The\npolynomial approximation yields 83.8% accuracy with 1,145 seconds per image,\nwhile our scheme-switching method improves accuracy to 89.8%, albeit with a\nlonger inference time of 1,697 seconds. These results underscore a critical\ntrade-off in FHE-based ML: faster activation functions often reduce accuracy,\nwhereas those preserving accuracy demand greater computational resources."
                },
                "authors": [
                    {
                        "name": "Nges Brian Njungle"
                    },
                    {
                        "name": "Michel A. Kinsy"
                    }
                ],
                "author_detail": {
                    "name": "Michel A. Kinsy"
                },
                "author": "Michel A. Kinsy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10603v2",
                "updated": "2025-08-15T16:07:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    7,
                    3,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-14T12:44:39Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    44,
                    39,
                    3,
                    226,
                    0
                ],
                "title": "Why Report Failed Interactions With Robots?! Towards Vignette-based\n  Interaction Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Report Failed Interactions With Robots?! Towards Vignette-based\n  Interaction Quality"
                },
                "summary": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods."
                },
                "authors": [
                    {
                        "name": "Agnes Axelsson"
                    },
                    {
                        "name": "Merle Reimann"
                    },
                    {
                        "name": "Ronald Cumbal"
                    },
                    {
                        "name": "Hannah Pelikan"
                    },
                    {
                        "name": "Divesh Lala"
                    }
                ],
                "author_detail": {
                    "name": "Divesh Lala"
                },
                "author": "Divesh Lala",
                "arxiv_comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025. 6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11548v1",
                "updated": "2025-08-15T15:50:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    50,
                    20,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T15:50:20Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    50,
                    20,
                    4,
                    227,
                    0
                ],
                "title": "Copyright Protection for Large Language Models: A Survey of Methods,\n  Challenges, and Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Copyright Protection for Large Language Models: A Survey of Methods,\n  Challenges, and Trends"
                },
                "summary": "Copyright protection for large language models is of critical importance,\ngiven their substantial development costs, proprietary value, and potential for\nmisuse. Existing surveys have predominantly focused on techniques for tracing\nLLM-generated content-namely, text watermarking-while a systematic exploration\nof methods for protecting the models themselves (i.e., model watermarking and\nmodel fingerprinting) remains absent. Moreover, the relationships and\ndistinctions among text watermarking, model watermarking, and model\nfingerprinting have not been comprehensively clarified. This work presents a\ncomprehensive survey of the current state of LLM copyright protection\ntechnologies, with a focus on model fingerprinting, covering the following\naspects: (1) clarifying the conceptual connection from text watermarking to\nmodel watermarking and fingerprinting, and adopting a unified terminology that\nincorporates model watermarking into the broader fingerprinting framework; (2)\nproviding an overview and comparison of diverse text watermarking techniques,\nhighlighting cases where such methods can function as model fingerprinting; (3)\nsystematically categorizing and comparing existing model fingerprinting\napproaches for LLM copyright protection; (4) presenting, for the first time,\ntechniques for fingerprint transfer and fingerprint removal; (5) summarizing\nevaluation metrics for model fingerprints, including effectiveness,\nharmlessness, robustness, stealthiness, and reliability; and (6) discussing\nopen challenges and future research directions. This survey aims to offer\nresearchers a thorough understanding of both text watermarking and model\nfingerprinting technologies in the era of LLMs, thereby fostering further\nadvances in protecting their intellectual property.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Copyright protection for large language models is of critical importance,\ngiven their substantial development costs, proprietary value, and potential for\nmisuse. Existing surveys have predominantly focused on techniques for tracing\nLLM-generated content-namely, text watermarking-while a systematic exploration\nof methods for protecting the models themselves (i.e., model watermarking and\nmodel fingerprinting) remains absent. Moreover, the relationships and\ndistinctions among text watermarking, model watermarking, and model\nfingerprinting have not been comprehensively clarified. This work presents a\ncomprehensive survey of the current state of LLM copyright protection\ntechnologies, with a focus on model fingerprinting, covering the following\naspects: (1) clarifying the conceptual connection from text watermarking to\nmodel watermarking and fingerprinting, and adopting a unified terminology that\nincorporates model watermarking into the broader fingerprinting framework; (2)\nproviding an overview and comparison of diverse text watermarking techniques,\nhighlighting cases where such methods can function as model fingerprinting; (3)\nsystematically categorizing and comparing existing model fingerprinting\napproaches for LLM copyright protection; (4) presenting, for the first time,\ntechniques for fingerprint transfer and fingerprint removal; (5) summarizing\nevaluation metrics for model fingerprints, including effectiveness,\nharmlessness, robustness, stealthiness, and reliability; and (6) discussing\nopen challenges and future research directions. This survey aims to offer\nresearchers a thorough understanding of both text watermarking and model\nfingerprinting technologies in the era of LLMs, thereby fostering further\nadvances in protecting their intellectual property."
                },
                "authors": [
                    {
                        "name": "Zhenhua Xu"
                    },
                    {
                        "name": "Xubin Yue"
                    },
                    {
                        "name": "Zhebo Wang"
                    },
                    {
                        "name": "Qichen Liu"
                    },
                    {
                        "name": "Xixiang Zhao"
                    },
                    {
                        "name": "Jingxuan Zhang"
                    },
                    {
                        "name": "Wenjun Zeng"
                    },
                    {
                        "name": "Wengpeng Xing"
                    },
                    {
                        "name": "Dezhang Kong"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10054v2",
                "updated": "2025-08-15T15:40:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    40,
                    50,
                    4,
                    227,
                    0
                ],
                "published": "2025-06-11T17:58:05Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    58,
                    5,
                    2,
                    162,
                    0
                ],
                "title": "Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of\n  LLMs"
                },
                "summary": "Direct Preference Optimization (DPO) has become a cornerstone of\nreinforcement learning from human feedback (RLHF) due to its simplicity and\nefficiency. However, existing DPO-based approaches typically treat all\npreference pairs uniformly, ignoring critical variations in their inherent\nquality and learning utility, leading to suboptimal data utilization and\nperformance. To address this challenge, we propose Omni-DPO, a dual-perspective\noptimization framework that jointly accounts for (1) the inherent quality of\neach preference pair and (2) the model's evolving performance on those pairs.\nBy adaptively weighting samples according to both data quality and the model's\nlearning dynamics during training, Omni-DPO enables more effective training\ndata utilization and achieves better performance. Experimental results on\nvarious models and benchmarks demonstrate the superiority and generalization\ncapabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it\nfinetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant\nmargin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning\ntasks, Omni-DPO consistently outperforms the baseline methods across all\nbenchmarks, providing strong empirical evidence for the effectiveness and\nrobustness of our approach. Code and models will be available at\nhttps://github.com/pspdada/Omni-DPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has become a cornerstone of\nreinforcement learning from human feedback (RLHF) due to its simplicity and\nefficiency. However, existing DPO-based approaches typically treat all\npreference pairs uniformly, ignoring critical variations in their inherent\nquality and learning utility, leading to suboptimal data utilization and\nperformance. To address this challenge, we propose Omni-DPO, a dual-perspective\noptimization framework that jointly accounts for (1) the inherent quality of\neach preference pair and (2) the model's evolving performance on those pairs.\nBy adaptively weighting samples according to both data quality and the model's\nlearning dynamics during training, Omni-DPO enables more effective training\ndata utilization and achieves better performance. Experimental results on\nvarious models and benchmarks demonstrate the superiority and generalization\ncapabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it\nfinetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant\nmargin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning\ntasks, Omni-DPO consistently outperforms the baseline methods across all\nbenchmarks, providing strong empirical evidence for the effectiveness and\nrobustness of our approach. Code and models will be available at\nhttps://github.com/pspdada/Omni-DPO."
                },
                "authors": [
                    {
                        "name": "Shangpin Peng"
                    },
                    {
                        "name": "Weinong Wang"
                    },
                    {
                        "name": "Zhuotao Tian"
                    },
                    {
                        "name": "Senqiao Yang"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Chengquan Zhang"
                    },
                    {
                        "name": "Takashi Isobe"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11542v1",
                "updated": "2025-08-15T15:38:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    38,
                    52,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T15:38:52Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    38,
                    52,
                    4,
                    227,
                    0
                ],
                "title": "Nested Operator Inference for Adaptive Data-Driven Learning of\n  Reduced-order Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nested Operator Inference for Adaptive Data-Driven Learning of\n  Reduced-order Models"
                },
                "summary": "This paper presents a data-driven, nested Operator Inference (OpInf) approach\nfor learning physics-informed reduced-order models (ROMs) from snapshot data of\nhigh-dimensional dynamical systems. The approach exploits the inherent\nhierarchy within the reduced space to iteratively construct initial guesses for\nthe OpInf learning problem that prioritize the interactions of the dominant\nmodes. The initial guess computed for any target reduced dimension corresponds\nto a ROM with provably smaller or equal snapshot reconstruction error than with\nstandard OpInf. Moreover, our nested OpInf algorithm can be warm-started from\npreviously learned models, enabling versatile application scenarios involving\ndynamic basis and model form updates. We demonstrate the performance of our\nalgorithm on a cubic heat conduction problem, with nested OpInf achieving a\nfour times smaller error than standard OpInf at a comparable offline time.\nFurther, we apply nested OpInf to a large-scale, parameterized model of the\nGreenland ice sheet where, despite model form approximation errors, it learns a\nROM with, on average, 3% error and computational speed-up factor above 19,000.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a data-driven, nested Operator Inference (OpInf) approach\nfor learning physics-informed reduced-order models (ROMs) from snapshot data of\nhigh-dimensional dynamical systems. The approach exploits the inherent\nhierarchy within the reduced space to iteratively construct initial guesses for\nthe OpInf learning problem that prioritize the interactions of the dominant\nmodes. The initial guess computed for any target reduced dimension corresponds\nto a ROM with provably smaller or equal snapshot reconstruction error than with\nstandard OpInf. Moreover, our nested OpInf algorithm can be warm-started from\npreviously learned models, enabling versatile application scenarios involving\ndynamic basis and model form updates. We demonstrate the performance of our\nalgorithm on a cubic heat conduction problem, with nested OpInf achieving a\nfour times smaller error than standard OpInf at a comparable offline time.\nFurther, we apply nested OpInf to a large-scale, parameterized model of the\nGreenland ice sheet where, despite model form approximation errors, it learns a\nROM with, on average, 3% error and computational speed-up factor above 19,000."
                },
                "authors": [
                    {
                        "name": "Nicole Aretz"
                    },
                    {
                        "name": "Karen Willcox"
                    }
                ],
                "author_detail": {
                    "name": "Karen Willcox"
                },
                "author": "Karen Willcox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11538v1",
                "updated": "2025-08-15T15:34:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    34,
                    56,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T15:34:56Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    34,
                    56,
                    4,
                    227,
                    0
                ],
                "title": "Reinforcing Video Reasoning Segmentation to Think Before It Segments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcing Video Reasoning Segmentation to Think Before It Segments"
                },
                "summary": "Video reasoning segmentation (VRS) endeavors to delineate referred objects in\nvideos guided by implicit instructions that encapsulate human intent and\ntemporal logic. Previous approaches leverage large vision language models\n(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.\nHowever, this paradigm suffers from limited interpretability during inference\nand suboptimal performance due to inadequate spatiotemporal reasoning. Drawing\ninspiration from seminal breakthroughs in reinforcement learning, we introduce\nVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in\nsegmentation. Veason-R1 is trained through Group Relative Policy Optimization\n(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we\ncurate high-quality CoT training data to instill structured reasoning\ntrajectories, bridging video-level semantics and frame-level spatial grounding,\nyielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO\nfine-tuning encourages efficient exploration of the reasoning space by\noptimizing reasoning chains. To this end, we incorporate a holistic reward\nmechanism that synergistically enhances spatial alignment and temporal\nconsistency, bolstering keyframe localization and fine-grained grounding.\nComprehensive empirical evaluations demonstrate that Veason-R1 achieves\nstate-of-the-art performance on multiple benchmarks, surpassing prior art by\nsignificant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),\nwhile exhibiting robustness to hallucinations (+8.8 R). Our code and model\nweights will be available at Veason-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video reasoning segmentation (VRS) endeavors to delineate referred objects in\nvideos guided by implicit instructions that encapsulate human intent and\ntemporal logic. Previous approaches leverage large vision language models\n(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.\nHowever, this paradigm suffers from limited interpretability during inference\nand suboptimal performance due to inadequate spatiotemporal reasoning. Drawing\ninspiration from seminal breakthroughs in reinforcement learning, we introduce\nVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in\nsegmentation. Veason-R1 is trained through Group Relative Policy Optimization\n(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we\ncurate high-quality CoT training data to instill structured reasoning\ntrajectories, bridging video-level semantics and frame-level spatial grounding,\nyielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO\nfine-tuning encourages efficient exploration of the reasoning space by\noptimizing reasoning chains. To this end, we incorporate a holistic reward\nmechanism that synergistically enhances spatial alignment and temporal\nconsistency, bolstering keyframe localization and fine-grained grounding.\nComprehensive empirical evaluations demonstrate that Veason-R1 achieves\nstate-of-the-art performance on multiple benchmarks, surpassing prior art by\nsignificant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),\nwhile exhibiting robustness to hallucinations (+8.8 R). Our code and model\nweights will be available at Veason-R1."
                },
                "authors": [
                    {
                        "name": "Sitong Gong"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Yunzhi Zhuge"
                    },
                    {
                        "name": "Xu Jia"
                    },
                    {
                        "name": "Pingping Zhang"
                    },
                    {
                        "name": "Huchuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Huchuan Lu"
                },
                "author": "Huchuan Lu",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11536v1",
                "updated": "2025-08-15T15:32:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    32,
                    19,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T15:32:19Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    32,
                    19,
                    4,
                    227,
                    0
                ],
                "title": "Language models align with brain regions that represent concepts across\n  modalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models align with brain regions that represent concepts across\n  modalities"
                },
                "summary": "Cognitive science and neuroscience have long faced the challenge of\ndisentangling representations of language from representations of conceptual\nmeaning. As the same problem arises in today's language models (LMs), we\ninvestigate the relationship between LM--brain alignment and two neural\nmetrics: (1) the level of brain activation during processing of sentences,\ntargeting linguistic processing, and (2) a novel measure of meaning consistency\nacross input modalities, which quantifies how consistently a brain region\nresponds to the same concept across paradigms (sentence, word cloud, image)\nusing an fMRI dataset (Pereira et al., 2018). Our experiments show that both\nlanguage-only and language-vision models predict the signal better in more\nmeaning-consistent areas of the brain, even when these areas are not strongly\nsensitive to language processing, suggesting that LMs might internally\nrepresent cross-modal conceptual meaning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive science and neuroscience have long faced the challenge of\ndisentangling representations of language from representations of conceptual\nmeaning. As the same problem arises in today's language models (LMs), we\ninvestigate the relationship between LM--brain alignment and two neural\nmetrics: (1) the level of brain activation during processing of sentences,\ntargeting linguistic processing, and (2) a novel measure of meaning consistency\nacross input modalities, which quantifies how consistently a brain region\nresponds to the same concept across paradigms (sentence, word cloud, image)\nusing an fMRI dataset (Pereira et al., 2018). Our experiments show that both\nlanguage-only and language-vision models predict the signal better in more\nmeaning-consistent areas of the brain, even when these areas are not strongly\nsensitive to language processing, suggesting that LMs might internally\nrepresent cross-modal conceptual meaning."
                },
                "authors": [
                    {
                        "name": "Maria Ryskina"
                    },
                    {
                        "name": "Greta Tuckute"
                    },
                    {
                        "name": "Alexander Fung"
                    },
                    {
                        "name": "Ashley Malkin"
                    },
                    {
                        "name": "Evelina Fedorenko"
                    }
                ],
                "author_detail": {
                    "name": "Evelina Fedorenko"
                },
                "author": "Evelina Fedorenko",
                "arxiv_comment": "Accepted to COLM 2025. Code and data can be found at\n  https://github.com/ryskina/concepts-brain-llms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08750v2",
                "updated": "2025-08-15T15:26:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    26,
                    46,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-12T08:55:45Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    55,
                    45,
                    1,
                    224,
                    0
                ],
                "title": "Challenging a binary neutron star merger interpretation of GW230529",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenging a binary neutron star merger interpretation of GW230529"
                },
                "summary": "GW230529_181500 represented the first gravitational-wave detection with one\nof the component objects' mass inferred to lie in the previously hypothesized\nmass gap between the heaviest neutron stars and the lightest observed black\nholes. Given the expected maximum mass values for neutron stars, this object\nwas identified as a black hole, and, with the secondary component being a\nneutron star, the detection was classified as a neutron star-black hole merger.\nHowever, due to the low signal-to-noise ratio and the known waveform degeneracy\nbetween the spin and mass ratio in the employed gravitational-wave models,\nGW230529_181500 could also be interpreted as a merger of two heavy ($\\gtrsim 2\n\\mathrm{M}_\\odot$) neutron stars with high spins. We investigate the\ndistinguishability of these scenarios by performing parameter estimation on\nsimulated signals obtained from numerical-relativity waveforms for both neutron\nstar-black hole and binary neutron star systems, with parameters consistent\nwith GW230529_181500, and comparing them to the analysis of the real event\ndata. We find that GW230529_181500 is more likely to have originated from a\nneutron star-black hole merger, though the possibility of a binary neutron star\norigin can not be ruled out. Moreover, we use the simulation data to estimate\nthe signatures of potential electromagnetic counterparts emitted by the\nsystems. We find them to be too dim to be located by current wide-field surveys\nif only the dynamical ejecta is considered, and detectable by the Vera C. Rubin\nObservatory during the first two days after merger if one accounts for\nadditional disk wind ejecta.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GW230529_181500 represented the first gravitational-wave detection with one\nof the component objects' mass inferred to lie in the previously hypothesized\nmass gap between the heaviest neutron stars and the lightest observed black\nholes. Given the expected maximum mass values for neutron stars, this object\nwas identified as a black hole, and, with the secondary component being a\nneutron star, the detection was classified as a neutron star-black hole merger.\nHowever, due to the low signal-to-noise ratio and the known waveform degeneracy\nbetween the spin and mass ratio in the employed gravitational-wave models,\nGW230529_181500 could also be interpreted as a merger of two heavy ($\\gtrsim 2\n\\mathrm{M}_\\odot$) neutron stars with high spins. We investigate the\ndistinguishability of these scenarios by performing parameter estimation on\nsimulated signals obtained from numerical-relativity waveforms for both neutron\nstar-black hole and binary neutron star systems, with parameters consistent\nwith GW230529_181500, and comparing them to the analysis of the real event\ndata. We find that GW230529_181500 is more likely to have originated from a\nneutron star-black hole merger, though the possibility of a binary neutron star\norigin can not be ruled out. Moreover, we use the simulation data to estimate\nthe signatures of potential electromagnetic counterparts emitted by the\nsystems. We find them to be too dim to be located by current wide-field surveys\nif only the dynamical ejecta is considered, and detectable by the Vera C. Rubin\nObservatory during the first two days after merger if one accounts for\nadditional disk wind ejecta."
                },
                "authors": [
                    {
                        "name": "Ivan Markin"
                    },
                    {
                        "name": "Anna Puecher"
                    },
                    {
                        "name": "Mattia Bulla"
                    },
                    {
                        "name": "Tim Dietrich"
                    }
                ],
                "author_detail": {
                    "name": "Tim Dietrich"
                },
                "author": "Tim Dietrich",
                "arxiv_comment": "20 pages, 9 figures, submitted to PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11534v1",
                "updated": "2025-08-15T15:22:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    22,
                    0,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T15:22:00Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    22,
                    0,
                    4,
                    227,
                    0
                ],
                "title": "Speciesism in AI: Evaluating Discrimination Against Animals in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speciesism in AI: Evaluating Discrimination Against Animals in Large\n  Language Models"
                },
                "summary": "As large language models (LLMs) become more widely deployed, it is crucial to\nexamine their ethical tendencies. Building on research on fairness and\ndiscrimination in AI, we investigate whether LLMs exhibit speciesist bias --\ndiscrimination based on species membership -- and how they value non-human\nanimals. We systematically examine this issue across three paradigms: (1)\nSpeciesismBench, a 1,003-item benchmark assessing recognition and moral\nevaluation of speciesist statements; (2) established psychological measures\ncomparing model responses with those of human participants; (3) text-generation\ntasks probing elaboration on, or resistance to, speciesist rationalizations. In\nour benchmark, LLMs reliably detected speciesist statements but rarely\ncondemned them, often treating speciesist attitudes as morally acceptable. On\npsychological measures, results were mixed: LLMs expressed slightly lower\nexplicit speciesism than people, yet in direct trade-offs they more often chose\nto save one human over multiple animals. A tentative interpretation is that\nLLMs may weight cognitive capacity rather than species per se: when capacities\nwere equal, they showed no species preference, and when an animal was described\nas more capable, they tended to prioritize it over a less capable human. In\nopen-ended text generation tasks, LLMs frequently normalized or rationalized\nharm toward farmed animals while refusing to do so for non-farmed animals.\nThese findings suggest that while LLMs reflect a mixture of progressive and\nmainstream human views, they nonetheless reproduce entrenched cultural norms\naround animal exploitation. We argue that expanding AI fairness and alignment\nframeworks to explicitly include non-human moral patients is essential for\nreducing these biases and preventing the entrenchment of speciesist attitudes\nin AI systems and the societies they influence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become more widely deployed, it is crucial to\nexamine their ethical tendencies. Building on research on fairness and\ndiscrimination in AI, we investigate whether LLMs exhibit speciesist bias --\ndiscrimination based on species membership -- and how they value non-human\nanimals. We systematically examine this issue across three paradigms: (1)\nSpeciesismBench, a 1,003-item benchmark assessing recognition and moral\nevaluation of speciesist statements; (2) established psychological measures\ncomparing model responses with those of human participants; (3) text-generation\ntasks probing elaboration on, or resistance to, speciesist rationalizations. In\nour benchmark, LLMs reliably detected speciesist statements but rarely\ncondemned them, often treating speciesist attitudes as morally acceptable. On\npsychological measures, results were mixed: LLMs expressed slightly lower\nexplicit speciesism than people, yet in direct trade-offs they more often chose\nto save one human over multiple animals. A tentative interpretation is that\nLLMs may weight cognitive capacity rather than species per se: when capacities\nwere equal, they showed no species preference, and when an animal was described\nas more capable, they tended to prioritize it over a less capable human. In\nopen-ended text generation tasks, LLMs frequently normalized or rationalized\nharm toward farmed animals while refusing to do so for non-farmed animals.\nThese findings suggest that while LLMs reflect a mixture of progressive and\nmainstream human views, they nonetheless reproduce entrenched cultural norms\naround animal exploitation. We argue that expanding AI fairness and alignment\nframeworks to explicitly include non-human moral patients is essential for\nreducing these biases and preventing the entrenchment of speciesist attitudes\nin AI systems and the societies they influence."
                },
                "authors": [
                    {
                        "name": "Monika Jotautaitė"
                    },
                    {
                        "name": "Lucius Caviola"
                    },
                    {
                        "name": "David A. Brewster"
                    },
                    {
                        "name": "Thilo Hagendorff"
                    }
                ],
                "author_detail": {
                    "name": "Thilo Hagendorff"
                },
                "author": "Thilo Hagendorff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01307v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01307v2",
                "updated": "2025-08-15T15:21:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    21,
                    46,
                    4,
                    227,
                    0
                ],
                "published": "2025-03-03T08:46:22Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    46,
                    22,
                    0,
                    62,
                    0
                ],
                "title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four\n  Habits of Highly Effective STaRs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four\n  Habits of Highly Effective STaRs"
                },
                "summary": "Test-time inference has emerged as a powerful paradigm for enabling language\nmodels to ``think'' longer and more carefully about complex challenges, much\nlike skilled human experts. While reinforcement learning (RL) can drive\nself-improvement in language models on verifiable tasks, some models exhibit\nsubstantial gains while others quickly plateau. For instance, we find that\nQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game\nof Countdown. This discrepancy raises a critical question: what intrinsic\nproperties enable effective self-improvement? We introduce a framework to\ninvestigate this question by analyzing four key cognitive behaviors --\nverification, backtracking, subgoal setting, and backward chaining -- that both\nexpert human problem solvers and successful language models employ. Our study\nreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama\ninitially lacks them. In systematic experimentation with controlled behavioral\ndatasets, we find that priming Llama with examples containing these reasoning\nbehaviors enables substantial improvements during RL, matching or exceeding\nQwen's performance. Importantly, the presence of reasoning behaviors, rather\nthan correctness of answers, proves to be the critical factor -- models primed\nwith incorrect solutions containing proper reasoning patterns achieve\ncomparable performance to those trained on correct solutions. Finally,\nleveraging continued pretraining with OpenWebMath data, filtered to amplify\nreasoning behaviors, enables the Llama model to match Qwen's self-improvement\ntrajectory. Our findings establish a fundamental relationship between initial\nreasoning behaviors and the capacity for improvement, explaining why some\nlanguage models effectively utilize additional computation while others\nplateau.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time inference has emerged as a powerful paradigm for enabling language\nmodels to ``think'' longer and more carefully about complex challenges, much\nlike skilled human experts. While reinforcement learning (RL) can drive\nself-improvement in language models on verifiable tasks, some models exhibit\nsubstantial gains while others quickly plateau. For instance, we find that\nQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game\nof Countdown. This discrepancy raises a critical question: what intrinsic\nproperties enable effective self-improvement? We introduce a framework to\ninvestigate this question by analyzing four key cognitive behaviors --\nverification, backtracking, subgoal setting, and backward chaining -- that both\nexpert human problem solvers and successful language models employ. Our study\nreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama\ninitially lacks them. In systematic experimentation with controlled behavioral\ndatasets, we find that priming Llama with examples containing these reasoning\nbehaviors enables substantial improvements during RL, matching or exceeding\nQwen's performance. Importantly, the presence of reasoning behaviors, rather\nthan correctness of answers, proves to be the critical factor -- models primed\nwith incorrect solutions containing proper reasoning patterns achieve\ncomparable performance to those trained on correct solutions. Finally,\nleveraging continued pretraining with OpenWebMath data, filtered to amplify\nreasoning behaviors, enables the Llama model to match Qwen's self-improvement\ntrajectory. Our findings establish a fundamental relationship between initial\nreasoning behaviors and the capacity for improvement, explaining why some\nlanguage models effectively utilize additional computation while others\nplateau."
                },
                "authors": [
                    {
                        "name": "Kanishk Gandhi"
                    },
                    {
                        "name": "Ayush Chakravarthy"
                    },
                    {
                        "name": "Anikait Singh"
                    },
                    {
                        "name": "Nathan Lile"
                    },
                    {
                        "name": "Noah D. Goodman"
                    }
                ],
                "author_detail": {
                    "name": "Noah D. Goodman"
                },
                "author": "Noah D. Goodman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01307v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01307v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11524v1",
                "updated": "2025-08-15T15:08:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    8,
                    7,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T15:08:07Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    8,
                    7,
                    4,
                    227,
                    0
                ],
                "title": "Inspire or Predict? Exploring New Paradigms in Assisting Classical\n  Planners with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspire or Predict? Exploring New Paradigms in Assisting Classical\n  Planners with Large Language Models"
                },
                "summary": "Addressing large-scale planning problems has become one of the central\nchallenges in the planning community, deriving from the state-space explosion\ncaused by growing objects and actions. Recently, researchers have explored the\neffectiveness of leveraging Large Language Models (LLMs) to generate helpful\nactions and states to prune the search space. However, prior works have largely\noverlooked integrating LLMs with domain-specific knowledge to ensure valid\nplans. In this paper, we propose a novel LLM-assisted planner integrated with\nproblem decomposition, which first decomposes large planning problems into\nmultiple simpler sub-tasks. Then we explore two novel paradigms to utilize\nLLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where\nLLM4Inspire provides heuristic guidance according to general knowledge and\nLLM4Predict employs domain-specific knowledge to infer intermediate conditions.\nWe empirically validate the effectiveness of our planner across multiple\ndomains, demonstrating the ability of search space partition when solving\nlarge-scale planning problems. The experimental results show that LLMs\neffectively locate feasible solutions when pruning the search space, where\ninfusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds\nparticular promise compared with LLM4Inspire, which offers general knowledge\nwithin LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing large-scale planning problems has become one of the central\nchallenges in the planning community, deriving from the state-space explosion\ncaused by growing objects and actions. Recently, researchers have explored the\neffectiveness of leveraging Large Language Models (LLMs) to generate helpful\nactions and states to prune the search space. However, prior works have largely\noverlooked integrating LLMs with domain-specific knowledge to ensure valid\nplans. In this paper, we propose a novel LLM-assisted planner integrated with\nproblem decomposition, which first decomposes large planning problems into\nmultiple simpler sub-tasks. Then we explore two novel paradigms to utilize\nLLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where\nLLM4Inspire provides heuristic guidance according to general knowledge and\nLLM4Predict employs domain-specific knowledge to infer intermediate conditions.\nWe empirically validate the effectiveness of our planner across multiple\ndomains, demonstrating the ability of search space partition when solving\nlarge-scale planning problems. The experimental results show that LLMs\neffectively locate feasible solutions when pruning the search space, where\ninfusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds\nparticular promise compared with LLM4Inspire, which offers general knowledge\nwithin LLMs."
                },
                "authors": [
                    {
                        "name": "Wenkai Yu"
                    },
                    {
                        "name": "Jianhang Tang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Shanjiang Tang"
                    },
                    {
                        "name": "Kebing Jin"
                    },
                    {
                        "name": "Hankz Hankui Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Hankz Hankui Zhuo"
                },
                "author": "Hankz Hankui Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01118v2",
                "updated": "2025-08-15T14:24:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    24,
                    5,
                    4,
                    227,
                    0
                ],
                "published": "2024-11-02T03:11:58Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    3,
                    11,
                    58,
                    5,
                    307,
                    0
                ],
                "title": "Variational Autoencoders for At-Source Data Reduction and Anomaly\n  Detection in High Energy Particle Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Autoencoders for At-Source Data Reduction and Anomaly\n  Detection in High Energy Particle Detectors"
                },
                "summary": "Detectors in next-generation high-energy physics experiments face several\ndaunting requirements, such as high data rates, damaging radiation exposure,\nand stringent constraints on power, space, and latency. To address these\nchallenges, machine learning in readout electronics can be leveraged for smart\ndetector designs, enabling intelligent inference and data reduction at-source.\nVariational autoencoders (VAEs) offer a variety of benefits for front-end\nreadout; an on-sensor encoder can perform efficient lossy data compression\nwhile simultaneously providing a latent space representation that can be used\nfor anomaly detection. Results are presented from low-latency and\nresource-efficient VAEs for front-end data processing in a futuristic silicon\npixel detector. Encoder-based data compression is found to preserve good\nperformance of off-detector analysis while significantly reducing the\noff-detector data rate as compared to a similarly sized data filtering\napproach. Furthermore, the latent space information is found to be a useful\ndiscriminator in the context of real-time sensor defect monitoring. Together,\nthese results highlight the multifaceted utility of autoencoder-based front-end\nreadout schemes and motivate their consideration in future detector designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detectors in next-generation high-energy physics experiments face several\ndaunting requirements, such as high data rates, damaging radiation exposure,\nand stringent constraints on power, space, and latency. To address these\nchallenges, machine learning in readout electronics can be leveraged for smart\ndetector designs, enabling intelligent inference and data reduction at-source.\nVariational autoencoders (VAEs) offer a variety of benefits for front-end\nreadout; an on-sensor encoder can perform efficient lossy data compression\nwhile simultaneously providing a latent space representation that can be used\nfor anomaly detection. Results are presented from low-latency and\nresource-efficient VAEs for front-end data processing in a futuristic silicon\npixel detector. Encoder-based data compression is found to preserve good\nperformance of off-detector analysis while significantly reducing the\noff-detector data rate as compared to a similarly sized data filtering\napproach. Furthermore, the latent space information is found to be a useful\ndiscriminator in the context of real-time sensor defect monitoring. Together,\nthese results highlight the multifaceted utility of autoencoder-based front-end\nreadout schemes and motivate their consideration in future detector designs."
                },
                "authors": [
                    {
                        "name": "Alexander Yue"
                    },
                    {
                        "name": "Haoyi Jia"
                    },
                    {
                        "name": "Julia Gonski"
                    }
                ],
                "author_detail": {
                    "name": "Julia Gonski"
                },
                "author": "Julia Gonski",
                "arxiv_doi": "10.1088/2632-2153/adf0c0",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/2632-2153/adf0c0",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 5 figures",
                "arxiv_journal_ref": "2025 Mach. Learn.: Sci. Technol. 6 035017",
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04996v3",
                "updated": "2025-08-15T14:21:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    21,
                    57,
                    4,
                    227,
                    0
                ],
                "published": "2025-07-07T13:34:49Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    34,
                    49,
                    0,
                    188,
                    0
                ],
                "title": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility\n  Systems"
                },
                "summary": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are viewed as vehicular systems capable of\nperceiving their environment and executing pre-programmed tasks independently\nof external input. However, both research and real-world deployments\nincreasingly showcase vehicles that demonstrate behaviors beyond this\ndefinition (including the SAE levels 0 to 5); Examples of this outpace include\nthe interaction with humans with natural language, goal adaptation, contextual\nreasoning, external tool use, and unseen ethical dilemma handling, largely\nempowered by multi-modal large language models (LLMs). These developments\nreveal a conceptual gap between technical autonomy and the broader cognitive\nand social capabilities needed for future human-centered mobility systems. To\naddress this gap, this paper introduces the concept of agentic vehicles (AgVs),\nreferring to vehicles that integrate agentic AI systems to reason, adapt, and\ninteract within complex environments. This paper proposes the term AgVs and\ntheir distinguishing characteristics from conventional AuVs. It synthesizes\nrelevant advances in integrating LLMs and AuVs and highlights how AgVs might\ntransform future mobility systems and ensure the systems are human-centered.\nThe paper concludes by identifying key challenges in the development and\ngovernance of AgVs, and how they can play a significant role in future agentic\ntransportation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are viewed as vehicular systems capable of\nperceiving their environment and executing pre-programmed tasks independently\nof external input. However, both research and real-world deployments\nincreasingly showcase vehicles that demonstrate behaviors beyond this\ndefinition (including the SAE levels 0 to 5); Examples of this outpace include\nthe interaction with humans with natural language, goal adaptation, contextual\nreasoning, external tool use, and unseen ethical dilemma handling, largely\nempowered by multi-modal large language models (LLMs). These developments\nreveal a conceptual gap between technical autonomy and the broader cognitive\nand social capabilities needed for future human-centered mobility systems. To\naddress this gap, this paper introduces the concept of agentic vehicles (AgVs),\nreferring to vehicles that integrate agentic AI systems to reason, adapt, and\ninteract within complex environments. This paper proposes the term AgVs and\ntheir distinguishing characteristics from conventional AuVs. It synthesizes\nrelevant advances in integrating LLMs and AuVs and highlights how AgVs might\ntransform future mobility systems and ensure the systems are human-centered.\nThe paper concludes by identifying key challenges in the development and\ngovernance of AgVs, and how they can play a significant role in future agentic\ntransportation systems."
                },
                "authors": [
                    {
                        "name": "Jiangbo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangbo Yu"
                },
                "author": "Jiangbo Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.06500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.06500v2",
                "updated": "2025-08-15T14:18:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    18,
                    48,
                    4,
                    227,
                    0
                ],
                "published": "2023-10-10T10:17:58Z",
                "published_parsed": [
                    2023,
                    10,
                    10,
                    10,
                    17,
                    58,
                    1,
                    283,
                    0
                ],
                "title": "MetaAgents: Large Language Model Based Agents for Decision-Making on\n  Teaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaAgents: Large Language Model Based Agents for Decision-Making on\n  Teaming"
                },
                "summary": "Significant advancements have occurred in the application of Large Language\nModels (LLMs) for social simulations. Despite this, their abilities to perform\nteaming in task-oriented social events are underexplored. Such capabilities are\ncrucial if LLMs are to effectively mimic human-like social behaviors and form\nefficient teams to solve tasks. To bridge this gap, we introduce MetaAgents, a\nsocial simulation framework populated with LLM-based agents. MetaAgents\nfacilitates agent engagement in conversations and a series of decision making\nwithin social contexts, serving as an appropriate platform for investigating\ninteractions and interpersonal decision-making of agents. In particular, we\nconstruct a job fair environment as a case study to scrutinize the team\nassembly and skill-matching behaviors of LLM-based agents. We take advantage of\nboth quantitative metrics evaluation and qualitative text analysis to assess\ntheir teaming abilities at the job fair. Our evaluation demonstrates that\nLLM-based agents perform competently in making rational decisions to develop\nefficient teams. However, we also identify limitations that hinder their\neffectiveness in more complex team assembly tasks. Our work provides valuable\ninsights into the role and evolution of LLMs in task-oriented social\nsimulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant advancements have occurred in the application of Large Language\nModels (LLMs) for social simulations. Despite this, their abilities to perform\nteaming in task-oriented social events are underexplored. Such capabilities are\ncrucial if LLMs are to effectively mimic human-like social behaviors and form\nefficient teams to solve tasks. To bridge this gap, we introduce MetaAgents, a\nsocial simulation framework populated with LLM-based agents. MetaAgents\nfacilitates agent engagement in conversations and a series of decision making\nwithin social contexts, serving as an appropriate platform for investigating\ninteractions and interpersonal decision-making of agents. In particular, we\nconstruct a job fair environment as a case study to scrutinize the team\nassembly and skill-matching behaviors of LLM-based agents. We take advantage of\nboth quantitative metrics evaluation and qualitative text analysis to assess\ntheir teaming abilities at the job fair. Our evaluation demonstrates that\nLLM-based agents perform competently in making rational decisions to develop\nefficient teams. However, we also identify limitations that hinder their\neffectiveness in more complex team assembly tasks. Our work provides valuable\ninsights into the role and evolution of LLMs in task-oriented social\nsimulations."
                },
                "authors": [
                    {
                        "name": "Yuan Li"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Yixuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Zhang"
                },
                "author": "Yixuan Zhang",
                "arxiv_doi": "10.1145/3711032",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711032",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.06500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.06500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08715v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08715v2",
                "updated": "2025-08-15T14:15:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    15,
                    19,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-12T07:58:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    7,
                    58,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation\n  Tutor with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation\n  Tutor with LLMs"
                },
                "summary": "Generative speech models have demonstrated significant potential in\npersonalizing teacher-student interactions, offering valuable real-world\napplications for language learning in children's education. However, achieving\nhigh-quality, child-friendly speech generation remains challenging,\nparticularly for low-resource languages across diverse languages and cultural\ncontexts. In this paper, we propose MultiAiTutor, an educational multilingual\ngenerative AI tutor with child-friendly designs, leveraging LLM architecture\nfor speech generation tailored for educational purposes. We propose to\nintegrate age-appropriate multilingual speech generation using LLM\narchitectures, facilitating young children's language learning through\nculturally relevant image-description tasks in three low-resource languages:\nSingaporean-accent Mandarin, Malay, and Tamil. Experimental results from both\nobjective metrics and subjective evaluations demonstrate the superior\nperformance of the proposed MultiAiTutor compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative speech models have demonstrated significant potential in\npersonalizing teacher-student interactions, offering valuable real-world\napplications for language learning in children's education. However, achieving\nhigh-quality, child-friendly speech generation remains challenging,\nparticularly for low-resource languages across diverse languages and cultural\ncontexts. In this paper, we propose MultiAiTutor, an educational multilingual\ngenerative AI tutor with child-friendly designs, leveraging LLM architecture\nfor speech generation tailored for educational purposes. We propose to\nintegrate age-appropriate multilingual speech generation using LLM\narchitectures, facilitating young children's language learning through\nculturally relevant image-description tasks in three low-resource languages:\nSingaporean-accent Mandarin, Malay, and Tamil. Experimental results from both\nobjective metrics and subjective evaluations demonstrate the superior\nperformance of the proposed MultiAiTutor compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Huayun Zhang"
                    },
                    {
                        "name": "Nancy F. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Nancy F. Chen"
                },
                "author": "Nancy F. Chen",
                "arxiv_comment": "We are withdrawing the manuscript to revise the title and contents of\n  figures for better alignment with the paper's contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08715v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08715v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11479v1",
                "updated": "2025-08-15T13:48:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    48,
                    15,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T13:48:15Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    48,
                    15,
                    4,
                    227,
                    0
                ],
                "title": "OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal\n  Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal\n  Navigation"
                },
                "summary": "Open-vocabulary Object Goal Navigation requires an embodied agent to reach\nobjects described by free-form language, including categories never seen during\ntraining. Existing end-to-end policies overfit small simulator datasets,\nachieving high success on training scenes but failing to generalize and\nexhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a\nlightweight transformer policy that tackles these issues with two synergistic\ncomponents. The first component is the semantic branch, which includes an\nencoder for the target binary mask and an auxiliary segmentation loss function,\ngrounding the textual goal and providing precise spatial cues. The second\ncomponent consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample\nscheduler that continuously balances imitation and reinforcement signals\naccording to the policy entropy, eliminating brittle manual phase switches.\nThese additions cut the sample complexity of training by 33%, and reduce\ncollision count in two times while keeping inference cost low (130M parameters,\nRGB-only input). On HM3D-OVON, our model matches the performance on unseen\ncategories to that on seen ones and establishes state-of-the-art results (40.1%\nSR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language\nmodels. Code is available at https://github.com/CognitiveAISystems/OVSegDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-vocabulary Object Goal Navigation requires an embodied agent to reach\nobjects described by free-form language, including categories never seen during\ntraining. Existing end-to-end policies overfit small simulator datasets,\nachieving high success on training scenes but failing to generalize and\nexhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a\nlightweight transformer policy that tackles these issues with two synergistic\ncomponents. The first component is the semantic branch, which includes an\nencoder for the target binary mask and an auxiliary segmentation loss function,\ngrounding the textual goal and providing precise spatial cues. The second\ncomponent consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample\nscheduler that continuously balances imitation and reinforcement signals\naccording to the policy entropy, eliminating brittle manual phase switches.\nThese additions cut the sample complexity of training by 33%, and reduce\ncollision count in two times while keeping inference cost low (130M parameters,\nRGB-only input). On HM3D-OVON, our model matches the performance on unseen\ncategories to that on seen ones and establishes state-of-the-art results (40.1%\nSR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language\nmodels. Code is available at https://github.com/CognitiveAISystems/OVSegDT."
                },
                "authors": [
                    {
                        "name": "Tatiana Zemskova"
                    },
                    {
                        "name": "Aleksei Staroverov"
                    },
                    {
                        "name": "Dmitry Yudin"
                    },
                    {
                        "name": "Aleksandr Panov"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandr Panov"
                },
                "author": "Aleksandr Panov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06972v2",
                "updated": "2025-08-15T13:42:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    42,
                    28,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-09T12:38:18Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    12,
                    38,
                    18,
                    5,
                    221,
                    0
                ],
                "title": "DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine\n  Learning"
                },
                "summary": "DSperse is a modular framework for distributed machine learning inference\nwith strategic cryptographic verification. Operating within the emerging\nparadigm of distributed zero-knowledge machine learning, DSperse avoids the\nhigh cost and rigidity of full-model circuitization by enabling targeted\nverification of strategically chosen subcomputations. These verifiable\nsegments, or \"slices\", may cover part or all of the inference pipeline, with\nglobal consistency enforced through audit, replication, or economic incentives.\nThis architecture supports a pragmatic form of trust minimization, localizing\nzero-knowledge proofs to the components where they provide the greatest value.\nWe evaluate DSperse using multiple proving systems and report empirical results\non memory usage, runtime, and circuit behavior under sliced and unsliced\nconfigurations. By allowing proof boundaries to align flexibly with the model's\nlogical structure, DSperse supports scalable, targeted verification strategies\nsuited to diverse deployment needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSperse is a modular framework for distributed machine learning inference\nwith strategic cryptographic verification. Operating within the emerging\nparadigm of distributed zero-knowledge machine learning, DSperse avoids the\nhigh cost and rigidity of full-model circuitization by enabling targeted\nverification of strategically chosen subcomputations. These verifiable\nsegments, or \"slices\", may cover part or all of the inference pipeline, with\nglobal consistency enforced through audit, replication, or economic incentives.\nThis architecture supports a pragmatic form of trust minimization, localizing\nzero-knowledge proofs to the components where they provide the greatest value.\nWe evaluate DSperse using multiple proving systems and report empirical results\non memory usage, runtime, and circuit behavior under sliced and unsliced\nconfigurations. By allowing proof boundaries to align flexibly with the model's\nlogical structure, DSperse supports scalable, targeted verification strategies\nsuited to diverse deployment needs."
                },
                "authors": [
                    {
                        "name": "Dan Ivanov"
                    },
                    {
                        "name": "Tristan Freiberg"
                    },
                    {
                        "name": "Shirin Shahabi"
                    },
                    {
                        "name": "Jonathan Gold"
                    },
                    {
                        "name": "Haruna Isah"
                    }
                ],
                "author_detail": {
                    "name": "Haruna Isah"
                },
                "author": "Haruna Isah",
                "arxiv_comment": "12 pages, 8 figures, and 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11469v1",
                "updated": "2025-08-15T13:34:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    34,
                    24,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T13:34:24Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    34,
                    24,
                    4,
                    227,
                    0
                ],
                "title": "CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement\n  Membrane Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement\n  Membrane Segmentation"
                },
                "summary": "Accurate segmentation of the glomerular basement membrane (GBM) in electron\nmicroscopy (EM) images is fundamental for quantifying membrane thickness and\nsupporting the diagnosis of various kidney diseases. While supervised deep\nlearning approaches achieve high segmentation accuracy, their reliance on\nextensive pixel-level annotation renders them impractical for clinical\nworkflows. Few-shot learning can reduce this annotation burden but often\nstruggles to capture the fine structural details necessary for GBM analysis. In\nthis study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot\nsegmentation pipeline designed for GBM delineation in EM images. CoFi first\ntrains a lightweight neural network using only three annotated images to\nproduce an initial coarse segmentation mask. This mask is then automatically\nprocessed to generate high-quality point prompts with morphology-aware pruning,\nwhich are subsequently used to guide SAM in refining the segmentation. The\nproposed method achieved exceptional GBM segmentation performance, with a Dice\ncoefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that\nCoFi not only alleviates the annotation and computational burdens associated\nwith conventional methods, but also achieves accurate and reliable segmentation\nresults. The pipeline's speed and annotation efficiency make it well-suited for\nresearch and hold strong potential for clinical applications in renal\npathology. The pipeline is publicly available at:\nhttps://github.com/ddrrnn123/CoFi.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate segmentation of the glomerular basement membrane (GBM) in electron\nmicroscopy (EM) images is fundamental for quantifying membrane thickness and\nsupporting the diagnosis of various kidney diseases. While supervised deep\nlearning approaches achieve high segmentation accuracy, their reliance on\nextensive pixel-level annotation renders them impractical for clinical\nworkflows. Few-shot learning can reduce this annotation burden but often\nstruggles to capture the fine structural details necessary for GBM analysis. In\nthis study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot\nsegmentation pipeline designed for GBM delineation in EM images. CoFi first\ntrains a lightweight neural network using only three annotated images to\nproduce an initial coarse segmentation mask. This mask is then automatically\nprocessed to generate high-quality point prompts with morphology-aware pruning,\nwhich are subsequently used to guide SAM in refining the segmentation. The\nproposed method achieved exceptional GBM segmentation performance, with a Dice\ncoefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that\nCoFi not only alleviates the annotation and computational burdens associated\nwith conventional methods, but also achieves accurate and reliable segmentation\nresults. The pipeline's speed and annotation efficiency make it well-suited for\nresearch and hold strong potential for clinical applications in renal\npathology. The pipeline is publicly available at:\nhttps://github.com/ddrrnn123/CoFi."
                },
                "authors": [
                    {
                        "name": "Hongjin Fang"
                    },
                    {
                        "name": "Daniel Reisenbüchler"
                    },
                    {
                        "name": "Kenji Ikemura"
                    },
                    {
                        "name": "Mert R. Sabuncu"
                    },
                    {
                        "name": "Yihe Yang"
                    },
                    {
                        "name": "Ruining Deng"
                    }
                ],
                "author_detail": {
                    "name": "Ruining Deng"
                },
                "author": "Ruining Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11468v1",
                "updated": "2025-08-15T13:33:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    33,
                    52,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T13:33:52Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    33,
                    52,
                    4,
                    227,
                    0
                ],
                "title": "TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation"
                },
                "summary": "Automatic code translation is a fundamental task in modern software\ndevelopment. While the advent of Large Language Models (LLMs) has significantly\nimproved the correctness of code translation, the critical dimension of\nexecution efficiency remains overlooked. To address this gap, we introduce\nTRACY, the first comprehensive benchmark designed to evaluate the execution\nefficiency of LLM-translated code. TRACY is constructed through an LLM-driven\ntwo-stage pipeline: an initial stage generates a suite of stress tests to\namplify performance differences, followed by an efficiency-oriented task\npruning stage that isolates the efficiency-distinguishing tasks. The resulting\nbenchmark comprises 1,011 code translation tasks across C++, Java, and Python,\neach accompanied by an average of 22.1 verified reference translations and 10\ncomputationally demanding tests. Our extensive evaluation of 26 representative\nLLMs reveals that even top-tier LLMs struggle to consistently produce efficient\ncode translations. For instance, Claude-4-think, the leading model for\ncorrectness, ranks eighth overall when time efficiency is taken into account,\nsurpassed by several smaller open-source models. We further pinpoint that\nalgorithmic flaws and improper resource handling are the most detrimental,\ncausing a median time slowdown of 5.6$\\times$ and memory increase of\n12.0$\\times$, respectively. Our work underscores the necessity of jointly\noptimizing for correctness and efficiency in future LLM-based code translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic code translation is a fundamental task in modern software\ndevelopment. While the advent of Large Language Models (LLMs) has significantly\nimproved the correctness of code translation, the critical dimension of\nexecution efficiency remains overlooked. To address this gap, we introduce\nTRACY, the first comprehensive benchmark designed to evaluate the execution\nefficiency of LLM-translated code. TRACY is constructed through an LLM-driven\ntwo-stage pipeline: an initial stage generates a suite of stress tests to\namplify performance differences, followed by an efficiency-oriented task\npruning stage that isolates the efficiency-distinguishing tasks. The resulting\nbenchmark comprises 1,011 code translation tasks across C++, Java, and Python,\neach accompanied by an average of 22.1 verified reference translations and 10\ncomputationally demanding tests. Our extensive evaluation of 26 representative\nLLMs reveals that even top-tier LLMs struggle to consistently produce efficient\ncode translations. For instance, Claude-4-think, the leading model for\ncorrectness, ranks eighth overall when time efficiency is taken into account,\nsurpassed by several smaller open-source models. We further pinpoint that\nalgorithmic flaws and improper resource handling are the most detrimental,\ncausing a median time slowdown of 5.6$\\times$ and memory increase of\n12.0$\\times$, respectively. Our work underscores the necessity of jointly\noptimizing for correctness and efficiency in future LLM-based code translation."
                },
                "authors": [
                    {
                        "name": "Zhihao Gong"
                    },
                    {
                        "name": "Zeyu Sun"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Qingyuan Liang"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Dan Hao"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hao"
                },
                "author": "Dan Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11460v1",
                "updated": "2025-08-15T13:17:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    17,
                    32,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T13:17:32Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    17,
                    32,
                    4,
                    227,
                    0
                ],
                "title": "Calibrated and uncertain? Evaluating uncertainty estimates in binary\n  classification models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated and uncertain? Evaluating uncertainty estimates in binary\n  classification models"
                },
                "summary": "Rigorous statistical methods, including parameter estimation with\naccompanying uncertainties, underpin the validity of scientific discovery,\nespecially in the natural sciences. With increasingly complex data models such\nas deep learning techniques, uncertainty quantification has become exceedingly\ndifficult and a plethora of techniques have been proposed. In this case study,\nwe use the unifying framework of approximate Bayesian inference combined with\nempirical tests on carefully created synthetic classification datasets to\ninvestigate qualitative properties of six different probabilistic machine\nlearning algorithms for class probability and uncertainty estimation: (i) a\nneural network ensemble, (ii) neural network ensemble with conflictual loss,\n(iii) evidential deep learning, (iv) a single neural network with Monte Carlo\nDropout, (v) Gaussian process classification and (vi) a Dirichlet process\nmixture model. We check if the algorithms produce uncertainty estimates which\nreflect commonly desired properties, such as being well calibrated and\nexhibiting an increase in uncertainty for out-of-distribution data points. Our\nresults indicate that all algorithms are well calibrated, but none of the deep\nlearning based algorithms provide uncertainties that consistently reflect lack\nof experimental evidence for out-of-distribution data points. We hope our study\nmay serve as a clarifying example for researchers developing new methods of\nuncertainty estimation for scientific data-driven modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous statistical methods, including parameter estimation with\naccompanying uncertainties, underpin the validity of scientific discovery,\nespecially in the natural sciences. With increasingly complex data models such\nas deep learning techniques, uncertainty quantification has become exceedingly\ndifficult and a plethora of techniques have been proposed. In this case study,\nwe use the unifying framework of approximate Bayesian inference combined with\nempirical tests on carefully created synthetic classification datasets to\ninvestigate qualitative properties of six different probabilistic machine\nlearning algorithms for class probability and uncertainty estimation: (i) a\nneural network ensemble, (ii) neural network ensemble with conflictual loss,\n(iii) evidential deep learning, (iv) a single neural network with Monte Carlo\nDropout, (v) Gaussian process classification and (vi) a Dirichlet process\nmixture model. We check if the algorithms produce uncertainty estimates which\nreflect commonly desired properties, such as being well calibrated and\nexhibiting an increase in uncertainty for out-of-distribution data points. Our\nresults indicate that all algorithms are well calibrated, but none of the deep\nlearning based algorithms provide uncertainties that consistently reflect lack\nof experimental evidence for out-of-distribution data points. We hope our study\nmay serve as a clarifying example for researchers developing new methods of\nuncertainty estimation for scientific data-driven modeling."
                },
                "authors": [
                    {
                        "name": "Aurora Grefsrud"
                    },
                    {
                        "name": "Nello Blaser"
                    },
                    {
                        "name": "Trygve Buanes"
                    }
                ],
                "author_detail": {
                    "name": "Trygve Buanes"
                },
                "author": "Trygve Buanes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13970v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13970v3",
                "updated": "2025-08-15T13:10:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    10,
                    24,
                    4,
                    227,
                    0
                ],
                "published": "2025-07-18T14:32:45Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    32,
                    45,
                    4,
                    199,
                    0
                ],
                "title": "A Segmented Robot Grasping Perception Neural Network for Edge AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Segmented Robot Grasping Perception Neural Network for Edge AI"
                },
                "summary": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation."
                },
                "authors": [
                    {
                        "name": "Casper Bröcheler"
                    },
                    {
                        "name": "Thomas Vroom"
                    },
                    {
                        "name": "Derrick Timmermans"
                    },
                    {
                        "name": "Alan van den Akker"
                    },
                    {
                        "name": "Guangzhi Tang"
                    },
                    {
                        "name": "Charalampos S. Kouzinopoulos"
                    },
                    {
                        "name": "Rico Möckel"
                    }
                ],
                "author_detail": {
                    "name": "Rico Möckel"
                },
                "author": "Rico Möckel",
                "arxiv_comment": "Accepted by SMC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13970v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13970v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.9; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11454v1",
                "updated": "2025-08-15T13:04:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    4,
                    32,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T13:04:32Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    4,
                    32,
                    4,
                    227,
                    0
                ],
                "title": "Reference Points in LLM Sentiment Analysis: The Role of Structured\n  Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference Points in LLM Sentiment Analysis: The Role of Structured\n  Context"
                },
                "summary": "Large language models (LLMs) are now widely used across many fields,\nincluding marketing research. Sentiment analysis, in particular, helps firms\nunderstand consumer preferences. While most NLP studies classify sentiment from\nreview text alone, marketing theories, such as prospect theory and\nexpectation--disconfirmation theory, point out that customer evaluations are\nshaped not only by the actual experience but also by additional reference\npoints. This study therefore investigates how the content and format of such\nsupplementary information affect sentiment analysis using LLMs. We compare\nnatural language (NL) and JSON-formatted prompts using a lightweight 3B\nparameter model suitable for practical marketing applications. Experiments on\ntwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt with\nadditional information outperforms all baselines without fine-tuning: Macro-F1\nrises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it\ndeployable in resource-constrained edge devices. Furthermore, a follow-up\nanalysis confirms that performance gains stem from genuine contextual reasoning\nrather than label proxying. This work demonstrates that structured prompting\ncan enable smaller models to achieve competitive performance, offering a\npractical alternative to large-scale model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are now widely used across many fields,\nincluding marketing research. Sentiment analysis, in particular, helps firms\nunderstand consumer preferences. While most NLP studies classify sentiment from\nreview text alone, marketing theories, such as prospect theory and\nexpectation--disconfirmation theory, point out that customer evaluations are\nshaped not only by the actual experience but also by additional reference\npoints. This study therefore investigates how the content and format of such\nsupplementary information affect sentiment analysis using LLMs. We compare\nnatural language (NL) and JSON-formatted prompts using a lightweight 3B\nparameter model suitable for practical marketing applications. Experiments on\ntwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt with\nadditional information outperforms all baselines without fine-tuning: Macro-F1\nrises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it\ndeployable in resource-constrained edge devices. Furthermore, a follow-up\nanalysis confirms that performance gains stem from genuine contextual reasoning\nrather than label proxying. This work demonstrates that structured prompting\ncan enable smaller models to achieve competitive performance, offering a\npractical alternative to large-scale model deployment."
                },
                "authors": [
                    {
                        "name": "Junichiro Niimi"
                    }
                ],
                "author_detail": {
                    "name": "Junichiro Niimi"
                },
                "author": "Junichiro Niimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11452v1",
                "updated": "2025-08-15T13:00:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    0,
                    7,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T13:00:07Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    0,
                    7,
                    4,
                    227,
                    0
                ],
                "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models\n  with Real-World Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models\n  with Real-World Apps"
                },
                "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking."
                },
                "authors": [
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Hongliang He"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Ruiqi Liang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "Our platform is publicly accessible at\n  https://doraemon.alipay.com/model-ranking",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11438v1",
                "updated": "2025-08-15T12:38:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    38,
                    48,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T12:38:48Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    38,
                    48,
                    4,
                    227,
                    0
                ],
                "title": "Simulation-based inference using splitting schemes for partially\n  observed diffusions in chemical reaction networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference using splitting schemes for partially\n  observed diffusions in chemical reaction networks"
                },
                "summary": "We address the problem of simulation and parameter inference for chemical\nreaction networks described by the chemical Langevin equation, a stochastic\ndifferential equation (SDE) representation of the dynamics of the chemical\nspecies. This is challenging for two main reasons. First, the\n(multi-dimensional) SDEs cannot be explicitly solved and are driven by\nmultiplicative and non-commutative noise, requiring the development of advanced\nnumerical schemes for their approximation and simulation. Second, not all\ncomponents of the SDEs are directly observed, as the available discrete-time\ndata are typically incomplete and/or perturbed with measurement error. We\ntackle these issues via three contributions. First, we show that these models\ncan be rewritten as perturbed conditionally Cox-Ingersoll-Ross-type SDEs, i.e.,\neach coordinate, conditioned on all other coordinates being fixed, follows an\nSDE with linear drift and square root diffusion coefficient perturbed by\nadditional Brownian motions. Second, for this class of SDEs, we develop a\nnumerical splitting scheme that preserves structural properties of the model,\nsuch as oscillations, state space and invariant distributions, unlike the\ncommonly used Euler-Maruyama scheme. Our numerical method is robust for large\nintegration time steps. Third, we propose a sequential Monte Carlo approximate\nBayesian computation algorithm incorporating \"data-conditional\" simulation and\nsequential learning of summary statistics, allowing inference for\nmultidimensional partially observed systems, further developing previous\nresults on fully observed systems based on the Euler-Maruyama scheme. We\nvalidate our approach on models of interest in chemical reaction networks, such\nas the stochastic Repressilator, Lotka-Volterra, and two-pool systems,\ndemonstrating its effectiveness, in terms of both numerical and inferential\naccuracy, and reduced computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of simulation and parameter inference for chemical\nreaction networks described by the chemical Langevin equation, a stochastic\ndifferential equation (SDE) representation of the dynamics of the chemical\nspecies. This is challenging for two main reasons. First, the\n(multi-dimensional) SDEs cannot be explicitly solved and are driven by\nmultiplicative and non-commutative noise, requiring the development of advanced\nnumerical schemes for their approximation and simulation. Second, not all\ncomponents of the SDEs are directly observed, as the available discrete-time\ndata are typically incomplete and/or perturbed with measurement error. We\ntackle these issues via three contributions. First, we show that these models\ncan be rewritten as perturbed conditionally Cox-Ingersoll-Ross-type SDEs, i.e.,\neach coordinate, conditioned on all other coordinates being fixed, follows an\nSDE with linear drift and square root diffusion coefficient perturbed by\nadditional Brownian motions. Second, for this class of SDEs, we develop a\nnumerical splitting scheme that preserves structural properties of the model,\nsuch as oscillations, state space and invariant distributions, unlike the\ncommonly used Euler-Maruyama scheme. Our numerical method is robust for large\nintegration time steps. Third, we propose a sequential Monte Carlo approximate\nBayesian computation algorithm incorporating \"data-conditional\" simulation and\nsequential learning of summary statistics, allowing inference for\nmultidimensional partially observed systems, further developing previous\nresults on fully observed systems based on the Euler-Maruyama scheme. We\nvalidate our approach on models of interest in chemical reaction networks, such\nas the stochastic Repressilator, Lotka-Volterra, and two-pool systems,\ndemonstrating its effectiveness, in terms of both numerical and inferential\naccuracy, and reduced computational cost."
                },
                "authors": [
                    {
                        "name": "Petar Jovanovski"
                    },
                    {
                        "name": "Andrew Golightly"
                    },
                    {
                        "name": "Umberto Picchini"
                    },
                    {
                        "name": "Massimiliano Tamborrino"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Tamborrino"
                },
                "author": "Massimiliano Tamborrino",
                "arxiv_comment": "35 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10649v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10649v3",
                "updated": "2025-08-15T12:25:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    25,
                    21,
                    4,
                    227,
                    0
                ],
                "published": "2024-10-14T16:00:20Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    0,
                    20,
                    0,
                    288,
                    0
                ],
                "title": "Vecchia Gaussian Processes: Probabilistic Properties, Minimax Rates and\n  Methodological Developments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vecchia Gaussian Processes: Probabilistic Properties, Minimax Rates and\n  Methodological Developments"
                },
                "summary": "Gaussian Processes (GPs) are widely used to model dependencies in spatial\nstatistics and machine learning; however, exact inference is computationally\nintractable, with a time complexity of $O(n^3)$. Vecchia approximation allows\nscalable Bayesian inference of GPs by introducing sparsity in the spatial\ndependency structure characterized by a directed acyclic graph (DAG). Despite\nits practical popularity, the approach lacks rigorous theoretical foundations,\nand the choice of DAG structure remains an open question. In this paper, we\nsystematically study Vecchia GPs as standalone stochastic processes, uncovering\nkey probabilistic and statistical properties. For probabilistic properties, we\nprove that the conditional distributions of the Mat\\'{e}rn GPs, as well as\ntheir Vecchia approximations, can be characterized by polynomial\ninterpolations. This allows us to prove a series of results regarding small\nball probabilities and Reproducing Kernel Hilbert Spaces (RKHSs) of Vecchia\nGPs. For the statistical methodology, we provide a principled guideline for\nselecting parent sets as norming sets with fixed cardinality, and we develop\nalgorithms along these guidelines. In terms of theoretical guarantees, we\nestablish posterior contraction rates for Vecchia GPs in the nonparametric\nregression model and show that minimax-optimal rates are attained under oracle\nrescaling or hierarchical Bayesian tuning. We demonstrate our theoretical\nresults and methodology through numerical studies and provide an efficient\nimplementation of our methods in C++, with interfaces in R.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Processes (GPs) are widely used to model dependencies in spatial\nstatistics and machine learning; however, exact inference is computationally\nintractable, with a time complexity of $O(n^3)$. Vecchia approximation allows\nscalable Bayesian inference of GPs by introducing sparsity in the spatial\ndependency structure characterized by a directed acyclic graph (DAG). Despite\nits practical popularity, the approach lacks rigorous theoretical foundations,\nand the choice of DAG structure remains an open question. In this paper, we\nsystematically study Vecchia GPs as standalone stochastic processes, uncovering\nkey probabilistic and statistical properties. For probabilistic properties, we\nprove that the conditional distributions of the Mat\\'{e}rn GPs, as well as\ntheir Vecchia approximations, can be characterized by polynomial\ninterpolations. This allows us to prove a series of results regarding small\nball probabilities and Reproducing Kernel Hilbert Spaces (RKHSs) of Vecchia\nGPs. For the statistical methodology, we provide a principled guideline for\nselecting parent sets as norming sets with fixed cardinality, and we develop\nalgorithms along these guidelines. In terms of theoretical guarantees, we\nestablish posterior contraction rates for Vecchia GPs in the nonparametric\nregression model and show that minimax-optimal rates are attained under oracle\nrescaling or hierarchical Bayesian tuning. We demonstrate our theoretical\nresults and methodology through numerical studies and provide an efficient\nimplementation of our methods in C++, with interfaces in R."
                },
                "authors": [
                    {
                        "name": "Botond Szabo"
                    },
                    {
                        "name": "Yichen Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yichen Zhu"
                },
                "author": "Yichen Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10649v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G08, 60G15, 62H11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11434v1",
                "updated": "2025-08-15T12:24:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    24,
                    22,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T12:24:22Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    24,
                    22,
                    4,
                    227,
                    0
                ],
                "title": "Online Anti-sexist Speech: Identifying Resistance to Gender Bias in\n  Political Discourse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Anti-sexist Speech: Identifying Resistance to Gender Bias in\n  Political Discourse"
                },
                "summary": "Anti-sexist speech, i.e., public expressions that challenge or resist\ngendered abuse and sexism, plays a vital role in shaping democratic debate\nonline. Yet automated content moderation systems, increasingly powered by large\nlanguage models (LLMs), may struggle to distinguish such resistance from the\nsexism it opposes. This study examines how five LLMs classify sexist,\nanti-sexist, and neutral political tweets from the UK, focusing on\nhigh-salience trigger events involving female Members of Parliament in the year\n2022. Our analysis show that models frequently misclassify anti-sexist speech\nas harmful, particularly during politically charged events where rhetorical\nstyles of harm and resistance converge. These errors risk silencing those who\nchallenge sexism, with disproportionate consequences for marginalised voices.\nWe argue that moderation design must move beyond binary harmful/not-harmful\nschemas, integrate human-in-the-loop review during sensitive events, and\nexplicitly include counter-speech in training data. By linking feminist\nscholarship, event-based analysis, and model evaluation, this work highlights\nthe sociotechnical challenges of safeguarding resistance speech in digital\npolitical spaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anti-sexist speech, i.e., public expressions that challenge or resist\ngendered abuse and sexism, plays a vital role in shaping democratic debate\nonline. Yet automated content moderation systems, increasingly powered by large\nlanguage models (LLMs), may struggle to distinguish such resistance from the\nsexism it opposes. This study examines how five LLMs classify sexist,\nanti-sexist, and neutral political tweets from the UK, focusing on\nhigh-salience trigger events involving female Members of Parliament in the year\n2022. Our analysis show that models frequently misclassify anti-sexist speech\nas harmful, particularly during politically charged events where rhetorical\nstyles of harm and resistance converge. These errors risk silencing those who\nchallenge sexism, with disproportionate consequences for marginalised voices.\nWe argue that moderation design must move beyond binary harmful/not-harmful\nschemas, integrate human-in-the-loop review during sensitive events, and\nexplicitly include counter-speech in training data. By linking feminist\nscholarship, event-based analysis, and model evaluation, this work highlights\nthe sociotechnical challenges of safeguarding resistance speech in digital\npolitical spaces."
                },
                "authors": [
                    {
                        "name": "Aditi Dutta"
                    },
                    {
                        "name": "Susan Banducci"
                    }
                ],
                "author_detail": {
                    "name": "Susan Banducci"
                },
                "author": "Susan Banducci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06371v2",
                "updated": "2025-08-15T12:22:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    22,
                    31,
                    4,
                    227,
                    0
                ],
                "published": "2025-06-04T12:11:05Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    12,
                    11,
                    5,
                    2,
                    155,
                    0
                ],
                "title": "Relationship Detection on Tabular Data Using Statistical Analysis and\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relationship Detection on Tabular Data Using Statistical Analysis and\n  Large Language Models"
                },
                "summary": "Over the past few years, table interpretation tasks have made significant\nprogress due to their importance and the introduction of new technologies and\nbenchmarks in the field. This work experiments with a hybrid approach for\ndetecting relationships among columns of unlabeled tabular data, using a\nKnowledge Graph (KG) as a reference point, a task known as CPA. This approach\nleverages large language models (LLMs) while employing statistical analysis to\nreduce the search space of potential KG relations. The main modules of this\napproach for reducing the search space are domain and range constraints\ndetection, as well as relation co-appearance analysis. The experimental\nevaluation on two benchmark datasets provided by the SemTab challenge assesses\nthe influence of each module and the effectiveness of different\nstate-of-the-art LLMs at various levels of quantization. The experiments were\nperformed, as well as at different prompting techniques. The proposed\nmethodology, which is publicly available on github, proved to be competitive\nwith state-of-the-art approaches on these datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past few years, table interpretation tasks have made significant\nprogress due to their importance and the introduction of new technologies and\nbenchmarks in the field. This work experiments with a hybrid approach for\ndetecting relationships among columns of unlabeled tabular data, using a\nKnowledge Graph (KG) as a reference point, a task known as CPA. This approach\nleverages large language models (LLMs) while employing statistical analysis to\nreduce the search space of potential KG relations. The main modules of this\napproach for reducing the search space are domain and range constraints\ndetection, as well as relation co-appearance analysis. The experimental\nevaluation on two benchmark datasets provided by the SemTab challenge assesses\nthe influence of each module and the effectiveness of different\nstate-of-the-art LLMs at various levels of quantization. The experiments were\nperformed, as well as at different prompting techniques. The proposed\nmethodology, which is publicly available on github, proved to be competitive\nwith state-of-the-art approaches on these datasets."
                },
                "authors": [
                    {
                        "name": "Panagiotis Koletsis"
                    },
                    {
                        "name": "Christos Panagiotopoulos"
                    },
                    {
                        "name": "Georgios Th. Papadopoulos"
                    },
                    {
                        "name": "Vasilis Efthymiou"
                    }
                ],
                "author_detail": {
                    "name": "Vasilis Efthymiou"
                },
                "author": "Vasilis Efthymiou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02363v2",
                "updated": "2025-08-15T12:20:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    20,
                    22,
                    4,
                    227,
                    0
                ],
                "published": "2025-02-04T14:46:08Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    46,
                    8,
                    1,
                    35,
                    0
                ],
                "title": "FAB-PPI: Frequentist, Assisted by Bayes, Prediction-Powered Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAB-PPI: Frequentist, Assisted by Bayes, Prediction-Powered Inference"
                },
                "summary": "Prediction-powered inference (PPI) enables valid statistical inference by\ncombining experimental data with machine learning predictions. When a\nsufficient number of high-quality predictions is available, PPI results in more\naccurate estimates and tighter confidence intervals than traditional methods.\nIn this paper, we propose to inform the PPI framework with prior knowledge on\nthe quality of the predictions. The resulting method, which we call\nfrequentist, assisted by Bayes, PPI (FAB-PPI), improves over PPI when the\nobserved prediction quality is likely under the prior, while maintaining its\nfrequentist guarantees. Furthermore, when using heavy-tailed priors, FAB-PPI\nadaptively reverts to standard PPI in low prior probability regions. We\ndemonstrate the benefits of FAB-PPI in real and synthetic examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction-powered inference (PPI) enables valid statistical inference by\ncombining experimental data with machine learning predictions. When a\nsufficient number of high-quality predictions is available, PPI results in more\naccurate estimates and tighter confidence intervals than traditional methods.\nIn this paper, we propose to inform the PPI framework with prior knowledge on\nthe quality of the predictions. The resulting method, which we call\nfrequentist, assisted by Bayes, PPI (FAB-PPI), improves over PPI when the\nobserved prediction quality is likely under the prior, while maintaining its\nfrequentist guarantees. Furthermore, when using heavy-tailed priors, FAB-PPI\nadaptively reverts to standard PPI in low prior probability regions. We\ndemonstrate the benefits of FAB-PPI in real and synthetic examples."
                },
                "authors": [
                    {
                        "name": "Stefano Cortinovis"
                    },
                    {
                        "name": "François Caron"
                    }
                ],
                "author_detail": {
                    "name": "François Caron"
                },
                "author": "François Caron",
                "arxiv_comment": "29 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11431v1",
                "updated": "2025-08-15T12:15:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    15,
                    6,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T12:15:06Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    15,
                    6,
                    4,
                    227,
                    0
                ],
                "title": "Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian\n  Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian\n  Splatting"
                },
                "summary": "Understanding what semantic information persists after object removal is\ncritical for privacy-preserving 3D reconstruction and editable scene\nrepresentations. In this work, we introduce a novel benchmark and evaluation\nframework to measure semantic residuals, the unintended semantic traces left\nbehind, after object removal in 3D Gaussian Splatting. We conduct experiments\nacross a diverse set of indoor and outdoor scenes, showing that current methods\ncan preserve semantic information despite the absence of visual geometry. We\nalso release Remove360, a dataset of pre/post-removal RGB images and\nobject-level masks captured in real-world environments. While prior datasets\nhave focused on isolated object instances, Remove360 covers a broader and more\ncomplex range of indoor and outdoor scenes, enabling evaluation of object\nremoval in the context of full-scene representations. Given ground truth images\nof a scene before and after object removal, we assess whether we can truly\neliminate semantic presence, and if downstream models can still infer what was\nremoved. Our findings reveal critical limitations in current 3D object removal\ntechniques and underscore the need for more robust solutions capable of\nhandling real-world complexity. The evaluation framework is available at\ngithub.com/spatial-intelligence-ai/Remove360.git. Data are available at\nhuggingface.co/datasets/simkoc/Remove360.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding what semantic information persists after object removal is\ncritical for privacy-preserving 3D reconstruction and editable scene\nrepresentations. In this work, we introduce a novel benchmark and evaluation\nframework to measure semantic residuals, the unintended semantic traces left\nbehind, after object removal in 3D Gaussian Splatting. We conduct experiments\nacross a diverse set of indoor and outdoor scenes, showing that current methods\ncan preserve semantic information despite the absence of visual geometry. We\nalso release Remove360, a dataset of pre/post-removal RGB images and\nobject-level masks captured in real-world environments. While prior datasets\nhave focused on isolated object instances, Remove360 covers a broader and more\ncomplex range of indoor and outdoor scenes, enabling evaluation of object\nremoval in the context of full-scene representations. Given ground truth images\nof a scene before and after object removal, we assess whether we can truly\neliminate semantic presence, and if downstream models can still infer what was\nremoved. Our findings reveal critical limitations in current 3D object removal\ntechniques and underscore the need for more robust solutions capable of\nhandling real-world complexity. The evaluation framework is available at\ngithub.com/spatial-intelligence-ai/Remove360.git. Data are available at\nhuggingface.co/datasets/simkoc/Remove360."
                },
                "authors": [
                    {
                        "name": "Simona Kocour"
                    },
                    {
                        "name": "Assia Benbihi"
                    },
                    {
                        "name": "Torsten Sattler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Sattler"
                },
                "author": "Torsten Sattler",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2503.17574",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05118v3",
                "updated": "2025-08-15T12:14:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    14,
                    24,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-07T07:51:38Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    51,
                    38,
                    3,
                    219,
                    0
                ],
                "title": "Exploring Superior Function Calls via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Superior Function Calls via Reinforcement Learning"
                },
                "summary": "Function calling capabilities are crucial for deploying Large Language Models\nin real-world applications, yet current training approaches fail to develop\nrobust reasoning strategies. Supervised fine-tuning produces models that rely\non superficial pattern matching, while standard reinforcement learning methods\nstruggle with the complex action space of structured function calls. We present\na novel reinforcement learning framework designed to enhance group relative\npolicy optimization through strategic entropy based exploration specifically\ntailored for function calling tasks. Our approach addresses three critical\nchallenges in function calling: insufficient exploration during policy\nlearning, lack of structured reasoning in chain-of-thought generation, and\ninadequate verification of parameter extraction. Our two-stage data preparation\npipeline ensures high-quality training samples through iterative LLM evaluation\nand abstract syntax tree validation. Extensive experiments on the Berkeley\nFunction Calling Leaderboard demonstrate that this framework achieves\nstate-of-the-art performance among open-source models with 86.02\\% overall\naccuracy, outperforming standard GRPO by up to 6\\% on complex multi-function\nscenarios. Notably, our method shows particularly strong improvements on\ncode-pretrained models, suggesting that structured language generation\ncapabilities provide an advantageous starting point for reinforcement learning\nin function calling tasks. We will release all the code, models and dataset to\nbenefit the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function calling capabilities are crucial for deploying Large Language Models\nin real-world applications, yet current training approaches fail to develop\nrobust reasoning strategies. Supervised fine-tuning produces models that rely\non superficial pattern matching, while standard reinforcement learning methods\nstruggle with the complex action space of structured function calls. We present\na novel reinforcement learning framework designed to enhance group relative\npolicy optimization through strategic entropy based exploration specifically\ntailored for function calling tasks. Our approach addresses three critical\nchallenges in function calling: insufficient exploration during policy\nlearning, lack of structured reasoning in chain-of-thought generation, and\ninadequate verification of parameter extraction. Our two-stage data preparation\npipeline ensures high-quality training samples through iterative LLM evaluation\nand abstract syntax tree validation. Extensive experiments on the Berkeley\nFunction Calling Leaderboard demonstrate that this framework achieves\nstate-of-the-art performance among open-source models with 86.02\\% overall\naccuracy, outperforming standard GRPO by up to 6\\% on complex multi-function\nscenarios. Notably, our method shows particularly strong improvements on\ncode-pretrained models, suggesting that structured language generation\ncapabilities provide an advantageous starting point for reinforcement learning\nin function calling tasks. We will release all the code, models and dataset to\nbenefit the community."
                },
                "authors": [
                    {
                        "name": "Bingguang Hao"
                    },
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Zengzhuang Xu"
                    },
                    {
                        "name": "Yicheng Chen"
                    },
                    {
                        "name": "Cunyin Peng"
                    },
                    {
                        "name": "Jinjie GU"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Chenyi Zhuang"
                },
                "author": "Chenyi Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01460v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01460v4",
                "updated": "2025-08-15T12:13:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    13,
                    58,
                    4,
                    227,
                    0
                ],
                "published": "2024-12-31T10:43:19Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    10,
                    43,
                    19,
                    1,
                    366,
                    0
                ],
                "title": "GDSR: Global-Detail Integration through Dual-Branch Network with Wavelet\n  Losses for Remote Sensing Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GDSR: Global-Detail Integration through Dual-Branch Network with Wavelet\n  Losses for Remote Sensing Image Super-Resolution"
                },
                "summary": "In recent years, deep neural networks, including Convolutional Neural\nNetworks, Transformers, and State Space Models, have achieved significant\nprogress in Remote Sensing Image (RSI) Super-Resolution (SR). However, existing\nSR methods typically overlook the complementary relationship between global and\nlocal dependencies. These methods either focus on capturing local information\nor prioritize global information, which results in models that are unable to\neffectively capture both global and local features simultaneously. Moreover,\ntheir computational cost becomes prohibitive when applied to large-scale RSIs.\nTo address these challenges, we introduce the novel application of Receptance\nWeighted Key Value (RWKV) to RSI-SR, which captures long-range dependencies\nwith linear complexity. To simultaneously model global and local features, we\npropose the Global-Detail dual-branch structure, GDSR, which performs SR by\nparalleling RWKV and convolutional operations to handle large-scale RSIs.\nFurthermore, we introduce the Global-Detail Reconstruction Module (GDRM) as an\nintermediary between the two branches to bridge their complementary roles. In\naddition, we propose the Dual-Group Multi-Scale Wavelet Loss, a wavelet-domain\nconstraint mechanism via dual-group subband strategy and cross-resolution\nfrequency alignment for enhanced reconstruction fidelity in RSI-SR. Extensive\nexperiments under two degradation methods on several benchmarks, including AID,\nUCMerced, and RSSRD-QH, demonstrate that GSDR outperforms the state-of-the-art\nTransformer-based method HAT by an average of 0.09 dB in PSNR, while using only\n63% of its parameters and 51% of its FLOPs, achieving an inference speed 3.2\ntimes faster.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, deep neural networks, including Convolutional Neural\nNetworks, Transformers, and State Space Models, have achieved significant\nprogress in Remote Sensing Image (RSI) Super-Resolution (SR). However, existing\nSR methods typically overlook the complementary relationship between global and\nlocal dependencies. These methods either focus on capturing local information\nor prioritize global information, which results in models that are unable to\neffectively capture both global and local features simultaneously. Moreover,\ntheir computational cost becomes prohibitive when applied to large-scale RSIs.\nTo address these challenges, we introduce the novel application of Receptance\nWeighted Key Value (RWKV) to RSI-SR, which captures long-range dependencies\nwith linear complexity. To simultaneously model global and local features, we\npropose the Global-Detail dual-branch structure, GDSR, which performs SR by\nparalleling RWKV and convolutional operations to handle large-scale RSIs.\nFurthermore, we introduce the Global-Detail Reconstruction Module (GDRM) as an\nintermediary between the two branches to bridge their complementary roles. In\naddition, we propose the Dual-Group Multi-Scale Wavelet Loss, a wavelet-domain\nconstraint mechanism via dual-group subband strategy and cross-resolution\nfrequency alignment for enhanced reconstruction fidelity in RSI-SR. Extensive\nexperiments under two degradation methods on several benchmarks, including AID,\nUCMerced, and RSSRD-QH, demonstrate that GSDR outperforms the state-of-the-art\nTransformer-based method HAT by an average of 0.09 dB in PSNR, while using only\n63% of its parameters and 51% of its FLOPs, achieving an inference speed 3.2\ntimes faster."
                },
                "authors": [
                    {
                        "name": "Qiwei Zhu"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Guojing Zhang"
                    },
                    {
                        "name": "Xiaoying Wang"
                    },
                    {
                        "name": "Jianqiang Huang"
                    },
                    {
                        "name": "Xilai Li"
                    }
                ],
                "author_detail": {
                    "name": "Xilai Li"
                },
                "author": "Xilai Li",
                "arxiv_comment": "GDSR: Global-Detail Integration through Dual-Branch Network with\n  Wavelet Losses for Remote Sensing Image Super-Resolution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01460v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01460v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22402v2",
                "updated": "2025-08-15T12:08:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    8,
                    35,
                    4,
                    227,
                    0
                ],
                "published": "2025-03-28T13:11:27Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    11,
                    27,
                    4,
                    87,
                    0
                ],
                "title": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing"
                },
                "summary": "Text-to-SQL automatically translates natural language queries to SQL,\nallowing non-technical users to retrieve data from databases without\nspecialized SQL knowledge. Despite the success of advanced LLM-based\nText-to-SQL approaches on leaderboards, their unsustainable computational\ncosts--often overlooked--stand as the \"elephant in the room\" in current\nleaderboard-driven research, limiting their economic practicability for\nreal-world deployment and widespread adoption. To tackle this, we exploratively\npropose EllieSQL, a complexity-aware routing framework that assigns queries to\nsuitable SQL generation pipelines based on estimated complexity. We investigate\nmultiple routers to direct simple queries to efficient approaches while\nreserving computationally intensive methods for complex cases. Drawing from\neconomics, we introduce the Token Elasticity of Performance (TEP) metric,\ncapturing cost-efficiency by quantifying the responsiveness of performance\ngains relative to token investment in SQL generation. Experiments show that\ncompared to always using the most advanced methods in our study, EllieSQL with\nthe Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising\nperformance on Bird development set, achieving more than a 2x boost in TEP over\nnon-routing approaches. This not only advances the pursuit of cost-efficient\nText-to-SQL but also invites the community to weigh resource efficiency\nalongside performance, contributing to progress in sustainable Text-to-SQL. Our\nsource code and model are available at https://elliesql.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL automatically translates natural language queries to SQL,\nallowing non-technical users to retrieve data from databases without\nspecialized SQL knowledge. Despite the success of advanced LLM-based\nText-to-SQL approaches on leaderboards, their unsustainable computational\ncosts--often overlooked--stand as the \"elephant in the room\" in current\nleaderboard-driven research, limiting their economic practicability for\nreal-world deployment and widespread adoption. To tackle this, we exploratively\npropose EllieSQL, a complexity-aware routing framework that assigns queries to\nsuitable SQL generation pipelines based on estimated complexity. We investigate\nmultiple routers to direct simple queries to efficient approaches while\nreserving computationally intensive methods for complex cases. Drawing from\neconomics, we introduce the Token Elasticity of Performance (TEP) metric,\ncapturing cost-efficiency by quantifying the responsiveness of performance\ngains relative to token investment in SQL generation. Experiments show that\ncompared to always using the most advanced methods in our study, EllieSQL with\nthe Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising\nperformance on Bird development set, achieving more than a 2x boost in TEP over\nnon-routing approaches. This not only advances the pursuit of cost-efficient\nText-to-SQL but also invites the community to weigh resource efficiency\nalongside performance, contributing to progress in sustainable Text-to-SQL. Our\nsource code and model are available at https://elliesql.github.io/."
                },
                "authors": [
                    {
                        "name": "Yizhang Zhu"
                    },
                    {
                        "name": "Runzhi Jiang"
                    },
                    {
                        "name": "Boyan Li"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Yuyu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Luo"
                },
                "author": "Yuyu Luo",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11429v1",
                "updated": "2025-08-15T12:07:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    7,
                    56,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T12:07:56Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    7,
                    56,
                    4,
                    227,
                    0
                ],
                "title": "HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor"
                },
                "summary": "Automated humor generation with Large Language Models (LLMs) often yields\njokes that feel generic, repetitive, or tone-deaf because humor is deeply\nsituated and hinges on the listener's cultural background, mindset, and\nimmediate context. We introduce HumorPlanSearch, a modular pipeline that\nexplicitly models context through: (1) Plan-Search for diverse, topic-tailored\nstrategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and\nstylistic reasoning; (3) a Knowledge Graph to retrieve and adapt\nhigh-performing historical strategies; (4) novelty filtering via semantic\nembeddings; and (5) an iterative judge-driven revision loop. To evaluate\ncontext sensitivity and comedic quality, we propose the Humor Generation Score\n(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,\nand topic relevance. In experiments across nine topics with feedback from 13\nhuman judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent\n(p < 0.05) over a strong baseline. By foregrounding context at every stage from\nstrategy planning to multi-signal evaluation, HumorPlanSearch advances\nAI-driven humor toward more coherent, adaptive, and culturally attuned comedy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated humor generation with Large Language Models (LLMs) often yields\njokes that feel generic, repetitive, or tone-deaf because humor is deeply\nsituated and hinges on the listener's cultural background, mindset, and\nimmediate context. We introduce HumorPlanSearch, a modular pipeline that\nexplicitly models context through: (1) Plan-Search for diverse, topic-tailored\nstrategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and\nstylistic reasoning; (3) a Knowledge Graph to retrieve and adapt\nhigh-performing historical strategies; (4) novelty filtering via semantic\nembeddings; and (5) an iterative judge-driven revision loop. To evaluate\ncontext sensitivity and comedic quality, we propose the Humor Generation Score\n(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,\nand topic relevance. In experiments across nine topics with feedback from 13\nhuman judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent\n(p < 0.05) over a strong baseline. By foregrounding context at every stage from\nstrategy planning to multi-signal evaluation, HumorPlanSearch advances\nAI-driven humor toward more coherent, adaptive, and culturally attuned comedy."
                },
                "authors": [
                    {
                        "name": "Shivam Dubey"
                    }
                ],
                "author_detail": {
                    "name": "Shivam Dubey"
                },
                "author": "Shivam Dubey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11425v1",
                "updated": "2025-08-15T12:02:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    2,
                    46,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T12:02:46Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    2,
                    46,
                    4,
                    227,
                    0
                ],
                "title": "Tapas are free! Training-Free Adaptation of Programmatic Agents via\n  LLM-Guided Program Synthesis in Dynamic Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tapas are free! Training-Free Adaptation of Programmatic Agents via\n  LLM-Guided Program Synthesis in Dynamic Environments"
                },
                "summary": "Autonomous agents in safety-critical applications must continuously adapt to\ndynamic conditions without compromising performance and reliability. This work\nintroduces TAPA (Training-free Adaptation of Programmatic Agents), a novel\nframework that positions large language models (LLMs) as intelligent moderators\nof the symbolic action space. Unlike prior programmatic agents that typically\ngenerate a monolithic policy program or rely on fixed symbolic action sets,\nTAPA synthesizes and adapts modular programs for individual high-level actions,\nreferred to as logical primitives. By decoupling strategic intent from\nexecution, TAPA enables meta-agents to operate over an abstract, interpretable\naction space while the LLM dynamically generates, composes, and refines\nsymbolic programs tailored to each primitive. Extensive experiments across\ncybersecurity and swarm intelligence domains validate TAPA's effectiveness. In\nautonomous DDoS defense scenarios, TAPA achieves 77.7% network uptime while\nmaintaining near-perfect detection accuracy in unknown dynamic environments. In\nswarm intelligence formation control under environmental and adversarial\ndisturbances, TAPA consistently preserves consensus at runtime where baseline\nmethods fail completely. This work promotes a paradigm shift for autonomous\nsystem design in evolving environments, from policy adaptation to dynamic\naction adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents in safety-critical applications must continuously adapt to\ndynamic conditions without compromising performance and reliability. This work\nintroduces TAPA (Training-free Adaptation of Programmatic Agents), a novel\nframework that positions large language models (LLMs) as intelligent moderators\nof the symbolic action space. Unlike prior programmatic agents that typically\ngenerate a monolithic policy program or rely on fixed symbolic action sets,\nTAPA synthesizes and adapts modular programs for individual high-level actions,\nreferred to as logical primitives. By decoupling strategic intent from\nexecution, TAPA enables meta-agents to operate over an abstract, interpretable\naction space while the LLM dynamically generates, composes, and refines\nsymbolic programs tailored to each primitive. Extensive experiments across\ncybersecurity and swarm intelligence domains validate TAPA's effectiveness. In\nautonomous DDoS defense scenarios, TAPA achieves 77.7% network uptime while\nmaintaining near-perfect detection accuracy in unknown dynamic environments. In\nswarm intelligence formation control under environmental and adversarial\ndisturbances, TAPA consistently preserves consensus at runtime where baseline\nmethods fail completely. This work promotes a paradigm shift for autonomous\nsystem design in evolving environments, from policy adaptation to dynamic\naction adaptation."
                },
                "authors": [
                    {
                        "name": "Jinwei Hu"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Youcheng Sun"
                    },
                    {
                        "name": "Xiaowei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Huang"
                },
                "author": "Xiaowei Huang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17811v2",
                "updated": "2025-08-15T11:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    48,
                    28,
                    4,
                    227,
                    0
                ],
                "published": "2025-03-22T16:22:53Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    16,
                    22,
                    53,
                    5,
                    81,
                    0
                ],
                "title": "Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model\n  Collaboration Paradigm for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model\n  Collaboration Paradigm for Small Language Models"
                },
                "summary": "Natural Language to SQL (NL2SQL) has seen significant advancements with large\nlanguage models (LLMs). However, these models often depend on closed-source\nsystems and high computational resources, posing challenges in data privacy and\ndeployment. In contrast, small language models (SLMs) struggle with NL2SQL\ntasks, exhibiting poor performance and incompatibility with existing\nframeworks. To address these issues, we introduce Feather-SQL, a new\nlightweight framework tailored for SLMs. Feather-SQL improves SQL executability\nand accuracy through 1) schema pruning and linking, 2) multi-path and\nmulti-candidate generation. Additionally, we introduce the 1+1 Model\nCollaboration Paradigm, which pairs a strong general-purpose chat model with a\nfine-tuned SQL specialist, combining strong analytical reasoning with\nhigh-precision SQL generation. Experimental results on BIRD demonstrate that\nFeather-SQL improves NL2SQL performance on SLMs, with around 10% boost for\nmodels without fine-tuning. The proposed paradigm raises the accuracy ceiling\nof SLMs to 54.76%, highlighting its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language to SQL (NL2SQL) has seen significant advancements with large\nlanguage models (LLMs). However, these models often depend on closed-source\nsystems and high computational resources, posing challenges in data privacy and\ndeployment. In contrast, small language models (SLMs) struggle with NL2SQL\ntasks, exhibiting poor performance and incompatibility with existing\nframeworks. To address these issues, we introduce Feather-SQL, a new\nlightweight framework tailored for SLMs. Feather-SQL improves SQL executability\nand accuracy through 1) schema pruning and linking, 2) multi-path and\nmulti-candidate generation. Additionally, we introduce the 1+1 Model\nCollaboration Paradigm, which pairs a strong general-purpose chat model with a\nfine-tuned SQL specialist, combining strong analytical reasoning with\nhigh-precision SQL generation. Experimental results on BIRD demonstrate that\nFeather-SQL improves NL2SQL performance on SLMs, with around 10% boost for\nmodels without fine-tuning. The proposed paradigm raises the accuracy ceiling\nof SLMs to 54.76%, highlighting its effectiveness."
                },
                "authors": [
                    {
                        "name": "Wenqi Pei"
                    },
                    {
                        "name": "Hailing Xu"
                    },
                    {
                        "name": "Hengyuan Zhao"
                    },
                    {
                        "name": "Shizheng Hou"
                    },
                    {
                        "name": "Han Chen"
                    },
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Pingyi Luo"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "arxiv_comment": "DL4C @ ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02736v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02736v2",
                "updated": "2025-08-15T11:45:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    45,
                    40,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-02T01:43:39Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    1,
                    43,
                    39,
                    5,
                    214,
                    0
                ],
                "title": "AgentSight: System-Level Observability for AI Agents Using eBPF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSight: System-Level Observability for AI Agents Using eBPF"
                },
                "summary": "Modern software infrastructure increasingly relies on LLM agents for\ndevelopment and maintenance, such as Claude Code and Gemini-cli. However, these\nAI agents differ fundamentally from traditional deterministic software, posing\na significant challenge to conventional monitoring and debugging. This creates\na critical semantic gap: existing tools observe either an agent's high-level\nintent (via LLM prompts) or its low-level actions (e.g., system calls), but\ncannot correlate these two views. This blindness makes it difficult to\ndistinguish between benign operations, malicious attacks, and costly failures.\nWe introduce AgentSight, an AgentOps observability framework that bridges this\nsemantic gap using a hybrid approach. Our approach, boundary tracing, monitors\nagents from outside their application code at stable system interfaces using\neBPF. AgentSight intercepts TLS-encrypted LLM traffic to extract semantic\nintent, monitors kernel events to observe system-wide effects, and causally\ncorrelates these two streams across process boundaries using a real-time engine\nand secondary LLM analysis. This instrumentation-free technique is\nframework-agnostic, resilient to rapid API changes, and incurs less than 3%\nperformance overhead. Our evaluation shows AgentSight detects prompt injection\nattacks, identifies resource-wasting reasoning loops, and reveals hidden\ncoordination bottlenecks in multi-agent systems. AgentSight is released as an\nopen-source project at https://github.com/agent-sight/agentsight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software infrastructure increasingly relies on LLM agents for\ndevelopment and maintenance, such as Claude Code and Gemini-cli. However, these\nAI agents differ fundamentally from traditional deterministic software, posing\na significant challenge to conventional monitoring and debugging. This creates\na critical semantic gap: existing tools observe either an agent's high-level\nintent (via LLM prompts) or its low-level actions (e.g., system calls), but\ncannot correlate these two views. This blindness makes it difficult to\ndistinguish between benign operations, malicious attacks, and costly failures.\nWe introduce AgentSight, an AgentOps observability framework that bridges this\nsemantic gap using a hybrid approach. Our approach, boundary tracing, monitors\nagents from outside their application code at stable system interfaces using\neBPF. AgentSight intercepts TLS-encrypted LLM traffic to extract semantic\nintent, monitors kernel events to observe system-wide effects, and causally\ncorrelates these two streams across process boundaries using a real-time engine\nand secondary LLM analysis. This instrumentation-free technique is\nframework-agnostic, resilient to rapid API changes, and incurs less than 3%\nperformance overhead. Our evaluation shows AgentSight detects prompt injection\nattacks, identifies resource-wasting reasoning loops, and reveals hidden\ncoordination bottlenecks in multi-agent systems. AgentSight is released as an\nopen-source project at https://github.com/agent-sight/agentsight."
                },
                "authors": [
                    {
                        "name": "Yusheng Zheng"
                    },
                    {
                        "name": "Yanpeng Hu"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Andi Quinn"
                    }
                ],
                "author_detail": {
                    "name": "Andi Quinn"
                },
                "author": "Andi Quinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02736v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02736v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11416v1",
                "updated": "2025-08-15T11:38:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    38,
                    19,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T11:38:19Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    38,
                    19,
                    4,
                    227,
                    0
                ],
                "title": "AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory\n  Manager",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory\n  Manager"
                },
                "summary": "Recent advances in mathematical reasoning and the long-term planning\ncapabilities of large language models (LLMs) have precipitated the development\nof agents, which are being increasingly leveraged in business operations\nprocesses. Decision models to optimize inventory levels are one of the core\nelements of operations management. However, the capabilities of the LLM agent\nin making inventory decisions in uncertain contexts, as well as the\ndecision-making biases (e.g. framing effect, etc.) of the agent, remain largely\nunexplored. This prompts concerns regarding the capacity of LLM agents to\neffectively address real-world problems, as well as the potential implications\nof biases that may be present. To address this gap, we introduce AIM-Bench, a\nnovel benchmark designed to assess the decision-making behaviour of LLM agents\nin uncertain supply chain management scenarios through a diverse series of\ninventory replenishment experiments. Our results reveal that different LLMs\ntypically exhibit varying degrees of decision bias that are similar to those\nobserved in human beings. In addition, we explored strategies to mitigate the\npull-to-centre effect and the bullwhip effect, namely cognitive reflection and\nimplementation of information sharing. These findings underscore the need for\ncareful consideration of the potential biases in deploying LLMs in Inventory\ndecision-making scenarios. We hope that these insights will pave the way for\nmitigating human decision bias and developing human-centred decision support\nsystems for supply chains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in mathematical reasoning and the long-term planning\ncapabilities of large language models (LLMs) have precipitated the development\nof agents, which are being increasingly leveraged in business operations\nprocesses. Decision models to optimize inventory levels are one of the core\nelements of operations management. However, the capabilities of the LLM agent\nin making inventory decisions in uncertain contexts, as well as the\ndecision-making biases (e.g. framing effect, etc.) of the agent, remain largely\nunexplored. This prompts concerns regarding the capacity of LLM agents to\neffectively address real-world problems, as well as the potential implications\nof biases that may be present. To address this gap, we introduce AIM-Bench, a\nnovel benchmark designed to assess the decision-making behaviour of LLM agents\nin uncertain supply chain management scenarios through a diverse series of\ninventory replenishment experiments. Our results reveal that different LLMs\ntypically exhibit varying degrees of decision bias that are similar to those\nobserved in human beings. In addition, we explored strategies to mitigate the\npull-to-centre effect and the bullwhip effect, namely cognitive reflection and\nimplementation of information sharing. These findings underscore the need for\ncareful consideration of the potential biases in deploying LLMs in Inventory\ndecision-making scenarios. We hope that these insights will pave the way for\nmitigating human decision bias and developing human-centred decision support\nsystems for supply chains."
                },
                "authors": [
                    {
                        "name": "Xuhua Zhao"
                    },
                    {
                        "name": "Yuxuan Xie"
                    },
                    {
                        "name": "Caihua Chen"
                    },
                    {
                        "name": "Yuxiang Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiang Sun"
                },
                "author": "Yuxiang Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11414v1",
                "updated": "2025-08-15T11:36:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    36,
                    17,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T11:36:17Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    36,
                    17,
                    4,
                    227,
                    0
                ],
                "title": "Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via\n  Survey Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via\n  Survey Questions"
                },
                "summary": "Large language models implicitly encode preferences over human values, yet\nsteering them often requires large training data. In this work, we investigate\na simple approach: Can we reliably modify a model's value system in downstream\nbehavior by training it to answer value survey questions accordingly? We first\nconstruct value profiles of several open-source LLMs by asking them to rate a\nseries of value-related descriptions spanning 20 distinct human values, which\nwe use as a baseline for subsequent experiments. We then investigate whether\nthe value system of a model can be governed by fine-tuning on the value\nsurveys. We evaluate the effect of finetuning on the model's behavior in two\nways; first, we assess how answers change on in-domain, held-out survey\nquestions. Second, we evaluate whether the model's behavior changes in\nout-of-domain settings (situational scenarios). To this end, we construct a\ncontextualized moral judgment dataset based on Reddit posts and evaluate\nchanges in the model's behavior in text-based adventure games. We demonstrate\nthat our simple approach can not only change the model's answers to in-domain\nsurvey questions, but also produces substantial shifts (value alignment) in\nimplicit downstream task behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models implicitly encode preferences over human values, yet\nsteering them often requires large training data. In this work, we investigate\na simple approach: Can we reliably modify a model's value system in downstream\nbehavior by training it to answer value survey questions accordingly? We first\nconstruct value profiles of several open-source LLMs by asking them to rate a\nseries of value-related descriptions spanning 20 distinct human values, which\nwe use as a baseline for subsequent experiments. We then investigate whether\nthe value system of a model can be governed by fine-tuning on the value\nsurveys. We evaluate the effect of finetuning on the model's behavior in two\nways; first, we assess how answers change on in-domain, held-out survey\nquestions. Second, we evaluate whether the model's behavior changes in\nout-of-domain settings (situational scenarios). To this end, we construct a\ncontextualized moral judgment dataset based on Reddit posts and evaluate\nchanges in the model's behavior in text-based adventure games. We demonstrate\nthat our simple approach can not only change the model's answers to in-domain\nsurvey questions, but also produces substantial shifts (value alignment) in\nimplicit downstream task behavior."
                },
                "authors": [
                    {
                        "name": "Shangrui Nie"
                    },
                    {
                        "name": "Florian Mai"
                    },
                    {
                        "name": "David Kaczér"
                    },
                    {
                        "name": "Charles Welch"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Lucie Flek"
                    }
                ],
                "author_detail": {
                    "name": "Lucie Flek"
                },
                "author": "Lucie Flek",
                "arxiv_comment": "7 pages 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11412v1",
                "updated": "2025-08-15T11:33:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    33,
                    3,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T11:33:03Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    33,
                    3,
                    4,
                    227,
                    0
                ],
                "title": "Towards Embodied Conversational Agents for Reducing Oral Exam Anxiety in\n  Extended Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Embodied Conversational Agents for Reducing Oral Exam Anxiety in\n  Extended Reality"
                },
                "summary": "Oral examinations are a prevalent but psychologically demanding form of\nassessment in higher education. Many students experience intense anxiety, which\ncan impair cognitive performance and hinder academic success. This position\npaper explores the potential of embodied conversational agents (ECAs) in\nextended reality (XR) environments to support students preparing for oral\nexams. We propose a system concept that integrates photorealistic ECAs with\nreal-time capable large language models (LLMs) to enable psychologically safe,\nadaptive, and repeatable rehearsal of oral examination scenarios. We also\ndiscuss the potential benefits and challenges of such an envisioned system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oral examinations are a prevalent but psychologically demanding form of\nassessment in higher education. Many students experience intense anxiety, which\ncan impair cognitive performance and hinder academic success. This position\npaper explores the potential of embodied conversational agents (ECAs) in\nextended reality (XR) environments to support students preparing for oral\nexams. We propose a system concept that integrates photorealistic ECAs with\nreal-time capable large language models (LLMs) to enable psychologically safe,\nadaptive, and repeatable rehearsal of oral examination scenarios. We also\ndiscuss the potential benefits and challenges of such an envisioned system."
                },
                "authors": [
                    {
                        "name": "Jens Grubert"
                    },
                    {
                        "name": "Yvonne Sedelmaier"
                    },
                    {
                        "name": "Dieter Landes"
                    }
                ],
                "author_detail": {
                    "name": "Dieter Landes"
                },
                "author": "Dieter Landes",
                "arxiv_comment": "Accepted to the IEEE ISMAR-Adjunct Proceedings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11409v1",
                "updated": "2025-08-15T11:20:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    20,
                    18,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T11:20:18Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    20,
                    18,
                    4,
                    227,
                    0
                ],
                "title": "RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator"
                },
                "summary": "Atmospheric turbulence severely degrades video quality by introducing\ndistortions such as geometric warping, blur, and temporal flickering, posing\nsignificant challenges to both visual clarity and temporal consistency. Current\nstate-of-the-art methods are based on transformer and 3D architectures and\nrequire multi-frame input, but their large computational cost and memory usage\nlimit real-time deployment, especially in resource-constrained scenarios. In\nthis work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric\nTurbulence Mitigator, designed for efficient and temporally consistent video\nrestoration under AT conditions. RMFAT adopts a lightweight recurrent framework\nthat restores each frame using only two inputs at a time, significantly\nreducing temporal window size and computational burden. It further integrates\nmulti-scale feature encoding and decoding with temporal warping modules at both\nencoder and decoder stages to enhance spatial detail and temporal coherence.\nExtensive experiments on synthetic and real-world atmospheric turbulence\ndatasets demonstrate that RMFAT not only outperforms existing methods in terms\nof clarity restoration (with nearly a 9\\% improvement in SSIM) but also\nachieves significantly improved inference speed (more than a fourfold reduction\nin runtime), making it particularly suitable for real-time atmospheric\nturbulence suppression tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atmospheric turbulence severely degrades video quality by introducing\ndistortions such as geometric warping, blur, and temporal flickering, posing\nsignificant challenges to both visual clarity and temporal consistency. Current\nstate-of-the-art methods are based on transformer and 3D architectures and\nrequire multi-frame input, but their large computational cost and memory usage\nlimit real-time deployment, especially in resource-constrained scenarios. In\nthis work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric\nTurbulence Mitigator, designed for efficient and temporally consistent video\nrestoration under AT conditions. RMFAT adopts a lightweight recurrent framework\nthat restores each frame using only two inputs at a time, significantly\nreducing temporal window size and computational burden. It further integrates\nmulti-scale feature encoding and decoding with temporal warping modules at both\nencoder and decoder stages to enhance spatial detail and temporal coherence.\nExtensive experiments on synthetic and real-world atmospheric turbulence\ndatasets demonstrate that RMFAT not only outperforms existing methods in terms\nof clarity restoration (with nearly a 9\\% improvement in SSIM) but also\nachieves significantly improved inference speed (more than a fourfold reduction\nin runtime), making it particularly suitable for real-time atmospheric\nturbulence suppression tasks."
                },
                "authors": [
                    {
                        "name": "Zhiming Liu"
                    },
                    {
                        "name": "Nantheera Anantrasirichai"
                    }
                ],
                "author_detail": {
                    "name": "Nantheera Anantrasirichai"
                },
                "author": "Nantheera Anantrasirichai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11408v1",
                "updated": "2025-08-15T11:20:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    20,
                    3,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T11:20:03Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    20,
                    3,
                    4,
                    227,
                    0
                ],
                "title": "On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised\n  Fine-Tuning and Reinforcement Learning via Dynamic Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised\n  Fine-Tuning and Reinforcement Learning via Dynamic Weighting"
                },
                "summary": "Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two\nprominent post-training paradigms for refining the capabilities and aligning\nthe behavior of Large Language Models (LLMs). Existing approaches that\nintegrate SFT and RL often face the risk of disrupting established model\npatterns and inducing overfitting to expert data. To address this, we present a\nnovel investigation into the unified view of SFT and RL through an off-policy\nversus on-policy lens. We propose CHORD, a framework for the Controllable\nHarmonization of On- and Off-Policy Reinforcement Learning via Dynamic\nWeighting, which reframes SFT not as a separate stage but as a dynamically\nweighted auxiliary objective within the on-policy RL process. Based on an\nanalysis of off-policy expert data's influence at both holistic and granular\nlevels, we incorporate a dual-control mechanism in CHORD. Specifically, the\nframework first employs a global coefficient to holistically guide the\ntransition from off-policy imitation to on-policy exploration, and then applies\na token-wise weighting function that enables granular learning from expert\ntokens, which preserves on-policy exploration and mitigates disruption from\noff-policy data. We conduct extensive experiments on widely used benchmarks,\nproviding empirical evidence that CHORD achieves a stable and efficient\nlearning process. By effectively harmonizing off-policy expert data with\non-policy exploration, CHORD demonstrates significant improvements over\nbaselines. We release the implementation at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to\ninspire further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two\nprominent post-training paradigms for refining the capabilities and aligning\nthe behavior of Large Language Models (LLMs). Existing approaches that\nintegrate SFT and RL often face the risk of disrupting established model\npatterns and inducing overfitting to expert data. To address this, we present a\nnovel investigation into the unified view of SFT and RL through an off-policy\nversus on-policy lens. We propose CHORD, a framework for the Controllable\nHarmonization of On- and Off-Policy Reinforcement Learning via Dynamic\nWeighting, which reframes SFT not as a separate stage but as a dynamically\nweighted auxiliary objective within the on-policy RL process. Based on an\nanalysis of off-policy expert data's influence at both holistic and granular\nlevels, we incorporate a dual-control mechanism in CHORD. Specifically, the\nframework first employs a global coefficient to holistically guide the\ntransition from off-policy imitation to on-policy exploration, and then applies\na token-wise weighting function that enables granular learning from expert\ntokens, which preserves on-policy exploration and mitigates disruption from\noff-policy data. We conduct extensive experiments on widely used benchmarks,\nproviding empirical evidence that CHORD achieves a stable and efficient\nlearning process. By effectively harmonizing off-policy expert data with\non-policy exploration, CHORD demonstrates significant improvements over\nbaselines. We release the implementation at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to\ninspire further research."
                },
                "authors": [
                    {
                        "name": "Wenhao Zhang"
                    },
                    {
                        "name": "Yuexiang Xie"
                    },
                    {
                        "name": "Yuchang Sun"
                    },
                    {
                        "name": "Yanxi Chen"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11401v1",
                "updated": "2025-08-15T11:10:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    10,
                    40,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T11:10:40Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    10,
                    40,
                    4,
                    227,
                    0
                ],
                "title": "FACET:Teacher-Centred LLM-Based Multi-Agent Systems-Towards Personalized\n  Educational Worksheets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FACET:Teacher-Centred LLM-Based Multi-Agent Systems-Towards Personalized\n  Educational Worksheets"
                },
                "summary": "The increasing heterogeneity of student populations poses significant\nchallenges for teachers, particularly in mathematics education, where\ncognitive, motivational, and emotional differences strongly influence learning\noutcomes. While AI-driven personalization tools have emerged, most remain\nperformance-focused, offering limited support for teachers and neglecting\nbroader pedagogical needs. This paper presents the FACET framework, a\nteacher-facing, large language model (LLM)-based multi-agent system designed to\ngenerate individualized classroom materials that integrate both cognitive and\nmotivational dimensions of learner profiles. The framework comprises three\nspecialized agents: (1) learner agents that simulate diverse profiles\nincorporating topic proficiency and intrinsic motivation, (2) a teacher agent\nthat adapts instructional content according to didactical principles, and (3)\nan evaluator agent that provides automated quality assurance. We tested the\nsystem using authentic grade 8 mathematics curriculum content and evaluated its\nfeasibility through a) automated agent-based assessment of output quality and\nb) exploratory feedback from K-12 in-service teachers. Results from ten\ninternal evaluations highlighted high stability and alignment between generated\nmaterials and learner profiles, and teacher feedback particularly highlighted\nstructure and suitability of tasks. The findings demonstrate the potential of\nmulti-agent LLM architectures to provide scalable, context-aware\npersonalization in heterogeneous classroom settings, and outline directions for\nextending the framework to richer learner profiles and real-world classroom\ntrials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing heterogeneity of student populations poses significant\nchallenges for teachers, particularly in mathematics education, where\ncognitive, motivational, and emotional differences strongly influence learning\noutcomes. While AI-driven personalization tools have emerged, most remain\nperformance-focused, offering limited support for teachers and neglecting\nbroader pedagogical needs. This paper presents the FACET framework, a\nteacher-facing, large language model (LLM)-based multi-agent system designed to\ngenerate individualized classroom materials that integrate both cognitive and\nmotivational dimensions of learner profiles. The framework comprises three\nspecialized agents: (1) learner agents that simulate diverse profiles\nincorporating topic proficiency and intrinsic motivation, (2) a teacher agent\nthat adapts instructional content according to didactical principles, and (3)\nan evaluator agent that provides automated quality assurance. We tested the\nsystem using authentic grade 8 mathematics curriculum content and evaluated its\nfeasibility through a) automated agent-based assessment of output quality and\nb) exploratory feedback from K-12 in-service teachers. Results from ten\ninternal evaluations highlighted high stability and alignment between generated\nmaterials and learner profiles, and teacher feedback particularly highlighted\nstructure and suitability of tasks. The findings demonstrate the potential of\nmulti-agent LLM architectures to provide scalable, context-aware\npersonalization in heterogeneous classroom settings, and outline directions for\nextending the framework to richer learner profiles and real-world classroom\ntrials."
                },
                "authors": [
                    {
                        "name": "Jana Gonnermann-Müller"
                    },
                    {
                        "name": "Jennifer Haase"
                    },
                    {
                        "name": "Konstantin Fackeldey"
                    },
                    {
                        "name": "Sebastian Pokutta"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pokutta"
                },
                "author": "Sebastian Pokutta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11398v1",
                "updated": "2025-08-15T11:08:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    8,
                    32,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T11:08:32Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    8,
                    32,
                    4,
                    227,
                    0
                ],
                "title": "Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling\n  and Explainable Mental Disorder Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling\n  and Explainable Mental Disorder Diagnosis"
                },
                "summary": "LLM-based agents have emerged as transformative tools capable of executing\ncomplex tasks through iterative planning and action, achieving significant\nadvancements in understanding and addressing user needs. Yet, their\neffectiveness remains limited in specialized domains such as mental health\ndiagnosis, where they underperform compared to general applications. Current\napproaches to integrating diagnostic capabilities into LLMs rely on scarce,\nhighly sensitive mental health datasets, which are challenging to acquire.\nThese methods also fail to emulate clinicians' proactive inquiry skills, lack\nmulti-turn conversational comprehension, and struggle to align outputs with\nexpert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the\nfirst LLM-based agent workflow designed to autonomously generate DSM-5 Level-1\ndiagnostic questionnaires. By simulating therapist-client dialogues with\nspecific client profiles, the framework delivers transparent, step-by-step\ndisorder predictions, producing explainable and trustworthy results. This\nworkflow serves as a complementary tool for mental health diagnosis, ensuring\nadherence to ethical and legal standards. Through comprehensive experiments, we\nevaluate leading LLMs across three critical dimensions: conversational realism,\ndiagnostic accuracy, and explainability. Our datasets and implementations are\nfully open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have emerged as transformative tools capable of executing\ncomplex tasks through iterative planning and action, achieving significant\nadvancements in understanding and addressing user needs. Yet, their\neffectiveness remains limited in specialized domains such as mental health\ndiagnosis, where they underperform compared to general applications. Current\napproaches to integrating diagnostic capabilities into LLMs rely on scarce,\nhighly sensitive mental health datasets, which are challenging to acquire.\nThese methods also fail to emulate clinicians' proactive inquiry skills, lack\nmulti-turn conversational comprehension, and struggle to align outputs with\nexpert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the\nfirst LLM-based agent workflow designed to autonomously generate DSM-5 Level-1\ndiagnostic questionnaires. By simulating therapist-client dialogues with\nspecific client profiles, the framework delivers transparent, step-by-step\ndisorder predictions, producing explainable and trustworthy results. This\nworkflow serves as a complementary tool for mental health diagnosis, ensuring\nadherence to ethical and legal standards. Through comprehensive experiments, we\nevaluate leading LLMs across three critical dimensions: conversational realism,\ndiagnostic accuracy, and explainability. Our datasets and implementations are\nfully open-sourced."
                },
                "authors": [
                    {
                        "name": "Mithat Can Ozgun"
                    },
                    {
                        "name": "Jiahuan Pei"
                    },
                    {
                        "name": "Koen Hindriks"
                    },
                    {
                        "name": "Lucia Donatelli"
                    },
                    {
                        "name": "Qingzhi Liu"
                    },
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Junxiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Junxiao Wang"
                },
                "author": "Junxiao Wang",
                "arxiv_comment": "Accepted by CIKM 2025 as a full paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10331v2",
                "updated": "2025-08-15T11:01:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    1,
                    26,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-14T04:11:09Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    4,
                    11,
                    9,
                    3,
                    226,
                    0
                ],
                "title": "Synthesizing Evidence: Data-Pooling as a Tool for Treatment Selection in\n  Online Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Evidence: Data-Pooling as a Tool for Treatment Selection in\n  Online Experiments"
                },
                "summary": "Randomized experiments are the gold standard for causal inference but face\nsignificant challenges in business applications, including limited traffic\nallocation, the need for heterogeneous treatment effect estimation, and the\ncomplexity of managing overlapping experiments. These factors lead to high\nvariability in treatment effect estimates, making data-driven policy roll out\ndifficult. To address these issues, we introduce the data pooling treatment\nroll-out (DPTR) framework, which enhances policy roll-out by pooling data\nacross experiments rather than focusing narrowly on individual ones. DPTR can\neffectively accommodate both overlapping and non-overlapping traffic scenarios,\nregardless of linear or nonlinear model specifications. We demonstrate the\nframework's robustness through a three-pronged validation: (a) theoretical\nanalysis shows that DPTR surpasses the traditional difference-in-mean and\nordinary least squares methods under non-overlapping experiments, particularly\nwhen the number of experiments is large; (b) synthetic simulations confirm its\nadaptability in complex scenarios with overlapping traffic, rich covariates and\nnonlinear specifications; and (c) empirical applications to two experimental\ndatasets from real world platforms, demonstrating its effectiveness in guiding\ncustomized policy roll-outs for subgroups within a single experiment, as well\nas in coordinating policy deployments across multiple experiments with\noverlapping scenarios. By reducing estimation variability to improve\ndecision-making effectiveness, DPTR provides a scalable, practical solution for\nonline platforms to better leverage their experimental data in today's\nincreasingly complex business environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomized experiments are the gold standard for causal inference but face\nsignificant challenges in business applications, including limited traffic\nallocation, the need for heterogeneous treatment effect estimation, and the\ncomplexity of managing overlapping experiments. These factors lead to high\nvariability in treatment effect estimates, making data-driven policy roll out\ndifficult. To address these issues, we introduce the data pooling treatment\nroll-out (DPTR) framework, which enhances policy roll-out by pooling data\nacross experiments rather than focusing narrowly on individual ones. DPTR can\neffectively accommodate both overlapping and non-overlapping traffic scenarios,\nregardless of linear or nonlinear model specifications. We demonstrate the\nframework's robustness through a three-pronged validation: (a) theoretical\nanalysis shows that DPTR surpasses the traditional difference-in-mean and\nordinary least squares methods under non-overlapping experiments, particularly\nwhen the number of experiments is large; (b) synthetic simulations confirm its\nadaptability in complex scenarios with overlapping traffic, rich covariates and\nnonlinear specifications; and (c) empirical applications to two experimental\ndatasets from real world platforms, demonstrating its effectiveness in guiding\ncustomized policy roll-outs for subgroups within a single experiment, as well\nas in coordinating policy deployments across multiple experiments with\noverlapping scenarios. By reducing estimation variability to improve\ndecision-making effectiveness, DPTR provides a scalable, practical solution for\nonline platforms to better leverage their experimental data in today's\nincreasingly complex business environments."
                },
                "authors": [
                    {
                        "name": "Zhenkang Peng"
                    },
                    {
                        "name": "Chengzhang Li"
                    },
                    {
                        "name": "Ying Rong"
                    },
                    {
                        "name": "Renyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Renyu Zhang"
                },
                "author": "Renyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11391v1",
                "updated": "2025-08-15T10:50:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    50,
                    38,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T10:50:38Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    50,
                    38,
                    4,
                    227,
                    0
                ],
                "title": "LKFMixer: Exploring Large Kernel Feature For Efficient Image\n  Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LKFMixer: Exploring Large Kernel Feature For Efficient Image\n  Super-Resolution"
                },
                "summary": "The success of self-attention (SA) in Transformer demonstrates the importance\nof non-local information to image super-resolution (SR), but the huge computing\npower required makes it difficult to implement lightweight models. To solve\nthis problem, we propose a pure convolutional neural network (CNN) model,\nLKFMixer, which utilizes large convolutional kernel to simulate the ability of\nself-attention to capture non-local features. Specifically, we increase the\nkernel size to 31 to obtain the larger receptive field as possible, and reduce\nthe parameters and computations by coordinate decomposition. Meanwhile, a\nspatial feature modulation block (SFMB) is designed to enhance the focus of\nfeature information on both spatial and channel dimension. In addition, by\nintroducing feature selection block (FSB), the model can adaptively adjust the\nweights between local features and non-local features. Extensive experiments\nshow that the proposed LKFMixer family outperform other state-of-the-art (SOTA)\nmethods in terms of SR performance and reconstruction quality. In particular,\ncompared with SwinIR-light on Manga109 dataset, LKFMixer-L achieves 0.6dB PSNR\nimprovement at $\\times$4 scale, while the inference speed is $\\times$5 times\nfaster. The code is available at https://github.com/Supereeeee/LKFMixer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of self-attention (SA) in Transformer demonstrates the importance\nof non-local information to image super-resolution (SR), but the huge computing\npower required makes it difficult to implement lightweight models. To solve\nthis problem, we propose a pure convolutional neural network (CNN) model,\nLKFMixer, which utilizes large convolutional kernel to simulate the ability of\nself-attention to capture non-local features. Specifically, we increase the\nkernel size to 31 to obtain the larger receptive field as possible, and reduce\nthe parameters and computations by coordinate decomposition. Meanwhile, a\nspatial feature modulation block (SFMB) is designed to enhance the focus of\nfeature information on both spatial and channel dimension. In addition, by\nintroducing feature selection block (FSB), the model can adaptively adjust the\nweights between local features and non-local features. Extensive experiments\nshow that the proposed LKFMixer family outperform other state-of-the-art (SOTA)\nmethods in terms of SR performance and reconstruction quality. In particular,\ncompared with SwinIR-light on Manga109 dataset, LKFMixer-L achieves 0.6dB PSNR\nimprovement at $\\times$4 scale, while the inference speed is $\\times$5 times\nfaster. The code is available at https://github.com/Supereeeee/LKFMixer."
                },
                "authors": [
                    {
                        "name": "Yinggan Tang"
                    },
                    {
                        "name": "Quanwei Hu"
                    }
                ],
                "author_detail": {
                    "name": "Quanwei Hu"
                },
                "author": "Quanwei Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08837v2",
                "updated": "2025-08-15T10:48:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    48,
                    46,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-12T10:54:08Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    54,
                    8,
                    1,
                    224,
                    0
                ],
                "title": "The Roots of International Perceptions: Simulating US Attitude Changes\n  Towards China with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Roots of International Perceptions: Simulating US Attitude Changes\n  Towards China with LLM Agents"
                },
                "summary": "The rise of LLMs poses new possibilities in modeling opinion evolution, a\nlong-standing task in simulation, by leveraging advanced reasoning abilities to\nrecreate complex, large-scale human cognitive trends. While most prior works\nfocus on opinion evolution surrounding specific isolated events or the views\nwithin a country, ours is the first to model the large-scale attitude evolution\nof a population representing an entire country towards another -- US citizens'\nperspectives towards China. To tackle the challenges of this broad scenario, we\npropose a framework that integrates media data collection, user profile\ncreation, and cognitive architecture for opinion updates to successfully\nreproduce the real trend of US attitudes towards China over a 20-year period\nfrom 2005 to today. We also leverage LLMs' capabilities to introduce debiased\nmedia exposure, extracting neutral events from typically subjective news\ncontents, to uncover the roots of polarized opinion formation, as well as a\ndevils advocate agent to help explain the rare reversal from negative to\npositive attitudes towards China, corresponding with changes in the way\nAmericans obtain information about the country. The simulation results, beyond\nvalidating our framework architecture, also reveal the impact of biased framing\nand selection bias in shaping attitudes. Overall, our work contributes to a new\nparadigm for LLM-based modeling of cognitive behaviors in a large-scale,\nlong-term, cross-border social context, providing insights into the formation\nof international biases and offering valuable implications for media consumers\nto better understand the factors shaping their perspectives, and ultimately\ncontributing to the larger social need for bias reduction and cross-cultural\ntolerance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of LLMs poses new possibilities in modeling opinion evolution, a\nlong-standing task in simulation, by leveraging advanced reasoning abilities to\nrecreate complex, large-scale human cognitive trends. While most prior works\nfocus on opinion evolution surrounding specific isolated events or the views\nwithin a country, ours is the first to model the large-scale attitude evolution\nof a population representing an entire country towards another -- US citizens'\nperspectives towards China. To tackle the challenges of this broad scenario, we\npropose a framework that integrates media data collection, user profile\ncreation, and cognitive architecture for opinion updates to successfully\nreproduce the real trend of US attitudes towards China over a 20-year period\nfrom 2005 to today. We also leverage LLMs' capabilities to introduce debiased\nmedia exposure, extracting neutral events from typically subjective news\ncontents, to uncover the roots of polarized opinion formation, as well as a\ndevils advocate agent to help explain the rare reversal from negative to\npositive attitudes towards China, corresponding with changes in the way\nAmericans obtain information about the country. The simulation results, beyond\nvalidating our framework architecture, also reveal the impact of biased framing\nand selection bias in shaping attitudes. Overall, our work contributes to a new\nparadigm for LLM-based modeling of cognitive behaviors in a large-scale,\nlong-term, cross-border social context, providing insights into the formation\nof international biases and offering valuable implications for media consumers\nto better understand the factors shaping their perspectives, and ultimately\ncontributing to the larger social need for bias reduction and cross-cultural\ntolerance."
                },
                "authors": [
                    {
                        "name": "Nicholas Sukiennik"
                    },
                    {
                        "name": "Yichuan Xu"
                    },
                    {
                        "name": "Yuqing Kan"
                    },
                    {
                        "name": "Jinghua Piao"
                    },
                    {
                        "name": "Yuwei Yan"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "Submitted to AAAI Social Impact 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07493v2",
                "updated": "2025-08-15T10:40:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    40,
                    56,
                    4,
                    227,
                    0
                ],
                "published": "2024-06-11T17:30:09Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    30,
                    9,
                    1,
                    163,
                    0
                ],
                "title": "Model-Independent Test of Prerecombination New Physics: Measuring the\n  Sound Horizon with Gravitational Wave Standard Sirens and the Baryon Acoustic\n  Oscillation Angular Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Independent Test of Prerecombination New Physics: Measuring the\n  Sound Horizon with Gravitational Wave Standard Sirens and the Baryon Acoustic\n  Oscillation Angular Scale"
                },
                "summary": "In a broad class of cosmological models where spacetime is described by a\npseudo-Riemannian manifold, photons propagate along null geodesics, and their\nnumber is conserved, upcoming Gravitational Wave (GW) observations can be\ncombined with measurements of the Baryon Acoustic Oscillation (BAO) angular\nscale to provide model-independent estimates of the sound horizon at the\nbaryon-drag epoch. By focusing on the accuracy expected from forthcoming\nsurveys such as LISA GW standard sirens and DESI or Euclid angular BAO\nmeasurements, we forecast a relative precision of $\\sigma_{r_{\\rm d}} /r_{\\rm\nd} \\sim 1.5\\%$ within the redshift range $z \\lesssim 1$. This approach will\noffer a unique model-independent measure of a fundamental scale characterizing\nthe early universe, which is competitive with model-dependent values inferred\nwithin specific theoretical frameworks. These measurements can serve as a\nconsistency test for $\\Lambda$CDM, potentially clarifying the nature of the\nHubble tension and confirming or ruling out new physics prior to recombination\nwith a statistical significance of $\\sim 4\\sigma$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a broad class of cosmological models where spacetime is described by a\npseudo-Riemannian manifold, photons propagate along null geodesics, and their\nnumber is conserved, upcoming Gravitational Wave (GW) observations can be\ncombined with measurements of the Baryon Acoustic Oscillation (BAO) angular\nscale to provide model-independent estimates of the sound horizon at the\nbaryon-drag epoch. By focusing on the accuracy expected from forthcoming\nsurveys such as LISA GW standard sirens and DESI or Euclid angular BAO\nmeasurements, we forecast a relative precision of $\\sigma_{r_{\\rm d}} /r_{\\rm\nd} \\sim 1.5\\%$ within the redshift range $z \\lesssim 1$. This approach will\noffer a unique model-independent measure of a fundamental scale characterizing\nthe early universe, which is competitive with model-dependent values inferred\nwithin specific theoretical frameworks. These measurements can serve as a\nconsistency test for $\\Lambda$CDM, potentially clarifying the nature of the\nHubble tension and confirming or ruling out new physics prior to recombination\nwith a statistical significance of $\\sim 4\\sigma$."
                },
                "authors": [
                    {
                        "name": "William Giarè"
                    },
                    {
                        "name": "Jonathan Betts"
                    },
                    {
                        "name": "Carsten van de Bruck"
                    },
                    {
                        "name": "Eleonora Di Valentino"
                    }
                ],
                "author_detail": {
                    "name": "Eleonora Di Valentino"
                },
                "author": "Eleonora Di Valentino",
                "arxiv_doi": "10.1103/k6mg-g23d",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/k6mg-g23d",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.07493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24 pages, 11 figures (main letter and supplementary material). V2:\n  new tests and discussions on forecast assumptions for GW detections,\n  dependence on the number of electromagnetic counterparts at different\n  redshifts, reconstruction methodology, and fiducial cosmology in the\n  supplementary material. Version accepted for publication in Physical Review\n  Letters",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02269v2",
                "updated": "2025-08-15T10:37:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    37,
                    36,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-04T10:21:47Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    21,
                    47,
                    0,
                    216,
                    0
                ],
                "title": "AirTrafficGen: Configurable Air Traffic Scenario Generation with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirTrafficGen: Configurable Air Traffic Scenario Generation with Large\n  Language Models"
                },
                "summary": "The manual design of scenarios for Air Traffic Control (ATC) training is a\ndemanding and time-consuming bottleneck that limits the diversity of\nsimulations available to controllers. To address this, we introduce a novel,\nend-to-end approach, $\\texttt{AirTrafficGen}$, that leverages large language\nmodels (LLMs) to automate and control the generation of complex ATC scenarios.\nOur method uses a purpose-built, graph-based representation to encode sector\ntopology (including airspace geometry, routes, and fixes) into a format LLMs\ncan process. Through rigorous benchmarking, we show that state-of-the-art\nmodels like Gemini 2.5 Pro, OpenAI o3, GPT-oss-120b and GPT-5 can generate\nhigh-traffic scenarios while maintaining operational realism. Our engineered\nprompting enables fine-grained control over interaction presence, type, and\nlocation. Initial findings suggest these models are also capable of iterative\nrefinement, correcting flawed scenarios based on simple textual feedback. This\napproach provides a scalable alternative to manual scenario design, addressing\nthe need for a greater volume and variety of ATC training and validation\nsimulations. More broadly, this work showcases the potential of LLMs for\ncomplex planning in safety-critical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The manual design of scenarios for Air Traffic Control (ATC) training is a\ndemanding and time-consuming bottleneck that limits the diversity of\nsimulations available to controllers. To address this, we introduce a novel,\nend-to-end approach, $\\texttt{AirTrafficGen}$, that leverages large language\nmodels (LLMs) to automate and control the generation of complex ATC scenarios.\nOur method uses a purpose-built, graph-based representation to encode sector\ntopology (including airspace geometry, routes, and fixes) into a format LLMs\ncan process. Through rigorous benchmarking, we show that state-of-the-art\nmodels like Gemini 2.5 Pro, OpenAI o3, GPT-oss-120b and GPT-5 can generate\nhigh-traffic scenarios while maintaining operational realism. Our engineered\nprompting enables fine-grained control over interaction presence, type, and\nlocation. Initial findings suggest these models are also capable of iterative\nrefinement, correcting flawed scenarios based on simple textual feedback. This\napproach provides a scalable alternative to manual scenario design, addressing\nthe need for a greater volume and variety of ATC training and validation\nsimulations. More broadly, this work showcases the potential of LLMs for\ncomplex planning in safety-critical domains."
                },
                "authors": [
                    {
                        "name": "Dewi Sid William Gould"
                    },
                    {
                        "name": "George De Ath"
                    },
                    {
                        "name": "Ben Carvell"
                    },
                    {
                        "name": "Nick Pepper"
                    }
                ],
                "author_detail": {
                    "name": "Nick Pepper"
                },
                "author": "Nick Pepper",
                "arxiv_comment": "9 pages and appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11383v1",
                "updated": "2025-08-15T10:32:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    32,
                    50,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T10:32:50Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    32,
                    50,
                    4,
                    227,
                    0
                ],
                "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness\n  Methods for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness\n  Methods for LLMs"
                },
                "summary": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters."
                },
                "authors": [
                    {
                        "name": "Mikhail Seleznyov"
                    },
                    {
                        "name": "Mikhail Chaichuk"
                    },
                    {
                        "name": "Gleb Ershov"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Elena Tutubalina"
                    },
                    {
                        "name": "Oleg Somov"
                    }
                ],
                "author_detail": {
                    "name": "Oleg Somov"
                },
                "author": "Oleg Somov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11379v1",
                "updated": "2025-08-15T10:25:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    25,
                    58,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T10:25:58Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    25,
                    58,
                    4,
                    227,
                    0
                ],
                "title": "G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior\n  Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior\n  Integration"
                },
                "summary": "We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene\nreconstruction that enhances the CUT3R model by integrating prior information.\nUnlike existing feed-forward methods that rely solely on input images, our\nmethod leverages auxiliary data, such as depth, camera calibrations, or camera\npositions, commonly available in real-world scenarios. We propose a lightweight\nmodification to CUT3R, incorporating a dedicated encoder for each modality to\nextract features, which are fused with RGB image tokens via zero convolution.\nThis flexible design enables seamless integration of any combination of prior\ninformation during inference. Evaluated across multiple benchmarks, including\n3D reconstruction and other multi-view tasks, our approach demonstrates\nsignificant performance improvements, showing its ability to effectively\nutilize available priors while maintaining compatibility with varying input\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene\nreconstruction that enhances the CUT3R model by integrating prior information.\nUnlike existing feed-forward methods that rely solely on input images, our\nmethod leverages auxiliary data, such as depth, camera calibrations, or camera\npositions, commonly available in real-world scenarios. We propose a lightweight\nmodification to CUT3R, incorporating a dedicated encoder for each modality to\nextract features, which are fused with RGB image tokens via zero convolution.\nThis flexible design enables seamless integration of any combination of prior\ninformation during inference. Evaluated across multiple benchmarks, including\n3D reconstruction and other multi-view tasks, our approach demonstrates\nsignificant performance improvements, showing its ability to effectively\nutilize available priors while maintaining compatibility with varying input\nmodalities."
                },
                "authors": [
                    {
                        "name": "Ramil Khafizov"
                    },
                    {
                        "name": "Artem Komarichev"
                    },
                    {
                        "name": "Ruslan Rakhimov"
                    },
                    {
                        "name": "Peter Wonka"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Burnaev"
                },
                "author": "Evgeny Burnaev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11364v1",
                "updated": "2025-08-15T09:59:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    59,
                    22,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T09:59:22Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    59,
                    22,
                    4,
                    227,
                    0
                ],
                "title": "Feedback Indicators: The Alignment between Llama and a Teacher in\n  Language Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback Indicators: The Alignment between Llama and a Teacher in\n  Language Learning"
                },
                "summary": "Automated feedback generation has the potential to enhance students' learning\nprogress by providing timely and targeted feedback. Moreover, it can assist\nteachers in optimizing their time, allowing them to focus on more strategic and\npersonalized aspects of teaching. To generate high-quality, information-rich\nformative feedback, it is essential first to extract relevant indicators, as\nthese serve as the foundation upon which the feedback is constructed. Teachers\noften employ feedback criteria grids composed of various indicators that they\nevaluate systematically. This study examines the initial phase of extracting\nsuch indicators from students' submissions of a language learning course using\nthe large language model Llama 3.1. Accordingly, the alignment between\nindicators generated by the LLM and human ratings across various feedback\ncriteria is investigated. The findings demonstrate statistically significant\nstrong correlations, even in cases involving unanticipated combinations of\nindicators and criteria. The methodology employed in this paper offers a\npromising foundation for extracting indicators from students' submissions using\nLLMs. Such indicators can potentially be utilized to auto-generate explainable\nand transparent formative feedback in future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated feedback generation has the potential to enhance students' learning\nprogress by providing timely and targeted feedback. Moreover, it can assist\nteachers in optimizing their time, allowing them to focus on more strategic and\npersonalized aspects of teaching. To generate high-quality, information-rich\nformative feedback, it is essential first to extract relevant indicators, as\nthese serve as the foundation upon which the feedback is constructed. Teachers\noften employ feedback criteria grids composed of various indicators that they\nevaluate systematically. This study examines the initial phase of extracting\nsuch indicators from students' submissions of a language learning course using\nthe large language model Llama 3.1. Accordingly, the alignment between\nindicators generated by the LLM and human ratings across various feedback\ncriteria is investigated. The findings demonstrate statistically significant\nstrong correlations, even in cases involving unanticipated combinations of\nindicators and criteria. The methodology employed in this paper offers a\npromising foundation for extracting indicators from students' submissions using\nLLMs. Such indicators can potentially be utilized to auto-generate explainable\nand transparent formative feedback in future research."
                },
                "authors": [
                    {
                        "name": "Sylvio Rüdian"
                    },
                    {
                        "name": "Yassin Elsir"
                    },
                    {
                        "name": "Marvin Kretschmer"
                    },
                    {
                        "name": "Sabine Cayrou"
                    },
                    {
                        "name": "Niels Pinkwart"
                    }
                ],
                "author_detail": {
                    "name": "Niels Pinkwart"
                },
                "author": "Niels Pinkwart",
                "arxiv_comment": "11 pages, one table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11359v1",
                "updated": "2025-08-15T09:54:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    54,
                    13,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T09:54:13Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    54,
                    13,
                    4,
                    227,
                    0
                ],
                "title": "Can We Tell if ChatGPT is a Parasite? Studying Human-AI Symbiosis with\n  Game Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Tell if ChatGPT is a Parasite? Studying Human-AI Symbiosis with\n  Game Theory"
                },
                "summary": "This work asks whether a human interacting with a generative AI system can\nmerge into a single individual through iterative, information-driven\ninteractions. We model the interactions between a human, a generative AI\nsystem, and the human's wider environment as a three-player stochastic game. We\nuse information-theoretic measures (entropy, mutual information, and transfer\nentropy) to show that our modelled human and generative AI are able to form an\naggregate individual in the sense of Krakauer et al. (2020). The model we\npresent is able to answer interesting questions around the symbiotic nature of\nhumans and AI systems, including whether LLM-driven chatbots are acting as\nparasites, feeding on the information provided by humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work asks whether a human interacting with a generative AI system can\nmerge into a single individual through iterative, information-driven\ninteractions. We model the interactions between a human, a generative AI\nsystem, and the human's wider environment as a three-player stochastic game. We\nuse information-theoretic measures (entropy, mutual information, and transfer\nentropy) to show that our modelled human and generative AI are able to form an\naggregate individual in the sense of Krakauer et al. (2020). The model we\npresent is able to answer interesting questions around the symbiotic nature of\nhumans and AI systems, including whether LLM-driven chatbots are acting as\nparasites, feeding on the information provided by humans."
                },
                "authors": [
                    {
                        "name": "Jiejun Hu-Bolz"
                    },
                    {
                        "name": "James Stovold"
                    }
                ],
                "author_detail": {
                    "name": "James Stovold"
                },
                "author": "James Stovold",
                "arxiv_comment": "8 pages, 6 figures, accepted in ALife 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11356v1",
                "updated": "2025-08-15T09:49:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    49,
                    14,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T09:49:14Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    49,
                    14,
                    4,
                    227,
                    0
                ],
                "title": "ETTRL: Balancing Exploration and Exploitation in LLM Test-Time\n  Reinforcement Learning Via Entropy Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETTRL: Balancing Exploration and Exploitation in LLM Test-Time\n  Reinforcement Learning Via Entropy Mechanism"
                },
                "summary": "Recent advancements in Large Language Models have yielded significant\nimprovements in complex reasoning tasks such as mathematics and programming.\nHowever, these models remain heavily dependent on annotated data and exhibit\nlimited adaptability in unsupervised scenarios. To address these limitations,\ntest-time reinforcement learning (TTRL) has been proposed, which enables\nself-optimization by leveraging model-generated pseudo-labels. Despite its\npromise, TTRL faces several key challenges, including high inference costs due\nto parallel rollouts and early-stage estimation bias that fosters\noverconfidence, reducing output diversity and causing performance plateaus. To\naddress these challenges, we introduce an entropy-based mechanism to enhance\nthe exploration-exploitation balance in test-time reinforcement learning\nthrough two strategies: Entropy-fork Tree Majority Rollout (ETMR) and\nEntropy-based Advantage Reshaping (EAR). Compared with the baseline, our\napproach enables Llama3.1-8B to achieve a 68 percent relative improvement in\nPass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of\nthe rollout tokens budget. This highlights our method's ability to effectively\noptimize the trade-off between inference efficiency, diversity, and estimation\nrobustness, thereby advancing unsupervised reinforcement learning for\nopen-domain reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models have yielded significant\nimprovements in complex reasoning tasks such as mathematics and programming.\nHowever, these models remain heavily dependent on annotated data and exhibit\nlimited adaptability in unsupervised scenarios. To address these limitations,\ntest-time reinforcement learning (TTRL) has been proposed, which enables\nself-optimization by leveraging model-generated pseudo-labels. Despite its\npromise, TTRL faces several key challenges, including high inference costs due\nto parallel rollouts and early-stage estimation bias that fosters\noverconfidence, reducing output diversity and causing performance plateaus. To\naddress these challenges, we introduce an entropy-based mechanism to enhance\nthe exploration-exploitation balance in test-time reinforcement learning\nthrough two strategies: Entropy-fork Tree Majority Rollout (ETMR) and\nEntropy-based Advantage Reshaping (EAR). Compared with the baseline, our\napproach enables Llama3.1-8B to achieve a 68 percent relative improvement in\nPass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of\nthe rollout tokens budget. This highlights our method's ability to effectively\noptimize the trade-off between inference efficiency, diversity, and estimation\nrobustness, thereby advancing unsupervised reinforcement learning for\nopen-domain reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "ChangYi He"
                    },
                    {
                        "name": "YingQiao Lin"
                    },
                    {
                        "name": "MingMin Yang"
                    },
                    {
                        "name": "FeiYang Shen"
                    },
                    {
                        "name": "ShaoGuo Liu"
                    },
                    {
                        "name": "TingTing Gao"
                    }
                ],
                "author_detail": {
                    "name": "TingTing Gao"
                },
                "author": "TingTing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16636v2",
                "updated": "2025-08-15T09:45:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    45,
                    15,
                    4,
                    227,
                    0
                ],
                "published": "2025-02-23T16:23:50Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    23,
                    50,
                    6,
                    54,
                    0
                ],
                "title": "Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation\n  for Visual Knowledge Intensive Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation\n  for Visual Knowledge Intensive Queries"
                },
                "summary": "Retrieval-augmented generation (RAG) is a paradigm that augments large\nlanguage models (LLMs) with external knowledge to tackle knowledge-intensive\nquestion answering. While several benchmarks evaluate Multimodal LLMs (MLLMs)\nunder Multimodal RAG settings, they predominantly retrieve from textual corpora\nand do not explicitly assess how models exploit visual evidence during\ngeneration. Consequently, there still lacks benchmark that isolates and\nmeasures the contribution of retrieved images in RAG. We introduce Visual-RAG,\na question-answering benchmark that targets visually grounded,\nknowledge-intensive questions. Unlike prior work, Visual-RAG requires\ntext-to-image retrieval and the integration of retrieved clue images to extract\nvisual evidence for answer generation. With Visual-RAG, we evaluate 5\nopen-source and 3 proprietary MLLMs, showcasing that images provide strong\nevidence in augmented generation. However, even state-of-the-art models\nstruggle to efficiently extract and utilize visual knowledge. Our results\nhighlight the need for improved visual retrieval, grounding, and attribution in\nmultimodal RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a paradigm that augments large\nlanguage models (LLMs) with external knowledge to tackle knowledge-intensive\nquestion answering. While several benchmarks evaluate Multimodal LLMs (MLLMs)\nunder Multimodal RAG settings, they predominantly retrieve from textual corpora\nand do not explicitly assess how models exploit visual evidence during\ngeneration. Consequently, there still lacks benchmark that isolates and\nmeasures the contribution of retrieved images in RAG. We introduce Visual-RAG,\na question-answering benchmark that targets visually grounded,\nknowledge-intensive questions. Unlike prior work, Visual-RAG requires\ntext-to-image retrieval and the integration of retrieved clue images to extract\nvisual evidence for answer generation. With Visual-RAG, we evaluate 5\nopen-source and 3 proprietary MLLMs, showcasing that images provide strong\nevidence in augmented generation. However, even state-of-the-art models\nstruggle to efficiently extract and utilize visual knowledge. Our results\nhighlight the need for improved visual retrieval, grounding, and attribution in\nmultimodal RAG systems."
                },
                "authors": [
                    {
                        "name": "Yin Wu"
                    },
                    {
                        "name": "Quanyu Long"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Jianfei Yu"
                    },
                    {
                        "name": "Wenya Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenya Wang"
                },
                "author": "Wenya Wang",
                "arxiv_comment": "21 pages, 6 figures, 17 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16502v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16502v4",
                "updated": "2025-08-15T09:41:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    41,
                    39,
                    4,
                    227,
                    0
                ],
                "published": "2024-10-21T20:48:16Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    20,
                    48,
                    16,
                    0,
                    295,
                    0
                ],
                "title": "RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic\n  and Human-like Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic\n  and Human-like Reasoning"
                },
                "summary": "Formal logic enables computers to reason in natural language by representing\nsentences in symbolic forms and applying rules to derive conclusions. However,\nin what our study characterizes as \"rulebreaker\" scenarios, this method can\nlead to conclusions that are typically not inferred or accepted by humans given\ntheir common sense and factual knowledge. Inspired by works in cognitive\nscience, we create RULEBREAKERS, the first dataset for rigorously evaluating\nthe ability of large language models (LLMs) to recognize and respond to\nrulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven\nLLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on\nRULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules\nunlike what is expected from typical human reasoners. Further analysis suggests\nthat this apparent failure is potentially associated with the models' poor\nutilization of their world knowledge and their attention distribution patterns.\nWhilst revealing a limitation of current LLMs, our study also provides a timely\ncounterbalance to a growing body of recent works that propose methods relying\non formal logic to improve LLMs' general reasoning capabilities, highlighting\ntheir risk of further increasing divergence between LLMs and human-like\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal logic enables computers to reason in natural language by representing\nsentences in symbolic forms and applying rules to derive conclusions. However,\nin what our study characterizes as \"rulebreaker\" scenarios, this method can\nlead to conclusions that are typically not inferred or accepted by humans given\ntheir common sense and factual knowledge. Inspired by works in cognitive\nscience, we create RULEBREAKERS, the first dataset for rigorously evaluating\nthe ability of large language models (LLMs) to recognize and respond to\nrulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven\nLLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on\nRULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules\nunlike what is expected from typical human reasoners. Further analysis suggests\nthat this apparent failure is potentially associated with the models' poor\nutilization of their world knowledge and their attention distribution patterns.\nWhilst revealing a limitation of current LLMs, our study also provides a timely\ncounterbalance to a growing body of recent works that propose methods relying\non formal logic to improve LLMs' general reasoning capabilities, highlighting\ntheir risk of further increasing divergence between LLMs and human-like\nreasoning."
                },
                "authors": [
                    {
                        "name": "Jason Chan"
                    },
                    {
                        "name": "Robert Gaizauskas"
                    },
                    {
                        "name": "Zhixue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhixue Zhao"
                },
                "author": "Zhixue Zhao",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16502v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16502v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11348v1",
                "updated": "2025-08-15T09:25:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    25,
                    40,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T09:25:40Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    25,
                    40,
                    4,
                    227,
                    0
                ],
                "title": "NeMo: A Neuron-Level Modularizing-While-Training Approach for\n  Decomposing DNN Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeMo: A Neuron-Level Modularizing-While-Training Approach for\n  Decomposing DNN Models"
                },
                "summary": "With the growing incorporation of deep neural network (DNN) models into\nmodern software systems, the prohibitive construction costs have become a\nsignificant challenge. Model reuse has been widely applied to reduce training\ncosts, but indiscriminately reusing entire models may incur significant\ninference overhead. Consequently, DNN modularization has gained attention,\nenabling module reuse by decomposing DNN models. The emerging\nmodularizing-while-training (MwT) paradigm, which incorporates modularization\ninto training, outperforms modularizing-after-training approaches. However,\nexisting MwT methods focus on small-scale CNN models at the convolutional\nkernel level and struggle with diverse DNNs and large-scale models,\nparticularly Transformer-based models. To address these limitations, we propose\nNeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron\nlevel fundamental component common to all DNNs-ensuring applicability to\nTransformers and various architectures. We design a contrastive learning-based\nmodular training method with an effective composite loss function, enabling\nscalability to large-scale models. Comprehensive experiments on two\nTransformer-based models and four CNN models across two classification datasets\ndemonstrate NeMo's superiority over state-of-the-art MwT methods. Results show\naverage gains of 1.72% in module classification accuracy and 58.10% reduction\nin module size, demonstrating efficacy across both CNN and large-scale\nTransformer-based models. A case study on open-source projects shows NeMo's\npotential benefits in practical scenarios, offering a promising approach for\nscalable and generalizable DNN modularization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing incorporation of deep neural network (DNN) models into\nmodern software systems, the prohibitive construction costs have become a\nsignificant challenge. Model reuse has been widely applied to reduce training\ncosts, but indiscriminately reusing entire models may incur significant\ninference overhead. Consequently, DNN modularization has gained attention,\nenabling module reuse by decomposing DNN models. The emerging\nmodularizing-while-training (MwT) paradigm, which incorporates modularization\ninto training, outperforms modularizing-after-training approaches. However,\nexisting MwT methods focus on small-scale CNN models at the convolutional\nkernel level and struggle with diverse DNNs and large-scale models,\nparticularly Transformer-based models. To address these limitations, we propose\nNeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron\nlevel fundamental component common to all DNNs-ensuring applicability to\nTransformers and various architectures. We design a contrastive learning-based\nmodular training method with an effective composite loss function, enabling\nscalability to large-scale models. Comprehensive experiments on two\nTransformer-based models and four CNN models across two classification datasets\ndemonstrate NeMo's superiority over state-of-the-art MwT methods. Results show\naverage gains of 1.72% in module classification accuracy and 58.10% reduction\nin module size, demonstrating efficacy across both CNN and large-scale\nTransformer-based models. A case study on open-source projects shows NeMo's\npotential benefits in practical scenarios, offering a promising approach for\nscalable and generalizable DNN modularization."
                },
                "authors": [
                    {
                        "name": "Xiaohan Bi"
                    },
                    {
                        "name": "Binhang Qi"
                    },
                    {
                        "name": "Hailong Sun"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Xiaojun Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Liang"
                },
                "author": "Xiaojun Liang",
                "arxiv_doi": "10.1145/3757740",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3757740",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.11348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "ACM Transactions on Software Engineering and Methodology 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13420v3",
                "updated": "2025-08-15T09:23:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    23,
                    48,
                    4,
                    227,
                    0
                ],
                "published": "2025-01-23T06:48:48Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    48,
                    48,
                    3,
                    23,
                    0
                ],
                "title": "LVFace: Progressive Cluster Optimization for Large Vision Models in Face\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVFace: Progressive Cluster Optimization for Large Vision Models in Face\n  Recognition"
                },
                "summary": "Vision Transformers (ViTs) have revolutionized large-scale visual modeling,\nyet remain underexplored in face recognition (FR) where CNNs still dominate. We\nidentify a critical bottleneck: CNN-inspired training paradigms fail to unlock\nViT's potential, leading to suboptimal performance and convergence\ninstability.To address this challenge, we propose LVFace, a ViT-based FR model\nthat integrates Progressive Cluster Optimization (PCO) to achieve superior\nresults. Specifically, PCO sequentially applies negative class sub-sampling\n(NCS) for robust and fast feature alignment from random initialization, feature\nexpectation penalties for centroid stabilization, performing cluster boundary\nrefinement through full-batch training without NCS constraints. LVFace\nestablishes a new state-of-the-art face recognition baseline, surpassing\nleading approaches such as UniFace and TopoFR across multiple benchmarks.\nExtensive experiments demonstrate that LVFace delivers consistent performance\ngains, while exhibiting scalability to large-scale datasets and compatibility\nwith mainstream VLMs and LLMs. Notably, LVFace secured 1st place in the ICCV\n2021 Masked Face Recognition (MFR)-Ongoing Challenge (March 2025), proving its\nefficacy in real-world scenarios. Project is available at\nhttps://github.com/bytedance/LVFace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have revolutionized large-scale visual modeling,\nyet remain underexplored in face recognition (FR) where CNNs still dominate. We\nidentify a critical bottleneck: CNN-inspired training paradigms fail to unlock\nViT's potential, leading to suboptimal performance and convergence\ninstability.To address this challenge, we propose LVFace, a ViT-based FR model\nthat integrates Progressive Cluster Optimization (PCO) to achieve superior\nresults. Specifically, PCO sequentially applies negative class sub-sampling\n(NCS) for robust and fast feature alignment from random initialization, feature\nexpectation penalties for centroid stabilization, performing cluster boundary\nrefinement through full-batch training without NCS constraints. LVFace\nestablishes a new state-of-the-art face recognition baseline, surpassing\nleading approaches such as UniFace and TopoFR across multiple benchmarks.\nExtensive experiments demonstrate that LVFace delivers consistent performance\ngains, while exhibiting scalability to large-scale datasets and compatibility\nwith mainstream VLMs and LLMs. Notably, LVFace secured 1st place in the ICCV\n2021 Masked Face Recognition (MFR)-Ongoing Challenge (March 2025), proving its\nefficacy in real-world scenarios. Project is available at\nhttps://github.com/bytedance/LVFace."
                },
                "authors": [
                    {
                        "name": "Jinghan You"
                    },
                    {
                        "name": "Shanglin Li"
                    },
                    {
                        "name": "Yuanrui Sun"
                    },
                    {
                        "name": "Jiangchuan Wei"
                    },
                    {
                        "name": "Mingyu Guo"
                    },
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Jiao Ran"
                    }
                ],
                "author_detail": {
                    "name": "Jiao Ran"
                },
                "author": "Jiao Ran",
                "arxiv_comment": "Accepted at ICCV25 as highlight paper, code released at\n  https://github.com/bytedance/LVFace",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11343v1",
                "updated": "2025-08-15T09:13:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    13,
                    42,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T09:13:42Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    13,
                    42,
                    4,
                    227,
                    0
                ],
                "title": "SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated\n  Text via Spectral Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated\n  Text via Spectral Analysis"
                },
                "summary": "The proliferation of high-quality text from Large Language Models (LLMs)\ndemands reliable and efficient detection methods. While existing training-free\napproaches show promise, they often rely on surface-level statistics and\noverlook fundamental signal properties of the text generation process. In this\nwork, we reframe detection as a signal processing problem, introducing a novel\nparadigm that analyzes the sequence of token log-probabilities in the frequency\ndomain. By systematically analyzing the signal's spectral properties using the\nglobal Discrete Fourier Transform (DFT) and the local Short-Time Fourier\nTransform (STFT), we find that human-written text consistently exhibits\nsignificantly higher spectral energy. This higher energy reflects the\nlarger-amplitude fluctuations inherent in human writing compared to the\nsuppressed dynamics of LLM-generated text. Based on this key insight, we\nconstruct SpecDetect, a detector built on a single, robust feature from the\nglobal DFT: DFT total energy. We also propose an enhanced version,\nSpecDetect++, which incorporates a sampling discrepancy mechanism to further\nboost robustness. Extensive experiments demonstrate that our approach\noutperforms the state-of-the-art model while running in nearly half the time.\nOur work introduces a new, efficient, and interpretable pathway for\nLLM-generated text detection, showing that classical signal processing\ntechniques offer a surprisingly powerful solution to this modern challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of high-quality text from Large Language Models (LLMs)\ndemands reliable and efficient detection methods. While existing training-free\napproaches show promise, they often rely on surface-level statistics and\noverlook fundamental signal properties of the text generation process. In this\nwork, we reframe detection as a signal processing problem, introducing a novel\nparadigm that analyzes the sequence of token log-probabilities in the frequency\ndomain. By systematically analyzing the signal's spectral properties using the\nglobal Discrete Fourier Transform (DFT) and the local Short-Time Fourier\nTransform (STFT), we find that human-written text consistently exhibits\nsignificantly higher spectral energy. This higher energy reflects the\nlarger-amplitude fluctuations inherent in human writing compared to the\nsuppressed dynamics of LLM-generated text. Based on this key insight, we\nconstruct SpecDetect, a detector built on a single, robust feature from the\nglobal DFT: DFT total energy. We also propose an enhanced version,\nSpecDetect++, which incorporates a sampling discrepancy mechanism to further\nboost robustness. Extensive experiments demonstrate that our approach\noutperforms the state-of-the-art model while running in nearly half the time.\nOur work introduces a new, efficient, and interpretable pathway for\nLLM-generated text detection, showing that classical signal processing\ntechniques offer a surprisingly powerful solution to this modern challenge."
                },
                "authors": [
                    {
                        "name": "Haitong Luo"
                    },
                    {
                        "name": "Weiyao Zhang"
                    },
                    {
                        "name": "Suhang Wang"
                    },
                    {
                        "name": "Wenji Zou"
                    },
                    {
                        "name": "Chungang Lin"
                    },
                    {
                        "name": "Xuying Meng"
                    },
                    {
                        "name": "Yujun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yujun Zhang"
                },
                "author": "Yujun Zhang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11342v1",
                "updated": "2025-08-15T09:12:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    12,
                    30,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T09:12:30Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    12,
                    30,
                    4,
                    227,
                    0
                ],
                "title": "CrossTrace: Efficient Cross-Thread and Cross-Service Span Correlation in\n  Distributed Tracing for Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrossTrace: Efficient Cross-Thread and Cross-Service Span Correlation in\n  Distributed Tracing for Microservices"
                },
                "summary": "Distributed tracing has become an essential technique for debugging and\ntroubleshooting modern microservice-based applications, enabling software\nengineers to detect performance bottlenecks, identify failures, and gain\ninsights into system behavior. However, implementing distributed tracing in\nlarge-scale applications remains challenging due to the need for extensive\ninstrumentation. To reduce this burden, zero-code instrumentation solutions,\nsuch as those based on eBPF, have emerged, allowing span data to be collected\nwithout modifying application code. Despite this promise, span correlation, the\nprocess of establishing causal relationships between spans, remains a critical\nchallenge in zero-code approaches. Existing solutions often rely on thread\naffinity, compromise system security by requiring the kernel integrity mode to\nbe disabled, or incur significant computational overhead due to complex\ninference algorithms. This paper presents CrossTrace, a practical and efficient\ndistributed tracing solution designed to support the debugging of microservice\napplications without requiring source code modifications. CrossTrace employs a\ngreedy algorithm to infer intra-service span relationships from delay patterns,\neliminating reliance on thread identifiers. For inter-service correlation,\nCrossTrace embeds span identifiers into TCP packet headers via eBPF, enabling\nsecure and efficient correlation compromising system security policies.\nEvaluation results show that CrossTrace can correlate thousands of spans within\nseconds with over 90% accuracy, making it suitable for production deployment\nand valuable for microservice observability and diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed tracing has become an essential technique for debugging and\ntroubleshooting modern microservice-based applications, enabling software\nengineers to detect performance bottlenecks, identify failures, and gain\ninsights into system behavior. However, implementing distributed tracing in\nlarge-scale applications remains challenging due to the need for extensive\ninstrumentation. To reduce this burden, zero-code instrumentation solutions,\nsuch as those based on eBPF, have emerged, allowing span data to be collected\nwithout modifying application code. Despite this promise, span correlation, the\nprocess of establishing causal relationships between spans, remains a critical\nchallenge in zero-code approaches. Existing solutions often rely on thread\naffinity, compromise system security by requiring the kernel integrity mode to\nbe disabled, or incur significant computational overhead due to complex\ninference algorithms. This paper presents CrossTrace, a practical and efficient\ndistributed tracing solution designed to support the debugging of microservice\napplications without requiring source code modifications. CrossTrace employs a\ngreedy algorithm to infer intra-service span relationships from delay patterns,\neliminating reliance on thread identifiers. For inter-service correlation,\nCrossTrace embeds span identifiers into TCP packet headers via eBPF, enabling\nsecure and efficient correlation compromising system security policies.\nEvaluation results show that CrossTrace can correlate thousands of spans within\nseconds with over 90% accuracy, making it suitable for production deployment\nand valuable for microservice observability and diagnosis."
                },
                "authors": [
                    {
                        "name": "Linh-An Phan"
                    },
                    {
                        "name": "MingXue Wang"
                    },
                    {
                        "name": "Guangyu Wu"
                    },
                    {
                        "name": "Wang Dawei"
                    },
                    {
                        "name": "Chen Liqun"
                    },
                    {
                        "name": "Li Jin"
                    }
                ],
                "author_detail": {
                    "name": "Li Jin"
                },
                "author": "Li Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16481v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16481v3",
                "updated": "2025-08-15T09:10:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    10,
                    3,
                    4,
                    227,
                    0
                ],
                "published": "2025-05-22T10:07:33Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    10,
                    7,
                    33,
                    3,
                    142,
                    0
                ],
                "title": "Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable\n  Structured Latent Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable\n  Structured Latent Modelling"
                },
                "summary": "Gaussian Process (GP) Variational Autoencoders (VAEs) extend standard VAEs by\nreplacing the fully factorised Gaussian prior with a GP prior, thereby\ncapturing richer correlations among latent variables. However, performing exact\nGP inference in large-scale GPVAEs is computationally prohibitive, often\nforcing existing approaches to rely on restrictive kernel assumptions or large\nsets of inducing points. In this work, we propose a neighbour-driven\napproximation strategy that exploits local adjacencies in the latent space to\nachieve scalable GPVAE inference. By confining computations to the nearest\nneighbours of each data point, our method preserves essential latent\ndependencies, allowing more flexible kernel choices and mitigating the need for\nnumerous inducing points. Through extensive experiments on tasks including\nrepresentation learning, data imputation, and conditional generation, we\ndemonstrate that our approach outperforms other GPVAE variants in both\npredictive performance and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Process (GP) Variational Autoencoders (VAEs) extend standard VAEs by\nreplacing the fully factorised Gaussian prior with a GP prior, thereby\ncapturing richer correlations among latent variables. However, performing exact\nGP inference in large-scale GPVAEs is computationally prohibitive, often\nforcing existing approaches to rely on restrictive kernel assumptions or large\nsets of inducing points. In this work, we propose a neighbour-driven\napproximation strategy that exploits local adjacencies in the latent space to\nachieve scalable GPVAE inference. By confining computations to the nearest\nneighbours of each data point, our method preserves essential latent\ndependencies, allowing more flexible kernel choices and mitigating the need for\nnumerous inducing points. Through extensive experiments on tasks including\nrepresentation learning, data imputation, and conditional generation, we\ndemonstrate that our approach outperforms other GPVAE variants in both\npredictive performance and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Xinxing Shi"
                    },
                    {
                        "name": "Xiaoyu Jiang"
                    },
                    {
                        "name": "Mauricio A. Álvarez"
                    }
                ],
                "author_detail": {
                    "name": "Mauricio A. Álvarez"
                },
                "author": "Mauricio A. Álvarez",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16481v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16481v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11301v2",
                "updated": "2025-08-15T08:59:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    59,
                    23,
                    4,
                    227,
                    0
                ],
                "published": "2025-04-15T15:44:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    44,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Learning to Be A Doctor: Searching for Effective Medical Agent\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Be A Doctor: Searching for Effective Medical Agent\n  Architectures"
                },
                "summary": "Large Language Model (LLM)-based agents have demonstrated strong capabilities\nacross a wide range of tasks, and their application in the medical domain holds\nparticular promise due to the demand for high generalizability and reliance on\ninterdisciplinary knowledge. However, existing medical agent systems often rely\non static, manually crafted workflows that lack the flexibility to accommodate\ndiverse diagnostic requirements and adapt to emerging clinical scenarios.\nMotivated by the success of automated machine learning (AutoML), this paper\nintroduces a novel framework for the automated design of medical agent\narchitectures. Specifically, we define a hierarchical and expressive agent\nsearch space that enables dynamic workflow adaptation through structured\nmodifications at the node, structural, and framework levels. Our framework\nconceptualizes medical agents as graph-based architectures composed of diverse,\nfunctional node types and supports iterative self-improvement guided by\ndiagnostic feedback. Experimental results on skin disease diagnosis tasks\ndemonstrate that the proposed method effectively evolves workflow structures\nand significantly enhances diagnostic accuracy over time. This work represents\nthe first fully automated framework for medical agent architecture design and\noffers a scalable, adaptable foundation for deploying intelligent agents in\nreal-world clinical environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have demonstrated strong capabilities\nacross a wide range of tasks, and their application in the medical domain holds\nparticular promise due to the demand for high generalizability and reliance on\ninterdisciplinary knowledge. However, existing medical agent systems often rely\non static, manually crafted workflows that lack the flexibility to accommodate\ndiverse diagnostic requirements and adapt to emerging clinical scenarios.\nMotivated by the success of automated machine learning (AutoML), this paper\nintroduces a novel framework for the automated design of medical agent\narchitectures. Specifically, we define a hierarchical and expressive agent\nsearch space that enables dynamic workflow adaptation through structured\nmodifications at the node, structural, and framework levels. Our framework\nconceptualizes medical agents as graph-based architectures composed of diverse,\nfunctional node types and supports iterative self-improvement guided by\ndiagnostic feedback. Experimental results on skin disease diagnosis tasks\ndemonstrate that the proposed method effectively evolves workflow structures\nand significantly enhances diagnostic accuracy over time. This work represents\nthe first fully automated framework for medical agent architecture design and\noffers a scalable, adaptable foundation for deploying intelligent agents in\nreal-world clinical environments."
                },
                "authors": [
                    {
                        "name": "Yangyang Zhuang"
                    },
                    {
                        "name": "Wenjia Jiang"
                    },
                    {
                        "name": "Jiayu Zhang"
                    },
                    {
                        "name": "Ze Yang"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    },
                    {
                        "name": "Chi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Zhang"
                },
                "author": "Chi Zhang",
                "arxiv_comment": "Accepted at ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11326v1",
                "updated": "2025-08-15T08:53:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    53,
                    56,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T08:53:56Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    53,
                    56,
                    4,
                    227,
                    0
                ],
                "title": "MoE-TTS: Enhancing Out-of-Domain Text Understanding for\n  Description-based TTS via Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-TTS: Enhancing Out-of-Domain Text Understanding for\n  Description-based TTS via Mixture-of-Experts"
                },
                "summary": "Description-based text-to-speech (TTS) models exhibit strong performance on\nin-domain text descriptions, i.e., those encountered during training. However,\nin real-world applications, the diverse range of user-generated descriptions\ninevitably introduces numerous out-of-domain inputs that challenge the text\nunderstanding capabilities of these systems. To address this issue, we propose\nMoE-TTS, a description-based TTS model designed to enhance the understanding of\nout-of-domain text descriptions. MoE-TTS employs a modality-based\nmixture-of-experts (MoE) approach to augment a pre-trained textual large\nlanguage model (LLM) with a set of specialized weights adapted to the speech\nmodality while maintaining the original LLM frozen during training. This\napproach allows MoE-TTS to effectively leverage the pre-trained knowledge and\ntext understanding abilities of textual LLMs. Our experimental results indicate\nthat: first, even the most advanced closed-source commercial products can be\nchallenged by carefully designed out-of-domain description test sets; second,\nMoE-TTS achieves superior performance in generating speech that more accurately\nreflects the descriptions. We encourage readers to listen to the demos at\nhttps://welkinyang.github.io/MoE-TTS/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Description-based text-to-speech (TTS) models exhibit strong performance on\nin-domain text descriptions, i.e., those encountered during training. However,\nin real-world applications, the diverse range of user-generated descriptions\ninevitably introduces numerous out-of-domain inputs that challenge the text\nunderstanding capabilities of these systems. To address this issue, we propose\nMoE-TTS, a description-based TTS model designed to enhance the understanding of\nout-of-domain text descriptions. MoE-TTS employs a modality-based\nmixture-of-experts (MoE) approach to augment a pre-trained textual large\nlanguage model (LLM) with a set of specialized weights adapted to the speech\nmodality while maintaining the original LLM frozen during training. This\napproach allows MoE-TTS to effectively leverage the pre-trained knowledge and\ntext understanding abilities of textual LLMs. Our experimental results indicate\nthat: first, even the most advanced closed-source commercial products can be\nchallenged by carefully designed out-of-domain description test sets; second,\nMoE-TTS achieves superior performance in generating speech that more accurately\nreflects the descriptions. We encourage readers to listen to the demos at\nhttps://welkinyang.github.io/MoE-TTS/."
                },
                "authors": [
                    {
                        "name": "Heyang Xue"
                    },
                    {
                        "name": "Xuchen Song"
                    },
                    {
                        "name": "Yu Tang"
                    },
                    {
                        "name": "Jianyu Chen"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yahui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yahui Zhou"
                },
                "author": "Yahui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11318v1",
                "updated": "2025-08-15T08:41:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    41,
                    20,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T08:41:20Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    41,
                    20,
                    4,
                    227,
                    0
                ],
                "title": "LLM Compression: How Far Can We Go in Balancing Size and Performance?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Compression: How Far Can We Go in Balancing Size and Performance?"
                },
                "summary": "Quantization is an essential and popular technique for improving the\naccessibility of large language models (LLMs) by reducing memory usage and\ncomputational costs while maintaining performance. In this study, we apply\n4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer\nQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their\nimpact across multiple NLP tasks. We benchmark these models on MS MARCO\n(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K\n(Mathematical Reasoning) datasets, assessing both accuracy and efficiency\nacross various tasks. The study measures the trade-offs between model\ncompression and task performance, analyzing key evaluation metrics, namely\naccuracy, inference latency, and throughput (total output tokens generated per\nsecond), providing insights into the suitability of low-bit quantization for\nreal-world deployment. Using the results, users can then make suitable\ndecisions based on the specifications that need to be met. We discuss the pros\nand cons of GSQ and GPTQ techniques on models of different sizes, which also\nserve as a benchmark for future experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is an essential and popular technique for improving the\naccessibility of large language models (LLMs) by reducing memory usage and\ncomputational costs while maintaining performance. In this study, we apply\n4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer\nQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their\nimpact across multiple NLP tasks. We benchmark these models on MS MARCO\n(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K\n(Mathematical Reasoning) datasets, assessing both accuracy and efficiency\nacross various tasks. The study measures the trade-offs between model\ncompression and task performance, analyzing key evaluation metrics, namely\naccuracy, inference latency, and throughput (total output tokens generated per\nsecond), providing insights into the suitability of low-bit quantization for\nreal-world deployment. Using the results, users can then make suitable\ndecisions based on the specifications that need to be met. We discuss the pros\nand cons of GSQ and GPTQ techniques on models of different sizes, which also\nserve as a benchmark for future experiments."
                },
                "authors": [
                    {
                        "name": "Sahil Sk"
                    },
                    {
                        "name": "Debasish Dhal"
                    },
                    {
                        "name": "Sonal Khosla"
                    },
                    {
                        "name": "Sk Shahid"
                    },
                    {
                        "name": "Sambit Shekhar"
                    },
                    {
                        "name": "Akash Dhaka"
                    },
                    {
                        "name": "Shantipriya Parida"
                    },
                    {
                        "name": "Dilip K. Prasad"
                    },
                    {
                        "name": "Ondřej Bojar"
                    }
                ],
                "author_detail": {
                    "name": "Ondřej Bojar"
                },
                "author": "Ondřej Bojar",
                "arxiv_comment": "This paper has been accepted for presentation at the RANLP 2025\n  conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11310v1",
                "updated": "2025-08-15T08:27:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    27,
                    58,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T08:27:58Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    27,
                    58,
                    4,
                    227,
                    0
                ],
                "title": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced\n  Benchmark for Automatic Survey Generation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced\n  Benchmark for Automatic Survey Generation Systems"
                },
                "summary": "The growing interest in automatic survey generation (ASG), a task that\ntraditionally required considerable time and effort, has been spurred by recent\nadvances in large language models (LLMs). With advancements in\nretrieval-augmented generation (RAG) and the rising popularity of multi-agent\nsystems (MASs), synthesizing academic surveys using LLMs has become a viable\napproach, thereby elevating the need for robust evaluation methods in this\ndomain. However, existing evaluation methods suffer from several limitations,\nincluding biased metrics, a lack of human preference, and an over-reliance on\nLLMs-as-judges. To address these challenges, we propose SGSimEval, a\ncomprehensive benchmark for Survey Generation with Similarity-Enhanced\nEvaluation that evaluates automatic survey generation systems by integrating\nassessments of the outline, content, and references, and also combines\nLLM-based scoring with quantitative metrics to provide a multifaceted\nevaluation framework. In SGSimEval, we also introduce human preference metrics\nthat emphasize both inherent quality and similarity to humans. Extensive\nexperiments reveal that current ASG systems demonstrate human-comparable\nsuperiority in outline generation, while showing significant room for\nimprovement in content and reference generation, and our evaluation metrics\nmaintain strong consistency with human assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in automatic survey generation (ASG), a task that\ntraditionally required considerable time and effort, has been spurred by recent\nadvances in large language models (LLMs). With advancements in\nretrieval-augmented generation (RAG) and the rising popularity of multi-agent\nsystems (MASs), synthesizing academic surveys using LLMs has become a viable\napproach, thereby elevating the need for robust evaluation methods in this\ndomain. However, existing evaluation methods suffer from several limitations,\nincluding biased metrics, a lack of human preference, and an over-reliance on\nLLMs-as-judges. To address these challenges, we propose SGSimEval, a\ncomprehensive benchmark for Survey Generation with Similarity-Enhanced\nEvaluation that evaluates automatic survey generation systems by integrating\nassessments of the outline, content, and references, and also combines\nLLM-based scoring with quantitative metrics to provide a multifaceted\nevaluation framework. In SGSimEval, we also introduce human preference metrics\nthat emphasize both inherent quality and similarity to humans. Extensive\nexperiments reveal that current ASG systems demonstrate human-comparable\nsuperiority in outline generation, while showing significant room for\nimprovement in content and reference generation, and our evaluation metrics\nmaintain strong consistency with human assessments."
                },
                "authors": [
                    {
                        "name": "Beichen Guo"
                    },
                    {
                        "name": "Zhiyuan Wen"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Ruosong Yang"
                    },
                    {
                        "name": "Jiaxing Shen"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxing Shen"
                },
                "author": "Jiaxing Shen",
                "arxiv_comment": "Accepted to The 21st International Conference on Advanced Data Mining\n  and Applications (ADMA2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11305v1",
                "updated": "2025-08-15T08:20:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    20,
                    9,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T08:20:09Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    20,
                    9,
                    4,
                    227,
                    0
                ],
                "title": "Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and\n  Reasoning"
                },
                "summary": "Logging code is written by developers to capture system runtime behavior and\nplays a vital role in debugging, performance analysis, and system monitoring.\nHowever, defects in logging code can undermine the usefulness of logs and lead\nto misinterpretations. Although prior work has identified several logging\ndefect patterns and provided valuable insights into logging practices, these\nstudies often focus on a narrow range of defect patterns derived from limited\nsources (e.g., commit histories) and lack a systematic and comprehensive\nanalysis. Moreover, large language models (LLMs) have demonstrated promising\ngeneralization and reasoning capabilities across a variety of code-related\ntasks, yet their potential for detecting logging code defects remains largely\nunexplored.\n  In this paper, we derive a comprehensive taxonomy of logging code defects,\nwhich encompasses seven logging code defect patterns with 14 detailed\nscenarios. We further construct a benchmark dataset, \\dataset, consisting of\n164 developer-verified real-world logging defects. Then we propose an automated\nframework that leverages various prompting strategies and contextual\ninformation to evaluate LLMs' capability in detecting and reasoning logging\ncode defects. Experimental results reveal that LLMs generally struggle to\naccurately detect and reason logging code defects based on the source code\nonly. However, incorporating proper knowledge (e.g., detailed scenarios of\ndefect patterns) can lead to 10.9\\% improvement in detection accuracy. Overall,\nour findings provide actionable guidance for practitioners to avoid common\ndefect patterns and establish a foundation for improving LLM-based reasoning in\nlogging code defect detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logging code is written by developers to capture system runtime behavior and\nplays a vital role in debugging, performance analysis, and system monitoring.\nHowever, defects in logging code can undermine the usefulness of logs and lead\nto misinterpretations. Although prior work has identified several logging\ndefect patterns and provided valuable insights into logging practices, these\nstudies often focus on a narrow range of defect patterns derived from limited\nsources (e.g., commit histories) and lack a systematic and comprehensive\nanalysis. Moreover, large language models (LLMs) have demonstrated promising\ngeneralization and reasoning capabilities across a variety of code-related\ntasks, yet their potential for detecting logging code defects remains largely\nunexplored.\n  In this paper, we derive a comprehensive taxonomy of logging code defects,\nwhich encompasses seven logging code defect patterns with 14 detailed\nscenarios. We further construct a benchmark dataset, \\dataset, consisting of\n164 developer-verified real-world logging defects. Then we propose an automated\nframework that leverages various prompting strategies and contextual\ninformation to evaluate LLMs' capability in detecting and reasoning logging\ncode defects. Experimental results reveal that LLMs generally struggle to\naccurately detect and reason logging code defects based on the source code\nonly. However, incorporating proper knowledge (e.g., detailed scenarios of\ndefect patterns) can lead to 10.9\\% improvement in detection accuracy. Overall,\nour findings provide actionable guidance for practitioners to avoid common\ndefect patterns and establish a foundation for improving LLM-based reasoning in\nlogging code defect detection."
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Zhenhao Li"
                    },
                    {
                        "name": "Zishuo Ding"
                    }
                ],
                "author_detail": {
                    "name": "Zishuo Ding"
                },
                "author": "Zishuo Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10210v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10210v2",
                "updated": "2025-08-15T08:12:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    12,
                    18,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-13T21:40:35Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    21,
                    40,
                    35,
                    2,
                    225,
                    0
                ],
                "title": "An Explainable AI based approach for Monitoring Animal Health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Explainable AI based approach for Monitoring Animal Health"
                },
                "summary": "Monitoring cattle health and optimizing yield are key challenges faced by\ndairy farmers due to difficulties in tracking all animals on the farm. This\nwork aims to showcase modern data-driven farming practices based on explainable\nmachine learning(ML) methods that explain the activity and behaviour of dairy\ncattle (cows). Continuous data collection of 3-axis accelerometer sensors and\nusage of robust ML methodologies and algorithms, provide farmers and\nresearchers with actionable information on cattle activity, allowing farmers to\nmake informed decisions and incorporate sustainable practices. This study\nutilizes Bluetooth-based Internet of Things (IoT) devices and 4G networks for\nseamless data transmission, immediate analysis, inference generation, and\nexplains the models performance with explainability frameworks. Special\nemphasis is put on the pre-processing of the accelerometers time series data,\nincluding the extraction of statistical characteristics, signal processing\ntechniques, and lag-based features using the sliding window technique. Various\nhyperparameter-optimized ML models are evaluated across varying window lengths\nfor activity classification. The k-nearest neighbour Classifier achieved the\nbest performance, with AUC of mean 0.98 and standard deviation of 0.0026 on the\ntraining set and 0.99 on testing set). In order to ensure transparency,\nExplainable AI based frameworks such as SHAP is used to interpret feature\nimportance that can be understood and used by practitioners. A detailed\ncomparison of the important features, along with the stability analysis of\nselected features, supports development of explainable and practical ML models\nfor sustainable livestock management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring cattle health and optimizing yield are key challenges faced by\ndairy farmers due to difficulties in tracking all animals on the farm. This\nwork aims to showcase modern data-driven farming practices based on explainable\nmachine learning(ML) methods that explain the activity and behaviour of dairy\ncattle (cows). Continuous data collection of 3-axis accelerometer sensors and\nusage of robust ML methodologies and algorithms, provide farmers and\nresearchers with actionable information on cattle activity, allowing farmers to\nmake informed decisions and incorporate sustainable practices. This study\nutilizes Bluetooth-based Internet of Things (IoT) devices and 4G networks for\nseamless data transmission, immediate analysis, inference generation, and\nexplains the models performance with explainability frameworks. Special\nemphasis is put on the pre-processing of the accelerometers time series data,\nincluding the extraction of statistical characteristics, signal processing\ntechniques, and lag-based features using the sliding window technique. Various\nhyperparameter-optimized ML models are evaluated across varying window lengths\nfor activity classification. The k-nearest neighbour Classifier achieved the\nbest performance, with AUC of mean 0.98 and standard deviation of 0.0026 on the\ntraining set and 0.99 on testing set). In order to ensure transparency,\nExplainable AI based frameworks such as SHAP is used to interpret feature\nimportance that can be understood and used by practitioners. A detailed\ncomparison of the important features, along with the stability analysis of\nselected features, supports development of explainable and practical ML models\nfor sustainable livestock management."
                },
                "authors": [
                    {
                        "name": "Rahul Jana"
                    },
                    {
                        "name": "Shubham Dixit"
                    },
                    {
                        "name": "Mrityunjay Sharma"
                    },
                    {
                        "name": "Ritesh Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ritesh Kumar"
                },
                "author": "Ritesh Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10210v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10210v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11736v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11736v2",
                "updated": "2025-08-15T08:12:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    12,
                    16,
                    4,
                    227,
                    0
                ],
                "published": "2024-12-16T12:57:19Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    57,
                    19,
                    0,
                    351,
                    0
                ],
                "title": "Personalized LLM for Generating Customized Responses to the Same Query\n  from Different Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized LLM for Generating Customized Responses to the Same Query\n  from Different Users"
                },
                "summary": "Existing work on large language model (LLM) personalization assigned\ndifferent responding roles to LLMs, but overlooked the diversity of queriers.\nIn this work, we propose a new form of querier-aware LLM personalization,\ngenerating different responses even for the same query from different queriers.\nWe design a dual-tower model architecture with a cross-querier general encoder\nand a querier-specific encoder. We further apply contrastive learning with\nmulti-view augmentation, pulling close the dialogue representations of the same\nquerier, while pulling apart those of different queriers. To mitigate the\nimpact of query diversity on querier-contrastive learning, we cluster the\ndialogues based on query similarity and restrict the scope of contrastive\nlearning within each cluster. To address the lack of datasets designed for\nquerier-aware personalization, we also build a multi-querier dataset from\nEnglish and Chinese scripts, as well as WeChat records, called MQDialog,\ncontaining 173 queriers and 12 responders. Extensive evaluations demonstrate\nthat our design significantly improves the quality of personalized response\ngeneration, achieving relative improvement of 8.4% to 48.7% in ROUGE-L scores\nand winning rates ranging from 54% to 82% compared with various baseline\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing work on large language model (LLM) personalization assigned\ndifferent responding roles to LLMs, but overlooked the diversity of queriers.\nIn this work, we propose a new form of querier-aware LLM personalization,\ngenerating different responses even for the same query from different queriers.\nWe design a dual-tower model architecture with a cross-querier general encoder\nand a querier-specific encoder. We further apply contrastive learning with\nmulti-view augmentation, pulling close the dialogue representations of the same\nquerier, while pulling apart those of different queriers. To mitigate the\nimpact of query diversity on querier-contrastive learning, we cluster the\ndialogues based on query similarity and restrict the scope of contrastive\nlearning within each cluster. To address the lack of datasets designed for\nquerier-aware personalization, we also build a multi-querier dataset from\nEnglish and Chinese scripts, as well as WeChat records, called MQDialog,\ncontaining 173 queriers and 12 responders. Extensive evaluations demonstrate\nthat our design significantly improves the quality of personalized response\ngeneration, achieving relative improvement of 8.4% to 48.7% in ROUGE-L scores\nand winning rates ranging from 54% to 82% compared with various baseline\nmethods."
                },
                "authors": [
                    {
                        "name": "Hang Zeng"
                    },
                    {
                        "name": "Chaoyue Niu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Accepted by CIKM'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11736v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11736v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10016v2",
                "updated": "2025-08-15T08:09:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    9,
                    53,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-06T16:17:29Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    17,
                    29,
                    2,
                    218,
                    0
                ],
                "title": "Training-Free Multimodal Large Language Model Orchestration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Multimodal Large Language Model Orchestration"
                },
                "summary": "Different Multimodal Large Language Models (MLLMs) cannot be integrated into\na unified multimodal input-output system directly. In previous work, training\nhas been considered as an inevitable component due to challenges in modal\nalignment, Text-to-Speech efficiency and other integration issues. In this\npaper, we introduce Multimodal Large Language Model Orchestration, an effective\napproach for creating interactive multimodal AI systems without additional\ntraining. MLLM Orchestration leverages the inherent reasoning capabilities of\nlarge language models to coordinate specialized models through explicit\nworkflows, enabling natural multimodal interactions while maintaining\nmodularity, improving interpretability, and significantly enhancing\ncomputational efficiency. Our orchestration framework is built upon three key\ninnovations: (1) a central controller LLM that analyzes user inputs and\ndynamically routes tasks to appropriate specialized models through carefully\ndesigned agents; (2) a parallel Text-to-Speech architecture that enables true\nfull-duplex interaction with seamless interruption handling and natural\nconversational flow; and (3) a cross-modal memory integration system that\nmaintains coherent context across modalities through intelligent information\nsynthesis and retrieval, selectively avoiding unnecessary modality calls in\ncertain scenarios to improve response speed. Extensive evaluations demonstrate\nthat MLLM Orchestration achieves comprehensive multimodal capabilities without\nadditional training, performance improvements of up to 7.8% over traditional\njointly-trained approaches on standard benchmarks, reduced latency by 10.3%,\nand significantly enhanced interpretability through explicit orchestration\nprocesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Different Multimodal Large Language Models (MLLMs) cannot be integrated into\na unified multimodal input-output system directly. In previous work, training\nhas been considered as an inevitable component due to challenges in modal\nalignment, Text-to-Speech efficiency and other integration issues. In this\npaper, we introduce Multimodal Large Language Model Orchestration, an effective\napproach for creating interactive multimodal AI systems without additional\ntraining. MLLM Orchestration leverages the inherent reasoning capabilities of\nlarge language models to coordinate specialized models through explicit\nworkflows, enabling natural multimodal interactions while maintaining\nmodularity, improving interpretability, and significantly enhancing\ncomputational efficiency. Our orchestration framework is built upon three key\ninnovations: (1) a central controller LLM that analyzes user inputs and\ndynamically routes tasks to appropriate specialized models through carefully\ndesigned agents; (2) a parallel Text-to-Speech architecture that enables true\nfull-duplex interaction with seamless interruption handling and natural\nconversational flow; and (3) a cross-modal memory integration system that\nmaintains coherent context across modalities through intelligent information\nsynthesis and retrieval, selectively avoiding unnecessary modality calls in\ncertain scenarios to improve response speed. Extensive evaluations demonstrate\nthat MLLM Orchestration achieves comprehensive multimodal capabilities without\nadditional training, performance improvements of up to 7.8% over traditional\njointly-trained approaches on standard benchmarks, reduced latency by 10.3%,\nand significantly enhanced interpretability through explicit orchestration\nprocesses."
                },
                "authors": [
                    {
                        "name": "Tianyu Xie"
                    },
                    {
                        "name": "Yuhang Wu"
                    },
                    {
                        "name": "Yongdong Luo"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Xiawu Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Xiawu Zheng"
                },
                "author": "Xiawu Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18932v2",
                "updated": "2025-08-15T08:08:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    8,
                    0,
                    4,
                    227,
                    0
                ],
                "published": "2025-07-25T03:58:07Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    3,
                    58,
                    7,
                    4,
                    206,
                    0
                ],
                "title": "MMESGBench: Pioneering Multimodal Understanding and Complex Reasoning\n  Benchmark for ESG Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMESGBench: Pioneering Multimodal Understanding and Complex Reasoning\n  Benchmark for ESG Tasks"
                },
                "summary": "Environmental, Social, and Governance (ESG) reports are essential for\nevaluating sustainability practices, ensuring regulatory compliance, and\npromoting financial transparency. However, these documents are often lengthy,\nstructurally diverse, and multimodal, comprising dense text, structured tables,\ncomplex figures, and layout-dependent semantics. Existing AI systems often\nstruggle to perform reliable document-level reasoning in such settings, and no\ndedicated benchmark currently exists in ESG domain. To fill the gap, we\nintroduce \\textbf{MMESGBench}, a first-of-its-kind benchmark dataset targeted\nto evaluate multimodal understanding and complex reasoning across structurally\ndiverse and multi-source ESG documents. This dataset is constructed via a\nhuman-AI collaborative, multi-stage pipeline. First, a multimodal LLM generates\ncandidate question-answer (QA) pairs by jointly interpreting rich textual,\ntabular, and visual information from layout-aware document pages. Second, an\nLLM verifies the semantic accuracy, completeness, and reasoning complexity of\neach QA pair. This automated process is followed by an expert-in-the-loop\nvalidation, where domain specialists validate and calibrate QA pairs to ensure\nquality, relevance, and diversity. MMESGBench comprises 933 validated QA pairs\nderived from 45 ESG documents, spanning across seven distinct document types\nand three major ESG source categories. Questions are categorized as\nsingle-page, cross-page, or unanswerable, with each accompanied by fine-grained\nmultimodal evidence. Initial experiments validate that multimodal and\nretrieval-augmented models substantially outperform text-only baselines,\nparticularly on visually grounded and cross-page tasks. MMESGBench is publicly\navailable as an open-source dataset at\nhttps://github.com/Zhanglei1103/MMESGBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environmental, Social, and Governance (ESG) reports are essential for\nevaluating sustainability practices, ensuring regulatory compliance, and\npromoting financial transparency. However, these documents are often lengthy,\nstructurally diverse, and multimodal, comprising dense text, structured tables,\ncomplex figures, and layout-dependent semantics. Existing AI systems often\nstruggle to perform reliable document-level reasoning in such settings, and no\ndedicated benchmark currently exists in ESG domain. To fill the gap, we\nintroduce \\textbf{MMESGBench}, a first-of-its-kind benchmark dataset targeted\nto evaluate multimodal understanding and complex reasoning across structurally\ndiverse and multi-source ESG documents. This dataset is constructed via a\nhuman-AI collaborative, multi-stage pipeline. First, a multimodal LLM generates\ncandidate question-answer (QA) pairs by jointly interpreting rich textual,\ntabular, and visual information from layout-aware document pages. Second, an\nLLM verifies the semantic accuracy, completeness, and reasoning complexity of\neach QA pair. This automated process is followed by an expert-in-the-loop\nvalidation, where domain specialists validate and calibrate QA pairs to ensure\nquality, relevance, and diversity. MMESGBench comprises 933 validated QA pairs\nderived from 45 ESG documents, spanning across seven distinct document types\nand three major ESG source categories. Questions are categorized as\nsingle-page, cross-page, or unanswerable, with each accompanied by fine-grained\nmultimodal evidence. Initial experiments validate that multimodal and\nretrieval-augmented models substantially outperform text-only baselines,\nparticularly on visually grounded and cross-page tasks. MMESGBench is publicly\navailable as an open-source dataset at\nhttps://github.com/Zhanglei1103/MMESGBench."
                },
                "authors": [
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Chaoyue He"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Chunyan Miao"
                    }
                ],
                "author_detail": {
                    "name": "Chunyan Miao"
                },
                "author": "Chunyan Miao",
                "arxiv_comment": "Accepted at ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11291v1",
                "updated": "2025-08-15T07:55:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:55:05Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "title": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks"
                },
                "summary": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks."
                },
                "authors": [
                    {
                        "name": "Rui Bao"
                    },
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Chen"
                },
                "author": "Zhiyong Chen",
                "arxiv_comment": "accepted by IEEE/CIC ICCC workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11290v1",
                "updated": "2025-08-15T07:54:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    54,
                    42,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:54:42Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    54,
                    42,
                    4,
                    227,
                    0
                ],
                "title": "SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through\n  Task-Specific Trajectory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through\n  Task-Specific Trajectory"
                },
                "summary": "LLMs increasingly exhibit over-refusal behavior, where safety mechanisms\ncause models to reject benign instructions that superficially resemble harmful\ncontent. This phenomena diminishes utility in production applications that\nrepeatedly rely on common prompt templates or applications that frequently rely\non LLMs for specific tasks (e.g. sentiment analysis, language translation).\nThrough comprehensive evaluation, we demonstrate that LLMs still tend to refuse\nresponses to harmful instructions when those instructions are reframed to\nappear as benign tasks. Our mechanistic analysis reveal that LLMs follow\ndistinct \"constellation\" patterns in embedding space as representations\ntraverse layers, with each task maintaining consistent trajectories that shift\npredictably between refusal and non-refusal cases. We introduce\nSafeConstellations, an inference-time trajectory-shifting approach that tracks\ntask-specific trajectory patterns and guides representations toward non-refusal\npathways. By selectively guiding model behavior only on tasks prone to\nover-refusal, and by preserving general model behavior, our method reduces\nover-refusal rates by up to 73% with minimal impact on utility-offering a\nprincipled approach to mitigating over-refusals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs increasingly exhibit over-refusal behavior, where safety mechanisms\ncause models to reject benign instructions that superficially resemble harmful\ncontent. This phenomena diminishes utility in production applications that\nrepeatedly rely on common prompt templates or applications that frequently rely\non LLMs for specific tasks (e.g. sentiment analysis, language translation).\nThrough comprehensive evaluation, we demonstrate that LLMs still tend to refuse\nresponses to harmful instructions when those instructions are reframed to\nappear as benign tasks. Our mechanistic analysis reveal that LLMs follow\ndistinct \"constellation\" patterns in embedding space as representations\ntraverse layers, with each task maintaining consistent trajectories that shift\npredictably between refusal and non-refusal cases. We introduce\nSafeConstellations, an inference-time trajectory-shifting approach that tracks\ntask-specific trajectory patterns and guides representations toward non-refusal\npathways. By selectively guiding model behavior only on tasks prone to\nover-refusal, and by preserving general model behavior, our method reduces\nover-refusal rates by up to 73% with minimal impact on utility-offering a\nprincipled approach to mitigating over-refusals."
                },
                "authors": [
                    {
                        "name": "Utsav Maskey"
                    },
                    {
                        "name": "Sumit Yadav"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11287v1",
                "updated": "2025-08-15T07:49:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    49,
                    22,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:49:22Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    49,
                    22,
                    4,
                    227,
                    0
                ],
                "title": "CSGO: Generalized Optimization for Cold Start in Wireless Collaborative\n  Edge LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSGO: Generalized Optimization for Cold Start in Wireless Collaborative\n  Edge LLM Systems"
                },
                "summary": "While deploying large language models on edge devices promises low-latency\nand privacy-preserving AI services, it is hindered by limited device resources.\nAlthough pipeline parallelism facilitates distributed inference, existing\napproaches often ignore the cold-start latency caused by on-demand model\nloading. In this paper, we propose a latency-aware scheduling framework that\noverlaps model loading with computation and communication to minimize total\ninference latency. Based on device and model parameters, the framework\ndynamically adjusts layer partitioning and allocation to effectively hide\nloading time, thereby eliminating as many idle periods as possible. We\nformulate the problem as a Mixed-Integer Non-Linear Program and design an\nefficient dynamic programming algorithm to optimize model partitioning and\ndevice assignment. Experimental results show that the proposed method\nsignificantly reduces cold-start latency compared to baseline strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While deploying large language models on edge devices promises low-latency\nand privacy-preserving AI services, it is hindered by limited device resources.\nAlthough pipeline parallelism facilitates distributed inference, existing\napproaches often ignore the cold-start latency caused by on-demand model\nloading. In this paper, we propose a latency-aware scheduling framework that\noverlaps model loading with computation and communication to minimize total\ninference latency. Based on device and model parameters, the framework\ndynamically adjusts layer partitioning and allocation to effectively hide\nloading time, thereby eliminating as many idle periods as possible. We\nformulate the problem as a Mixed-Integer Non-Linear Program and design an\nefficient dynamic programming algorithm to optimize model partitioning and\ndevice assignment. Experimental results show that the proposed method\nsignificantly reduces cold-start latency compared to baseline strategies."
                },
                "authors": [
                    {
                        "name": "Xuran Liu"
                    },
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Rui Bao"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Shuguang Cui"
                    }
                ],
                "author_detail": {
                    "name": "Shuguang Cui"
                },
                "author": "Shuguang Cui",
                "arxiv_comment": "submitted to Journal of Communications and Information Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11285v1",
                "updated": "2025-08-15T07:47:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    47,
                    10,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:47:10Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    47,
                    10,
                    4,
                    227,
                    0
                ],
                "title": "AI in Mental Health: Emotional and Sentiment Analysis of Large Language\n  Models' Responses to Depression, Anxiety, and Stress Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI in Mental Health: Emotional and Sentiment Analysis of Large Language\n  Models' Responses to Depression, Anxiety, and Stress Queries"
                },
                "summary": "Depression, anxiety, and stress are widespread mental health concerns that\nincreasingly drive individuals to seek information from Large Language Models\n(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini\nPro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty\npragmatic questions about depression, anxiety, and stress when those questions\nare framed for six user profiles (baseline, woman, man, young, old, and\nuniversity student). The models generated 2,880 answers, which we scored for\nsentiment and emotions using state-of-the-art tools. Our analysis revealed that\noptimism, fear, and sadness dominated the emotional landscape across all\noutputs, with neutral sentiment maintaining consistently high values.\nGratitude, joy, and trust appeared at moderate levels, while emotions such as\nanger, disgust, and love were rarely expressed. The choice of LLM significantly\ninfluenced emotional expression patterns. Mixtral exhibited the highest levels\nof negative emotions including disapproval, annoyance, and sadness, while Llama\ndemonstrated the most optimistic and joyful responses. The type of mental\nhealth condition dramatically shaped emotional responses: anxiety prompts\nelicited extraordinarily high fear scores (0.974), depression prompts generated\nelevated sadness (0.686) and the highest negative sentiment, while\nstress-related queries produced the most optimistic responses (0.755) with\nelevated joy and trust. In contrast, demographic framing of queries produced\nonly marginal variations in emotional tone. Statistical analyses confirmed\nsignificant model-specific and condition-specific differences, while\ndemographic influences remained minimal. These findings highlight the critical\nimportance of model selection in mental health applications, as each LLM\nexhibits a distinct emotional signature that could significantly impact user\nexperience and outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depression, anxiety, and stress are widespread mental health concerns that\nincreasingly drive individuals to seek information from Large Language Models\n(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini\nPro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty\npragmatic questions about depression, anxiety, and stress when those questions\nare framed for six user profiles (baseline, woman, man, young, old, and\nuniversity student). The models generated 2,880 answers, which we scored for\nsentiment and emotions using state-of-the-art tools. Our analysis revealed that\noptimism, fear, and sadness dominated the emotional landscape across all\noutputs, with neutral sentiment maintaining consistently high values.\nGratitude, joy, and trust appeared at moderate levels, while emotions such as\nanger, disgust, and love were rarely expressed. The choice of LLM significantly\ninfluenced emotional expression patterns. Mixtral exhibited the highest levels\nof negative emotions including disapproval, annoyance, and sadness, while Llama\ndemonstrated the most optimistic and joyful responses. The type of mental\nhealth condition dramatically shaped emotional responses: anxiety prompts\nelicited extraordinarily high fear scores (0.974), depression prompts generated\nelevated sadness (0.686) and the highest negative sentiment, while\nstress-related queries produced the most optimistic responses (0.755) with\nelevated joy and trust. In contrast, demographic framing of queries produced\nonly marginal variations in emotional tone. Statistical analyses confirmed\nsignificant model-specific and condition-specific differences, while\ndemographic influences remained minimal. These findings highlight the critical\nimportance of model selection in mental health applications, as each LLM\nexhibits a distinct emotional signature that could significantly impact user\nexperience and outcomes."
                },
                "authors": [
                    {
                        "name": "Arya VarastehNezhad"
                    },
                    {
                        "name": "Reza Tavasoli"
                    },
                    {
                        "name": "Soroush Elyasi"
                    },
                    {
                        "name": "MohammadHossein LotfiNia"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11281v1",
                "updated": "2025-08-15T07:40:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    40,
                    41,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:40:41Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    40,
                    41,
                    4,
                    227,
                    0
                ],
                "title": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT\n  Fine-Tuning for French Toxicity Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT\n  Fine-Tuning for French Toxicity Detection"
                },
                "summary": "Detecting toxic content using language models is crucial yet challenging.\nWhile substantial progress has been made in English, toxicity detection in\nFrench remains underdeveloped, primarily due to the lack of culturally\nrelevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new\npublic benchmark of 53,622 French online comments, constructed via a\nsemi-automated annotation pipeline that reduces manual labeling to only 10%\nthrough high-confidence LLM-based pre-annotation and human verification. Then,\nwe benchmark a broad range of models and uncover a counterintuitive insight:\nSmall Language Models (SLMs) outperform many larger models in robustness and\ngeneralization under the toxicity detection task. Motivated by this finding, we\npropose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic\nweighted loss that progressively emphasizes the model's final decision,\nsignificantly improving faithfulness. Our fine-tuned 4B model achieves\nstate-of-the-art performance, improving its F1 score by 13% over its baseline\nand outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a\ncross-lingual toxicity benchmark demonstrates strong multilingual ability,\nsuggesting that our methodology can be effectively extended to other languages\nand safety-critical classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting toxic content using language models is crucial yet challenging.\nWhile substantial progress has been made in English, toxicity detection in\nFrench remains underdeveloped, primarily due to the lack of culturally\nrelevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new\npublic benchmark of 53,622 French online comments, constructed via a\nsemi-automated annotation pipeline that reduces manual labeling to only 10%\nthrough high-confidence LLM-based pre-annotation and human verification. Then,\nwe benchmark a broad range of models and uncover a counterintuitive insight:\nSmall Language Models (SLMs) outperform many larger models in robustness and\ngeneralization under the toxicity detection task. Motivated by this finding, we\npropose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic\nweighted loss that progressively emphasizes the model's final decision,\nsignificantly improving faithfulness. Our fine-tuned 4B model achieves\nstate-of-the-art performance, improving its F1 score by 13% over its baseline\nand outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a\ncross-lingual toxicity benchmark demonstrates strong multilingual ability,\nsuggesting that our methodology can be effectively extended to other languages\nand safety-critical classification tasks."
                },
                "authors": [
                    {
                        "name": "Axel Delaval"
                    },
                    {
                        "name": "Shujian Yang"
                    },
                    {
                        "name": "Haicheng Wang"
                    },
                    {
                        "name": "Han Qiu"
                    },
                    {
                        "name": "Jialiang Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jialiang Lu"
                },
                "author": "Jialiang Lu",
                "arxiv_comment": "14 pages, 5 figures, 8 tables. This paper introduces TOXIFRENCH, a\n  new large-scale benchmark for French toxicity detection, and proposes a\n  Chain-of-Thought (CoT) fine-tuning method with a dynamic weighted loss. The\n  resulting fine-tuned 4B parameter model, ToxiFrench, achieves\n  state-of-the-art performance, outperforming larger models like GPT-4o",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01212v2",
                "updated": "2025-08-15T07:40:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    40,
                    14,
                    4,
                    227,
                    0
                ],
                "published": "2025-01-02T11:41:43Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    41,
                    43,
                    3,
                    2,
                    0
                ],
                "title": "Towards Consumer-Grade Cybersickness Prediction: Multi-Model Alignment\n  for Real-Time Vision-Only Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Consumer-Grade Cybersickness Prediction: Multi-Model Alignment\n  for Real-Time Vision-Only Inference"
                },
                "summary": "Cybersickness remains a major obstacle to the widespread adoption of\nimmersive virtual reality (VR), particularly in consumer-grade environments.\nWhile prior methods rely on invasive signals such as electroencephalography\n(EEG) for high predictive accuracy, these approaches require specialized\nhardware and are impractical for real-world applications. In this work, we\npropose a scalable, deployable framework for personalized cybersickness\nprediction leveraging only non-invasive signals readily available from\ncommercial VR headsets, including head motion, eye tracking, and physiological\nresponses. Our model employs a modality-specific graph neural network enhanced\nwith a Difference Attention Module to extract temporal-spatial embeddings\ncapturing dynamic changes across modalities. A cross-modal alignment module\njointly trains the video encoder to learn personalized traits by aligning video\nfeatures with sensor-derived representations. Consequently, the model\naccurately predicts individual cybersickness using only video input during\ninference. Experimental results show our model achieves 88.4\\% accuracy,\nclosely matching EEG-based approaches (89.16\\%), while reducing deployment\ncomplexity. With an average inference latency of 90ms, our framework supports\nreal-time applications, ideal for integration into consumer-grade VR platforms\nwithout compromising personalization or performance. The code will be relesed\nat https://github.com/U235-Aurora/PTGNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cybersickness remains a major obstacle to the widespread adoption of\nimmersive virtual reality (VR), particularly in consumer-grade environments.\nWhile prior methods rely on invasive signals such as electroencephalography\n(EEG) for high predictive accuracy, these approaches require specialized\nhardware and are impractical for real-world applications. In this work, we\npropose a scalable, deployable framework for personalized cybersickness\nprediction leveraging only non-invasive signals readily available from\ncommercial VR headsets, including head motion, eye tracking, and physiological\nresponses. Our model employs a modality-specific graph neural network enhanced\nwith a Difference Attention Module to extract temporal-spatial embeddings\ncapturing dynamic changes across modalities. A cross-modal alignment module\njointly trains the video encoder to learn personalized traits by aligning video\nfeatures with sensor-derived representations. Consequently, the model\naccurately predicts individual cybersickness using only video input during\ninference. Experimental results show our model achieves 88.4\\% accuracy,\nclosely matching EEG-based approaches (89.16\\%), while reducing deployment\ncomplexity. With an average inference latency of 90ms, our framework supports\nreal-time applications, ideal for integration into consumer-grade VR platforms\nwithout compromising personalization or performance. The code will be relesed\nat https://github.com/U235-Aurora/PTGNN."
                },
                "authors": [
                    {
                        "name": "Yitong Zhu"
                    },
                    {
                        "name": "Zhuowen Liang"
                    },
                    {
                        "name": "Yiming Wu"
                    },
                    {
                        "name": "Tangyao Li"
                    },
                    {
                        "name": "Yuyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuyang Wang"
                },
                "author": "Yuyang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11280v1",
                "updated": "2025-08-15T07:37:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    37,
                    12,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:37:12Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    37,
                    12,
                    4,
                    227,
                    0
                ],
                "title": "LETToT: Label-Free Evaluation of Large Language Models On Tourism Using\n  Expert Tree-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LETToT: Label-Free Evaluation of Large Language Models On Tourism Using\n  Expert Tree-of-Thought"
                },
                "summary": "Evaluating large language models (LLMs) in specific domain like tourism\nremains challenging due to the prohibitive cost of annotated benchmarks and\npersistent issues like hallucinations. We propose $\\textbf{L}$able-Free\n$\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert\n$\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that\nleverages expert-derived reasoning structures-instead of labeled data-to access\nLLMs in tourism. First, we iteratively refine and validate hierarchical ToT\ncomponents through alignment with generic quality dimensions and expert\nfeedback. Results demonstrate the effectiveness of our systematically optimized\nexpert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we\napply LETToT's optimized expert ToT to evaluate models of varying scales\n(32B-671B parameters), revealing: (1) Scaling laws persist in specialized\ndomains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,\nDeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit\nreasoning architectures outperform counterparts in accuracy and conciseness\n($p<0.05$). Our work established a scalable, label-free paradigm for\ndomain-specific LLM evaluation, offering a robust alternative to conventional\nannotated benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) in specific domain like tourism\nremains challenging due to the prohibitive cost of annotated benchmarks and\npersistent issues like hallucinations. We propose $\\textbf{L}$able-Free\n$\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert\n$\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that\nleverages expert-derived reasoning structures-instead of labeled data-to access\nLLMs in tourism. First, we iteratively refine and validate hierarchical ToT\ncomponents through alignment with generic quality dimensions and expert\nfeedback. Results demonstrate the effectiveness of our systematically optimized\nexpert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we\napply LETToT's optimized expert ToT to evaluate models of varying scales\n(32B-671B parameters), revealing: (1) Scaling laws persist in specialized\ndomains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,\nDeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit\nreasoning architectures outperform counterparts in accuracy and conciseness\n($p<0.05$). Our work established a scalable, label-free paradigm for\ndomain-specific LLM evaluation, offering a robust alternative to conventional\nannotated benchmarks."
                },
                "authors": [
                    {
                        "name": "Ruiyan Qi"
                    },
                    {
                        "name": "Congding Wen"
                    },
                    {
                        "name": "Weibo Zhou"
                    },
                    {
                        "name": "Shangsong Liang"
                    },
                    {
                        "name": "Lingbo Li"
                    }
                ],
                "author_detail": {
                    "name": "Lingbo Li"
                },
                "author": "Lingbo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11278v1",
                "updated": "2025-08-15T07:29:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    29,
                    46,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:29:46Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    29,
                    46,
                    4,
                    227,
                    0
                ],
                "title": "Is General-Purpose AI Reasoning Sensitive to Data-Induced Cognitive\n  Biases? Dynamic Benchmarking on Typical Software Engineering Dilemmas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is General-Purpose AI Reasoning Sensitive to Data-Induced Cognitive\n  Biases? Dynamic Benchmarking on Typical Software Engineering Dilemmas"
                },
                "summary": "Human cognitive biases in software engineering can lead to costly errors.\nWhile general-purpose AI (GPAI) systems may help mitigate these biases due to\ntheir non-human nature, their training on human-generated data raises a\ncritical question: Do GPAI systems themselves exhibit cognitive biases?\n  To investigate this, we present the first dynamic benchmarking framework to\nevaluate data-induced cognitive biases in GPAI within software engineering\nworkflows. Starting with a seed set of 16 hand-crafted realistic tasks, each\nfeaturing one of 8 cognitive biases (e.g., anchoring, framing) and\ncorresponding unbiased variants, we test whether bias-inducing linguistic cues\nunrelated to task logic can lead GPAI systems from correct to incorrect\nconclusions.\n  To scale the benchmark and ensure realism, we develop an on-demand\naugmentation pipeline relying on GPAI systems to generate task variants that\npreserve bias-inducing cues while varying surface details. This pipeline\nensures correctness (88--99% on average, according to human evaluation),\npromotes diversity, and controls reasoning complexity by leveraging\nProlog-based reasoning and LLM-as-a-judge validation. It also verifies that the\nembedded biases are both harmful and undetectable by logic-based, unbiased\nreasoners.\n  We evaluate leading GPAI systems (GPT, LLaMA, DeepSeek) and find a consistent\ntendency to rely on shallow linguistic heuristics over deep reasoning. All\nsystems exhibit cognitive biases (ranging from 5.9% to 35% across types), with\nbias sensitivity increasing sharply with task complexity (up to 49%),\nhighlighting critical risks in real-world software engineering deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognitive biases in software engineering can lead to costly errors.\nWhile general-purpose AI (GPAI) systems may help mitigate these biases due to\ntheir non-human nature, their training on human-generated data raises a\ncritical question: Do GPAI systems themselves exhibit cognitive biases?\n  To investigate this, we present the first dynamic benchmarking framework to\nevaluate data-induced cognitive biases in GPAI within software engineering\nworkflows. Starting with a seed set of 16 hand-crafted realistic tasks, each\nfeaturing one of 8 cognitive biases (e.g., anchoring, framing) and\ncorresponding unbiased variants, we test whether bias-inducing linguistic cues\nunrelated to task logic can lead GPAI systems from correct to incorrect\nconclusions.\n  To scale the benchmark and ensure realism, we develop an on-demand\naugmentation pipeline relying on GPAI systems to generate task variants that\npreserve bias-inducing cues while varying surface details. This pipeline\nensures correctness (88--99% on average, according to human evaluation),\npromotes diversity, and controls reasoning complexity by leveraging\nProlog-based reasoning and LLM-as-a-judge validation. It also verifies that the\nembedded biases are both harmful and undetectable by logic-based, unbiased\nreasoners.\n  We evaluate leading GPAI systems (GPT, LLaMA, DeepSeek) and find a consistent\ntendency to rely on shallow linguistic heuristics over deep reasoning. All\nsystems exhibit cognitive biases (ranging from 5.9% to 35% across types), with\nbias sensitivity increasing sharply with task complexity (up to 49%),\nhighlighting critical risks in real-world software engineering deployments."
                },
                "authors": [
                    {
                        "name": "Francesco Sovrano"
                    },
                    {
                        "name": "Gabriele Dominici"
                    },
                    {
                        "name": "Rita Sevastjanova"
                    },
                    {
                        "name": "Alessandra Stramiglio"
                    },
                    {
                        "name": "Alberto Bacchelli"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Bacchelli"
                },
                "author": "Alberto Bacchelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11277v1",
                "updated": "2025-08-15T07:29:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    29,
                    42,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:29:42Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    29,
                    42,
                    4,
                    227,
                    0
                ],
                "title": "Probing the Representational Power of Sparse Autoencoders in Vision\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Representational Power of Sparse Autoencoders in Vision\n  Models"
                },
                "summary": "Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting\nthe hidden states of large language models (LLMs). By learning to reconstruct\nactivations from a sparse bottleneck layer, SAEs discover interpretable\nfeatures from the high-dimensional internal representations of LLMs. Despite\ntheir popularity with language models, SAEs remain understudied in the visual\ndomain. In this work, we provide an extensive evaluation the representational\npower of SAEs for vision models using a broad range of image-based tasks. Our\nexperimental results demonstrate that SAE features are semantically meaningful,\nimprove out-of-distribution generalization, and enable controllable generation\nacross three vision model architectures: vision embedding models, multi-modal\nLMMs and diffusion models. In vision embedding models, we find that learned SAE\nfeatures can be used for OOD detection and provide evidence that they recover\nthe ontological structure of the underlying model. For diffusion models, we\ndemonstrate that SAEs enable semantic steering through text encoder\nmanipulation and develop an automated pipeline for discovering\nhuman-interpretable attributes. Finally, we conduct exploratory experiments on\nmulti-modal LLMs, finding evidence that SAE features reveal shared\nrepresentations across vision and language modalities. Our study provides a\nfoundation for SAE evaluation in vision models, highlighting their strong\npotential improving interpretability, generalization, and steerability in the\nvisual domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting\nthe hidden states of large language models (LLMs). By learning to reconstruct\nactivations from a sparse bottleneck layer, SAEs discover interpretable\nfeatures from the high-dimensional internal representations of LLMs. Despite\ntheir popularity with language models, SAEs remain understudied in the visual\ndomain. In this work, we provide an extensive evaluation the representational\npower of SAEs for vision models using a broad range of image-based tasks. Our\nexperimental results demonstrate that SAE features are semantically meaningful,\nimprove out-of-distribution generalization, and enable controllable generation\nacross three vision model architectures: vision embedding models, multi-modal\nLMMs and diffusion models. In vision embedding models, we find that learned SAE\nfeatures can be used for OOD detection and provide evidence that they recover\nthe ontological structure of the underlying model. For diffusion models, we\ndemonstrate that SAEs enable semantic steering through text encoder\nmanipulation and develop an automated pipeline for discovering\nhuman-interpretable attributes. Finally, we conduct exploratory experiments on\nmulti-modal LLMs, finding evidence that SAE features reveal shared\nrepresentations across vision and language modalities. Our study provides a\nfoundation for SAE evaluation in vision models, highlighting their strong\npotential improving interpretability, generalization, and steerability in the\nvisual domain."
                },
                "authors": [
                    {
                        "name": "Matthew Lyle Olson"
                    },
                    {
                        "name": "Musashi Hinck"
                    },
                    {
                        "name": "Neale Ratzlaff"
                    },
                    {
                        "name": "Changbai Li"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Vasudev Lal"
                    },
                    {
                        "name": "Shao-Yen Tseng"
                    }
                ],
                "author_detail": {
                    "name": "Shao-Yen Tseng"
                },
                "author": "Shao-Yen Tseng",
                "arxiv_comment": "ICCV 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11269v1",
                "updated": "2025-08-15T07:08:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    8,
                    46,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:08:46Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    8,
                    46,
                    4,
                    227,
                    0
                ],
                "title": "Inference performance evaluation for LLMs on edge devices with a novel\n  benchmarking framework and metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference performance evaluation for LLMs on edge devices with a novel\n  benchmarking framework and metric"
                },
                "summary": "With the significant success achieved by large language models (LLMs) like\nLLaMA, edge computing-based LLM inference services for mobile and PC are in\nhigh demand for data privacy. However, different edge platforms have different\nhardware characteristics and the large demand for memory capacity and bandwidth\nmakes it very challenging to deploy and benchmark LLMs on edge devices. In this\npaper, we introduce a benchmarking tool named ELIB (edge LLM inference\nbenchmarking) to evaluate LLM inference performance of different edge\nplatforms, and propose a novel metric named MBU to indicate the percentage of\nthe theoretically efficient use of available memory bandwidth for a specific\nmodel running on edge hardware to optimize memory usage. We deploy ELIB on\nthree edge platforms and benchmark using five quantized models to optimize MBU\nin combination with other metrics such as FLOPS, throughput, latency and\naccuracy. And we analyze the results to derive the key factors, constraints,\nunpredictability in optimizing MBU that can guide deploying LLMs on more edge\nplatforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the significant success achieved by large language models (LLMs) like\nLLaMA, edge computing-based LLM inference services for mobile and PC are in\nhigh demand for data privacy. However, different edge platforms have different\nhardware characteristics and the large demand for memory capacity and bandwidth\nmakes it very challenging to deploy and benchmark LLMs on edge devices. In this\npaper, we introduce a benchmarking tool named ELIB (edge LLM inference\nbenchmarking) to evaluate LLM inference performance of different edge\nplatforms, and propose a novel metric named MBU to indicate the percentage of\nthe theoretically efficient use of available memory bandwidth for a specific\nmodel running on edge hardware to optimize memory usage. We deploy ELIB on\nthree edge platforms and benchmark using five quantized models to optimize MBU\nin combination with other metrics such as FLOPS, throughput, latency and\naccuracy. And we analyze the results to derive the key factors, constraints,\nunpredictability in optimizing MBU that can guide deploying LLMs on more edge\nplatforms."
                },
                "authors": [
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Cong Tian"
                    },
                    {
                        "name": "Zixuan He"
                    },
                    {
                        "name": "Bin Yu"
                    },
                    {
                        "name": "Yepang Liu"
                    },
                    {
                        "name": "Jialun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jialun Cao"
                },
                "author": "Jialun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11260v1",
                "updated": "2025-08-15T06:53:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:53:28Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?"
                },
                "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages."
                },
                "authors": [
                    {
                        "name": "Mukund Choudhary"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Gaurja Aeron"
                    },
                    {
                        "name": "Antara Raaghavi Bhattacharya"
                    },
                    {
                        "name": "Dang Khoa Dang Dinh"
                    },
                    {
                        "name": "Ikhlasul Akmal Hanif"
                    },
                    {
                        "name": "Daria Kotova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11258v1",
                "updated": "2025-08-15T06:50:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    50,
                    29,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:50:29Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    50,
                    29,
                    4,
                    227,
                    0
                ],
                "title": "Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed\n  LLMs via Post-Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed\n  LLMs via Post-Processing"
                },
                "summary": "Instruction fine-tuned large language models (LLMs) enable a simple zero-shot\nor few-shot prompting paradigm, also known as in-context learning, for building\nprediction models. This convenience, combined with continued advances in LLM\ncapability, has the potential to drive their adoption across a broad range of\ndomains, including high-stakes applications where group fairness -- preventing\ndisparate impacts across demographic groups -- is essential. The majority of\nexisting approaches to enforcing group fairness on LLM-based classifiers rely\non traditional fair algorithms applied via model fine-tuning or head-tuning on\nfinal-layer embeddings, but they are no longer applicable to closed-weight LLMs\nunder the in-context learning setting, which include some of the most capable\ncommercial models today, such as GPT-4, Gemini, and Claude. In this paper, we\npropose a framework for deriving fair classifiers from closed-weight LLMs via\nprompting: the LLM is treated as a feature extractor, and features are elicited\nfrom its probabilistic predictions (e.g., token log probabilities) using\nprompts strategically designed for the specified fairness criterion to obtain\nsufficient statistics for fair classification; a fair algorithm is then applied\nto these features to train a lightweight fair classifier in a post-hoc manner.\nExperiments on five datasets, including three tabular ones, demonstrate strong\naccuracy-fairness tradeoffs for the classifiers derived by our framework from\nboth open-weight and closed-weight LLMs; in particular, our framework is\ndata-efficient and outperforms fair classifiers trained on LLM embeddings\n(i.e., head-tuning) or from scratch on raw tabular features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction fine-tuned large language models (LLMs) enable a simple zero-shot\nor few-shot prompting paradigm, also known as in-context learning, for building\nprediction models. This convenience, combined with continued advances in LLM\ncapability, has the potential to drive their adoption across a broad range of\ndomains, including high-stakes applications where group fairness -- preventing\ndisparate impacts across demographic groups -- is essential. The majority of\nexisting approaches to enforcing group fairness on LLM-based classifiers rely\non traditional fair algorithms applied via model fine-tuning or head-tuning on\nfinal-layer embeddings, but they are no longer applicable to closed-weight LLMs\nunder the in-context learning setting, which include some of the most capable\ncommercial models today, such as GPT-4, Gemini, and Claude. In this paper, we\npropose a framework for deriving fair classifiers from closed-weight LLMs via\nprompting: the LLM is treated as a feature extractor, and features are elicited\nfrom its probabilistic predictions (e.g., token log probabilities) using\nprompts strategically designed for the specified fairness criterion to obtain\nsufficient statistics for fair classification; a fair algorithm is then applied\nto these features to train a lightweight fair classifier in a post-hoc manner.\nExperiments on five datasets, including three tabular ones, demonstrate strong\naccuracy-fairness tradeoffs for the classifiers derived by our framework from\nboth open-weight and closed-weight LLMs; in particular, our framework is\ndata-efficient and outperforms fair classifiers trained on LLM embeddings\n(i.e., head-tuning) or from scratch on raw tabular features."
                },
                "authors": [
                    {
                        "name": "Ruicheng Xian"
                    },
                    {
                        "name": "Yuxuan Wan"
                    },
                    {
                        "name": "Han Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Han Zhao"
                },
                "author": "Han Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11257v1",
                "updated": "2025-08-15T06:46:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    46,
                    50,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:46:50Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    46,
                    50,
                    4,
                    227,
                    0
                ],
                "title": "Hallucination in LLM-Based Code Generation: An Automotive Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination in LLM-Based Code Generation: An Automotive Case Study"
                },
                "summary": "Large Language Models (LLMs) have shown significant potential in automating\ncode generation tasks offering new opportunities across software engineering\ndomains. However, their practical application remains limited due to\nhallucinations - outputs that appear plausible but are factually incorrect,\nunverifiable or nonsensical. This paper investigates hallucination phenomena in\nthe context of code generation with a specific focus on the automotive domain.\nA case study is presented that evaluates multiple code LLMs for three different\nprompting complexities ranging from a minimal one-liner prompt to a prompt with\nCovesa Vehicle Signal Specifications (VSS) as additional context and finally to\na prompt with an additional code skeleton. The evaluation reveals a high\nfrequency of syntax violations, invalid reference errors and API knowledge\nconflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the\nevaluated models, only GPT-4.1 and GPT-4o were able to produce a correct\nsolution when given the most context-rich prompt. Simpler prompting strategies\nfailed to yield a working result, even after multiple refinement iterations.\nThese findings highlight the need for effective mitigation techniques to ensure\nthe safe and reliable use of LLM generated code, especially in safety-critical\ndomains such as automotive software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant potential in automating\ncode generation tasks offering new opportunities across software engineering\ndomains. However, their practical application remains limited due to\nhallucinations - outputs that appear plausible but are factually incorrect,\nunverifiable or nonsensical. This paper investigates hallucination phenomena in\nthe context of code generation with a specific focus on the automotive domain.\nA case study is presented that evaluates multiple code LLMs for three different\nprompting complexities ranging from a minimal one-liner prompt to a prompt with\nCovesa Vehicle Signal Specifications (VSS) as additional context and finally to\na prompt with an additional code skeleton. The evaluation reveals a high\nfrequency of syntax violations, invalid reference errors and API knowledge\nconflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the\nevaluated models, only GPT-4.1 and GPT-4o were able to produce a correct\nsolution when given the most context-rich prompt. Simpler prompting strategies\nfailed to yield a working result, even after multiple refinement iterations.\nThese findings highlight the need for effective mitigation techniques to ensure\nthe safe and reliable use of LLM generated code, especially in safety-critical\ndomains such as automotive software systems."
                },
                "authors": [
                    {
                        "name": "Marc Pavel"
                    },
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Lukasz Mazur"
                    },
                    {
                        "name": "Vahid Zolfaghari"
                    },
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08684v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08684v3",
                "updated": "2025-08-15T06:37:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    37,
                    14,
                    4,
                    227,
                    0
                ],
                "published": "2023-12-14T06:46:35Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    6,
                    46,
                    35,
                    3,
                    348,
                    0
                ],
                "title": "A Computationally Efficient Maximum A Posteriori Sequence Estimation via\n  Stein Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Computationally Efficient Maximum A Posteriori Sequence Estimation via\n  Stein Variational Inference"
                },
                "summary": "State estimation in robotic systems presents significant challenges,\nparticularly due to the prevalence of multimodal posterior distributions in\nreal-world scenarios. One effective strategy for handling such complexity is to\ncompute maximum a posteriori (MAP) sequences over a discretized or sampled\nstate space, which enables a concise representation of the most likely state\ntrajectory. However, this approach often incurs substantial computational\ncosts, especially in high-dimensional settings. In this article, we propose a\nnovel MAP sequence estimation method, \\textsf{Stein-MAP-Seq}, which effectively\naddresses multimodality while substantially reducing computational and memory\noverhead. Our key contribution is a sequential variational inference framework\nthat captures temporal dependencies in dynamical system models and integrates\nStein variational gradient descent (SVGD) into a Viterbi-style dynamic\nprogramming algorithm, enabling computationally efficient MAP sequence\nestimation. \\textsf{Stein-MAP-Seq} achieves a computational complexity of\n$\\mathcal{O}(M^2)$, where $M$ is the number of particles, in contrast to the\n$\\mathcal{O}(N^2)$ complexity of conventional MAP sequence estimators, with $N\n\\gg M$. Furthermore, the method inherits SVGD's parallelism, enabling efficient\ncomputation for real-time deployment on GPU-equipped autonomous systems. We\nvalidate the proposed method in various multimodal scenarios, including those\narising from nonlinear dynamics with ambiguous observations, unknown data\nassociations, and temporary unobservability, demonstrating substantial\nimprovements in estimation accuracy and robustness to multimodality over\nexisting approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State estimation in robotic systems presents significant challenges,\nparticularly due to the prevalence of multimodal posterior distributions in\nreal-world scenarios. One effective strategy for handling such complexity is to\ncompute maximum a posteriori (MAP) sequences over a discretized or sampled\nstate space, which enables a concise representation of the most likely state\ntrajectory. However, this approach often incurs substantial computational\ncosts, especially in high-dimensional settings. In this article, we propose a\nnovel MAP sequence estimation method, \\textsf{Stein-MAP-Seq}, which effectively\naddresses multimodality while substantially reducing computational and memory\noverhead. Our key contribution is a sequential variational inference framework\nthat captures temporal dependencies in dynamical system models and integrates\nStein variational gradient descent (SVGD) into a Viterbi-style dynamic\nprogramming algorithm, enabling computationally efficient MAP sequence\nestimation. \\textsf{Stein-MAP-Seq} achieves a computational complexity of\n$\\mathcal{O}(M^2)$, where $M$ is the number of particles, in contrast to the\n$\\mathcal{O}(N^2)$ complexity of conventional MAP sequence estimators, with $N\n\\gg M$. Furthermore, the method inherits SVGD's parallelism, enabling efficient\ncomputation for real-time deployment on GPU-equipped autonomous systems. We\nvalidate the proposed method in various multimodal scenarios, including those\narising from nonlinear dynamics with ambiguous observations, unknown data\nassociations, and temporary unobservability, demonstrating substantial\nimprovements in estimation accuracy and robustness to multimodality over\nexisting approaches."
                },
                "authors": [
                    {
                        "name": "Min-Won Seo"
                    },
                    {
                        "name": "Solmaz S. Kia"
                    }
                ],
                "author_detail": {
                    "name": "Solmaz S. Kia"
                },
                "author": "Solmaz S. Kia",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08684v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08684v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11247v1",
                "updated": "2025-08-15T06:36:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    36,
                    13,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:36:13Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    36,
                    13,
                    4,
                    227,
                    0
                ],
                "title": "Cross-Granularity Hypergraph Retrieval-Augmented Generation for\n  Multi-hop Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Granularity Hypergraph Retrieval-Augmented Generation for\n  Multi-hop Question Answering"
                },
                "summary": "Multi-hop question answering (MHQA) requires integrating knowledge scattered\nacross multiple passages to derive the correct answer. Traditional\nretrieval-augmented generation (RAG) methods primarily focus on coarse-grained\ntextual semantic similarity and ignore structural associations among dispersed\nknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods\naddress this by leveraging knowledge graphs (KGs) to capture structural\nassociations, but they tend to overly rely on structural information and\nfine-grained word- or phrase-level retrieval, resulting in an underutilization\nof textual semantics. In this paper, we propose a novel RAG approach called\nHGRAG for MHQA that achieves cross-granularity integration of structural and\nsemantic information via hypergraphs. Structurally, we construct an entity\nhypergraph where fine-grained entities serve as nodes and coarse-grained\npassages as hyperedges, and establish knowledge association through shared\nentities. Semantically, we design a hypergraph retrieval method that integrates\nfine-grained entity similarity and coarse-grained passage similarity via\nhypergraph diffusion. Finally, we employ a retrieval enhancement module, which\nfurther refines the retrieved results both semantically and structurally, to\nobtain the most relevant passages as context for answer generation with the\nLLM. Experimental results on benchmark datasets demonstrate that our approach\noutperforms state-of-the-art methods in QA performance, and achieves a\n6$\\times$ speedup in retrieval efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop question answering (MHQA) requires integrating knowledge scattered\nacross multiple passages to derive the correct answer. Traditional\nretrieval-augmented generation (RAG) methods primarily focus on coarse-grained\ntextual semantic similarity and ignore structural associations among dispersed\nknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods\naddress this by leveraging knowledge graphs (KGs) to capture structural\nassociations, but they tend to overly rely on structural information and\nfine-grained word- or phrase-level retrieval, resulting in an underutilization\nof textual semantics. In this paper, we propose a novel RAG approach called\nHGRAG for MHQA that achieves cross-granularity integration of structural and\nsemantic information via hypergraphs. Structurally, we construct an entity\nhypergraph where fine-grained entities serve as nodes and coarse-grained\npassages as hyperedges, and establish knowledge association through shared\nentities. Semantically, we design a hypergraph retrieval method that integrates\nfine-grained entity similarity and coarse-grained passage similarity via\nhypergraph diffusion. Finally, we employ a retrieval enhancement module, which\nfurther refines the retrieved results both semantically and structurally, to\nobtain the most relevant passages as context for answer generation with the\nLLM. Experimental results on benchmark datasets demonstrate that our approach\noutperforms state-of-the-art methods in QA performance, and achieves a\n6$\\times$ speedup in retrieval efficiency."
                },
                "authors": [
                    {
                        "name": "Changjian Wang"
                    },
                    {
                        "name": "Weihong Deng"
                    },
                    {
                        "name": "Weili Guan"
                    },
                    {
                        "name": "Quan Lu"
                    },
                    {
                        "name": "Ning Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Ning Jiang"
                },
                "author": "Ning Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11239v1",
                "updated": "2025-08-15T05:57:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    5,
                    57,
                    38,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T05:57:38Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    5,
                    57,
                    38,
                    4,
                    227,
                    0
                ],
                "title": "Mitigating Filter Bubble from the Perspective of Community Detection: A\n  Universal Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Filter Bubble from the Perspective of Community Detection: A\n  Universal Framework"
                },
                "summary": "In recent years, recommender systems have primarily focused on improving\naccuracy at the expense of diversity, which exacerbates the well-known filter\nbubble effect. This paper proposes a universal framework called CD-CGCN to\naddress the filter bubble issue in recommender systems from a community\ndetection perspective. By analyzing user-item interaction histories with a\ncommunity detection algorithm, we reveal that state-of-the-art recommendations\noften focus on intra-community items, worsening the filter bubble effect.\nCD-CGCN, a model-agnostic framework, integrates a Conditional Discriminator and\na Community-reweighted Graph Convolutional Network which can be plugged into\nmost recommender models. Using adversarial learning based on community labels,\nit counteracts the extracted community attributes and incorporates an inference\nstrategy tailored to the user's specific filter bubble state. Extensive\nexperiments on real-world datasets with multiple base models validate its\neffectiveness in mitigating filter bubbles while preserving recommendation\nquality. Additionally, by applying community debiasing to the original test set\nto construct an unbiased test set, we observe that CD-CGCN demonstrates\nsuperior performance in capturing users' inter-community preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, recommender systems have primarily focused on improving\naccuracy at the expense of diversity, which exacerbates the well-known filter\nbubble effect. This paper proposes a universal framework called CD-CGCN to\naddress the filter bubble issue in recommender systems from a community\ndetection perspective. By analyzing user-item interaction histories with a\ncommunity detection algorithm, we reveal that state-of-the-art recommendations\noften focus on intra-community items, worsening the filter bubble effect.\nCD-CGCN, a model-agnostic framework, integrates a Conditional Discriminator and\na Community-reweighted Graph Convolutional Network which can be plugged into\nmost recommender models. Using adversarial learning based on community labels,\nit counteracts the extracted community attributes and incorporates an inference\nstrategy tailored to the user's specific filter bubble state. Extensive\nexperiments on real-world datasets with multiple base models validate its\neffectiveness in mitigating filter bubbles while preserving recommendation\nquality. Additionally, by applying community debiasing to the original test set\nto construct an unbiased test set, we observe that CD-CGCN demonstrates\nsuperior performance in capturing users' inter-community preferences."
                },
                "authors": [
                    {
                        "name": "Ming Tang"
                    },
                    {
                        "name": "Xiaowen Huang"
                    },
                    {
                        "name": "Jitao Sang"
                    }
                ],
                "author_detail": {
                    "name": "Jitao Sang"
                },
                "author": "Jitao Sang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11234v1",
                "updated": "2025-08-15T05:49:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    5,
                    49,
                    40,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T05:49:40Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    5,
                    49,
                    40,
                    4,
                    227,
                    0
                ],
                "title": "Enabling low-power massive MIMO with ternary ADCs for AIoT sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling low-power massive MIMO with ternary ADCs for AIoT sensing"
                },
                "summary": "The proliferation of networked devices and the surging demand for ubiquitous\nintelligence have given rise to the artificial intelligence of things (AIoT).\nHowever, the utilization of high-resolution analog-to-digital converters (ADCs)\nand numerous radio frequency chains significantly raises power consumption.\nThis paper explores a cost-effective solution using ternary ADCs (T-ADCs) in\nmassive multiple-input-multiple-output (MIMO) systems for low-power AIoT and\nspecifically addresses channel sensing challenges. The channel is first\nestimated through a pilot-aided scheme and refined using a joint-pilot-and-data\n(JPD) approach. To assess the performance limits of this two-threshold ADC\nsystem, the analysis includes its hardware-ideal counterpart, the parallel\none-bit ADCs (PO-ADCs) and a realistic scenario where noise variance is unknown\nat the receiver is considered. Analytical findings indicate that the JPD scheme\neffectively mitigates performance degradation in channel estimation due to\ncoarse quantization effects under mild conditions, without necessitating\nadditional pilot overhead. For deterministic and random channels, we propose\nmodified expectation maximization (EM) and variational inference EM estimators,\nrespectively. Extensive simulations validate the theoretical results and\ndemonstrate the effectiveness of the proposed estimators in terms of mean\nsquare error and symbol error rate, which showcases the feasibility of\nimplementing T-ADCs and the associated JPD scheme for greener AIoT smart\nsensing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of networked devices and the surging demand for ubiquitous\nintelligence have given rise to the artificial intelligence of things (AIoT).\nHowever, the utilization of high-resolution analog-to-digital converters (ADCs)\nand numerous radio frequency chains significantly raises power consumption.\nThis paper explores a cost-effective solution using ternary ADCs (T-ADCs) in\nmassive multiple-input-multiple-output (MIMO) systems for low-power AIoT and\nspecifically addresses channel sensing challenges. The channel is first\nestimated through a pilot-aided scheme and refined using a joint-pilot-and-data\n(JPD) approach. To assess the performance limits of this two-threshold ADC\nsystem, the analysis includes its hardware-ideal counterpart, the parallel\none-bit ADCs (PO-ADCs) and a realistic scenario where noise variance is unknown\nat the receiver is considered. Analytical findings indicate that the JPD scheme\neffectively mitigates performance degradation in channel estimation due to\ncoarse quantization effects under mild conditions, without necessitating\nadditional pilot overhead. For deterministic and random channels, we propose\nmodified expectation maximization (EM) and variational inference EM estimators,\nrespectively. Extensive simulations validate the theoretical results and\ndemonstrate the effectiveness of the proposed estimators in terms of mean\nsquare error and symbol error rate, which showcases the feasibility of\nimplementing T-ADCs and the associated JPD scheme for greener AIoT smart\nsensing."
                },
                "authors": [
                    {
                        "name": "Shengheng Liu"
                    },
                    {
                        "name": "Ningning Fu"
                    }
                ],
                "author_detail": {
                    "name": "Ningning Fu"
                },
                "author": "Ningning Fu",
                "arxiv_doi": "10.1145/3722220",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3722220",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.11234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Already published in ACM TOSN. 27 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11232v1",
                "updated": "2025-08-15T05:43:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    5,
                    43,
                    41,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T05:43:41Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    5,
                    43,
                    41,
                    4,
                    227,
                    0
                ],
                "title": "Embodied Edge Intelligence Meets Near Field Communication: Concept,\n  Design, and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Edge Intelligence Meets Near Field Communication: Concept,\n  Design, and Verification"
                },
                "summary": "Realizing embodied artificial intelligence is challenging due to the huge\ncomputation demands of large models (LMs). To support LMs while ensuring\nreal-time inference, embodied edge intelligence (EEI) is a promising paradigm,\nwhich leverages an LM edge to provide computing powers in close proximity to\nembodied robots. Due to embodied data exchange, EEI requires higher spectral\nefficiency, enhanced communication security, and reduced inter-user\ninterference. To meet these requirements, near-field communication (NFC), which\nleverages extremely large antenna arrays as its hardware foundation, is an\nideal solution. Therefore, this paper advocates the integration of EEI and NFC,\nresulting in a near-field EEI (NEEI) paradigm. However, NEEI also introduces\nnew challenges that cannot be adequately addressed by isolated EEI or NFC\ndesigns, creating research opportunities for joint optimization of both\nfunctionalities. To this end, we propose radio-friendly embodied planning for\nEEI-assisted NFC scenarios and view-guided beam-focusing for NFC-assisted EEI\nscenarios. We also elaborate how to realize resource-efficient NEEI through\nopportunistic collaborative navigation. Experimental results are provided to\nconfirm the superiority of the proposed techniques compared with various\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realizing embodied artificial intelligence is challenging due to the huge\ncomputation demands of large models (LMs). To support LMs while ensuring\nreal-time inference, embodied edge intelligence (EEI) is a promising paradigm,\nwhich leverages an LM edge to provide computing powers in close proximity to\nembodied robots. Due to embodied data exchange, EEI requires higher spectral\nefficiency, enhanced communication security, and reduced inter-user\ninterference. To meet these requirements, near-field communication (NFC), which\nleverages extremely large antenna arrays as its hardware foundation, is an\nideal solution. Therefore, this paper advocates the integration of EEI and NFC,\nresulting in a near-field EEI (NEEI) paradigm. However, NEEI also introduces\nnew challenges that cannot be adequately addressed by isolated EEI or NFC\ndesigns, creating research opportunities for joint optimization of both\nfunctionalities. To this end, we propose radio-friendly embodied planning for\nEEI-assisted NFC scenarios and view-guided beam-focusing for NFC-assisted EEI\nscenarios. We also elaborate how to realize resource-efficient NEEI through\nopportunistic collaborative navigation. Experimental results are provided to\nconfirm the superiority of the proposed techniques compared with various\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Xibin Jin"
                    },
                    {
                        "name": "Yujie Wan"
                    },
                    {
                        "name": "Chenxuan Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "9 pages, 6 figures, to appear in IEEE Network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10450v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10450v3",
                "updated": "2025-08-15T05:34:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    5,
                    34,
                    6,
                    4,
                    227,
                    0
                ],
                "published": "2024-06-15T00:07:44Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    0,
                    7,
                    44,
                    5,
                    167,
                    0
                ],
                "title": "TokenRec: Learning to Tokenize ID for LLM-based Generative\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenRec: Learning to Tokenize ID for LLM-based Generative\n  Recommendation"
                },
                "summary": "There is a growing interest in utilizing large-scale language models (LLMs)\nto advance next-generation Recommender Systems (RecSys), driven by their\noutstanding language understanding and in-context learning capabilities. In\nthis scenario, tokenizing (i.e., indexing) users and items becomes essential\nfor ensuring a seamless alignment of LLMs with recommendations. While several\nstudies have made progress in representing users and items through textual\ncontents or latent representations, challenges remain in efficiently capturing\nhigh-order collaborative knowledge into discrete tokens that are compatible\nwith LLMs. Additionally, the majority of existing tokenization approaches often\nface difficulties in generalizing effectively to new/unseen users or items that\nwere not in the training corpus. To address these challenges, we propose a\nnovel framework called TokenRec, which introduces not only an effective ID\ntokenization strategy but also an efficient retrieval paradigm for LLM-based\nrecommendations. Specifically, our tokenization strategy, Masked\nVector-Quantized (MQ) Tokenizer, involves quantizing the masked user/item\nrepresentations learned from collaborative filtering into discrete tokens, thus\nachieving a smooth incorporation of high-order collaborative knowledge and a\ngeneralizable tokenization of users and items for LLM-based RecSys. Meanwhile,\nour generative retrieval paradigm is designed to efficiently recommend top-$K$\nitems for users to eliminate the need for the time-consuming auto-regressive\ndecoding and beam search processes used by LLMs, thus significantly reducing\ninference time. Comprehensive experiments validate the effectiveness of the\nproposed methods, demonstrating that TokenRec outperforms competitive\nbenchmarks, including both traditional recommender systems and emerging\nLLM-based recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing interest in utilizing large-scale language models (LLMs)\nto advance next-generation Recommender Systems (RecSys), driven by their\noutstanding language understanding and in-context learning capabilities. In\nthis scenario, tokenizing (i.e., indexing) users and items becomes essential\nfor ensuring a seamless alignment of LLMs with recommendations. While several\nstudies have made progress in representing users and items through textual\ncontents or latent representations, challenges remain in efficiently capturing\nhigh-order collaborative knowledge into discrete tokens that are compatible\nwith LLMs. Additionally, the majority of existing tokenization approaches often\nface difficulties in generalizing effectively to new/unseen users or items that\nwere not in the training corpus. To address these challenges, we propose a\nnovel framework called TokenRec, which introduces not only an effective ID\ntokenization strategy but also an efficient retrieval paradigm for LLM-based\nrecommendations. Specifically, our tokenization strategy, Masked\nVector-Quantized (MQ) Tokenizer, involves quantizing the masked user/item\nrepresentations learned from collaborative filtering into discrete tokens, thus\nachieving a smooth incorporation of high-order collaborative knowledge and a\ngeneralizable tokenization of users and items for LLM-based RecSys. Meanwhile,\nour generative retrieval paradigm is designed to efficiently recommend top-$K$\nitems for users to eliminate the need for the time-consuming auto-regressive\ndecoding and beam search processes used by LLMs, thus significantly reducing\ninference time. Comprehensive experiments validate the effectiveness of the\nproposed methods, demonstrating that TokenRec outperforms competitive\nbenchmarks, including both traditional recommender systems and emerging\nLLM-based recommender systems."
                },
                "authors": [
                    {
                        "name": "Haohao Qu"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Zihuai Zhao"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "Accepted by IEEE TKDE. Codes and data are available at\n  https://github.com/Quhaoh233/TokenRec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10450v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10450v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06834v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06834v3",
                "updated": "2025-08-15T05:27:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    5,
                    27,
                    45,
                    4,
                    227,
                    0
                ],
                "published": "2024-04-10T08:52:12Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    8,
                    52,
                    12,
                    2,
                    101,
                    0
                ],
                "title": "An Efficient Deep Learning Approach for Approximating\n  Parameter-to-Solution Maps of PDEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Deep Learning Approach for Approximating\n  Parameter-to-Solution Maps of PDEs"
                },
                "summary": "In this paper, we consider approximating the parameter-to-solution maps of\nparametric partial differential equations (PPDEs) using deep neural networks\n(DNNs). We propose an efficient approach combining reduced collocation methods\n(RCMs) and DNNs. In the approximation analysis section, we rigorously derive\nsharp upper bounds on the complexity of the neural networks. These bounds only\ndepend on the reduced basis dimension rather than the high-fidelity\ndiscretization dimension, thereby theoretically guaranteeing the computational\nefficiency of our approach. In numerical experiments, we implement the RCM\nusing radial basis function finite differences (RBF-FD) and proper orthogonal\ndecomposition (POD), and propose the POD-DNN algorithm. We consider various\ntypes of PPDEs and compare the accuracy and efficiency of different solvers.\nThe POD-DNN has demonstrated significantly accelerated inference speeds\ncompared with conventional numerical methods owing to the offline-online\ncomputation strategy. Furthermore, by employing the reduced basis methods\n(RBMs), it also outperforms standard DNNs in computational efficiency while\nmaintaining comparable accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider approximating the parameter-to-solution maps of\nparametric partial differential equations (PPDEs) using deep neural networks\n(DNNs). We propose an efficient approach combining reduced collocation methods\n(RCMs) and DNNs. In the approximation analysis section, we rigorously derive\nsharp upper bounds on the complexity of the neural networks. These bounds only\ndepend on the reduced basis dimension rather than the high-fidelity\ndiscretization dimension, thereby theoretically guaranteeing the computational\nefficiency of our approach. In numerical experiments, we implement the RCM\nusing radial basis function finite differences (RBF-FD) and proper orthogonal\ndecomposition (POD), and propose the POD-DNN algorithm. We consider various\ntypes of PPDEs and compare the accuracy and efficiency of different solvers.\nThe POD-DNN has demonstrated significantly accelerated inference speeds\ncompared with conventional numerical methods owing to the offline-online\ncomputation strategy. Furthermore, by employing the reduced basis methods\n(RBMs), it also outperforms standard DNNs in computational efficiency while\nmaintaining comparable accuracy."
                },
                "authors": [
                    {
                        "name": "Guanhang Lei"
                    },
                    {
                        "name": "Zhen Lei"
                    },
                    {
                        "name": "Lei Shi"
                    },
                    {
                        "name": "Chenyu Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Chenyu Zeng"
                },
                "author": "Chenyu Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06834v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06834v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.11628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11628v1",
                "updated": "2025-08-15T17:56:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    56,
                    24,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T17:56:24Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    56,
                    24,
                    4,
                    227,
                    0
                ],
                "title": "Is ChatGPT-5 Ready for Mammogram VQA?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is ChatGPT-5 Ready for Mammogram VQA?"
                },
                "summary": "Mammogram visual question answering (VQA) integrates image interpretation\nwith clinical reasoning and has potential to support breast cancer screening.\nWe systematically evaluated the GPT-5 family and GPT-4o model on four public\nmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,\nabnormality detection, and malignancy classification tasks. GPT-5 consistently\nwas the best performing model but lagged behind both human experts and\ndomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores\namong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),\ncalcification (63.5%), and malignancy (52.8%) classification. On InBreast, it\nattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%\nmalignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection\nand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS\naccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared\nwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and\nspecificity (52.3%). While GPT-5 exhibits promising capabilities for screening\ntasks, its performance remains insufficient for high-stakes clinical imaging\napplications without targeted domain adaptation and optimization. However, the\ntremendous improvements in performance from GPT-4o to GPT-5 show a promising\ntrend in the potential for general large language models (LLMs) to assist with\nmammography VQA tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mammogram visual question answering (VQA) integrates image interpretation\nwith clinical reasoning and has potential to support breast cancer screening.\nWe systematically evaluated the GPT-5 family and GPT-4o model on four public\nmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,\nabnormality detection, and malignancy classification tasks. GPT-5 consistently\nwas the best performing model but lagged behind both human experts and\ndomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores\namong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),\ncalcification (63.5%), and malignancy (52.8%) classification. On InBreast, it\nattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%\nmalignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection\nand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS\naccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared\nwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and\nspecificity (52.3%). While GPT-5 exhibits promising capabilities for screening\ntasks, its performance remains insufficient for high-stakes clinical imaging\napplications without targeted domain adaptation and optimization. However, the\ntremendous improvements in performance from GPT-4o to GPT-5 show a promising\ntrend in the potential for general large language models (LLMs) to assist with\nmammography VQA tasks."
                },
                "authors": [
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Shansong Wang"
                    },
                    {
                        "name": "Mingzhe Hu"
                    },
                    {
                        "name": "Mojtaba Safari"
                    },
                    {
                        "name": "Zachary Eidex"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Yang"
                },
                "author": "Xiaofeng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11616v1",
                "updated": "2025-08-15T17:29:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    29,
                    6,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T17:29:06Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    29,
                    6,
                    4,
                    227,
                    0
                ],
                "title": "Controlling Multimodal LLMs via Reward-guided Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Multimodal LLMs via Reward-guided Decoding"
                },
                "summary": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods."
                },
                "authors": [
                    {
                        "name": "Oscar Mañas"
                    },
                    {
                        "name": "Pierluca D'Oro"
                    },
                    {
                        "name": "Koustuv Sinha"
                    },
                    {
                        "name": "Adriana Romero-Soriano"
                    },
                    {
                        "name": "Michal Drozdzal"
                    },
                    {
                        "name": "Aishwarya Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Aishwarya Agrawal"
                },
                "author": "Aishwarya Agrawal",
                "arxiv_comment": "Published at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12052v2",
                "updated": "2025-08-15T17:10:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    10,
                    35,
                    4,
                    227,
                    0
                ],
                "published": "2025-02-17T17:22:49Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    17,
                    22,
                    49,
                    0,
                    48,
                    0
                ],
                "title": "A Dual-Perspective NLG Meta-Evaluation Framework with Automatic\n  Benchmark and Better Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dual-Perspective NLG Meta-Evaluation Framework with Automatic\n  Benchmark and Better Interpretability"
                },
                "summary": "In NLG meta-evaluation, evaluation metrics are typically assessed based on\ntheir consistency with humans. However, we identify some limitations in\ntraditional NLG meta-evaluation approaches, such as issues in handling human\nratings and ambiguous selections of correlation measures, which undermine the\neffectiveness of meta-evaluation. In this work, we propose a dual-perspective\nNLG meta-evaluation framework that focuses on different evaluation\ncapabilities, thereby providing better interpretability. In addition, we\nintroduce a method of automatically constructing the corresponding benchmarks\nwithout requiring new human annotations. Furthermore, we conduct experiments\nwith 16 representative LLMs as the evaluators based on our proposed framework,\ncomprehensively analyzing their evaluation performance from different\nperspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In NLG meta-evaluation, evaluation metrics are typically assessed based on\ntheir consistency with humans. However, we identify some limitations in\ntraditional NLG meta-evaluation approaches, such as issues in handling human\nratings and ambiguous selections of correlation measures, which undermine the\neffectiveness of meta-evaluation. In this work, we propose a dual-perspective\nNLG meta-evaluation framework that focuses on different evaluation\ncapabilities, thereby providing better interpretability. In addition, we\nintroduce a method of automatically constructing the corresponding benchmarks\nwithout requiring new human annotations. Furthermore, we conduct experiments\nwith 16 representative LLMs as the evaluators based on our proposed framework,\ncomprehensively analyzing their evaluation performance from different\nperspectives."
                },
                "authors": [
                    {
                        "name": "Xinyu Hu"
                    },
                    {
                        "name": "Mingqi Gao"
                    },
                    {
                        "name": "Li Lin"
                    },
                    {
                        "name": "Zhenghan Yu"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11599v1",
                "updated": "2025-08-15T17:07:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    7,
                    54,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T17:07:54Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    17,
                    7,
                    54,
                    4,
                    227,
                    0
                ],
                "title": "CryptoScope: Utilizing Large Language Models for Automated Cryptographic\n  Logic Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CryptoScope: Utilizing Large Language Models for Automated Cryptographic\n  Logic Vulnerability Detection"
                },
                "summary": "Cryptographic algorithms are fundamental to modern security, yet their\nimplementations frequently harbor subtle logic flaws that are hard to detect.\nWe introduce CryptoScope, a novel framework for automated cryptographic\nvulnerability detection powered by Large Language Models (LLMs). CryptoScope\ncombines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation\n(RAG), guided by a curated cryptographic knowledge base containing over 12,000\nentries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily\nderived from real-world CVE vulnerabilities, complemented by cryptographic\nchallenges from major Capture The Flag (CTF) competitions and synthetic\nexamples across 11 programming languages. CryptoScope consistently improves\nperformance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,\nGPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9\npreviously undisclosed flaws in widely used open-source cryptographic projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptographic algorithms are fundamental to modern security, yet their\nimplementations frequently harbor subtle logic flaws that are hard to detect.\nWe introduce CryptoScope, a novel framework for automated cryptographic\nvulnerability detection powered by Large Language Models (LLMs). CryptoScope\ncombines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation\n(RAG), guided by a curated cryptographic knowledge base containing over 12,000\nentries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily\nderived from real-world CVE vulnerabilities, complemented by cryptographic\nchallenges from major Capture The Flag (CTF) competitions and synthetic\nexamples across 11 programming languages. CryptoScope consistently improves\nperformance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,\nGPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9\npreviously undisclosed flaws in widely used open-source cryptographic projects."
                },
                "authors": [
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Zimo Ji"
                    },
                    {
                        "name": "Tao Zheng"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Xiao Lan"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Lan"
                },
                "author": "Xiao Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11594v1",
                "updated": "2025-08-15T16:58:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    58,
                    10,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T16:58:10Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    58,
                    10,
                    4,
                    227,
                    0
                ],
                "title": "It's not a FAD: first results in using Flows for unsupervised Anomaly\n  Detection at 40 MHz at the Large Hadron Collider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It's not a FAD: first results in using Flows for unsupervised Anomaly\n  Detection at 40 MHz at the Large Hadron Collider"
                },
                "summary": "We present the first implementation of a Continuous Normalizing Flow (CNF)\nmodel for unsupervised anomaly detection within the realistic, high-rate\nenvironment of the Large Hadron Collider's L1 trigger systems. While CNFs\ntypically define an anomaly score via a probabilistic likelihood, calculating\nthis score requires solving an Ordinary Differential Equation, a procedure too\ncomplex for FPGA deployment. To overcome this, we propose a novel,\nhardware-friendly anomaly score defined as the squared norm of the model's\nvector field output. This score is based on the intuition that anomalous events\nrequire a larger transformation by the flow. Our model, trained via Flow\nMatching on Standard Model-like data, is synthesized for an FPGA using the\nhls4ml library. We demonstrate that our approach effectively identifies a\nvariety of beyond-the-Standard-Model signatures with performance comparable to\nexisting machine learning-based triggers. The algorithm achieves a latency of a\nfew hundred nanoseconds and requires minimal FPGA resources, establishing CNFs\nas a viable new tool for real-time, data-driven discovery at 40 MHz.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first implementation of a Continuous Normalizing Flow (CNF)\nmodel for unsupervised anomaly detection within the realistic, high-rate\nenvironment of the Large Hadron Collider's L1 trigger systems. While CNFs\ntypically define an anomaly score via a probabilistic likelihood, calculating\nthis score requires solving an Ordinary Differential Equation, a procedure too\ncomplex for FPGA deployment. To overcome this, we propose a novel,\nhardware-friendly anomaly score defined as the squared norm of the model's\nvector field output. This score is based on the intuition that anomalous events\nrequire a larger transformation by the flow. Our model, trained via Flow\nMatching on Standard Model-like data, is synthesized for an FPGA using the\nhls4ml library. We demonstrate that our approach effectively identifies a\nvariety of beyond-the-Standard-Model signatures with performance comparable to\nexisting machine learning-based triggers. The algorithm achieves a latency of a\nfew hundred nanoseconds and requires minimal FPGA resources, establishing CNFs\nas a viable new tool for real-time, data-driven discovery at 40 MHz."
                },
                "authors": [
                    {
                        "name": "Francesco Vaselli"
                    },
                    {
                        "name": "Maurizio Pierini"
                    },
                    {
                        "name": "Maciej Mikolaj Glowacki"
                    },
                    {
                        "name": "Thea Aarrestad"
                    },
                    {
                        "name": "Katya Govorkova"
                    },
                    {
                        "name": "Vladimir Loncar"
                    },
                    {
                        "name": "Dimitrios Danopoulos"
                    },
                    {
                        "name": "Felice Pantaleo"
                    }
                ],
                "author_detail": {
                    "name": "Felice Pantaleo"
                },
                "author": "Felice Pantaleo",
                "arxiv_comment": "7 pages, 4 figures, presented at ML4Jets 2025, to be submitted to\n  journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05147v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05147v2",
                "updated": "2025-08-15T16:46:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    46,
                    42,
                    4,
                    227,
                    0
                ],
                "published": "2025-04-07T14:52:40Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    52,
                    40,
                    0,
                    97,
                    0
                ],
                "title": "Pr$εε$mpt: Sanitizing Sensitive Prompts for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pr$εε$mpt: Sanitizing Sensitive Prompts for LLMs"
                },
                "summary": "The rise of large language models (LLMs) has introduced new privacy\nchallenges, particularly during inference where sensitive information in\nprompts may be exposed to proprietary LLM APIs. In this paper, we address the\nproblem of formally protecting the sensitive information contained in a prompt\nwhile maintaining response quality. To this end, first, we introduce a\ncryptographically inspired notion of a prompt sanitizer which transforms an\ninput prompt to protect its sensitive tokens. Second, we propose\nPr$\\epsilon\\epsilon$mpt, a novel system that implements a prompt sanitizer.\nPr$\\epsilon\\epsilon$mpt categorizes sensitive tokens into two types: (1) those\nwhere the LLM's response depends solely on the format (such as SSNs, credit\ncard numbers), for which we use format-preserving encryption (FPE); and (2)\nthose where the response depends on specific values, (such as age, salary) for\nwhich we apply metric differential privacy (mDP). Our evaluation demonstrates\nthat Pr$\\epsilon\\epsilon$mpt is a practical method to achieve meaningful\nprivacy guarantees, while maintaining high utility compared to unsanitized\nprompts, and outperforming prior methods",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has introduced new privacy\nchallenges, particularly during inference where sensitive information in\nprompts may be exposed to proprietary LLM APIs. In this paper, we address the\nproblem of formally protecting the sensitive information contained in a prompt\nwhile maintaining response quality. To this end, first, we introduce a\ncryptographically inspired notion of a prompt sanitizer which transforms an\ninput prompt to protect its sensitive tokens. Second, we propose\nPr$\\epsilon\\epsilon$mpt, a novel system that implements a prompt sanitizer.\nPr$\\epsilon\\epsilon$mpt categorizes sensitive tokens into two types: (1) those\nwhere the LLM's response depends solely on the format (such as SSNs, credit\ncard numbers), for which we use format-preserving encryption (FPE); and (2)\nthose where the response depends on specific values, (such as age, salary) for\nwhich we apply metric differential privacy (mDP). Our evaluation demonstrates\nthat Pr$\\epsilon\\epsilon$mpt is a practical method to achieve meaningful\nprivacy guarantees, while maintaining high utility compared to unsanitized\nprompts, and outperforming prior methods"
                },
                "authors": [
                    {
                        "name": "Amrita Roy Chowdhury"
                    },
                    {
                        "name": "David Glukhov"
                    },
                    {
                        "name": "Divyam Anshumaan"
                    },
                    {
                        "name": "Prasad Chalasani"
                    },
                    {
                        "name": "Nicolas Papernot"
                    },
                    {
                        "name": "Somesh Jha"
                    },
                    {
                        "name": "Mihir Bellare"
                    }
                ],
                "author_detail": {
                    "name": "Mihir Bellare"
                },
                "author": "Mihir Bellare",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05147v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05147v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11582v1",
                "updated": "2025-08-15T16:40:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    40,
                    29,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T16:40:29Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    40,
                    29,
                    4,
                    227,
                    0
                ],
                "title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme\n  Reasoning Efficiency in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme\n  Reasoning Efficiency in Large Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "HuiKang Su"
                    },
                    {
                        "name": "Jiannan Guan"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10603v2",
                "updated": "2025-08-15T16:07:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    16,
                    7,
                    3,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-14T12:44:39Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    12,
                    44,
                    39,
                    3,
                    226,
                    0
                ],
                "title": "Why Report Failed Interactions With Robots?! Towards Vignette-based\n  Interaction Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Report Failed Interactions With Robots?! Towards Vignette-based\n  Interaction Quality"
                },
                "summary": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods."
                },
                "authors": [
                    {
                        "name": "Agnes Axelsson"
                    },
                    {
                        "name": "Merle Reimann"
                    },
                    {
                        "name": "Ronald Cumbal"
                    },
                    {
                        "name": "Hannah Pelikan"
                    },
                    {
                        "name": "Divesh Lala"
                    }
                ],
                "author_detail": {
                    "name": "Divesh Lala"
                },
                "author": "Divesh Lala",
                "arxiv_comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025. 6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11553v1",
                "updated": "2025-08-15T15:55:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    55,
                    37,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T15:55:37Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    55,
                    37,
                    4,
                    227,
                    0
                ],
                "title": "SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving\n  Bubble-Free Pipelines via Tag Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving\n  Bubble-Free Pipelines via Tag Scheduling"
                },
                "summary": "We introduce SeamlessFlow, a server based reinforcement learning (RL)\nframework that addresses two core challenges in industrial scale RL: (1)\ndecoupling RL training from the complex execution flow of agents; (2)\nmaximizing GPU utilization with minimal idle time while preserving the\nstability and scalability required for large-scale deployments. First,\nSeamlessFlow introduces a data plane that decouples the RL trainer from\ndiverse, complex agent implementations while sustaining high throughput. A\ncentral trajectory manager maintains complete interaction histories and\nsupports partial rollout, allowing rollout to pause for weight updates and\nresume seamlessly, keeping agents unaware of service interruptions. Second, we\npropose a tag driven scheduling paradigm that abstracts hardware into\ncapability tagged resources, unifying colocated and disaggregated\narchitectures. Based on this, SeamlessFlow introduces a spatiotemporal\nmultiplexing pipeline that dynamically reassigns idle training nodes to rollout\nin a train rollout separated setup, eliminating pipeline bubbles and fully\nexploiting heterogeneous cluster resources. By combining these innovations,\nSeamlessFlow delivers both stability and high performance, making it well\nsuited for multi agent, long horizon, and other complex RL tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SeamlessFlow, a server based reinforcement learning (RL)\nframework that addresses two core challenges in industrial scale RL: (1)\ndecoupling RL training from the complex execution flow of agents; (2)\nmaximizing GPU utilization with minimal idle time while preserving the\nstability and scalability required for large-scale deployments. First,\nSeamlessFlow introduces a data plane that decouples the RL trainer from\ndiverse, complex agent implementations while sustaining high throughput. A\ncentral trajectory manager maintains complete interaction histories and\nsupports partial rollout, allowing rollout to pause for weight updates and\nresume seamlessly, keeping agents unaware of service interruptions. Second, we\npropose a tag driven scheduling paradigm that abstracts hardware into\ncapability tagged resources, unifying colocated and disaggregated\narchitectures. Based on this, SeamlessFlow introduces a spatiotemporal\nmultiplexing pipeline that dynamically reassigns idle training nodes to rollout\nin a train rollout separated setup, eliminating pipeline bubbles and fully\nexploiting heterogeneous cluster resources. By combining these innovations,\nSeamlessFlow delivers both stability and high performance, making it well\nsuited for multi agent, long horizon, and other complex RL tasks."
                },
                "authors": [
                    {
                        "name": "Jinghui Wang"
                    },
                    {
                        "name": "Shaojie Wang"
                    },
                    {
                        "name": "Yinghan Cui"
                    },
                    {
                        "name": "Xuxing Chen"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Xiaojiang Zhang"
                    },
                    {
                        "name": "Minglei Zhang"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Wenhao Zhuang"
                    },
                    {
                        "name": "Yuchen Cao"
                    },
                    {
                        "name": "Wankang Bao"
                    },
                    {
                        "name": "Haimo Li"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Huiming Wang"
                    },
                    {
                        "name": "Haoyang Huang"
                    },
                    {
                        "name": "Zongxian Feng"
                    },
                    {
                        "name": "Zizheng Zhan"
                    },
                    {
                        "name": "Ken Deng"
                    },
                    {
                        "name": "Wen Xiang"
                    },
                    {
                        "name": "Huaixi Tang"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Mengtong Li"
                    },
                    {
                        "name": "Mengfei Xie"
                    },
                    {
                        "name": "Junyi Peng"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Bing Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Yu"
                },
                "author": "Bing Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11548v1",
                "updated": "2025-08-15T15:50:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    50,
                    20,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T15:50:20Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    50,
                    20,
                    4,
                    227,
                    0
                ],
                "title": "Copyright Protection for Large Language Models: A Survey of Methods,\n  Challenges, and Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Copyright Protection for Large Language Models: A Survey of Methods,\n  Challenges, and Trends"
                },
                "summary": "Copyright protection for large language models is of critical importance,\ngiven their substantial development costs, proprietary value, and potential for\nmisuse. Existing surveys have predominantly focused on techniques for tracing\nLLM-generated content-namely, text watermarking-while a systematic exploration\nof methods for protecting the models themselves (i.e., model watermarking and\nmodel fingerprinting) remains absent. Moreover, the relationships and\ndistinctions among text watermarking, model watermarking, and model\nfingerprinting have not been comprehensively clarified. This work presents a\ncomprehensive survey of the current state of LLM copyright protection\ntechnologies, with a focus on model fingerprinting, covering the following\naspects: (1) clarifying the conceptual connection from text watermarking to\nmodel watermarking and fingerprinting, and adopting a unified terminology that\nincorporates model watermarking into the broader fingerprinting framework; (2)\nproviding an overview and comparison of diverse text watermarking techniques,\nhighlighting cases where such methods can function as model fingerprinting; (3)\nsystematically categorizing and comparing existing model fingerprinting\napproaches for LLM copyright protection; (4) presenting, for the first time,\ntechniques for fingerprint transfer and fingerprint removal; (5) summarizing\nevaluation metrics for model fingerprints, including effectiveness,\nharmlessness, robustness, stealthiness, and reliability; and (6) discussing\nopen challenges and future research directions. This survey aims to offer\nresearchers a thorough understanding of both text watermarking and model\nfingerprinting technologies in the era of LLMs, thereby fostering further\nadvances in protecting their intellectual property.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Copyright protection for large language models is of critical importance,\ngiven their substantial development costs, proprietary value, and potential for\nmisuse. Existing surveys have predominantly focused on techniques for tracing\nLLM-generated content-namely, text watermarking-while a systematic exploration\nof methods for protecting the models themselves (i.e., model watermarking and\nmodel fingerprinting) remains absent. Moreover, the relationships and\ndistinctions among text watermarking, model watermarking, and model\nfingerprinting have not been comprehensively clarified. This work presents a\ncomprehensive survey of the current state of LLM copyright protection\ntechnologies, with a focus on model fingerprinting, covering the following\naspects: (1) clarifying the conceptual connection from text watermarking to\nmodel watermarking and fingerprinting, and adopting a unified terminology that\nincorporates model watermarking into the broader fingerprinting framework; (2)\nproviding an overview and comparison of diverse text watermarking techniques,\nhighlighting cases where such methods can function as model fingerprinting; (3)\nsystematically categorizing and comparing existing model fingerprinting\napproaches for LLM copyright protection; (4) presenting, for the first time,\ntechniques for fingerprint transfer and fingerprint removal; (5) summarizing\nevaluation metrics for model fingerprints, including effectiveness,\nharmlessness, robustness, stealthiness, and reliability; and (6) discussing\nopen challenges and future research directions. This survey aims to offer\nresearchers a thorough understanding of both text watermarking and model\nfingerprinting technologies in the era of LLMs, thereby fostering further\nadvances in protecting their intellectual property."
                },
                "authors": [
                    {
                        "name": "Zhenhua Xu"
                    },
                    {
                        "name": "Xubin Yue"
                    },
                    {
                        "name": "Zhebo Wang"
                    },
                    {
                        "name": "Qichen Liu"
                    },
                    {
                        "name": "Xixiang Zhao"
                    },
                    {
                        "name": "Jingxuan Zhang"
                    },
                    {
                        "name": "Wenjun Zeng"
                    },
                    {
                        "name": "Wengpeng Xing"
                    },
                    {
                        "name": "Dezhang Kong"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11547v1",
                "updated": "2025-08-15T15:48:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    48,
                    42,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T15:48:42Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    48,
                    42,
                    4,
                    227,
                    0
                ],
                "title": "Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs\n  with Suspended Payloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs\n  with Suspended Payloads"
                },
                "summary": "This paper addresses the problem of tracking the position of a\ncable-suspended payload carried by an unmanned aerial vehicle, with a focus on\nreal-world deployment and minimal hardware requirements. In contrast to many\nexisting approaches that rely on motion-capture systems, additional onboard\ncameras, or instrumented payloads, we propose a framework that uses only\nstandard onboard sensors--specifically, real-time kinematic global navigation\nsatellite system measurements and data from the onboard inertial measurement\nunit--to estimate and control the payload's position. The system models the\nfull coupled dynamics of the aerial vehicle and payload, and integrates a\nlinear Kalman filter for state estimation, a model predictive contouring\ncontrol planner, and an incremental model predictive controller. The control\narchitecture is designed to remain effective despite sensing limitations and\nestimation uncertainty. Extensive simulations demonstrate that the proposed\nsystem achieves performance comparable to control based on ground-truth\nmeasurements, with only minor degradation (< 6%). The system also shows strong\nrobustness to variations in payload parameters. Field experiments further\nvalidate the framework, confirming its practical applicability and reliable\nperformance in outdoor environments using only off-the-shelf aerial vehicle\nhardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the problem of tracking the position of a\ncable-suspended payload carried by an unmanned aerial vehicle, with a focus on\nreal-world deployment and minimal hardware requirements. In contrast to many\nexisting approaches that rely on motion-capture systems, additional onboard\ncameras, or instrumented payloads, we propose a framework that uses only\nstandard onboard sensors--specifically, real-time kinematic global navigation\nsatellite system measurements and data from the onboard inertial measurement\nunit--to estimate and control the payload's position. The system models the\nfull coupled dynamics of the aerial vehicle and payload, and integrates a\nlinear Kalman filter for state estimation, a model predictive contouring\ncontrol planner, and an incremental model predictive controller. The control\narchitecture is designed to remain effective despite sensing limitations and\nestimation uncertainty. Extensive simulations demonstrate that the proposed\nsystem achieves performance comparable to control based on ground-truth\nmeasurements, with only minor degradation (< 6%). The system also shows strong\nrobustness to variations in payload parameters. Field experiments further\nvalidate the framework, confirming its practical applicability and reliable\nperformance in outdoor environments using only off-the-shelf aerial vehicle\nhardware."
                },
                "authors": [
                    {
                        "name": "Martin Jiroušek"
                    },
                    {
                        "name": "Tomáš Báča"
                    },
                    {
                        "name": "Martin Saska"
                    }
                ],
                "author_detail": {
                    "name": "Martin Saska"
                },
                "author": "Martin Saska",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10054v2",
                "updated": "2025-08-15T15:40:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    40,
                    50,
                    4,
                    227,
                    0
                ],
                "published": "2025-06-11T17:58:05Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    58,
                    5,
                    2,
                    162,
                    0
                ],
                "title": "Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of\n  LLMs"
                },
                "summary": "Direct Preference Optimization (DPO) has become a cornerstone of\nreinforcement learning from human feedback (RLHF) due to its simplicity and\nefficiency. However, existing DPO-based approaches typically treat all\npreference pairs uniformly, ignoring critical variations in their inherent\nquality and learning utility, leading to suboptimal data utilization and\nperformance. To address this challenge, we propose Omni-DPO, a dual-perspective\noptimization framework that jointly accounts for (1) the inherent quality of\neach preference pair and (2) the model's evolving performance on those pairs.\nBy adaptively weighting samples according to both data quality and the model's\nlearning dynamics during training, Omni-DPO enables more effective training\ndata utilization and achieves better performance. Experimental results on\nvarious models and benchmarks demonstrate the superiority and generalization\ncapabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it\nfinetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant\nmargin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning\ntasks, Omni-DPO consistently outperforms the baseline methods across all\nbenchmarks, providing strong empirical evidence for the effectiveness and\nrobustness of our approach. Code and models will be available at\nhttps://github.com/pspdada/Omni-DPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has become a cornerstone of\nreinforcement learning from human feedback (RLHF) due to its simplicity and\nefficiency. However, existing DPO-based approaches typically treat all\npreference pairs uniformly, ignoring critical variations in their inherent\nquality and learning utility, leading to suboptimal data utilization and\nperformance. To address this challenge, we propose Omni-DPO, a dual-perspective\noptimization framework that jointly accounts for (1) the inherent quality of\neach preference pair and (2) the model's evolving performance on those pairs.\nBy adaptively weighting samples according to both data quality and the model's\nlearning dynamics during training, Omni-DPO enables more effective training\ndata utilization and achieves better performance. Experimental results on\nvarious models and benchmarks demonstrate the superiority and generalization\ncapabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it\nfinetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant\nmargin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning\ntasks, Omni-DPO consistently outperforms the baseline methods across all\nbenchmarks, providing strong empirical evidence for the effectiveness and\nrobustness of our approach. Code and models will be available at\nhttps://github.com/pspdada/Omni-DPO."
                },
                "authors": [
                    {
                        "name": "Shangpin Peng"
                    },
                    {
                        "name": "Weinong Wang"
                    },
                    {
                        "name": "Zhuotao Tian"
                    },
                    {
                        "name": "Senqiao Yang"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Chengquan Zhang"
                    },
                    {
                        "name": "Takashi Isobe"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11536v1",
                "updated": "2025-08-15T15:32:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    32,
                    19,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T15:32:19Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    32,
                    19,
                    4,
                    227,
                    0
                ],
                "title": "Language models align with brain regions that represent concepts across\n  modalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models align with brain regions that represent concepts across\n  modalities"
                },
                "summary": "Cognitive science and neuroscience have long faced the challenge of\ndisentangling representations of language from representations of conceptual\nmeaning. As the same problem arises in today's language models (LMs), we\ninvestigate the relationship between LM--brain alignment and two neural\nmetrics: (1) the level of brain activation during processing of sentences,\ntargeting linguistic processing, and (2) a novel measure of meaning consistency\nacross input modalities, which quantifies how consistently a brain region\nresponds to the same concept across paradigms (sentence, word cloud, image)\nusing an fMRI dataset (Pereira et al., 2018). Our experiments show that both\nlanguage-only and language-vision models predict the signal better in more\nmeaning-consistent areas of the brain, even when these areas are not strongly\nsensitive to language processing, suggesting that LMs might internally\nrepresent cross-modal conceptual meaning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive science and neuroscience have long faced the challenge of\ndisentangling representations of language from representations of conceptual\nmeaning. As the same problem arises in today's language models (LMs), we\ninvestigate the relationship between LM--brain alignment and two neural\nmetrics: (1) the level of brain activation during processing of sentences,\ntargeting linguistic processing, and (2) a novel measure of meaning consistency\nacross input modalities, which quantifies how consistently a brain region\nresponds to the same concept across paradigms (sentence, word cloud, image)\nusing an fMRI dataset (Pereira et al., 2018). Our experiments show that both\nlanguage-only and language-vision models predict the signal better in more\nmeaning-consistent areas of the brain, even when these areas are not strongly\nsensitive to language processing, suggesting that LMs might internally\nrepresent cross-modal conceptual meaning."
                },
                "authors": [
                    {
                        "name": "Maria Ryskina"
                    },
                    {
                        "name": "Greta Tuckute"
                    },
                    {
                        "name": "Alexander Fung"
                    },
                    {
                        "name": "Ashley Malkin"
                    },
                    {
                        "name": "Evelina Fedorenko"
                    }
                ],
                "author_detail": {
                    "name": "Evelina Fedorenko"
                },
                "author": "Evelina Fedorenko",
                "arxiv_comment": "Accepted to COLM 2025. Code and data can be found at\n  https://github.com/ryskina/concepts-brain-llms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11534v1",
                "updated": "2025-08-15T15:22:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    22,
                    0,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T15:22:00Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    22,
                    0,
                    4,
                    227,
                    0
                ],
                "title": "Speciesism in AI: Evaluating Discrimination Against Animals in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speciesism in AI: Evaluating Discrimination Against Animals in Large\n  Language Models"
                },
                "summary": "As large language models (LLMs) become more widely deployed, it is crucial to\nexamine their ethical tendencies. Building on research on fairness and\ndiscrimination in AI, we investigate whether LLMs exhibit speciesist bias --\ndiscrimination based on species membership -- and how they value non-human\nanimals. We systematically examine this issue across three paradigms: (1)\nSpeciesismBench, a 1,003-item benchmark assessing recognition and moral\nevaluation of speciesist statements; (2) established psychological measures\ncomparing model responses with those of human participants; (3) text-generation\ntasks probing elaboration on, or resistance to, speciesist rationalizations. In\nour benchmark, LLMs reliably detected speciesist statements but rarely\ncondemned them, often treating speciesist attitudes as morally acceptable. On\npsychological measures, results were mixed: LLMs expressed slightly lower\nexplicit speciesism than people, yet in direct trade-offs they more often chose\nto save one human over multiple animals. A tentative interpretation is that\nLLMs may weight cognitive capacity rather than species per se: when capacities\nwere equal, they showed no species preference, and when an animal was described\nas more capable, they tended to prioritize it over a less capable human. In\nopen-ended text generation tasks, LLMs frequently normalized or rationalized\nharm toward farmed animals while refusing to do so for non-farmed animals.\nThese findings suggest that while LLMs reflect a mixture of progressive and\nmainstream human views, they nonetheless reproduce entrenched cultural norms\naround animal exploitation. We argue that expanding AI fairness and alignment\nframeworks to explicitly include non-human moral patients is essential for\nreducing these biases and preventing the entrenchment of speciesist attitudes\nin AI systems and the societies they influence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become more widely deployed, it is crucial to\nexamine their ethical tendencies. Building on research on fairness and\ndiscrimination in AI, we investigate whether LLMs exhibit speciesist bias --\ndiscrimination based on species membership -- and how they value non-human\nanimals. We systematically examine this issue across three paradigms: (1)\nSpeciesismBench, a 1,003-item benchmark assessing recognition and moral\nevaluation of speciesist statements; (2) established psychological measures\ncomparing model responses with those of human participants; (3) text-generation\ntasks probing elaboration on, or resistance to, speciesist rationalizations. In\nour benchmark, LLMs reliably detected speciesist statements but rarely\ncondemned them, often treating speciesist attitudes as morally acceptable. On\npsychological measures, results were mixed: LLMs expressed slightly lower\nexplicit speciesism than people, yet in direct trade-offs they more often chose\nto save one human over multiple animals. A tentative interpretation is that\nLLMs may weight cognitive capacity rather than species per se: when capacities\nwere equal, they showed no species preference, and when an animal was described\nas more capable, they tended to prioritize it over a less capable human. In\nopen-ended text generation tasks, LLMs frequently normalized or rationalized\nharm toward farmed animals while refusing to do so for non-farmed animals.\nThese findings suggest that while LLMs reflect a mixture of progressive and\nmainstream human views, they nonetheless reproduce entrenched cultural norms\naround animal exploitation. We argue that expanding AI fairness and alignment\nframeworks to explicitly include non-human moral patients is essential for\nreducing these biases and preventing the entrenchment of speciesist attitudes\nin AI systems and the societies they influence."
                },
                "authors": [
                    {
                        "name": "Monika Jotautaitė"
                    },
                    {
                        "name": "Lucius Caviola"
                    },
                    {
                        "name": "David A. Brewster"
                    },
                    {
                        "name": "Thilo Hagendorff"
                    }
                ],
                "author_detail": {
                    "name": "Thilo Hagendorff"
                },
                "author": "Thilo Hagendorff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11532v1",
                "updated": "2025-08-15T15:20:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    20,
                    25,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T15:20:25Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    20,
                    25,
                    4,
                    227,
                    0
                ],
                "title": "An Efficient Medical Image Classification Method Based on a Lightweight\n  Improved ConvNeXt-Tiny Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Medical Image Classification Method Based on a Lightweight\n  Improved ConvNeXt-Tiny Architecture"
                },
                "summary": "Intelligent analysis of medical imaging plays a crucial role in assisting\nclinical diagnosis. However, achieving efficient and high-accuracy image\nclassification in resource-constrained computational environments remains\nchallenging. This study proposes a medical image classification method based on\nan improved ConvNeXt-Tiny architecture. Through structural optimization and\nloss function design, the proposed method enhances feature extraction\ncapability and classification performance while reducing computational\ncomplexity. Specifically, the method introduces a dual global pooling (Global\nAverage Pooling and Global Max Pooling) feature fusion strategy into the\nConvNeXt-Tiny backbone to simultaneously preserve global statistical features\nand salient response information. A lightweight channel attention module,\ntermed Squeeze-and-Excitation Vector (SEVector), is designed to improve the\nadaptive allocation of channel weights while minimizing parameter overhead.\nAdditionally, a Feature Smoothing Loss is incorporated into the loss function\nto enhance intra-class feature consistency and suppress intra-class variance.\nUnder CPU-only conditions (8 threads), the method achieves a maximum\nclassification accuracy of 89.10% on the test set within 10 training epochs,\nexhibiting a stable convergence trend in loss values. Experimental results\ndemonstrate that the proposed method effectively improves medical image\nclassification performance in resource-limited settings, providing a feasible\nand efficient solution for the deployment and promotion of medical imaging\nanalysis models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent analysis of medical imaging plays a crucial role in assisting\nclinical diagnosis. However, achieving efficient and high-accuracy image\nclassification in resource-constrained computational environments remains\nchallenging. This study proposes a medical image classification method based on\nan improved ConvNeXt-Tiny architecture. Through structural optimization and\nloss function design, the proposed method enhances feature extraction\ncapability and classification performance while reducing computational\ncomplexity. Specifically, the method introduces a dual global pooling (Global\nAverage Pooling and Global Max Pooling) feature fusion strategy into the\nConvNeXt-Tiny backbone to simultaneously preserve global statistical features\nand salient response information. A lightweight channel attention module,\ntermed Squeeze-and-Excitation Vector (SEVector), is designed to improve the\nadaptive allocation of channel weights while minimizing parameter overhead.\nAdditionally, a Feature Smoothing Loss is incorporated into the loss function\nto enhance intra-class feature consistency and suppress intra-class variance.\nUnder CPU-only conditions (8 threads), the method achieves a maximum\nclassification accuracy of 89.10% on the test set within 10 training epochs,\nexhibiting a stable convergence trend in loss values. Experimental results\ndemonstrate that the proposed method effectively improves medical image\nclassification performance in resource-limited settings, providing a feasible\nand efficient solution for the deployment and promotion of medical imaging\nanalysis models."
                },
                "authors": [
                    {
                        "name": "Jingsong Xia"
                    },
                    {
                        "name": "Yue Yin"
                    },
                    {
                        "name": "Xiuhan Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiuhan Li"
                },
                "author": "Xiuhan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11529v1",
                "updated": "2025-08-15T15:15:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    15,
                    25,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T15:15:25Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    15,
                    25,
                    4,
                    227,
                    0
                ],
                "title": "A Comprehensive Perspective on Explainable AI across the Machine\n  Learning Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Perspective on Explainable AI across the Machine\n  Learning Workflow"
                },
                "summary": "Artificial intelligence is reshaping science and industry, yet many users\nstill regard its models as opaque \"black boxes\". Conventional explainable\nartificial-intelligence methods clarify individual predictions but overlook the\nupstream decisions and downstream quality checks that determine whether\ninsights can be trusted. In this work, we present Holistic Explainable\nArtificial Intelligence (HXAI), a user-centric framework that embeds\nexplanation into every stage of the data-analysis workflow and tailors those\nexplanations to users. HXAI unifies six components (data, analysis set-up,\nlearning process, model output, model quality, communication channel) into a\nsingle taxonomy and aligns each component with the needs of domain experts,\ndata analysts and data scientists. A 112-item question bank covers these needs;\nour survey of contemporary tools highlights critical coverage gaps. Grounded in\ntheories of human explanation, principles from human-computer interaction and\nfindings from empirical user studies, HXAI identifies the characteristics that\nmake explanations clear, actionable and cognitively manageable. A comprehensive\ntaxonomy operationalises these insights, reducing terminological ambiguity and\nenabling rigorous coverage analysis of existing toolchains. We further\ndemonstrate how AI agents that embed large-language models can orchestrate\ndiverse explanation techniques, translating technical artifacts into\nstakeholder-specific narratives that bridge the gap between AI developers and\ndomain experts. Departing from traditional surveys or perspective articles,\nthis work melds concepts from multiple disciplines, lessons from real-world\nprojects and a critical synthesis of the literature to advance a novel,\nend-to-end viewpoint on transparency, trustworthiness and responsible AI\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence is reshaping science and industry, yet many users\nstill regard its models as opaque \"black boxes\". Conventional explainable\nartificial-intelligence methods clarify individual predictions but overlook the\nupstream decisions and downstream quality checks that determine whether\ninsights can be trusted. In this work, we present Holistic Explainable\nArtificial Intelligence (HXAI), a user-centric framework that embeds\nexplanation into every stage of the data-analysis workflow and tailors those\nexplanations to users. HXAI unifies six components (data, analysis set-up,\nlearning process, model output, model quality, communication channel) into a\nsingle taxonomy and aligns each component with the needs of domain experts,\ndata analysts and data scientists. A 112-item question bank covers these needs;\nour survey of contemporary tools highlights critical coverage gaps. Grounded in\ntheories of human explanation, principles from human-computer interaction and\nfindings from empirical user studies, HXAI identifies the characteristics that\nmake explanations clear, actionable and cognitively manageable. A comprehensive\ntaxonomy operationalises these insights, reducing terminological ambiguity and\nenabling rigorous coverage analysis of existing toolchains. We further\ndemonstrate how AI agents that embed large-language models can orchestrate\ndiverse explanation techniques, translating technical artifacts into\nstakeholder-specific narratives that bridge the gap between AI developers and\ndomain experts. Departing from traditional surveys or perspective articles,\nthis work melds concepts from multiple disciplines, lessons from real-world\nprojects and a critical synthesis of the literature to advance a novel,\nend-to-end viewpoint on transparency, trustworthiness and responsible AI\ndeployment."
                },
                "authors": [
                    {
                        "name": "George Paterakis"
                    },
                    {
                        "name": "Andrea Castellani"
                    },
                    {
                        "name": "George Papoutsoglou"
                    },
                    {
                        "name": "Tobias Rodemann"
                    },
                    {
                        "name": "Ioannis Tsamardinos"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Tsamardinos"
                },
                "author": "Ioannis Tsamardinos",
                "arxiv_comment": "Preprint. Currently under review at \"Artificial Intelligence Review\"\n  journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11524v1",
                "updated": "2025-08-15T15:08:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    8,
                    7,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T15:08:07Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    15,
                    8,
                    7,
                    4,
                    227,
                    0
                ],
                "title": "Inspire or Predict? Exploring New Paradigms in Assisting Classical\n  Planners with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspire or Predict? Exploring New Paradigms in Assisting Classical\n  Planners with Large Language Models"
                },
                "summary": "Addressing large-scale planning problems has become one of the central\nchallenges in the planning community, deriving from the state-space explosion\ncaused by growing objects and actions. Recently, researchers have explored the\neffectiveness of leveraging Large Language Models (LLMs) to generate helpful\nactions and states to prune the search space. However, prior works have largely\noverlooked integrating LLMs with domain-specific knowledge to ensure valid\nplans. In this paper, we propose a novel LLM-assisted planner integrated with\nproblem decomposition, which first decomposes large planning problems into\nmultiple simpler sub-tasks. Then we explore two novel paradigms to utilize\nLLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where\nLLM4Inspire provides heuristic guidance according to general knowledge and\nLLM4Predict employs domain-specific knowledge to infer intermediate conditions.\nWe empirically validate the effectiveness of our planner across multiple\ndomains, demonstrating the ability of search space partition when solving\nlarge-scale planning problems. The experimental results show that LLMs\neffectively locate feasible solutions when pruning the search space, where\ninfusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds\nparticular promise compared with LLM4Inspire, which offers general knowledge\nwithin LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing large-scale planning problems has become one of the central\nchallenges in the planning community, deriving from the state-space explosion\ncaused by growing objects and actions. Recently, researchers have explored the\neffectiveness of leveraging Large Language Models (LLMs) to generate helpful\nactions and states to prune the search space. However, prior works have largely\noverlooked integrating LLMs with domain-specific knowledge to ensure valid\nplans. In this paper, we propose a novel LLM-assisted planner integrated with\nproblem decomposition, which first decomposes large planning problems into\nmultiple simpler sub-tasks. Then we explore two novel paradigms to utilize\nLLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where\nLLM4Inspire provides heuristic guidance according to general knowledge and\nLLM4Predict employs domain-specific knowledge to infer intermediate conditions.\nWe empirically validate the effectiveness of our planner across multiple\ndomains, demonstrating the ability of search space partition when solving\nlarge-scale planning problems. The experimental results show that LLMs\neffectively locate feasible solutions when pruning the search space, where\ninfusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds\nparticular promise compared with LLM4Inspire, which offers general knowledge\nwithin LLMs."
                },
                "authors": [
                    {
                        "name": "Wenkai Yu"
                    },
                    {
                        "name": "Jianhang Tang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Shanjiang Tang"
                    },
                    {
                        "name": "Kebing Jin"
                    },
                    {
                        "name": "Hankz Hankui Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Hankz Hankui Zhuo"
                },
                "author": "Hankz Hankui Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11514v1",
                "updated": "2025-08-15T14:51:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    51,
                    45,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T14:51:45Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    51,
                    45,
                    4,
                    227,
                    0
                ],
                "title": "DiCriTest: Testing Scenario Generation for Decision-Making Agents\n  Considering Diversity and Criticality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCriTest: Testing Scenario Generation for Decision-Making Agents\n  Considering Diversity and Criticality"
                },
                "summary": "The growing deployment of decision-making agents in dynamic environments\nincreases the demand for safety verification. While critical testing scenario\ngeneration has emerged as an appealing verification methodology, effectively\nbalancing diversity and criticality remains a key challenge for existing\nmethods, particularly due to local optima entrapment in high-dimensional\nscenario spaces. To address this limitation, we propose a dual-space guided\ntesting framework that coordinates scenario parameter space and agent behavior\nspace, aiming to generate testing scenarios considering diversity and\ncriticality. Specifically, in the scenario parameter space, a hierarchical\nrepresentation framework combines dimensionality reduction and\nmulti-dimensional subspace evaluation to efficiently localize diverse and\ncritical subspaces. This guides dynamic coordination between two generation\nmodes: local perturbation and global exploration, optimizing critical scenario\nquantity and diversity. Complementarily, in the agent behavior space,\nagent-environment interaction data are leveraged to quantify behavioral\ncriticality/diversity and adaptively support generation mode switching, forming\na closed feedback loop that continuously enhances scenario characterization and\nexploration within the parameter space. Experiments show our framework improves\ncritical scenario generation by an average of 56.23\\% and demonstrates greater\ndiversity under novel parameter-behavior co-driven metrics when tested on five\ndecision-making agents, outperforming state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing deployment of decision-making agents in dynamic environments\nincreases the demand for safety verification. While critical testing scenario\ngeneration has emerged as an appealing verification methodology, effectively\nbalancing diversity and criticality remains a key challenge for existing\nmethods, particularly due to local optima entrapment in high-dimensional\nscenario spaces. To address this limitation, we propose a dual-space guided\ntesting framework that coordinates scenario parameter space and agent behavior\nspace, aiming to generate testing scenarios considering diversity and\ncriticality. Specifically, in the scenario parameter space, a hierarchical\nrepresentation framework combines dimensionality reduction and\nmulti-dimensional subspace evaluation to efficiently localize diverse and\ncritical subspaces. This guides dynamic coordination between two generation\nmodes: local perturbation and global exploration, optimizing critical scenario\nquantity and diversity. Complementarily, in the agent behavior space,\nagent-environment interaction data are leveraged to quantify behavioral\ncriticality/diversity and adaptively support generation mode switching, forming\na closed feedback loop that continuously enhances scenario characterization and\nexploration within the parameter space. Experiments show our framework improves\ncritical scenario generation by an average of 56.23\\% and demonstrates greater\ndiversity under novel parameter-behavior co-driven metrics when tested on five\ndecision-making agents, outperforming state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Qitong Chu"
                    },
                    {
                        "name": "Yufeng Yue"
                    },
                    {
                        "name": "Danya Yao"
                    },
                    {
                        "name": "Huaxin Pei"
                    }
                ],
                "author_detail": {
                    "name": "Huaxin Pei"
                },
                "author": "Huaxin Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11513v1",
                "updated": "2025-08-15T14:44:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    44,
                    11,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T14:44:11Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    44,
                    11,
                    4,
                    227,
                    0
                ],
                "title": "Towards Faithful Class-level Self-explainability in Graph Neural\n  Networks by Subgraph Dependencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Faithful Class-level Self-explainability in Graph Neural\n  Networks by Subgraph Dependencies"
                },
                "summary": "Enhancing the interpretability of graph neural networks (GNNs) is crucial to\nensure their safe and fair deployment. Recent work has introduced\nself-explainable GNNs that generate explanations as part of training, improving\nboth faithfulness and efficiency. Some of these models, such as ProtGNN and\nPGIB, learn class-specific prototypes, offering a potential pathway toward\nclass-level explanations. However, their evaluations focus solely on\ninstance-level explanations, leaving open the question of whether these\nprototypes meaningfully generalize across instances of the same class. In this\npaper, we introduce GraphOracle, a novel self-explainable GNN framework\ndesigned to generate and evaluate class-level explanations for GNNs. Our model\njointly learns a GNN classifier and a set of structured, sparse subgraphs that\nare discriminative for each class. We propose a novel integrated training that\ncaptures graph$\\unicode{x2013}$subgraph$\\unicode{x2013}$prediction dependencies\nefficiently and faithfully, validated through a masking-based evaluation\nstrategy. This strategy enables us to retroactively assess whether prior\nmethods like ProtGNN and PGIB deliver effective class-level explanations. Our\nresults show that they do not. In contrast, GraphOracle achieves superior\nfidelity, explainability, and scalability across a range of graph\nclassification tasks. We further demonstrate that GraphOracle avoids the\ncomputational bottlenecks of previous methods$\\unicode{x2014}$like Monte Carlo\nTree Search$\\unicode{x2014}$by using entropy-regularized subgraph selection and\nlightweight random walk extraction, enabling faster and more scalable training.\nThese findings position GraphOracle as a practical and principled solution for\nfaithful class-level self-explainability in GNNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the interpretability of graph neural networks (GNNs) is crucial to\nensure their safe and fair deployment. Recent work has introduced\nself-explainable GNNs that generate explanations as part of training, improving\nboth faithfulness and efficiency. Some of these models, such as ProtGNN and\nPGIB, learn class-specific prototypes, offering a potential pathway toward\nclass-level explanations. However, their evaluations focus solely on\ninstance-level explanations, leaving open the question of whether these\nprototypes meaningfully generalize across instances of the same class. In this\npaper, we introduce GraphOracle, a novel self-explainable GNN framework\ndesigned to generate and evaluate class-level explanations for GNNs. Our model\njointly learns a GNN classifier and a set of structured, sparse subgraphs that\nare discriminative for each class. We propose a novel integrated training that\ncaptures graph$\\unicode{x2013}$subgraph$\\unicode{x2013}$prediction dependencies\nefficiently and faithfully, validated through a masking-based evaluation\nstrategy. This strategy enables us to retroactively assess whether prior\nmethods like ProtGNN and PGIB deliver effective class-level explanations. Our\nresults show that they do not. In contrast, GraphOracle achieves superior\nfidelity, explainability, and scalability across a range of graph\nclassification tasks. We further demonstrate that GraphOracle avoids the\ncomputational bottlenecks of previous methods$\\unicode{x2014}$like Monte Carlo\nTree Search$\\unicode{x2014}$by using entropy-regularized subgraph selection and\nlightweight random walk extraction, enabling faster and more scalable training.\nThese findings position GraphOracle as a practical and principled solution for\nfaithful class-level self-explainability in GNNs."
                },
                "authors": [
                    {
                        "name": "Fanzhen Liu"
                    },
                    {
                        "name": "Xiaoxiao Ma"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Kristen Moore"
                    },
                    {
                        "name": "Surya Nepal"
                    },
                    {
                        "name": "Cecile Paris"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Jia Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jia Wu"
                },
                "author": "Jia Wu",
                "arxiv_comment": "14 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11503v1",
                "updated": "2025-08-15T14:30:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    30,
                    7,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T14:30:07Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    30,
                    7,
                    4,
                    227,
                    0
                ],
                "title": "Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media"
                },
                "summary": "Reliable autonomous navigation across the unstructured terrains of distant\nplanetary surfaces is a critical enabler for future space exploration. However,\nthe deployment of learning-based controllers is hindered by the inherent\nsim-to-real gap, particularly for the complex dynamics of wheel interactions\nwith granular media. This work presents a complete sim-to-real framework for\ndeveloping and validating robust control policies for dynamic waypoint tracking\non such challenging surfaces. We leverage massively parallel simulation to\ntrain reinforcement learning agents across a vast distribution of procedurally\ngenerated environments with randomized physics. These policies are then\ntransferred zero-shot to a physical wheeled rover operating in a lunar-analogue\nfacility. Our experiments systematically compare multiple reinforcement\nlearning algorithms and action smoothing filters to identify the most effective\ncombinations for real-world deployment. Crucially, we provide strong empirical\nevidence that agents trained with procedural diversity achieve superior\nzero-shot performance compared to those trained on static scenarios. We also\nanalyze the trade-offs of fine-tuning with high-fidelity particle physics,\nwhich offers minor gains in low-speed precision at a significant computational\ncost. Together, these contributions establish a validated workflow for creating\nreliable learning-based navigation systems, marking a critical step towards\ndeploying autonomous robots in the final frontier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable autonomous navigation across the unstructured terrains of distant\nplanetary surfaces is a critical enabler for future space exploration. However,\nthe deployment of learning-based controllers is hindered by the inherent\nsim-to-real gap, particularly for the complex dynamics of wheel interactions\nwith granular media. This work presents a complete sim-to-real framework for\ndeveloping and validating robust control policies for dynamic waypoint tracking\non such challenging surfaces. We leverage massively parallel simulation to\ntrain reinforcement learning agents across a vast distribution of procedurally\ngenerated environments with randomized physics. These policies are then\ntransferred zero-shot to a physical wheeled rover operating in a lunar-analogue\nfacility. Our experiments systematically compare multiple reinforcement\nlearning algorithms and action smoothing filters to identify the most effective\ncombinations for real-world deployment. Crucially, we provide strong empirical\nevidence that agents trained with procedural diversity achieve superior\nzero-shot performance compared to those trained on static scenarios. We also\nanalyze the trade-offs of fine-tuning with high-fidelity particle physics,\nwhich offers minor gains in low-speed precision at a significant computational\ncost. Together, these contributions establish a validated workflow for creating\nreliable learning-based navigation systems, marking a critical step towards\ndeploying autonomous robots in the final frontier."
                },
                "authors": [
                    {
                        "name": "Andrej Orsula"
                    },
                    {
                        "name": "Matthieu Geist"
                    },
                    {
                        "name": "Miguel Olivares-Mendez"
                    },
                    {
                        "name": "Carol Martinez"
                    }
                ],
                "author_detail": {
                    "name": "Carol Martinez"
                },
                "author": "Carol Martinez",
                "arxiv_comment": "The source code is available at\n  https://github.com/AndrejOrsula/space_robotics_bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04996v3",
                "updated": "2025-08-15T14:21:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    21,
                    57,
                    4,
                    227,
                    0
                ],
                "published": "2025-07-07T13:34:49Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    34,
                    49,
                    0,
                    188,
                    0
                ],
                "title": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility\n  Systems"
                },
                "summary": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are viewed as vehicular systems capable of\nperceiving their environment and executing pre-programmed tasks independently\nof external input. However, both research and real-world deployments\nincreasingly showcase vehicles that demonstrate behaviors beyond this\ndefinition (including the SAE levels 0 to 5); Examples of this outpace include\nthe interaction with humans with natural language, goal adaptation, contextual\nreasoning, external tool use, and unseen ethical dilemma handling, largely\nempowered by multi-modal large language models (LLMs). These developments\nreveal a conceptual gap between technical autonomy and the broader cognitive\nand social capabilities needed for future human-centered mobility systems. To\naddress this gap, this paper introduces the concept of agentic vehicles (AgVs),\nreferring to vehicles that integrate agentic AI systems to reason, adapt, and\ninteract within complex environments. This paper proposes the term AgVs and\ntheir distinguishing characteristics from conventional AuVs. It synthesizes\nrelevant advances in integrating LLMs and AuVs and highlights how AgVs might\ntransform future mobility systems and ensure the systems are human-centered.\nThe paper concludes by identifying key challenges in the development and\ngovernance of AgVs, and how they can play a significant role in future agentic\ntransportation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are viewed as vehicular systems capable of\nperceiving their environment and executing pre-programmed tasks independently\nof external input. However, both research and real-world deployments\nincreasingly showcase vehicles that demonstrate behaviors beyond this\ndefinition (including the SAE levels 0 to 5); Examples of this outpace include\nthe interaction with humans with natural language, goal adaptation, contextual\nreasoning, external tool use, and unseen ethical dilemma handling, largely\nempowered by multi-modal large language models (LLMs). These developments\nreveal a conceptual gap between technical autonomy and the broader cognitive\nand social capabilities needed for future human-centered mobility systems. To\naddress this gap, this paper introduces the concept of agentic vehicles (AgVs),\nreferring to vehicles that integrate agentic AI systems to reason, adapt, and\ninteract within complex environments. This paper proposes the term AgVs and\ntheir distinguishing characteristics from conventional AuVs. It synthesizes\nrelevant advances in integrating LLMs and AuVs and highlights how AgVs might\ntransform future mobility systems and ensure the systems are human-centered.\nThe paper concludes by identifying key challenges in the development and\ngovernance of AgVs, and how they can play a significant role in future agentic\ntransportation systems."
                },
                "authors": [
                    {
                        "name": "Jiangbo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangbo Yu"
                },
                "author": "Jiangbo Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.06500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.06500v2",
                "updated": "2025-08-15T14:18:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    18,
                    48,
                    4,
                    227,
                    0
                ],
                "published": "2023-10-10T10:17:58Z",
                "published_parsed": [
                    2023,
                    10,
                    10,
                    10,
                    17,
                    58,
                    1,
                    283,
                    0
                ],
                "title": "MetaAgents: Large Language Model Based Agents for Decision-Making on\n  Teaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaAgents: Large Language Model Based Agents for Decision-Making on\n  Teaming"
                },
                "summary": "Significant advancements have occurred in the application of Large Language\nModels (LLMs) for social simulations. Despite this, their abilities to perform\nteaming in task-oriented social events are underexplored. Such capabilities are\ncrucial if LLMs are to effectively mimic human-like social behaviors and form\nefficient teams to solve tasks. To bridge this gap, we introduce MetaAgents, a\nsocial simulation framework populated with LLM-based agents. MetaAgents\nfacilitates agent engagement in conversations and a series of decision making\nwithin social contexts, serving as an appropriate platform for investigating\ninteractions and interpersonal decision-making of agents. In particular, we\nconstruct a job fair environment as a case study to scrutinize the team\nassembly and skill-matching behaviors of LLM-based agents. We take advantage of\nboth quantitative metrics evaluation and qualitative text analysis to assess\ntheir teaming abilities at the job fair. Our evaluation demonstrates that\nLLM-based agents perform competently in making rational decisions to develop\nefficient teams. However, we also identify limitations that hinder their\neffectiveness in more complex team assembly tasks. Our work provides valuable\ninsights into the role and evolution of LLMs in task-oriented social\nsimulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant advancements have occurred in the application of Large Language\nModels (LLMs) for social simulations. Despite this, their abilities to perform\nteaming in task-oriented social events are underexplored. Such capabilities are\ncrucial if LLMs are to effectively mimic human-like social behaviors and form\nefficient teams to solve tasks. To bridge this gap, we introduce MetaAgents, a\nsocial simulation framework populated with LLM-based agents. MetaAgents\nfacilitates agent engagement in conversations and a series of decision making\nwithin social contexts, serving as an appropriate platform for investigating\ninteractions and interpersonal decision-making of agents. In particular, we\nconstruct a job fair environment as a case study to scrutinize the team\nassembly and skill-matching behaviors of LLM-based agents. We take advantage of\nboth quantitative metrics evaluation and qualitative text analysis to assess\ntheir teaming abilities at the job fair. Our evaluation demonstrates that\nLLM-based agents perform competently in making rational decisions to develop\nefficient teams. However, we also identify limitations that hinder their\neffectiveness in more complex team assembly tasks. Our work provides valuable\ninsights into the role and evolution of LLMs in task-oriented social\nsimulations."
                },
                "authors": [
                    {
                        "name": "Yuan Li"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Yixuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Zhang"
                },
                "author": "Yixuan Zhang",
                "arxiv_doi": "10.1145/3711032",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711032",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.06500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.06500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08715v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08715v2",
                "updated": "2025-08-15T14:15:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    15,
                    19,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-12T07:58:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    7,
                    58,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation\n  Tutor with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation\n  Tutor with LLMs"
                },
                "summary": "Generative speech models have demonstrated significant potential in\npersonalizing teacher-student interactions, offering valuable real-world\napplications for language learning in children's education. However, achieving\nhigh-quality, child-friendly speech generation remains challenging,\nparticularly for low-resource languages across diverse languages and cultural\ncontexts. In this paper, we propose MultiAiTutor, an educational multilingual\ngenerative AI tutor with child-friendly designs, leveraging LLM architecture\nfor speech generation tailored for educational purposes. We propose to\nintegrate age-appropriate multilingual speech generation using LLM\narchitectures, facilitating young children's language learning through\nculturally relevant image-description tasks in three low-resource languages:\nSingaporean-accent Mandarin, Malay, and Tamil. Experimental results from both\nobjective metrics and subjective evaluations demonstrate the superior\nperformance of the proposed MultiAiTutor compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative speech models have demonstrated significant potential in\npersonalizing teacher-student interactions, offering valuable real-world\napplications for language learning in children's education. However, achieving\nhigh-quality, child-friendly speech generation remains challenging,\nparticularly for low-resource languages across diverse languages and cultural\ncontexts. In this paper, we propose MultiAiTutor, an educational multilingual\ngenerative AI tutor with child-friendly designs, leveraging LLM architecture\nfor speech generation tailored for educational purposes. We propose to\nintegrate age-appropriate multilingual speech generation using LLM\narchitectures, facilitating young children's language learning through\nculturally relevant image-description tasks in three low-resource languages:\nSingaporean-accent Mandarin, Malay, and Tamil. Experimental results from both\nobjective metrics and subjective evaluations demonstrate the superior\nperformance of the proposed MultiAiTutor compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Huayun Zhang"
                    },
                    {
                        "name": "Nancy F. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Nancy F. Chen"
                },
                "author": "Nancy F. Chen",
                "arxiv_comment": "We are withdrawing the manuscript to revise the title and contents of\n  figures for better alignment with the paper's contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08715v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08715v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06972v2",
                "updated": "2025-08-15T13:42:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    42,
                    28,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-09T12:38:18Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    12,
                    38,
                    18,
                    5,
                    221,
                    0
                ],
                "title": "DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine\n  Learning"
                },
                "summary": "DSperse is a modular framework for distributed machine learning inference\nwith strategic cryptographic verification. Operating within the emerging\nparadigm of distributed zero-knowledge machine learning, DSperse avoids the\nhigh cost and rigidity of full-model circuitization by enabling targeted\nverification of strategically chosen subcomputations. These verifiable\nsegments, or \"slices\", may cover part or all of the inference pipeline, with\nglobal consistency enforced through audit, replication, or economic incentives.\nThis architecture supports a pragmatic form of trust minimization, localizing\nzero-knowledge proofs to the components where they provide the greatest value.\nWe evaluate DSperse using multiple proving systems and report empirical results\non memory usage, runtime, and circuit behavior under sliced and unsliced\nconfigurations. By allowing proof boundaries to align flexibly with the model's\nlogical structure, DSperse supports scalable, targeted verification strategies\nsuited to diverse deployment needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSperse is a modular framework for distributed machine learning inference\nwith strategic cryptographic verification. Operating within the emerging\nparadigm of distributed zero-knowledge machine learning, DSperse avoids the\nhigh cost and rigidity of full-model circuitization by enabling targeted\nverification of strategically chosen subcomputations. These verifiable\nsegments, or \"slices\", may cover part or all of the inference pipeline, with\nglobal consistency enforced through audit, replication, or economic incentives.\nThis architecture supports a pragmatic form of trust minimization, localizing\nzero-knowledge proofs to the components where they provide the greatest value.\nWe evaluate DSperse using multiple proving systems and report empirical results\non memory usage, runtime, and circuit behavior under sliced and unsliced\nconfigurations. By allowing proof boundaries to align flexibly with the model's\nlogical structure, DSperse supports scalable, targeted verification strategies\nsuited to diverse deployment needs."
                },
                "authors": [
                    {
                        "name": "Dan Ivanov"
                    },
                    {
                        "name": "Tristan Freiberg"
                    },
                    {
                        "name": "Shirin Shahabi"
                    },
                    {
                        "name": "Jonathan Gold"
                    },
                    {
                        "name": "Haruna Isah"
                    }
                ],
                "author_detail": {
                    "name": "Haruna Isah"
                },
                "author": "Haruna Isah",
                "arxiv_comment": "12 pages, 8 figures, and 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11468v1",
                "updated": "2025-08-15T13:33:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    33,
                    52,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T13:33:52Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    33,
                    52,
                    4,
                    227,
                    0
                ],
                "title": "TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation"
                },
                "summary": "Automatic code translation is a fundamental task in modern software\ndevelopment. While the advent of Large Language Models (LLMs) has significantly\nimproved the correctness of code translation, the critical dimension of\nexecution efficiency remains overlooked. To address this gap, we introduce\nTRACY, the first comprehensive benchmark designed to evaluate the execution\nefficiency of LLM-translated code. TRACY is constructed through an LLM-driven\ntwo-stage pipeline: an initial stage generates a suite of stress tests to\namplify performance differences, followed by an efficiency-oriented task\npruning stage that isolates the efficiency-distinguishing tasks. The resulting\nbenchmark comprises 1,011 code translation tasks across C++, Java, and Python,\neach accompanied by an average of 22.1 verified reference translations and 10\ncomputationally demanding tests. Our extensive evaluation of 26 representative\nLLMs reveals that even top-tier LLMs struggle to consistently produce efficient\ncode translations. For instance, Claude-4-think, the leading model for\ncorrectness, ranks eighth overall when time efficiency is taken into account,\nsurpassed by several smaller open-source models. We further pinpoint that\nalgorithmic flaws and improper resource handling are the most detrimental,\ncausing a median time slowdown of 5.6$\\times$ and memory increase of\n12.0$\\times$, respectively. Our work underscores the necessity of jointly\noptimizing for correctness and efficiency in future LLM-based code translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic code translation is a fundamental task in modern software\ndevelopment. While the advent of Large Language Models (LLMs) has significantly\nimproved the correctness of code translation, the critical dimension of\nexecution efficiency remains overlooked. To address this gap, we introduce\nTRACY, the first comprehensive benchmark designed to evaluate the execution\nefficiency of LLM-translated code. TRACY is constructed through an LLM-driven\ntwo-stage pipeline: an initial stage generates a suite of stress tests to\namplify performance differences, followed by an efficiency-oriented task\npruning stage that isolates the efficiency-distinguishing tasks. The resulting\nbenchmark comprises 1,011 code translation tasks across C++, Java, and Python,\neach accompanied by an average of 22.1 verified reference translations and 10\ncomputationally demanding tests. Our extensive evaluation of 26 representative\nLLMs reveals that even top-tier LLMs struggle to consistently produce efficient\ncode translations. For instance, Claude-4-think, the leading model for\ncorrectness, ranks eighth overall when time efficiency is taken into account,\nsurpassed by several smaller open-source models. We further pinpoint that\nalgorithmic flaws and improper resource handling are the most detrimental,\ncausing a median time slowdown of 5.6$\\times$ and memory increase of\n12.0$\\times$, respectively. Our work underscores the necessity of jointly\noptimizing for correctness and efficiency in future LLM-based code translation."
                },
                "authors": [
                    {
                        "name": "Zhihao Gong"
                    },
                    {
                        "name": "Zeyu Sun"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Qingyuan Liang"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Dan Hao"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hao"
                },
                "author": "Dan Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11454v1",
                "updated": "2025-08-15T13:04:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    4,
                    32,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T13:04:32Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    4,
                    32,
                    4,
                    227,
                    0
                ],
                "title": "Reference Points in LLM Sentiment Analysis: The Role of Structured\n  Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference Points in LLM Sentiment Analysis: The Role of Structured\n  Context"
                },
                "summary": "Large language models (LLMs) are now widely used across many fields,\nincluding marketing research. Sentiment analysis, in particular, helps firms\nunderstand consumer preferences. While most NLP studies classify sentiment from\nreview text alone, marketing theories, such as prospect theory and\nexpectation--disconfirmation theory, point out that customer evaluations are\nshaped not only by the actual experience but also by additional reference\npoints. This study therefore investigates how the content and format of such\nsupplementary information affect sentiment analysis using LLMs. We compare\nnatural language (NL) and JSON-formatted prompts using a lightweight 3B\nparameter model suitable for practical marketing applications. Experiments on\ntwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt with\nadditional information outperforms all baselines without fine-tuning: Macro-F1\nrises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it\ndeployable in resource-constrained edge devices. Furthermore, a follow-up\nanalysis confirms that performance gains stem from genuine contextual reasoning\nrather than label proxying. This work demonstrates that structured prompting\ncan enable smaller models to achieve competitive performance, offering a\npractical alternative to large-scale model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are now widely used across many fields,\nincluding marketing research. Sentiment analysis, in particular, helps firms\nunderstand consumer preferences. While most NLP studies classify sentiment from\nreview text alone, marketing theories, such as prospect theory and\nexpectation--disconfirmation theory, point out that customer evaluations are\nshaped not only by the actual experience but also by additional reference\npoints. This study therefore investigates how the content and format of such\nsupplementary information affect sentiment analysis using LLMs. We compare\nnatural language (NL) and JSON-formatted prompts using a lightweight 3B\nparameter model suitable for practical marketing applications. Experiments on\ntwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt with\nadditional information outperforms all baselines without fine-tuning: Macro-F1\nrises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it\ndeployable in resource-constrained edge devices. Furthermore, a follow-up\nanalysis confirms that performance gains stem from genuine contextual reasoning\nrather than label proxying. This work demonstrates that structured prompting\ncan enable smaller models to achieve competitive performance, offering a\npractical alternative to large-scale model deployment."
                },
                "authors": [
                    {
                        "name": "Junichiro Niimi"
                    }
                ],
                "author_detail": {
                    "name": "Junichiro Niimi"
                },
                "author": "Junichiro Niimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11453v1",
                "updated": "2025-08-15T13:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    0,
                    55,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T13:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    0,
                    55,
                    4,
                    227,
                    0
                ],
                "title": "EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State\n  Feedback"
                },
                "summary": "Recent years have witnessed remarkable progress in autonomous driving, with\nsystems evolving from modular pipelines to end-to-end architectures. However,\nmost existing methods are trained offline and lack mechanisms to adapt to new\nenvironments during deployment. As a result, their generalization ability\ndiminishes when faced with unseen variations in real-world driving scenarios.\nIn this paper, we break away from the conventional \"train once, deploy forever\"\nparadigm and propose EvoPSF, a novel online Evolution framework for autonomous\ndriving based on Planning-State Feedback. We argue that planning failures are\nprimarily caused by inaccurate object-level motion predictions, and such\nfailures are often reflected in the form of increased planner uncertainty. To\naddress this, we treat planner uncertainty as a trigger for online evolution,\nusing it as a diagnostic signal to initiate targeted model updates. Rather than\nperforming blind updates, we leverage the planner's agent-agent attention to\nidentify the specific objects that the ego vehicle attends to most, which are\nprimarily responsible for the planning failures. For these critical objects, we\ncompute a targeted self-supervised loss by comparing their predicted waypoints\nfrom the prediction module with their actual future positions, selected from\nthe perception module's outputs with high confidence scores. This loss is then\nbackpropagated to adapt the model online. As a result, our method improves the\nmodel's robustness to environmental changes, leads to more precise motion\npredictions, and therefore enables more accurate and stable planning behaviors.\nExperiments on both cross-region and corrupted variants of the nuScenes dataset\ndemonstrate that EvoPSF consistently improves planning performance under\nchallenging conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed remarkable progress in autonomous driving, with\nsystems evolving from modular pipelines to end-to-end architectures. However,\nmost existing methods are trained offline and lack mechanisms to adapt to new\nenvironments during deployment. As a result, their generalization ability\ndiminishes when faced with unseen variations in real-world driving scenarios.\nIn this paper, we break away from the conventional \"train once, deploy forever\"\nparadigm and propose EvoPSF, a novel online Evolution framework for autonomous\ndriving based on Planning-State Feedback. We argue that planning failures are\nprimarily caused by inaccurate object-level motion predictions, and such\nfailures are often reflected in the form of increased planner uncertainty. To\naddress this, we treat planner uncertainty as a trigger for online evolution,\nusing it as a diagnostic signal to initiate targeted model updates. Rather than\nperforming blind updates, we leverage the planner's agent-agent attention to\nidentify the specific objects that the ego vehicle attends to most, which are\nprimarily responsible for the planning failures. For these critical objects, we\ncompute a targeted self-supervised loss by comparing their predicted waypoints\nfrom the prediction module with their actual future positions, selected from\nthe perception module's outputs with high confidence scores. This loss is then\nbackpropagated to adapt the model online. As a result, our method improves the\nmodel's robustness to environmental changes, leads to more precise motion\npredictions, and therefore enables more accurate and stable planning behaviors.\nExperiments on both cross-region and corrupted variants of the nuScenes dataset\ndemonstrate that EvoPSF consistently improves planning performance under\nchallenging conditions."
                },
                "authors": [
                    {
                        "name": "Jiayue Jin"
                    },
                    {
                        "name": "Lang Qian"
                    },
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Chuanyu Ju"
                    },
                    {
                        "name": "Liang Song"
                    }
                ],
                "author_detail": {
                    "name": "Liang Song"
                },
                "author": "Liang Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11452v1",
                "updated": "2025-08-15T13:00:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    0,
                    7,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T13:00:07Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    0,
                    7,
                    4,
                    227,
                    0
                ],
                "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models\n  with Real-World Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models\n  with Real-World Apps"
                },
                "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking."
                },
                "authors": [
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Hongliang He"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Ruiqi Liang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "Our platform is publicly accessible at\n  https://doraemon.alipay.com/model-ranking",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11446v1",
                "updated": "2025-08-15T12:54:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    54,
                    13,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T12:54:13Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    54,
                    13,
                    4,
                    227,
                    0
                ],
                "title": "Inside Knowledge: Graph-based Path Generation with Explainable Data\n  Augmentation and Curriculum Learning for Visual Indoor Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inside Knowledge: Graph-based Path Generation with Explainable Data\n  Augmentation and Curriculum Learning for Visual Indoor Navigation"
                },
                "summary": "Indoor navigation is a difficult task, as it generally comes with poor GPS\naccess, forcing solutions to rely on other sources of information. While\nsignificant progress continues to be made in this area, deployment to\nproduction applications is still lacking, given the complexity and additional\nrequirements of current solutions. Here, we introduce an efficient, real-time\nand easily deployable deep learning approach, based on visual input only, that\ncan predict the direction towards a target from images captured by a mobile\ndevice. Our technical approach, based on a novel graph-based path generation\nmethod, combined with explainable data augmentation and curriculum learning,\nincludes contributions that make the process of data collection, annotation and\ntraining, as automatic as possible, efficient and robust. On the practical\nside, we introduce a novel largescale dataset, with video footage inside a\nrelatively large shopping mall, in which each frame is annotated with the\ncorrect next direction towards different specific target destinations.\nDifferent from current methods, ours relies solely on vision, avoiding the need\nof special sensors, additional markers placed along the path, knowledge of the\nscene map or internet access. We also created an easy to use application for\nAndroid, which we plan to make publicly available. We make all our data and\ncode available along with visual demos on our project site",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor navigation is a difficult task, as it generally comes with poor GPS\naccess, forcing solutions to rely on other sources of information. While\nsignificant progress continues to be made in this area, deployment to\nproduction applications is still lacking, given the complexity and additional\nrequirements of current solutions. Here, we introduce an efficient, real-time\nand easily deployable deep learning approach, based on visual input only, that\ncan predict the direction towards a target from images captured by a mobile\ndevice. Our technical approach, based on a novel graph-based path generation\nmethod, combined with explainable data augmentation and curriculum learning,\nincludes contributions that make the process of data collection, annotation and\ntraining, as automatic as possible, efficient and robust. On the practical\nside, we introduce a novel largescale dataset, with video footage inside a\nrelatively large shopping mall, in which each frame is annotated with the\ncorrect next direction towards different specific target destinations.\nDifferent from current methods, ours relies solely on vision, avoiding the need\nof special sensors, additional markers placed along the path, knowledge of the\nscene map or internet access. We also created an easy to use application for\nAndroid, which we plan to make publicly available. We make all our data and\ncode available along with visual demos on our project site"
                },
                "authors": [
                    {
                        "name": "Daniel Airinei"
                    },
                    {
                        "name": "Elena Burceanu"
                    },
                    {
                        "name": "Marius Leordeanu"
                    }
                ],
                "author_detail": {
                    "name": "Marius Leordeanu"
                },
                "author": "Marius Leordeanu",
                "arxiv_comment": "Accepted at the International Conference on Computer Vision Workshops\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11434v1",
                "updated": "2025-08-15T12:24:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    24,
                    22,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T12:24:22Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    24,
                    22,
                    4,
                    227,
                    0
                ],
                "title": "Online Anti-sexist Speech: Identifying Resistance to Gender Bias in\n  Political Discourse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Anti-sexist Speech: Identifying Resistance to Gender Bias in\n  Political Discourse"
                },
                "summary": "Anti-sexist speech, i.e., public expressions that challenge or resist\ngendered abuse and sexism, plays a vital role in shaping democratic debate\nonline. Yet automated content moderation systems, increasingly powered by large\nlanguage models (LLMs), may struggle to distinguish such resistance from the\nsexism it opposes. This study examines how five LLMs classify sexist,\nanti-sexist, and neutral political tweets from the UK, focusing on\nhigh-salience trigger events involving female Members of Parliament in the year\n2022. Our analysis show that models frequently misclassify anti-sexist speech\nas harmful, particularly during politically charged events where rhetorical\nstyles of harm and resistance converge. These errors risk silencing those who\nchallenge sexism, with disproportionate consequences for marginalised voices.\nWe argue that moderation design must move beyond binary harmful/not-harmful\nschemas, integrate human-in-the-loop review during sensitive events, and\nexplicitly include counter-speech in training data. By linking feminist\nscholarship, event-based analysis, and model evaluation, this work highlights\nthe sociotechnical challenges of safeguarding resistance speech in digital\npolitical spaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anti-sexist speech, i.e., public expressions that challenge or resist\ngendered abuse and sexism, plays a vital role in shaping democratic debate\nonline. Yet automated content moderation systems, increasingly powered by large\nlanguage models (LLMs), may struggle to distinguish such resistance from the\nsexism it opposes. This study examines how five LLMs classify sexist,\nanti-sexist, and neutral political tweets from the UK, focusing on\nhigh-salience trigger events involving female Members of Parliament in the year\n2022. Our analysis show that models frequently misclassify anti-sexist speech\nas harmful, particularly during politically charged events where rhetorical\nstyles of harm and resistance converge. These errors risk silencing those who\nchallenge sexism, with disproportionate consequences for marginalised voices.\nWe argue that moderation design must move beyond binary harmful/not-harmful\nschemas, integrate human-in-the-loop review during sensitive events, and\nexplicitly include counter-speech in training data. By linking feminist\nscholarship, event-based analysis, and model evaluation, this work highlights\nthe sociotechnical challenges of safeguarding resistance speech in digital\npolitical spaces."
                },
                "authors": [
                    {
                        "name": "Aditi Dutta"
                    },
                    {
                        "name": "Susan Banducci"
                    }
                ],
                "author_detail": {
                    "name": "Susan Banducci"
                },
                "author": "Susan Banducci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06371v2",
                "updated": "2025-08-15T12:22:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    22,
                    31,
                    4,
                    227,
                    0
                ],
                "published": "2025-06-04T12:11:05Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    12,
                    11,
                    5,
                    2,
                    155,
                    0
                ],
                "title": "Relationship Detection on Tabular Data Using Statistical Analysis and\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relationship Detection on Tabular Data Using Statistical Analysis and\n  Large Language Models"
                },
                "summary": "Over the past few years, table interpretation tasks have made significant\nprogress due to their importance and the introduction of new technologies and\nbenchmarks in the field. This work experiments with a hybrid approach for\ndetecting relationships among columns of unlabeled tabular data, using a\nKnowledge Graph (KG) as a reference point, a task known as CPA. This approach\nleverages large language models (LLMs) while employing statistical analysis to\nreduce the search space of potential KG relations. The main modules of this\napproach for reducing the search space are domain and range constraints\ndetection, as well as relation co-appearance analysis. The experimental\nevaluation on two benchmark datasets provided by the SemTab challenge assesses\nthe influence of each module and the effectiveness of different\nstate-of-the-art LLMs at various levels of quantization. The experiments were\nperformed, as well as at different prompting techniques. The proposed\nmethodology, which is publicly available on github, proved to be competitive\nwith state-of-the-art approaches on these datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past few years, table interpretation tasks have made significant\nprogress due to their importance and the introduction of new technologies and\nbenchmarks in the field. This work experiments with a hybrid approach for\ndetecting relationships among columns of unlabeled tabular data, using a\nKnowledge Graph (KG) as a reference point, a task known as CPA. This approach\nleverages large language models (LLMs) while employing statistical analysis to\nreduce the search space of potential KG relations. The main modules of this\napproach for reducing the search space are domain and range constraints\ndetection, as well as relation co-appearance analysis. The experimental\nevaluation on two benchmark datasets provided by the SemTab challenge assesses\nthe influence of each module and the effectiveness of different\nstate-of-the-art LLMs at various levels of quantization. The experiments were\nperformed, as well as at different prompting techniques. The proposed\nmethodology, which is publicly available on github, proved to be competitive\nwith state-of-the-art approaches on these datasets."
                },
                "authors": [
                    {
                        "name": "Panagiotis Koletsis"
                    },
                    {
                        "name": "Christos Panagiotopoulos"
                    },
                    {
                        "name": "Georgios Th. Papadopoulos"
                    },
                    {
                        "name": "Vasilis Efthymiou"
                    }
                ],
                "author_detail": {
                    "name": "Vasilis Efthymiou"
                },
                "author": "Vasilis Efthymiou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05118v3",
                "updated": "2025-08-15T12:14:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    14,
                    24,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-07T07:51:38Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    51,
                    38,
                    3,
                    219,
                    0
                ],
                "title": "Exploring Superior Function Calls via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Superior Function Calls via Reinforcement Learning"
                },
                "summary": "Function calling capabilities are crucial for deploying Large Language Models\nin real-world applications, yet current training approaches fail to develop\nrobust reasoning strategies. Supervised fine-tuning produces models that rely\non superficial pattern matching, while standard reinforcement learning methods\nstruggle with the complex action space of structured function calls. We present\na novel reinforcement learning framework designed to enhance group relative\npolicy optimization through strategic entropy based exploration specifically\ntailored for function calling tasks. Our approach addresses three critical\nchallenges in function calling: insufficient exploration during policy\nlearning, lack of structured reasoning in chain-of-thought generation, and\ninadequate verification of parameter extraction. Our two-stage data preparation\npipeline ensures high-quality training samples through iterative LLM evaluation\nand abstract syntax tree validation. Extensive experiments on the Berkeley\nFunction Calling Leaderboard demonstrate that this framework achieves\nstate-of-the-art performance among open-source models with 86.02\\% overall\naccuracy, outperforming standard GRPO by up to 6\\% on complex multi-function\nscenarios. Notably, our method shows particularly strong improvements on\ncode-pretrained models, suggesting that structured language generation\ncapabilities provide an advantageous starting point for reinforcement learning\nin function calling tasks. We will release all the code, models and dataset to\nbenefit the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function calling capabilities are crucial for deploying Large Language Models\nin real-world applications, yet current training approaches fail to develop\nrobust reasoning strategies. Supervised fine-tuning produces models that rely\non superficial pattern matching, while standard reinforcement learning methods\nstruggle with the complex action space of structured function calls. We present\na novel reinforcement learning framework designed to enhance group relative\npolicy optimization through strategic entropy based exploration specifically\ntailored for function calling tasks. Our approach addresses three critical\nchallenges in function calling: insufficient exploration during policy\nlearning, lack of structured reasoning in chain-of-thought generation, and\ninadequate verification of parameter extraction. Our two-stage data preparation\npipeline ensures high-quality training samples through iterative LLM evaluation\nand abstract syntax tree validation. Extensive experiments on the Berkeley\nFunction Calling Leaderboard demonstrate that this framework achieves\nstate-of-the-art performance among open-source models with 86.02\\% overall\naccuracy, outperforming standard GRPO by up to 6\\% on complex multi-function\nscenarios. Notably, our method shows particularly strong improvements on\ncode-pretrained models, suggesting that structured language generation\ncapabilities provide an advantageous starting point for reinforcement learning\nin function calling tasks. We will release all the code, models and dataset to\nbenefit the community."
                },
                "authors": [
                    {
                        "name": "Bingguang Hao"
                    },
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Zengzhuang Xu"
                    },
                    {
                        "name": "Yicheng Chen"
                    },
                    {
                        "name": "Cunyin Peng"
                    },
                    {
                        "name": "Jinjie GU"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Chenyi Zhuang"
                },
                "author": "Chenyi Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22402v2",
                "updated": "2025-08-15T12:08:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    8,
                    35,
                    4,
                    227,
                    0
                ],
                "published": "2025-03-28T13:11:27Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    11,
                    27,
                    4,
                    87,
                    0
                ],
                "title": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing"
                },
                "summary": "Text-to-SQL automatically translates natural language queries to SQL,\nallowing non-technical users to retrieve data from databases without\nspecialized SQL knowledge. Despite the success of advanced LLM-based\nText-to-SQL approaches on leaderboards, their unsustainable computational\ncosts--often overlooked--stand as the \"elephant in the room\" in current\nleaderboard-driven research, limiting their economic practicability for\nreal-world deployment and widespread adoption. To tackle this, we exploratively\npropose EllieSQL, a complexity-aware routing framework that assigns queries to\nsuitable SQL generation pipelines based on estimated complexity. We investigate\nmultiple routers to direct simple queries to efficient approaches while\nreserving computationally intensive methods for complex cases. Drawing from\neconomics, we introduce the Token Elasticity of Performance (TEP) metric,\ncapturing cost-efficiency by quantifying the responsiveness of performance\ngains relative to token investment in SQL generation. Experiments show that\ncompared to always using the most advanced methods in our study, EllieSQL with\nthe Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising\nperformance on Bird development set, achieving more than a 2x boost in TEP over\nnon-routing approaches. This not only advances the pursuit of cost-efficient\nText-to-SQL but also invites the community to weigh resource efficiency\nalongside performance, contributing to progress in sustainable Text-to-SQL. Our\nsource code and model are available at https://elliesql.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL automatically translates natural language queries to SQL,\nallowing non-technical users to retrieve data from databases without\nspecialized SQL knowledge. Despite the success of advanced LLM-based\nText-to-SQL approaches on leaderboards, their unsustainable computational\ncosts--often overlooked--stand as the \"elephant in the room\" in current\nleaderboard-driven research, limiting their economic practicability for\nreal-world deployment and widespread adoption. To tackle this, we exploratively\npropose EllieSQL, a complexity-aware routing framework that assigns queries to\nsuitable SQL generation pipelines based on estimated complexity. We investigate\nmultiple routers to direct simple queries to efficient approaches while\nreserving computationally intensive methods for complex cases. Drawing from\neconomics, we introduce the Token Elasticity of Performance (TEP) metric,\ncapturing cost-efficiency by quantifying the responsiveness of performance\ngains relative to token investment in SQL generation. Experiments show that\ncompared to always using the most advanced methods in our study, EllieSQL with\nthe Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising\nperformance on Bird development set, achieving more than a 2x boost in TEP over\nnon-routing approaches. This not only advances the pursuit of cost-efficient\nText-to-SQL but also invites the community to weigh resource efficiency\nalongside performance, contributing to progress in sustainable Text-to-SQL. Our\nsource code and model are available at https://elliesql.github.io/."
                },
                "authors": [
                    {
                        "name": "Yizhang Zhu"
                    },
                    {
                        "name": "Runzhi Jiang"
                    },
                    {
                        "name": "Boyan Li"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Yuyu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Luo"
                },
                "author": "Yuyu Luo",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11429v1",
                "updated": "2025-08-15T12:07:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    7,
                    56,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T12:07:56Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    7,
                    56,
                    4,
                    227,
                    0
                ],
                "title": "HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor"
                },
                "summary": "Automated humor generation with Large Language Models (LLMs) often yields\njokes that feel generic, repetitive, or tone-deaf because humor is deeply\nsituated and hinges on the listener's cultural background, mindset, and\nimmediate context. We introduce HumorPlanSearch, a modular pipeline that\nexplicitly models context through: (1) Plan-Search for diverse, topic-tailored\nstrategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and\nstylistic reasoning; (3) a Knowledge Graph to retrieve and adapt\nhigh-performing historical strategies; (4) novelty filtering via semantic\nembeddings; and (5) an iterative judge-driven revision loop. To evaluate\ncontext sensitivity and comedic quality, we propose the Humor Generation Score\n(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,\nand topic relevance. In experiments across nine topics with feedback from 13\nhuman judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent\n(p < 0.05) over a strong baseline. By foregrounding context at every stage from\nstrategy planning to multi-signal evaluation, HumorPlanSearch advances\nAI-driven humor toward more coherent, adaptive, and culturally attuned comedy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated humor generation with Large Language Models (LLMs) often yields\njokes that feel generic, repetitive, or tone-deaf because humor is deeply\nsituated and hinges on the listener's cultural background, mindset, and\nimmediate context. We introduce HumorPlanSearch, a modular pipeline that\nexplicitly models context through: (1) Plan-Search for diverse, topic-tailored\nstrategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and\nstylistic reasoning; (3) a Knowledge Graph to retrieve and adapt\nhigh-performing historical strategies; (4) novelty filtering via semantic\nembeddings; and (5) an iterative judge-driven revision loop. To evaluate\ncontext sensitivity and comedic quality, we propose the Humor Generation Score\n(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,\nand topic relevance. In experiments across nine topics with feedback from 13\nhuman judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent\n(p < 0.05) over a strong baseline. By foregrounding context at every stage from\nstrategy planning to multi-signal evaluation, HumorPlanSearch advances\nAI-driven humor toward more coherent, adaptive, and culturally attuned comedy."
                },
                "authors": [
                    {
                        "name": "Shivam Dubey"
                    }
                ],
                "author_detail": {
                    "name": "Shivam Dubey"
                },
                "author": "Shivam Dubey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11425v1",
                "updated": "2025-08-15T12:02:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    2,
                    46,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T12:02:46Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    2,
                    46,
                    4,
                    227,
                    0
                ],
                "title": "Tapas are free! Training-Free Adaptation of Programmatic Agents via\n  LLM-Guided Program Synthesis in Dynamic Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tapas are free! Training-Free Adaptation of Programmatic Agents via\n  LLM-Guided Program Synthesis in Dynamic Environments"
                },
                "summary": "Autonomous agents in safety-critical applications must continuously adapt to\ndynamic conditions without compromising performance and reliability. This work\nintroduces TAPA (Training-free Adaptation of Programmatic Agents), a novel\nframework that positions large language models (LLMs) as intelligent moderators\nof the symbolic action space. Unlike prior programmatic agents that typically\ngenerate a monolithic policy program or rely on fixed symbolic action sets,\nTAPA synthesizes and adapts modular programs for individual high-level actions,\nreferred to as logical primitives. By decoupling strategic intent from\nexecution, TAPA enables meta-agents to operate over an abstract, interpretable\naction space while the LLM dynamically generates, composes, and refines\nsymbolic programs tailored to each primitive. Extensive experiments across\ncybersecurity and swarm intelligence domains validate TAPA's effectiveness. In\nautonomous DDoS defense scenarios, TAPA achieves 77.7% network uptime while\nmaintaining near-perfect detection accuracy in unknown dynamic environments. In\nswarm intelligence formation control under environmental and adversarial\ndisturbances, TAPA consistently preserves consensus at runtime where baseline\nmethods fail completely. This work promotes a paradigm shift for autonomous\nsystem design in evolving environments, from policy adaptation to dynamic\naction adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents in safety-critical applications must continuously adapt to\ndynamic conditions without compromising performance and reliability. This work\nintroduces TAPA (Training-free Adaptation of Programmatic Agents), a novel\nframework that positions large language models (LLMs) as intelligent moderators\nof the symbolic action space. Unlike prior programmatic agents that typically\ngenerate a monolithic policy program or rely on fixed symbolic action sets,\nTAPA synthesizes and adapts modular programs for individual high-level actions,\nreferred to as logical primitives. By decoupling strategic intent from\nexecution, TAPA enables meta-agents to operate over an abstract, interpretable\naction space while the LLM dynamically generates, composes, and refines\nsymbolic programs tailored to each primitive. Extensive experiments across\ncybersecurity and swarm intelligence domains validate TAPA's effectiveness. In\nautonomous DDoS defense scenarios, TAPA achieves 77.7% network uptime while\nmaintaining near-perfect detection accuracy in unknown dynamic environments. In\nswarm intelligence formation control under environmental and adversarial\ndisturbances, TAPA consistently preserves consensus at runtime where baseline\nmethods fail completely. This work promotes a paradigm shift for autonomous\nsystem design in evolving environments, from policy adaptation to dynamic\naction adaptation."
                },
                "authors": [
                    {
                        "name": "Jinwei Hu"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Youcheng Sun"
                    },
                    {
                        "name": "Xiaowei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Huang"
                },
                "author": "Xiaowei Huang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17811v2",
                "updated": "2025-08-15T11:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    48,
                    28,
                    4,
                    227,
                    0
                ],
                "published": "2025-03-22T16:22:53Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    16,
                    22,
                    53,
                    5,
                    81,
                    0
                ],
                "title": "Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model\n  Collaboration Paradigm for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model\n  Collaboration Paradigm for Small Language Models"
                },
                "summary": "Natural Language to SQL (NL2SQL) has seen significant advancements with large\nlanguage models (LLMs). However, these models often depend on closed-source\nsystems and high computational resources, posing challenges in data privacy and\ndeployment. In contrast, small language models (SLMs) struggle with NL2SQL\ntasks, exhibiting poor performance and incompatibility with existing\nframeworks. To address these issues, we introduce Feather-SQL, a new\nlightweight framework tailored for SLMs. Feather-SQL improves SQL executability\nand accuracy through 1) schema pruning and linking, 2) multi-path and\nmulti-candidate generation. Additionally, we introduce the 1+1 Model\nCollaboration Paradigm, which pairs a strong general-purpose chat model with a\nfine-tuned SQL specialist, combining strong analytical reasoning with\nhigh-precision SQL generation. Experimental results on BIRD demonstrate that\nFeather-SQL improves NL2SQL performance on SLMs, with around 10% boost for\nmodels without fine-tuning. The proposed paradigm raises the accuracy ceiling\nof SLMs to 54.76%, highlighting its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language to SQL (NL2SQL) has seen significant advancements with large\nlanguage models (LLMs). However, these models often depend on closed-source\nsystems and high computational resources, posing challenges in data privacy and\ndeployment. In contrast, small language models (SLMs) struggle with NL2SQL\ntasks, exhibiting poor performance and incompatibility with existing\nframeworks. To address these issues, we introduce Feather-SQL, a new\nlightweight framework tailored for SLMs. Feather-SQL improves SQL executability\nand accuracy through 1) schema pruning and linking, 2) multi-path and\nmulti-candidate generation. Additionally, we introduce the 1+1 Model\nCollaboration Paradigm, which pairs a strong general-purpose chat model with a\nfine-tuned SQL specialist, combining strong analytical reasoning with\nhigh-precision SQL generation. Experimental results on BIRD demonstrate that\nFeather-SQL improves NL2SQL performance on SLMs, with around 10% boost for\nmodels without fine-tuning. The proposed paradigm raises the accuracy ceiling\nof SLMs to 54.76%, highlighting its effectiveness."
                },
                "authors": [
                    {
                        "name": "Wenqi Pei"
                    },
                    {
                        "name": "Hailing Xu"
                    },
                    {
                        "name": "Hengyuan Zhao"
                    },
                    {
                        "name": "Shizheng Hou"
                    },
                    {
                        "name": "Han Chen"
                    },
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Pingyi Luo"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "arxiv_comment": "DL4C @ ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02736v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02736v2",
                "updated": "2025-08-15T11:45:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    45,
                    40,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-02T01:43:39Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    1,
                    43,
                    39,
                    5,
                    214,
                    0
                ],
                "title": "AgentSight: System-Level Observability for AI Agents Using eBPF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSight: System-Level Observability for AI Agents Using eBPF"
                },
                "summary": "Modern software infrastructure increasingly relies on LLM agents for\ndevelopment and maintenance, such as Claude Code and Gemini-cli. However, these\nAI agents differ fundamentally from traditional deterministic software, posing\na significant challenge to conventional monitoring and debugging. This creates\na critical semantic gap: existing tools observe either an agent's high-level\nintent (via LLM prompts) or its low-level actions (e.g., system calls), but\ncannot correlate these two views. This blindness makes it difficult to\ndistinguish between benign operations, malicious attacks, and costly failures.\nWe introduce AgentSight, an AgentOps observability framework that bridges this\nsemantic gap using a hybrid approach. Our approach, boundary tracing, monitors\nagents from outside their application code at stable system interfaces using\neBPF. AgentSight intercepts TLS-encrypted LLM traffic to extract semantic\nintent, monitors kernel events to observe system-wide effects, and causally\ncorrelates these two streams across process boundaries using a real-time engine\nand secondary LLM analysis. This instrumentation-free technique is\nframework-agnostic, resilient to rapid API changes, and incurs less than 3%\nperformance overhead. Our evaluation shows AgentSight detects prompt injection\nattacks, identifies resource-wasting reasoning loops, and reveals hidden\ncoordination bottlenecks in multi-agent systems. AgentSight is released as an\nopen-source project at https://github.com/agent-sight/agentsight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software infrastructure increasingly relies on LLM agents for\ndevelopment and maintenance, such as Claude Code and Gemini-cli. However, these\nAI agents differ fundamentally from traditional deterministic software, posing\na significant challenge to conventional monitoring and debugging. This creates\na critical semantic gap: existing tools observe either an agent's high-level\nintent (via LLM prompts) or its low-level actions (e.g., system calls), but\ncannot correlate these two views. This blindness makes it difficult to\ndistinguish between benign operations, malicious attacks, and costly failures.\nWe introduce AgentSight, an AgentOps observability framework that bridges this\nsemantic gap using a hybrid approach. Our approach, boundary tracing, monitors\nagents from outside their application code at stable system interfaces using\neBPF. AgentSight intercepts TLS-encrypted LLM traffic to extract semantic\nintent, monitors kernel events to observe system-wide effects, and causally\ncorrelates these two streams across process boundaries using a real-time engine\nand secondary LLM analysis. This instrumentation-free technique is\nframework-agnostic, resilient to rapid API changes, and incurs less than 3%\nperformance overhead. Our evaluation shows AgentSight detects prompt injection\nattacks, identifies resource-wasting reasoning loops, and reveals hidden\ncoordination bottlenecks in multi-agent systems. AgentSight is released as an\nopen-source project at https://github.com/agent-sight/agentsight."
                },
                "authors": [
                    {
                        "name": "Yusheng Zheng"
                    },
                    {
                        "name": "Yanpeng Hu"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Andi Quinn"
                    }
                ],
                "author_detail": {
                    "name": "Andi Quinn"
                },
                "author": "Andi Quinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02736v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02736v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11416v1",
                "updated": "2025-08-15T11:38:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    38,
                    19,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T11:38:19Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    38,
                    19,
                    4,
                    227,
                    0
                ],
                "title": "AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory\n  Manager",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory\n  Manager"
                },
                "summary": "Recent advances in mathematical reasoning and the long-term planning\ncapabilities of large language models (LLMs) have precipitated the development\nof agents, which are being increasingly leveraged in business operations\nprocesses. Decision models to optimize inventory levels are one of the core\nelements of operations management. However, the capabilities of the LLM agent\nin making inventory decisions in uncertain contexts, as well as the\ndecision-making biases (e.g. framing effect, etc.) of the agent, remain largely\nunexplored. This prompts concerns regarding the capacity of LLM agents to\neffectively address real-world problems, as well as the potential implications\nof biases that may be present. To address this gap, we introduce AIM-Bench, a\nnovel benchmark designed to assess the decision-making behaviour of LLM agents\nin uncertain supply chain management scenarios through a diverse series of\ninventory replenishment experiments. Our results reveal that different LLMs\ntypically exhibit varying degrees of decision bias that are similar to those\nobserved in human beings. In addition, we explored strategies to mitigate the\npull-to-centre effect and the bullwhip effect, namely cognitive reflection and\nimplementation of information sharing. These findings underscore the need for\ncareful consideration of the potential biases in deploying LLMs in Inventory\ndecision-making scenarios. We hope that these insights will pave the way for\nmitigating human decision bias and developing human-centred decision support\nsystems for supply chains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in mathematical reasoning and the long-term planning\ncapabilities of large language models (LLMs) have precipitated the development\nof agents, which are being increasingly leveraged in business operations\nprocesses. Decision models to optimize inventory levels are one of the core\nelements of operations management. However, the capabilities of the LLM agent\nin making inventory decisions in uncertain contexts, as well as the\ndecision-making biases (e.g. framing effect, etc.) of the agent, remain largely\nunexplored. This prompts concerns regarding the capacity of LLM agents to\neffectively address real-world problems, as well as the potential implications\nof biases that may be present. To address this gap, we introduce AIM-Bench, a\nnovel benchmark designed to assess the decision-making behaviour of LLM agents\nin uncertain supply chain management scenarios through a diverse series of\ninventory replenishment experiments. Our results reveal that different LLMs\ntypically exhibit varying degrees of decision bias that are similar to those\nobserved in human beings. In addition, we explored strategies to mitigate the\npull-to-centre effect and the bullwhip effect, namely cognitive reflection and\nimplementation of information sharing. These findings underscore the need for\ncareful consideration of the potential biases in deploying LLMs in Inventory\ndecision-making scenarios. We hope that these insights will pave the way for\nmitigating human decision bias and developing human-centred decision support\nsystems for supply chains."
                },
                "authors": [
                    {
                        "name": "Xuhua Zhao"
                    },
                    {
                        "name": "Yuxuan Xie"
                    },
                    {
                        "name": "Caihua Chen"
                    },
                    {
                        "name": "Yuxiang Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiang Sun"
                },
                "author": "Yuxiang Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11414v1",
                "updated": "2025-08-15T11:36:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    36,
                    17,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T11:36:17Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    36,
                    17,
                    4,
                    227,
                    0
                ],
                "title": "Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via\n  Survey Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via\n  Survey Questions"
                },
                "summary": "Large language models implicitly encode preferences over human values, yet\nsteering them often requires large training data. In this work, we investigate\na simple approach: Can we reliably modify a model's value system in downstream\nbehavior by training it to answer value survey questions accordingly? We first\nconstruct value profiles of several open-source LLMs by asking them to rate a\nseries of value-related descriptions spanning 20 distinct human values, which\nwe use as a baseline for subsequent experiments. We then investigate whether\nthe value system of a model can be governed by fine-tuning on the value\nsurveys. We evaluate the effect of finetuning on the model's behavior in two\nways; first, we assess how answers change on in-domain, held-out survey\nquestions. Second, we evaluate whether the model's behavior changes in\nout-of-domain settings (situational scenarios). To this end, we construct a\ncontextualized moral judgment dataset based on Reddit posts and evaluate\nchanges in the model's behavior in text-based adventure games. We demonstrate\nthat our simple approach can not only change the model's answers to in-domain\nsurvey questions, but also produces substantial shifts (value alignment) in\nimplicit downstream task behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models implicitly encode preferences over human values, yet\nsteering them often requires large training data. In this work, we investigate\na simple approach: Can we reliably modify a model's value system in downstream\nbehavior by training it to answer value survey questions accordingly? We first\nconstruct value profiles of several open-source LLMs by asking them to rate a\nseries of value-related descriptions spanning 20 distinct human values, which\nwe use as a baseline for subsequent experiments. We then investigate whether\nthe value system of a model can be governed by fine-tuning on the value\nsurveys. We evaluate the effect of finetuning on the model's behavior in two\nways; first, we assess how answers change on in-domain, held-out survey\nquestions. Second, we evaluate whether the model's behavior changes in\nout-of-domain settings (situational scenarios). To this end, we construct a\ncontextualized moral judgment dataset based on Reddit posts and evaluate\nchanges in the model's behavior in text-based adventure games. We demonstrate\nthat our simple approach can not only change the model's answers to in-domain\nsurvey questions, but also produces substantial shifts (value alignment) in\nimplicit downstream task behavior."
                },
                "authors": [
                    {
                        "name": "Shangrui Nie"
                    },
                    {
                        "name": "Florian Mai"
                    },
                    {
                        "name": "David Kaczér"
                    },
                    {
                        "name": "Charles Welch"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Lucie Flek"
                    }
                ],
                "author_detail": {
                    "name": "Lucie Flek"
                },
                "author": "Lucie Flek",
                "arxiv_comment": "7 pages 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11412v1",
                "updated": "2025-08-15T11:33:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    33,
                    3,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T11:33:03Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    33,
                    3,
                    4,
                    227,
                    0
                ],
                "title": "Towards Embodied Conversational Agents for Reducing Oral Exam Anxiety in\n  Extended Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Embodied Conversational Agents for Reducing Oral Exam Anxiety in\n  Extended Reality"
                },
                "summary": "Oral examinations are a prevalent but psychologically demanding form of\nassessment in higher education. Many students experience intense anxiety, which\ncan impair cognitive performance and hinder academic success. This position\npaper explores the potential of embodied conversational agents (ECAs) in\nextended reality (XR) environments to support students preparing for oral\nexams. We propose a system concept that integrates photorealistic ECAs with\nreal-time capable large language models (LLMs) to enable psychologically safe,\nadaptive, and repeatable rehearsal of oral examination scenarios. We also\ndiscuss the potential benefits and challenges of such an envisioned system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oral examinations are a prevalent but psychologically demanding form of\nassessment in higher education. Many students experience intense anxiety, which\ncan impair cognitive performance and hinder academic success. This position\npaper explores the potential of embodied conversational agents (ECAs) in\nextended reality (XR) environments to support students preparing for oral\nexams. We propose a system concept that integrates photorealistic ECAs with\nreal-time capable large language models (LLMs) to enable psychologically safe,\nadaptive, and repeatable rehearsal of oral examination scenarios. We also\ndiscuss the potential benefits and challenges of such an envisioned system."
                },
                "authors": [
                    {
                        "name": "Jens Grubert"
                    },
                    {
                        "name": "Yvonne Sedelmaier"
                    },
                    {
                        "name": "Dieter Landes"
                    }
                ],
                "author_detail": {
                    "name": "Dieter Landes"
                },
                "author": "Dieter Landes",
                "arxiv_comment": "Accepted to the IEEE ISMAR-Adjunct Proceedings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11409v1",
                "updated": "2025-08-15T11:20:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    20,
                    18,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T11:20:18Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    20,
                    18,
                    4,
                    227,
                    0
                ],
                "title": "RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator"
                },
                "summary": "Atmospheric turbulence severely degrades video quality by introducing\ndistortions such as geometric warping, blur, and temporal flickering, posing\nsignificant challenges to both visual clarity and temporal consistency. Current\nstate-of-the-art methods are based on transformer and 3D architectures and\nrequire multi-frame input, but their large computational cost and memory usage\nlimit real-time deployment, especially in resource-constrained scenarios. In\nthis work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric\nTurbulence Mitigator, designed for efficient and temporally consistent video\nrestoration under AT conditions. RMFAT adopts a lightweight recurrent framework\nthat restores each frame using only two inputs at a time, significantly\nreducing temporal window size and computational burden. It further integrates\nmulti-scale feature encoding and decoding with temporal warping modules at both\nencoder and decoder stages to enhance spatial detail and temporal coherence.\nExtensive experiments on synthetic and real-world atmospheric turbulence\ndatasets demonstrate that RMFAT not only outperforms existing methods in terms\nof clarity restoration (with nearly a 9\\% improvement in SSIM) but also\nachieves significantly improved inference speed (more than a fourfold reduction\nin runtime), making it particularly suitable for real-time atmospheric\nturbulence suppression tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atmospheric turbulence severely degrades video quality by introducing\ndistortions such as geometric warping, blur, and temporal flickering, posing\nsignificant challenges to both visual clarity and temporal consistency. Current\nstate-of-the-art methods are based on transformer and 3D architectures and\nrequire multi-frame input, but their large computational cost and memory usage\nlimit real-time deployment, especially in resource-constrained scenarios. In\nthis work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric\nTurbulence Mitigator, designed for efficient and temporally consistent video\nrestoration under AT conditions. RMFAT adopts a lightweight recurrent framework\nthat restores each frame using only two inputs at a time, significantly\nreducing temporal window size and computational burden. It further integrates\nmulti-scale feature encoding and decoding with temporal warping modules at both\nencoder and decoder stages to enhance spatial detail and temporal coherence.\nExtensive experiments on synthetic and real-world atmospheric turbulence\ndatasets demonstrate that RMFAT not only outperforms existing methods in terms\nof clarity restoration (with nearly a 9\\% improvement in SSIM) but also\nachieves significantly improved inference speed (more than a fourfold reduction\nin runtime), making it particularly suitable for real-time atmospheric\nturbulence suppression tasks."
                },
                "authors": [
                    {
                        "name": "Zhiming Liu"
                    },
                    {
                        "name": "Nantheera Anantrasirichai"
                    }
                ],
                "author_detail": {
                    "name": "Nantheera Anantrasirichai"
                },
                "author": "Nantheera Anantrasirichai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11408v1",
                "updated": "2025-08-15T11:20:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    20,
                    3,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T11:20:03Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    20,
                    3,
                    4,
                    227,
                    0
                ],
                "title": "On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised\n  Fine-Tuning and Reinforcement Learning via Dynamic Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised\n  Fine-Tuning and Reinforcement Learning via Dynamic Weighting"
                },
                "summary": "Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two\nprominent post-training paradigms for refining the capabilities and aligning\nthe behavior of Large Language Models (LLMs). Existing approaches that\nintegrate SFT and RL often face the risk of disrupting established model\npatterns and inducing overfitting to expert data. To address this, we present a\nnovel investigation into the unified view of SFT and RL through an off-policy\nversus on-policy lens. We propose CHORD, a framework for the Controllable\nHarmonization of On- and Off-Policy Reinforcement Learning via Dynamic\nWeighting, which reframes SFT not as a separate stage but as a dynamically\nweighted auxiliary objective within the on-policy RL process. Based on an\nanalysis of off-policy expert data's influence at both holistic and granular\nlevels, we incorporate a dual-control mechanism in CHORD. Specifically, the\nframework first employs a global coefficient to holistically guide the\ntransition from off-policy imitation to on-policy exploration, and then applies\na token-wise weighting function that enables granular learning from expert\ntokens, which preserves on-policy exploration and mitigates disruption from\noff-policy data. We conduct extensive experiments on widely used benchmarks,\nproviding empirical evidence that CHORD achieves a stable and efficient\nlearning process. By effectively harmonizing off-policy expert data with\non-policy exploration, CHORD demonstrates significant improvements over\nbaselines. We release the implementation at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to\ninspire further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two\nprominent post-training paradigms for refining the capabilities and aligning\nthe behavior of Large Language Models (LLMs). Existing approaches that\nintegrate SFT and RL often face the risk of disrupting established model\npatterns and inducing overfitting to expert data. To address this, we present a\nnovel investigation into the unified view of SFT and RL through an off-policy\nversus on-policy lens. We propose CHORD, a framework for the Controllable\nHarmonization of On- and Off-Policy Reinforcement Learning via Dynamic\nWeighting, which reframes SFT not as a separate stage but as a dynamically\nweighted auxiliary objective within the on-policy RL process. Based on an\nanalysis of off-policy expert data's influence at both holistic and granular\nlevels, we incorporate a dual-control mechanism in CHORD. Specifically, the\nframework first employs a global coefficient to holistically guide the\ntransition from off-policy imitation to on-policy exploration, and then applies\na token-wise weighting function that enables granular learning from expert\ntokens, which preserves on-policy exploration and mitigates disruption from\noff-policy data. We conduct extensive experiments on widely used benchmarks,\nproviding empirical evidence that CHORD achieves a stable and efficient\nlearning process. By effectively harmonizing off-policy expert data with\non-policy exploration, CHORD demonstrates significant improvements over\nbaselines. We release the implementation at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to\ninspire further research."
                },
                "authors": [
                    {
                        "name": "Wenhao Zhang"
                    },
                    {
                        "name": "Yuexiang Xie"
                    },
                    {
                        "name": "Yuchang Sun"
                    },
                    {
                        "name": "Yanxi Chen"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11401v1",
                "updated": "2025-08-15T11:10:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    10,
                    40,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T11:10:40Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    10,
                    40,
                    4,
                    227,
                    0
                ],
                "title": "FACET:Teacher-Centred LLM-Based Multi-Agent Systems-Towards Personalized\n  Educational Worksheets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FACET:Teacher-Centred LLM-Based Multi-Agent Systems-Towards Personalized\n  Educational Worksheets"
                },
                "summary": "The increasing heterogeneity of student populations poses significant\nchallenges for teachers, particularly in mathematics education, where\ncognitive, motivational, and emotional differences strongly influence learning\noutcomes. While AI-driven personalization tools have emerged, most remain\nperformance-focused, offering limited support for teachers and neglecting\nbroader pedagogical needs. This paper presents the FACET framework, a\nteacher-facing, large language model (LLM)-based multi-agent system designed to\ngenerate individualized classroom materials that integrate both cognitive and\nmotivational dimensions of learner profiles. The framework comprises three\nspecialized agents: (1) learner agents that simulate diverse profiles\nincorporating topic proficiency and intrinsic motivation, (2) a teacher agent\nthat adapts instructional content according to didactical principles, and (3)\nan evaluator agent that provides automated quality assurance. We tested the\nsystem using authentic grade 8 mathematics curriculum content and evaluated its\nfeasibility through a) automated agent-based assessment of output quality and\nb) exploratory feedback from K-12 in-service teachers. Results from ten\ninternal evaluations highlighted high stability and alignment between generated\nmaterials and learner profiles, and teacher feedback particularly highlighted\nstructure and suitability of tasks. The findings demonstrate the potential of\nmulti-agent LLM architectures to provide scalable, context-aware\npersonalization in heterogeneous classroom settings, and outline directions for\nextending the framework to richer learner profiles and real-world classroom\ntrials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing heterogeneity of student populations poses significant\nchallenges for teachers, particularly in mathematics education, where\ncognitive, motivational, and emotional differences strongly influence learning\noutcomes. While AI-driven personalization tools have emerged, most remain\nperformance-focused, offering limited support for teachers and neglecting\nbroader pedagogical needs. This paper presents the FACET framework, a\nteacher-facing, large language model (LLM)-based multi-agent system designed to\ngenerate individualized classroom materials that integrate both cognitive and\nmotivational dimensions of learner profiles. The framework comprises three\nspecialized agents: (1) learner agents that simulate diverse profiles\nincorporating topic proficiency and intrinsic motivation, (2) a teacher agent\nthat adapts instructional content according to didactical principles, and (3)\nan evaluator agent that provides automated quality assurance. We tested the\nsystem using authentic grade 8 mathematics curriculum content and evaluated its\nfeasibility through a) automated agent-based assessment of output quality and\nb) exploratory feedback from K-12 in-service teachers. Results from ten\ninternal evaluations highlighted high stability and alignment between generated\nmaterials and learner profiles, and teacher feedback particularly highlighted\nstructure and suitability of tasks. The findings demonstrate the potential of\nmulti-agent LLM architectures to provide scalable, context-aware\npersonalization in heterogeneous classroom settings, and outline directions for\nextending the framework to richer learner profiles and real-world classroom\ntrials."
                },
                "authors": [
                    {
                        "name": "Jana Gonnermann-Müller"
                    },
                    {
                        "name": "Jennifer Haase"
                    },
                    {
                        "name": "Konstantin Fackeldey"
                    },
                    {
                        "name": "Sebastian Pokutta"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pokutta"
                },
                "author": "Sebastian Pokutta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11398v1",
                "updated": "2025-08-15T11:08:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    8,
                    32,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T11:08:32Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    8,
                    32,
                    4,
                    227,
                    0
                ],
                "title": "Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling\n  and Explainable Mental Disorder Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling\n  and Explainable Mental Disorder Diagnosis"
                },
                "summary": "LLM-based agents have emerged as transformative tools capable of executing\ncomplex tasks through iterative planning and action, achieving significant\nadvancements in understanding and addressing user needs. Yet, their\neffectiveness remains limited in specialized domains such as mental health\ndiagnosis, where they underperform compared to general applications. Current\napproaches to integrating diagnostic capabilities into LLMs rely on scarce,\nhighly sensitive mental health datasets, which are challenging to acquire.\nThese methods also fail to emulate clinicians' proactive inquiry skills, lack\nmulti-turn conversational comprehension, and struggle to align outputs with\nexpert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the\nfirst LLM-based agent workflow designed to autonomously generate DSM-5 Level-1\ndiagnostic questionnaires. By simulating therapist-client dialogues with\nspecific client profiles, the framework delivers transparent, step-by-step\ndisorder predictions, producing explainable and trustworthy results. This\nworkflow serves as a complementary tool for mental health diagnosis, ensuring\nadherence to ethical and legal standards. Through comprehensive experiments, we\nevaluate leading LLMs across three critical dimensions: conversational realism,\ndiagnostic accuracy, and explainability. Our datasets and implementations are\nfully open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have emerged as transformative tools capable of executing\ncomplex tasks through iterative planning and action, achieving significant\nadvancements in understanding and addressing user needs. Yet, their\neffectiveness remains limited in specialized domains such as mental health\ndiagnosis, where they underperform compared to general applications. Current\napproaches to integrating diagnostic capabilities into LLMs rely on scarce,\nhighly sensitive mental health datasets, which are challenging to acquire.\nThese methods also fail to emulate clinicians' proactive inquiry skills, lack\nmulti-turn conversational comprehension, and struggle to align outputs with\nexpert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the\nfirst LLM-based agent workflow designed to autonomously generate DSM-5 Level-1\ndiagnostic questionnaires. By simulating therapist-client dialogues with\nspecific client profiles, the framework delivers transparent, step-by-step\ndisorder predictions, producing explainable and trustworthy results. This\nworkflow serves as a complementary tool for mental health diagnosis, ensuring\nadherence to ethical and legal standards. Through comprehensive experiments, we\nevaluate leading LLMs across three critical dimensions: conversational realism,\ndiagnostic accuracy, and explainability. Our datasets and implementations are\nfully open-sourced."
                },
                "authors": [
                    {
                        "name": "Mithat Can Ozgun"
                    },
                    {
                        "name": "Jiahuan Pei"
                    },
                    {
                        "name": "Koen Hindriks"
                    },
                    {
                        "name": "Lucia Donatelli"
                    },
                    {
                        "name": "Qingzhi Liu"
                    },
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Junxiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Junxiao Wang"
                },
                "author": "Junxiao Wang",
                "arxiv_comment": "Accepted by CIKM 2025 as a full paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10331v2",
                "updated": "2025-08-15T11:01:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    11,
                    1,
                    26,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-14T04:11:09Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    4,
                    11,
                    9,
                    3,
                    226,
                    0
                ],
                "title": "Synthesizing Evidence: Data-Pooling as a Tool for Treatment Selection in\n  Online Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Evidence: Data-Pooling as a Tool for Treatment Selection in\n  Online Experiments"
                },
                "summary": "Randomized experiments are the gold standard for causal inference but face\nsignificant challenges in business applications, including limited traffic\nallocation, the need for heterogeneous treatment effect estimation, and the\ncomplexity of managing overlapping experiments. These factors lead to high\nvariability in treatment effect estimates, making data-driven policy roll out\ndifficult. To address these issues, we introduce the data pooling treatment\nroll-out (DPTR) framework, which enhances policy roll-out by pooling data\nacross experiments rather than focusing narrowly on individual ones. DPTR can\neffectively accommodate both overlapping and non-overlapping traffic scenarios,\nregardless of linear or nonlinear model specifications. We demonstrate the\nframework's robustness through a three-pronged validation: (a) theoretical\nanalysis shows that DPTR surpasses the traditional difference-in-mean and\nordinary least squares methods under non-overlapping experiments, particularly\nwhen the number of experiments is large; (b) synthetic simulations confirm its\nadaptability in complex scenarios with overlapping traffic, rich covariates and\nnonlinear specifications; and (c) empirical applications to two experimental\ndatasets from real world platforms, demonstrating its effectiveness in guiding\ncustomized policy roll-outs for subgroups within a single experiment, as well\nas in coordinating policy deployments across multiple experiments with\noverlapping scenarios. By reducing estimation variability to improve\ndecision-making effectiveness, DPTR provides a scalable, practical solution for\nonline platforms to better leverage their experimental data in today's\nincreasingly complex business environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomized experiments are the gold standard for causal inference but face\nsignificant challenges in business applications, including limited traffic\nallocation, the need for heterogeneous treatment effect estimation, and the\ncomplexity of managing overlapping experiments. These factors lead to high\nvariability in treatment effect estimates, making data-driven policy roll out\ndifficult. To address these issues, we introduce the data pooling treatment\nroll-out (DPTR) framework, which enhances policy roll-out by pooling data\nacross experiments rather than focusing narrowly on individual ones. DPTR can\neffectively accommodate both overlapping and non-overlapping traffic scenarios,\nregardless of linear or nonlinear model specifications. We demonstrate the\nframework's robustness through a three-pronged validation: (a) theoretical\nanalysis shows that DPTR surpasses the traditional difference-in-mean and\nordinary least squares methods under non-overlapping experiments, particularly\nwhen the number of experiments is large; (b) synthetic simulations confirm its\nadaptability in complex scenarios with overlapping traffic, rich covariates and\nnonlinear specifications; and (c) empirical applications to two experimental\ndatasets from real world platforms, demonstrating its effectiveness in guiding\ncustomized policy roll-outs for subgroups within a single experiment, as well\nas in coordinating policy deployments across multiple experiments with\noverlapping scenarios. By reducing estimation variability to improve\ndecision-making effectiveness, DPTR provides a scalable, practical solution for\nonline platforms to better leverage their experimental data in today's\nincreasingly complex business environments."
                },
                "authors": [
                    {
                        "name": "Zhenkang Peng"
                    },
                    {
                        "name": "Chengzhang Li"
                    },
                    {
                        "name": "Ying Rong"
                    },
                    {
                        "name": "Renyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Renyu Zhang"
                },
                "author": "Renyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08837v2",
                "updated": "2025-08-15T10:48:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    48,
                    46,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-12T10:54:08Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    54,
                    8,
                    1,
                    224,
                    0
                ],
                "title": "The Roots of International Perceptions: Simulating US Attitude Changes\n  Towards China with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Roots of International Perceptions: Simulating US Attitude Changes\n  Towards China with LLM Agents"
                },
                "summary": "The rise of LLMs poses new possibilities in modeling opinion evolution, a\nlong-standing task in simulation, by leveraging advanced reasoning abilities to\nrecreate complex, large-scale human cognitive trends. While most prior works\nfocus on opinion evolution surrounding specific isolated events or the views\nwithin a country, ours is the first to model the large-scale attitude evolution\nof a population representing an entire country towards another -- US citizens'\nperspectives towards China. To tackle the challenges of this broad scenario, we\npropose a framework that integrates media data collection, user profile\ncreation, and cognitive architecture for opinion updates to successfully\nreproduce the real trend of US attitudes towards China over a 20-year period\nfrom 2005 to today. We also leverage LLMs' capabilities to introduce debiased\nmedia exposure, extracting neutral events from typically subjective news\ncontents, to uncover the roots of polarized opinion formation, as well as a\ndevils advocate agent to help explain the rare reversal from negative to\npositive attitudes towards China, corresponding with changes in the way\nAmericans obtain information about the country. The simulation results, beyond\nvalidating our framework architecture, also reveal the impact of biased framing\nand selection bias in shaping attitudes. Overall, our work contributes to a new\nparadigm for LLM-based modeling of cognitive behaviors in a large-scale,\nlong-term, cross-border social context, providing insights into the formation\nof international biases and offering valuable implications for media consumers\nto better understand the factors shaping their perspectives, and ultimately\ncontributing to the larger social need for bias reduction and cross-cultural\ntolerance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of LLMs poses new possibilities in modeling opinion evolution, a\nlong-standing task in simulation, by leveraging advanced reasoning abilities to\nrecreate complex, large-scale human cognitive trends. While most prior works\nfocus on opinion evolution surrounding specific isolated events or the views\nwithin a country, ours is the first to model the large-scale attitude evolution\nof a population representing an entire country towards another -- US citizens'\nperspectives towards China. To tackle the challenges of this broad scenario, we\npropose a framework that integrates media data collection, user profile\ncreation, and cognitive architecture for opinion updates to successfully\nreproduce the real trend of US attitudes towards China over a 20-year period\nfrom 2005 to today. We also leverage LLMs' capabilities to introduce debiased\nmedia exposure, extracting neutral events from typically subjective news\ncontents, to uncover the roots of polarized opinion formation, as well as a\ndevils advocate agent to help explain the rare reversal from negative to\npositive attitudes towards China, corresponding with changes in the way\nAmericans obtain information about the country. The simulation results, beyond\nvalidating our framework architecture, also reveal the impact of biased framing\nand selection bias in shaping attitudes. Overall, our work contributes to a new\nparadigm for LLM-based modeling of cognitive behaviors in a large-scale,\nlong-term, cross-border social context, providing insights into the formation\nof international biases and offering valuable implications for media consumers\nto better understand the factors shaping their perspectives, and ultimately\ncontributing to the larger social need for bias reduction and cross-cultural\ntolerance."
                },
                "authors": [
                    {
                        "name": "Nicholas Sukiennik"
                    },
                    {
                        "name": "Yichuan Xu"
                    },
                    {
                        "name": "Yuqing Kan"
                    },
                    {
                        "name": "Jinghua Piao"
                    },
                    {
                        "name": "Yuwei Yan"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "Submitted to AAAI Social Impact 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11386v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11386v1",
                "updated": "2025-08-15T10:38:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    38,
                    15,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T10:38:15Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    38,
                    15,
                    4,
                    227,
                    0
                ],
                "title": "Retrieval-augmented reasoning with lean language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented reasoning with lean language models"
                },
                "summary": "This technical report details a novel approach to combining reasoning and\nretrieval augmented generation (RAG) within a single, lean language model\narchitecture. While existing RAG systems typically rely on large-scale models\nand external APIs, our work addresses the increasing demand for performant and\nprivacy-preserving solutions deployable in resource-constrained or secure\nenvironments. Building on recent developments in test-time scaling and\nsmall-scale reasoning models, we develop a retrieval augmented conversational\nagent capable of interpreting complex, domain-specific queries using a\nlightweight backbone model. Our system integrates a dense retriever with\nfine-tuned Qwen2.5-Instruct models, using synthetic query generation and\nreasoning traces derived from frontier models (e.g., DeepSeek-R1) over a\ncurated corpus, in this case, the NHS A-to-Z condition pages. We explore the\nimpact of summarisation-based document compression, synthetic data design, and\nreasoning-aware fine-tuning on model performance. Evaluation against both\nnon-reasoning and general-purpose lean models demonstrates that our\ndomain-specific fine-tuning approach yields substantial gains in answer\naccuracy and consistency, approaching frontier-level performance while\nremaining feasible for local deployment. All implementation details and code\nare publicly released to support reproducibility and adaptation across domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report details a novel approach to combining reasoning and\nretrieval augmented generation (RAG) within a single, lean language model\narchitecture. While existing RAG systems typically rely on large-scale models\nand external APIs, our work addresses the increasing demand for performant and\nprivacy-preserving solutions deployable in resource-constrained or secure\nenvironments. Building on recent developments in test-time scaling and\nsmall-scale reasoning models, we develop a retrieval augmented conversational\nagent capable of interpreting complex, domain-specific queries using a\nlightweight backbone model. Our system integrates a dense retriever with\nfine-tuned Qwen2.5-Instruct models, using synthetic query generation and\nreasoning traces derived from frontier models (e.g., DeepSeek-R1) over a\ncurated corpus, in this case, the NHS A-to-Z condition pages. We explore the\nimpact of summarisation-based document compression, synthetic data design, and\nreasoning-aware fine-tuning on model performance. Evaluation against both\nnon-reasoning and general-purpose lean models demonstrates that our\ndomain-specific fine-tuning approach yields substantial gains in answer\naccuracy and consistency, approaching frontier-level performance while\nremaining feasible for local deployment. All implementation details and code\nare publicly released to support reproducibility and adaptation across domains."
                },
                "authors": [
                    {
                        "name": "Ryan Sze-Yin Chan"
                    },
                    {
                        "name": "Federico Nanni"
                    },
                    {
                        "name": "Tomas Lazauskas"
                    },
                    {
                        "name": "Rosie Wood"
                    },
                    {
                        "name": "Penelope Yong"
                    },
                    {
                        "name": "Lionel Tarassenko"
                    },
                    {
                        "name": "Mark Girolami"
                    },
                    {
                        "name": "James Geddes"
                    },
                    {
                        "name": "Andrew Duncan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Duncan"
                },
                "author": "Andrew Duncan",
                "arxiv_doi": "10.5281/zenodo.16408412",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.16408412",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.11386v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11386v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02269v2",
                "updated": "2025-08-15T10:37:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    37,
                    36,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-04T10:21:47Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    21,
                    47,
                    0,
                    216,
                    0
                ],
                "title": "AirTrafficGen: Configurable Air Traffic Scenario Generation with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirTrafficGen: Configurable Air Traffic Scenario Generation with Large\n  Language Models"
                },
                "summary": "The manual design of scenarios for Air Traffic Control (ATC) training is a\ndemanding and time-consuming bottleneck that limits the diversity of\nsimulations available to controllers. To address this, we introduce a novel,\nend-to-end approach, $\\texttt{AirTrafficGen}$, that leverages large language\nmodels (LLMs) to automate and control the generation of complex ATC scenarios.\nOur method uses a purpose-built, graph-based representation to encode sector\ntopology (including airspace geometry, routes, and fixes) into a format LLMs\ncan process. Through rigorous benchmarking, we show that state-of-the-art\nmodels like Gemini 2.5 Pro, OpenAI o3, GPT-oss-120b and GPT-5 can generate\nhigh-traffic scenarios while maintaining operational realism. Our engineered\nprompting enables fine-grained control over interaction presence, type, and\nlocation. Initial findings suggest these models are also capable of iterative\nrefinement, correcting flawed scenarios based on simple textual feedback. This\napproach provides a scalable alternative to manual scenario design, addressing\nthe need for a greater volume and variety of ATC training and validation\nsimulations. More broadly, this work showcases the potential of LLMs for\ncomplex planning in safety-critical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The manual design of scenarios for Air Traffic Control (ATC) training is a\ndemanding and time-consuming bottleneck that limits the diversity of\nsimulations available to controllers. To address this, we introduce a novel,\nend-to-end approach, $\\texttt{AirTrafficGen}$, that leverages large language\nmodels (LLMs) to automate and control the generation of complex ATC scenarios.\nOur method uses a purpose-built, graph-based representation to encode sector\ntopology (including airspace geometry, routes, and fixes) into a format LLMs\ncan process. Through rigorous benchmarking, we show that state-of-the-art\nmodels like Gemini 2.5 Pro, OpenAI o3, GPT-oss-120b and GPT-5 can generate\nhigh-traffic scenarios while maintaining operational realism. Our engineered\nprompting enables fine-grained control over interaction presence, type, and\nlocation. Initial findings suggest these models are also capable of iterative\nrefinement, correcting flawed scenarios based on simple textual feedback. This\napproach provides a scalable alternative to manual scenario design, addressing\nthe need for a greater volume and variety of ATC training and validation\nsimulations. More broadly, this work showcases the potential of LLMs for\ncomplex planning in safety-critical domains."
                },
                "authors": [
                    {
                        "name": "Dewi Sid William Gould"
                    },
                    {
                        "name": "George De Ath"
                    },
                    {
                        "name": "Ben Carvell"
                    },
                    {
                        "name": "Nick Pepper"
                    }
                ],
                "author_detail": {
                    "name": "Nick Pepper"
                },
                "author": "Nick Pepper",
                "arxiv_comment": "9 pages and appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11383v1",
                "updated": "2025-08-15T10:32:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    32,
                    50,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T10:32:50Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    32,
                    50,
                    4,
                    227,
                    0
                ],
                "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness\n  Methods for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness\n  Methods for LLMs"
                },
                "summary": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters."
                },
                "authors": [
                    {
                        "name": "Mikhail Seleznyov"
                    },
                    {
                        "name": "Mikhail Chaichuk"
                    },
                    {
                        "name": "Gleb Ershov"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Elena Tutubalina"
                    },
                    {
                        "name": "Oleg Somov"
                    }
                ],
                "author_detail": {
                    "name": "Oleg Somov"
                },
                "author": "Oleg Somov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11376v1",
                "updated": "2025-08-15T10:20:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    20,
                    29,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T10:20:29Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    10,
                    20,
                    29,
                    4,
                    227,
                    0
                ],
                "title": "Unified Knowledge Distillation Framework: Fine-Grained Alignment and\n  Geometric Relationship Preservation for Deep Face Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Knowledge Distillation Framework: Fine-Grained Alignment and\n  Geometric Relationship Preservation for Deep Face Recognition"
                },
                "summary": "Knowledge Distillation is crucial for optimizing face recognition models for\ndeployment in computationally limited settings, such as edge devices.\nTraditional KD methods, such as Raw L2 Feature Distillation or Feature\nConsistency loss, often fail to capture both fine-grained instance-level\ndetails and complex relational structures, leading to suboptimal performance.\nWe propose a unified approach that integrates two novel loss functions,\nInstance-Level Embedding Distillation and Relation-Based Pairwise Similarity\nDistillation. Instance-Level Embedding Distillation focuses on aligning\nindividual feature embeddings by leveraging a dynamic hard mining strategy,\nthereby enhancing learning from challenging examples. Relation-Based Pairwise\nSimilarity Distillation captures relational information through pairwise\nsimilarity relationships, employing a memory bank mechanism and a sample mining\nstrategy. This unified framework ensures both effective instance-level\nalignment and preservation of geometric relationships between samples, leading\nto a more comprehensive distillation process. Our unified framework outperforms\nstate-of-the-art distillation methods across multiple benchmark face\nrecognition datasets, as demonstrated by extensive experimental evaluations.\nInterestingly, when using strong teacher networks compared to the student, our\nunified KD enables the student to even surpass the teacher's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation is crucial for optimizing face recognition models for\ndeployment in computationally limited settings, such as edge devices.\nTraditional KD methods, such as Raw L2 Feature Distillation or Feature\nConsistency loss, often fail to capture both fine-grained instance-level\ndetails and complex relational structures, leading to suboptimal performance.\nWe propose a unified approach that integrates two novel loss functions,\nInstance-Level Embedding Distillation and Relation-Based Pairwise Similarity\nDistillation. Instance-Level Embedding Distillation focuses on aligning\nindividual feature embeddings by leveraging a dynamic hard mining strategy,\nthereby enhancing learning from challenging examples. Relation-Based Pairwise\nSimilarity Distillation captures relational information through pairwise\nsimilarity relationships, employing a memory bank mechanism and a sample mining\nstrategy. This unified framework ensures both effective instance-level\nalignment and preservation of geometric relationships between samples, leading\nto a more comprehensive distillation process. Our unified framework outperforms\nstate-of-the-art distillation methods across multiple benchmark face\nrecognition datasets, as demonstrated by extensive experimental evaluations.\nInterestingly, when using strong teacher networks compared to the student, our\nunified KD enables the student to even surpass the teacher's accuracy."
                },
                "authors": [
                    {
                        "name": "Durgesh Mishra"
                    },
                    {
                        "name": "Rishabh Uikey"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Uikey"
                },
                "author": "Rishabh Uikey",
                "arxiv_comment": "The paper spans a total of 14 pages, 10 pages for the main content\n  (including references) and 4 pages for the appendix. The main paper contains\n  3 figures and 1 table, while the appendix includes 1 pseudo-code algorithm\n  and 4 tables. The work was recently accepted for publication at IJCB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11364v1",
                "updated": "2025-08-15T09:59:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    59,
                    22,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T09:59:22Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    59,
                    22,
                    4,
                    227,
                    0
                ],
                "title": "Feedback Indicators: The Alignment between Llama and a Teacher in\n  Language Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback Indicators: The Alignment between Llama and a Teacher in\n  Language Learning"
                },
                "summary": "Automated feedback generation has the potential to enhance students' learning\nprogress by providing timely and targeted feedback. Moreover, it can assist\nteachers in optimizing their time, allowing them to focus on more strategic and\npersonalized aspects of teaching. To generate high-quality, information-rich\nformative feedback, it is essential first to extract relevant indicators, as\nthese serve as the foundation upon which the feedback is constructed. Teachers\noften employ feedback criteria grids composed of various indicators that they\nevaluate systematically. This study examines the initial phase of extracting\nsuch indicators from students' submissions of a language learning course using\nthe large language model Llama 3.1. Accordingly, the alignment between\nindicators generated by the LLM and human ratings across various feedback\ncriteria is investigated. The findings demonstrate statistically significant\nstrong correlations, even in cases involving unanticipated combinations of\nindicators and criteria. The methodology employed in this paper offers a\npromising foundation for extracting indicators from students' submissions using\nLLMs. Such indicators can potentially be utilized to auto-generate explainable\nand transparent formative feedback in future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated feedback generation has the potential to enhance students' learning\nprogress by providing timely and targeted feedback. Moreover, it can assist\nteachers in optimizing their time, allowing them to focus on more strategic and\npersonalized aspects of teaching. To generate high-quality, information-rich\nformative feedback, it is essential first to extract relevant indicators, as\nthese serve as the foundation upon which the feedback is constructed. Teachers\noften employ feedback criteria grids composed of various indicators that they\nevaluate systematically. This study examines the initial phase of extracting\nsuch indicators from students' submissions of a language learning course using\nthe large language model Llama 3.1. Accordingly, the alignment between\nindicators generated by the LLM and human ratings across various feedback\ncriteria is investigated. The findings demonstrate statistically significant\nstrong correlations, even in cases involving unanticipated combinations of\nindicators and criteria. The methodology employed in this paper offers a\npromising foundation for extracting indicators from students' submissions using\nLLMs. Such indicators can potentially be utilized to auto-generate explainable\nand transparent formative feedback in future research."
                },
                "authors": [
                    {
                        "name": "Sylvio Rüdian"
                    },
                    {
                        "name": "Yassin Elsir"
                    },
                    {
                        "name": "Marvin Kretschmer"
                    },
                    {
                        "name": "Sabine Cayrou"
                    },
                    {
                        "name": "Niels Pinkwart"
                    }
                ],
                "author_detail": {
                    "name": "Niels Pinkwart"
                },
                "author": "Niels Pinkwart",
                "arxiv_comment": "11 pages, one table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11359v1",
                "updated": "2025-08-15T09:54:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    54,
                    13,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T09:54:13Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    54,
                    13,
                    4,
                    227,
                    0
                ],
                "title": "Can We Tell if ChatGPT is a Parasite? Studying Human-AI Symbiosis with\n  Game Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Tell if ChatGPT is a Parasite? Studying Human-AI Symbiosis with\n  Game Theory"
                },
                "summary": "This work asks whether a human interacting with a generative AI system can\nmerge into a single individual through iterative, information-driven\ninteractions. We model the interactions between a human, a generative AI\nsystem, and the human's wider environment as a three-player stochastic game. We\nuse information-theoretic measures (entropy, mutual information, and transfer\nentropy) to show that our modelled human and generative AI are able to form an\naggregate individual in the sense of Krakauer et al. (2020). The model we\npresent is able to answer interesting questions around the symbiotic nature of\nhumans and AI systems, including whether LLM-driven chatbots are acting as\nparasites, feeding on the information provided by humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work asks whether a human interacting with a generative AI system can\nmerge into a single individual through iterative, information-driven\ninteractions. We model the interactions between a human, a generative AI\nsystem, and the human's wider environment as a three-player stochastic game. We\nuse information-theoretic measures (entropy, mutual information, and transfer\nentropy) to show that our modelled human and generative AI are able to form an\naggregate individual in the sense of Krakauer et al. (2020). The model we\npresent is able to answer interesting questions around the symbiotic nature of\nhumans and AI systems, including whether LLM-driven chatbots are acting as\nparasites, feeding on the information provided by humans."
                },
                "authors": [
                    {
                        "name": "Jiejun Hu-Bolz"
                    },
                    {
                        "name": "James Stovold"
                    }
                ],
                "author_detail": {
                    "name": "James Stovold"
                },
                "author": "James Stovold",
                "arxiv_comment": "8 pages, 6 figures, accepted in ALife 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11356v1",
                "updated": "2025-08-15T09:49:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    49,
                    14,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T09:49:14Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    49,
                    14,
                    4,
                    227,
                    0
                ],
                "title": "ETTRL: Balancing Exploration and Exploitation in LLM Test-Time\n  Reinforcement Learning Via Entropy Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETTRL: Balancing Exploration and Exploitation in LLM Test-Time\n  Reinforcement Learning Via Entropy Mechanism"
                },
                "summary": "Recent advancements in Large Language Models have yielded significant\nimprovements in complex reasoning tasks such as mathematics and programming.\nHowever, these models remain heavily dependent on annotated data and exhibit\nlimited adaptability in unsupervised scenarios. To address these limitations,\ntest-time reinforcement learning (TTRL) has been proposed, which enables\nself-optimization by leveraging model-generated pseudo-labels. Despite its\npromise, TTRL faces several key challenges, including high inference costs due\nto parallel rollouts and early-stage estimation bias that fosters\noverconfidence, reducing output diversity and causing performance plateaus. To\naddress these challenges, we introduce an entropy-based mechanism to enhance\nthe exploration-exploitation balance in test-time reinforcement learning\nthrough two strategies: Entropy-fork Tree Majority Rollout (ETMR) and\nEntropy-based Advantage Reshaping (EAR). Compared with the baseline, our\napproach enables Llama3.1-8B to achieve a 68 percent relative improvement in\nPass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of\nthe rollout tokens budget. This highlights our method's ability to effectively\noptimize the trade-off between inference efficiency, diversity, and estimation\nrobustness, thereby advancing unsupervised reinforcement learning for\nopen-domain reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models have yielded significant\nimprovements in complex reasoning tasks such as mathematics and programming.\nHowever, these models remain heavily dependent on annotated data and exhibit\nlimited adaptability in unsupervised scenarios. To address these limitations,\ntest-time reinforcement learning (TTRL) has been proposed, which enables\nself-optimization by leveraging model-generated pseudo-labels. Despite its\npromise, TTRL faces several key challenges, including high inference costs due\nto parallel rollouts and early-stage estimation bias that fosters\noverconfidence, reducing output diversity and causing performance plateaus. To\naddress these challenges, we introduce an entropy-based mechanism to enhance\nthe exploration-exploitation balance in test-time reinforcement learning\nthrough two strategies: Entropy-fork Tree Majority Rollout (ETMR) and\nEntropy-based Advantage Reshaping (EAR). Compared with the baseline, our\napproach enables Llama3.1-8B to achieve a 68 percent relative improvement in\nPass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of\nthe rollout tokens budget. This highlights our method's ability to effectively\noptimize the trade-off between inference efficiency, diversity, and estimation\nrobustness, thereby advancing unsupervised reinforcement learning for\nopen-domain reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "ChangYi He"
                    },
                    {
                        "name": "YingQiao Lin"
                    },
                    {
                        "name": "MingMin Yang"
                    },
                    {
                        "name": "FeiYang Shen"
                    },
                    {
                        "name": "ShaoGuo Liu"
                    },
                    {
                        "name": "TingTing Gao"
                    }
                ],
                "author_detail": {
                    "name": "TingTing Gao"
                },
                "author": "TingTing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16636v2",
                "updated": "2025-08-15T09:45:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    45,
                    15,
                    4,
                    227,
                    0
                ],
                "published": "2025-02-23T16:23:50Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    23,
                    50,
                    6,
                    54,
                    0
                ],
                "title": "Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation\n  for Visual Knowledge Intensive Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation\n  for Visual Knowledge Intensive Queries"
                },
                "summary": "Retrieval-augmented generation (RAG) is a paradigm that augments large\nlanguage models (LLMs) with external knowledge to tackle knowledge-intensive\nquestion answering. While several benchmarks evaluate Multimodal LLMs (MLLMs)\nunder Multimodal RAG settings, they predominantly retrieve from textual corpora\nand do not explicitly assess how models exploit visual evidence during\ngeneration. Consequently, there still lacks benchmark that isolates and\nmeasures the contribution of retrieved images in RAG. We introduce Visual-RAG,\na question-answering benchmark that targets visually grounded,\nknowledge-intensive questions. Unlike prior work, Visual-RAG requires\ntext-to-image retrieval and the integration of retrieved clue images to extract\nvisual evidence for answer generation. With Visual-RAG, we evaluate 5\nopen-source and 3 proprietary MLLMs, showcasing that images provide strong\nevidence in augmented generation. However, even state-of-the-art models\nstruggle to efficiently extract and utilize visual knowledge. Our results\nhighlight the need for improved visual retrieval, grounding, and attribution in\nmultimodal RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a paradigm that augments large\nlanguage models (LLMs) with external knowledge to tackle knowledge-intensive\nquestion answering. While several benchmarks evaluate Multimodal LLMs (MLLMs)\nunder Multimodal RAG settings, they predominantly retrieve from textual corpora\nand do not explicitly assess how models exploit visual evidence during\ngeneration. Consequently, there still lacks benchmark that isolates and\nmeasures the contribution of retrieved images in RAG. We introduce Visual-RAG,\na question-answering benchmark that targets visually grounded,\nknowledge-intensive questions. Unlike prior work, Visual-RAG requires\ntext-to-image retrieval and the integration of retrieved clue images to extract\nvisual evidence for answer generation. With Visual-RAG, we evaluate 5\nopen-source and 3 proprietary MLLMs, showcasing that images provide strong\nevidence in augmented generation. However, even state-of-the-art models\nstruggle to efficiently extract and utilize visual knowledge. Our results\nhighlight the need for improved visual retrieval, grounding, and attribution in\nmultimodal RAG systems."
                },
                "authors": [
                    {
                        "name": "Yin Wu"
                    },
                    {
                        "name": "Quanyu Long"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Jianfei Yu"
                    },
                    {
                        "name": "Wenya Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenya Wang"
                },
                "author": "Wenya Wang",
                "arxiv_comment": "21 pages, 6 figures, 17 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16502v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16502v4",
                "updated": "2025-08-15T09:41:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    41,
                    39,
                    4,
                    227,
                    0
                ],
                "published": "2024-10-21T20:48:16Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    20,
                    48,
                    16,
                    0,
                    295,
                    0
                ],
                "title": "RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic\n  and Human-like Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic\n  and Human-like Reasoning"
                },
                "summary": "Formal logic enables computers to reason in natural language by representing\nsentences in symbolic forms and applying rules to derive conclusions. However,\nin what our study characterizes as \"rulebreaker\" scenarios, this method can\nlead to conclusions that are typically not inferred or accepted by humans given\ntheir common sense and factual knowledge. Inspired by works in cognitive\nscience, we create RULEBREAKERS, the first dataset for rigorously evaluating\nthe ability of large language models (LLMs) to recognize and respond to\nrulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven\nLLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on\nRULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules\nunlike what is expected from typical human reasoners. Further analysis suggests\nthat this apparent failure is potentially associated with the models' poor\nutilization of their world knowledge and their attention distribution patterns.\nWhilst revealing a limitation of current LLMs, our study also provides a timely\ncounterbalance to a growing body of recent works that propose methods relying\non formal logic to improve LLMs' general reasoning capabilities, highlighting\ntheir risk of further increasing divergence between LLMs and human-like\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal logic enables computers to reason in natural language by representing\nsentences in symbolic forms and applying rules to derive conclusions. However,\nin what our study characterizes as \"rulebreaker\" scenarios, this method can\nlead to conclusions that are typically not inferred or accepted by humans given\ntheir common sense and factual knowledge. Inspired by works in cognitive\nscience, we create RULEBREAKERS, the first dataset for rigorously evaluating\nthe ability of large language models (LLMs) to recognize and respond to\nrulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven\nLLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on\nRULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules\nunlike what is expected from typical human reasoners. Further analysis suggests\nthat this apparent failure is potentially associated with the models' poor\nutilization of their world knowledge and their attention distribution patterns.\nWhilst revealing a limitation of current LLMs, our study also provides a timely\ncounterbalance to a growing body of recent works that propose methods relying\non formal logic to improve LLMs' general reasoning capabilities, highlighting\ntheir risk of further increasing divergence between LLMs and human-like\nreasoning."
                },
                "authors": [
                    {
                        "name": "Jason Chan"
                    },
                    {
                        "name": "Robert Gaizauskas"
                    },
                    {
                        "name": "Zhixue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhixue Zhao"
                },
                "author": "Zhixue Zhao",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16502v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16502v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13420v3",
                "updated": "2025-08-15T09:23:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    23,
                    48,
                    4,
                    227,
                    0
                ],
                "published": "2025-01-23T06:48:48Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    48,
                    48,
                    3,
                    23,
                    0
                ],
                "title": "LVFace: Progressive Cluster Optimization for Large Vision Models in Face\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVFace: Progressive Cluster Optimization for Large Vision Models in Face\n  Recognition"
                },
                "summary": "Vision Transformers (ViTs) have revolutionized large-scale visual modeling,\nyet remain underexplored in face recognition (FR) where CNNs still dominate. We\nidentify a critical bottleneck: CNN-inspired training paradigms fail to unlock\nViT's potential, leading to suboptimal performance and convergence\ninstability.To address this challenge, we propose LVFace, a ViT-based FR model\nthat integrates Progressive Cluster Optimization (PCO) to achieve superior\nresults. Specifically, PCO sequentially applies negative class sub-sampling\n(NCS) for robust and fast feature alignment from random initialization, feature\nexpectation penalties for centroid stabilization, performing cluster boundary\nrefinement through full-batch training without NCS constraints. LVFace\nestablishes a new state-of-the-art face recognition baseline, surpassing\nleading approaches such as UniFace and TopoFR across multiple benchmarks.\nExtensive experiments demonstrate that LVFace delivers consistent performance\ngains, while exhibiting scalability to large-scale datasets and compatibility\nwith mainstream VLMs and LLMs. Notably, LVFace secured 1st place in the ICCV\n2021 Masked Face Recognition (MFR)-Ongoing Challenge (March 2025), proving its\nefficacy in real-world scenarios. Project is available at\nhttps://github.com/bytedance/LVFace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have revolutionized large-scale visual modeling,\nyet remain underexplored in face recognition (FR) where CNNs still dominate. We\nidentify a critical bottleneck: CNN-inspired training paradigms fail to unlock\nViT's potential, leading to suboptimal performance and convergence\ninstability.To address this challenge, we propose LVFace, a ViT-based FR model\nthat integrates Progressive Cluster Optimization (PCO) to achieve superior\nresults. Specifically, PCO sequentially applies negative class sub-sampling\n(NCS) for robust and fast feature alignment from random initialization, feature\nexpectation penalties for centroid stabilization, performing cluster boundary\nrefinement through full-batch training without NCS constraints. LVFace\nestablishes a new state-of-the-art face recognition baseline, surpassing\nleading approaches such as UniFace and TopoFR across multiple benchmarks.\nExtensive experiments demonstrate that LVFace delivers consistent performance\ngains, while exhibiting scalability to large-scale datasets and compatibility\nwith mainstream VLMs and LLMs. Notably, LVFace secured 1st place in the ICCV\n2021 Masked Face Recognition (MFR)-Ongoing Challenge (March 2025), proving its\nefficacy in real-world scenarios. Project is available at\nhttps://github.com/bytedance/LVFace."
                },
                "authors": [
                    {
                        "name": "Jinghan You"
                    },
                    {
                        "name": "Shanglin Li"
                    },
                    {
                        "name": "Yuanrui Sun"
                    },
                    {
                        "name": "Jiangchuan Wei"
                    },
                    {
                        "name": "Mingyu Guo"
                    },
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Jiao Ran"
                    }
                ],
                "author_detail": {
                    "name": "Jiao Ran"
                },
                "author": "Jiao Ran",
                "arxiv_comment": "Accepted at ICCV25 as highlight paper, code released at\n  https://github.com/bytedance/LVFace",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11343v1",
                "updated": "2025-08-15T09:13:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    13,
                    42,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T09:13:42Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    13,
                    42,
                    4,
                    227,
                    0
                ],
                "title": "SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated\n  Text via Spectral Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated\n  Text via Spectral Analysis"
                },
                "summary": "The proliferation of high-quality text from Large Language Models (LLMs)\ndemands reliable and efficient detection methods. While existing training-free\napproaches show promise, they often rely on surface-level statistics and\noverlook fundamental signal properties of the text generation process. In this\nwork, we reframe detection as a signal processing problem, introducing a novel\nparadigm that analyzes the sequence of token log-probabilities in the frequency\ndomain. By systematically analyzing the signal's spectral properties using the\nglobal Discrete Fourier Transform (DFT) and the local Short-Time Fourier\nTransform (STFT), we find that human-written text consistently exhibits\nsignificantly higher spectral energy. This higher energy reflects the\nlarger-amplitude fluctuations inherent in human writing compared to the\nsuppressed dynamics of LLM-generated text. Based on this key insight, we\nconstruct SpecDetect, a detector built on a single, robust feature from the\nglobal DFT: DFT total energy. We also propose an enhanced version,\nSpecDetect++, which incorporates a sampling discrepancy mechanism to further\nboost robustness. Extensive experiments demonstrate that our approach\noutperforms the state-of-the-art model while running in nearly half the time.\nOur work introduces a new, efficient, and interpretable pathway for\nLLM-generated text detection, showing that classical signal processing\ntechniques offer a surprisingly powerful solution to this modern challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of high-quality text from Large Language Models (LLMs)\ndemands reliable and efficient detection methods. While existing training-free\napproaches show promise, they often rely on surface-level statistics and\noverlook fundamental signal properties of the text generation process. In this\nwork, we reframe detection as a signal processing problem, introducing a novel\nparadigm that analyzes the sequence of token log-probabilities in the frequency\ndomain. By systematically analyzing the signal's spectral properties using the\nglobal Discrete Fourier Transform (DFT) and the local Short-Time Fourier\nTransform (STFT), we find that human-written text consistently exhibits\nsignificantly higher spectral energy. This higher energy reflects the\nlarger-amplitude fluctuations inherent in human writing compared to the\nsuppressed dynamics of LLM-generated text. Based on this key insight, we\nconstruct SpecDetect, a detector built on a single, robust feature from the\nglobal DFT: DFT total energy. We also propose an enhanced version,\nSpecDetect++, which incorporates a sampling discrepancy mechanism to further\nboost robustness. Extensive experiments demonstrate that our approach\noutperforms the state-of-the-art model while running in nearly half the time.\nOur work introduces a new, efficient, and interpretable pathway for\nLLM-generated text detection, showing that classical signal processing\ntechniques offer a surprisingly powerful solution to this modern challenge."
                },
                "authors": [
                    {
                        "name": "Haitong Luo"
                    },
                    {
                        "name": "Weiyao Zhang"
                    },
                    {
                        "name": "Suhang Wang"
                    },
                    {
                        "name": "Wenji Zou"
                    },
                    {
                        "name": "Chungang Lin"
                    },
                    {
                        "name": "Xuying Meng"
                    },
                    {
                        "name": "Yujun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yujun Zhang"
                },
                "author": "Yujun Zhang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11342v1",
                "updated": "2025-08-15T09:12:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    12,
                    30,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T09:12:30Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    12,
                    30,
                    4,
                    227,
                    0
                ],
                "title": "CrossTrace: Efficient Cross-Thread and Cross-Service Span Correlation in\n  Distributed Tracing for Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrossTrace: Efficient Cross-Thread and Cross-Service Span Correlation in\n  Distributed Tracing for Microservices"
                },
                "summary": "Distributed tracing has become an essential technique for debugging and\ntroubleshooting modern microservice-based applications, enabling software\nengineers to detect performance bottlenecks, identify failures, and gain\ninsights into system behavior. However, implementing distributed tracing in\nlarge-scale applications remains challenging due to the need for extensive\ninstrumentation. To reduce this burden, zero-code instrumentation solutions,\nsuch as those based on eBPF, have emerged, allowing span data to be collected\nwithout modifying application code. Despite this promise, span correlation, the\nprocess of establishing causal relationships between spans, remains a critical\nchallenge in zero-code approaches. Existing solutions often rely on thread\naffinity, compromise system security by requiring the kernel integrity mode to\nbe disabled, or incur significant computational overhead due to complex\ninference algorithms. This paper presents CrossTrace, a practical and efficient\ndistributed tracing solution designed to support the debugging of microservice\napplications without requiring source code modifications. CrossTrace employs a\ngreedy algorithm to infer intra-service span relationships from delay patterns,\neliminating reliance on thread identifiers. For inter-service correlation,\nCrossTrace embeds span identifiers into TCP packet headers via eBPF, enabling\nsecure and efficient correlation compromising system security policies.\nEvaluation results show that CrossTrace can correlate thousands of spans within\nseconds with over 90% accuracy, making it suitable for production deployment\nand valuable for microservice observability and diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed tracing has become an essential technique for debugging and\ntroubleshooting modern microservice-based applications, enabling software\nengineers to detect performance bottlenecks, identify failures, and gain\ninsights into system behavior. However, implementing distributed tracing in\nlarge-scale applications remains challenging due to the need for extensive\ninstrumentation. To reduce this burden, zero-code instrumentation solutions,\nsuch as those based on eBPF, have emerged, allowing span data to be collected\nwithout modifying application code. Despite this promise, span correlation, the\nprocess of establishing causal relationships between spans, remains a critical\nchallenge in zero-code approaches. Existing solutions often rely on thread\naffinity, compromise system security by requiring the kernel integrity mode to\nbe disabled, or incur significant computational overhead due to complex\ninference algorithms. This paper presents CrossTrace, a practical and efficient\ndistributed tracing solution designed to support the debugging of microservice\napplications without requiring source code modifications. CrossTrace employs a\ngreedy algorithm to infer intra-service span relationships from delay patterns,\neliminating reliance on thread identifiers. For inter-service correlation,\nCrossTrace embeds span identifiers into TCP packet headers via eBPF, enabling\nsecure and efficient correlation compromising system security policies.\nEvaluation results show that CrossTrace can correlate thousands of spans within\nseconds with over 90% accuracy, making it suitable for production deployment\nand valuable for microservice observability and diagnosis."
                },
                "authors": [
                    {
                        "name": "Linh-An Phan"
                    },
                    {
                        "name": "MingXue Wang"
                    },
                    {
                        "name": "Guangyu Wu"
                    },
                    {
                        "name": "Wang Dawei"
                    },
                    {
                        "name": "Chen Liqun"
                    },
                    {
                        "name": "Li Jin"
                    }
                ],
                "author_detail": {
                    "name": "Li Jin"
                },
                "author": "Li Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11301v2",
                "updated": "2025-08-15T08:59:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    59,
                    23,
                    4,
                    227,
                    0
                ],
                "published": "2025-04-15T15:44:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    44,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Learning to Be A Doctor: Searching for Effective Medical Agent\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Be A Doctor: Searching for Effective Medical Agent\n  Architectures"
                },
                "summary": "Large Language Model (LLM)-based agents have demonstrated strong capabilities\nacross a wide range of tasks, and their application in the medical domain holds\nparticular promise due to the demand for high generalizability and reliance on\ninterdisciplinary knowledge. However, existing medical agent systems often rely\non static, manually crafted workflows that lack the flexibility to accommodate\ndiverse diagnostic requirements and adapt to emerging clinical scenarios.\nMotivated by the success of automated machine learning (AutoML), this paper\nintroduces a novel framework for the automated design of medical agent\narchitectures. Specifically, we define a hierarchical and expressive agent\nsearch space that enables dynamic workflow adaptation through structured\nmodifications at the node, structural, and framework levels. Our framework\nconceptualizes medical agents as graph-based architectures composed of diverse,\nfunctional node types and supports iterative self-improvement guided by\ndiagnostic feedback. Experimental results on skin disease diagnosis tasks\ndemonstrate that the proposed method effectively evolves workflow structures\nand significantly enhances diagnostic accuracy over time. This work represents\nthe first fully automated framework for medical agent architecture design and\noffers a scalable, adaptable foundation for deploying intelligent agents in\nreal-world clinical environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have demonstrated strong capabilities\nacross a wide range of tasks, and their application in the medical domain holds\nparticular promise due to the demand for high generalizability and reliance on\ninterdisciplinary knowledge. However, existing medical agent systems often rely\non static, manually crafted workflows that lack the flexibility to accommodate\ndiverse diagnostic requirements and adapt to emerging clinical scenarios.\nMotivated by the success of automated machine learning (AutoML), this paper\nintroduces a novel framework for the automated design of medical agent\narchitectures. Specifically, we define a hierarchical and expressive agent\nsearch space that enables dynamic workflow adaptation through structured\nmodifications at the node, structural, and framework levels. Our framework\nconceptualizes medical agents as graph-based architectures composed of diverse,\nfunctional node types and supports iterative self-improvement guided by\ndiagnostic feedback. Experimental results on skin disease diagnosis tasks\ndemonstrate that the proposed method effectively evolves workflow structures\nand significantly enhances diagnostic accuracy over time. This work represents\nthe first fully automated framework for medical agent architecture design and\noffers a scalable, adaptable foundation for deploying intelligent agents in\nreal-world clinical environments."
                },
                "authors": [
                    {
                        "name": "Yangyang Zhuang"
                    },
                    {
                        "name": "Wenjia Jiang"
                    },
                    {
                        "name": "Jiayu Zhang"
                    },
                    {
                        "name": "Ze Yang"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    },
                    {
                        "name": "Chi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Zhang"
                },
                "author": "Chi Zhang",
                "arxiv_comment": "Accepted at ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11326v1",
                "updated": "2025-08-15T08:53:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    53,
                    56,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T08:53:56Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    53,
                    56,
                    4,
                    227,
                    0
                ],
                "title": "MoE-TTS: Enhancing Out-of-Domain Text Understanding for\n  Description-based TTS via Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-TTS: Enhancing Out-of-Domain Text Understanding for\n  Description-based TTS via Mixture-of-Experts"
                },
                "summary": "Description-based text-to-speech (TTS) models exhibit strong performance on\nin-domain text descriptions, i.e., those encountered during training. However,\nin real-world applications, the diverse range of user-generated descriptions\ninevitably introduces numerous out-of-domain inputs that challenge the text\nunderstanding capabilities of these systems. To address this issue, we propose\nMoE-TTS, a description-based TTS model designed to enhance the understanding of\nout-of-domain text descriptions. MoE-TTS employs a modality-based\nmixture-of-experts (MoE) approach to augment a pre-trained textual large\nlanguage model (LLM) with a set of specialized weights adapted to the speech\nmodality while maintaining the original LLM frozen during training. This\napproach allows MoE-TTS to effectively leverage the pre-trained knowledge and\ntext understanding abilities of textual LLMs. Our experimental results indicate\nthat: first, even the most advanced closed-source commercial products can be\nchallenged by carefully designed out-of-domain description test sets; second,\nMoE-TTS achieves superior performance in generating speech that more accurately\nreflects the descriptions. We encourage readers to listen to the demos at\nhttps://welkinyang.github.io/MoE-TTS/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Description-based text-to-speech (TTS) models exhibit strong performance on\nin-domain text descriptions, i.e., those encountered during training. However,\nin real-world applications, the diverse range of user-generated descriptions\ninevitably introduces numerous out-of-domain inputs that challenge the text\nunderstanding capabilities of these systems. To address this issue, we propose\nMoE-TTS, a description-based TTS model designed to enhance the understanding of\nout-of-domain text descriptions. MoE-TTS employs a modality-based\nmixture-of-experts (MoE) approach to augment a pre-trained textual large\nlanguage model (LLM) with a set of specialized weights adapted to the speech\nmodality while maintaining the original LLM frozen during training. This\napproach allows MoE-TTS to effectively leverage the pre-trained knowledge and\ntext understanding abilities of textual LLMs. Our experimental results indicate\nthat: first, even the most advanced closed-source commercial products can be\nchallenged by carefully designed out-of-domain description test sets; second,\nMoE-TTS achieves superior performance in generating speech that more accurately\nreflects the descriptions. We encourage readers to listen to the demos at\nhttps://welkinyang.github.io/MoE-TTS/."
                },
                "authors": [
                    {
                        "name": "Heyang Xue"
                    },
                    {
                        "name": "Xuchen Song"
                    },
                    {
                        "name": "Yu Tang"
                    },
                    {
                        "name": "Jianyu Chen"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yahui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yahui Zhou"
                },
                "author": "Yahui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11318v1",
                "updated": "2025-08-15T08:41:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    41,
                    20,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T08:41:20Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    41,
                    20,
                    4,
                    227,
                    0
                ],
                "title": "LLM Compression: How Far Can We Go in Balancing Size and Performance?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Compression: How Far Can We Go in Balancing Size and Performance?"
                },
                "summary": "Quantization is an essential and popular technique for improving the\naccessibility of large language models (LLMs) by reducing memory usage and\ncomputational costs while maintaining performance. In this study, we apply\n4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer\nQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their\nimpact across multiple NLP tasks. We benchmark these models on MS MARCO\n(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K\n(Mathematical Reasoning) datasets, assessing both accuracy and efficiency\nacross various tasks. The study measures the trade-offs between model\ncompression and task performance, analyzing key evaluation metrics, namely\naccuracy, inference latency, and throughput (total output tokens generated per\nsecond), providing insights into the suitability of low-bit quantization for\nreal-world deployment. Using the results, users can then make suitable\ndecisions based on the specifications that need to be met. We discuss the pros\nand cons of GSQ and GPTQ techniques on models of different sizes, which also\nserve as a benchmark for future experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is an essential and popular technique for improving the\naccessibility of large language models (LLMs) by reducing memory usage and\ncomputational costs while maintaining performance. In this study, we apply\n4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer\nQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their\nimpact across multiple NLP tasks. We benchmark these models on MS MARCO\n(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K\n(Mathematical Reasoning) datasets, assessing both accuracy and efficiency\nacross various tasks. The study measures the trade-offs between model\ncompression and task performance, analyzing key evaluation metrics, namely\naccuracy, inference latency, and throughput (total output tokens generated per\nsecond), providing insights into the suitability of low-bit quantization for\nreal-world deployment. Using the results, users can then make suitable\ndecisions based on the specifications that need to be met. We discuss the pros\nand cons of GSQ and GPTQ techniques on models of different sizes, which also\nserve as a benchmark for future experiments."
                },
                "authors": [
                    {
                        "name": "Sahil Sk"
                    },
                    {
                        "name": "Debasish Dhal"
                    },
                    {
                        "name": "Sonal Khosla"
                    },
                    {
                        "name": "Sk Shahid"
                    },
                    {
                        "name": "Sambit Shekhar"
                    },
                    {
                        "name": "Akash Dhaka"
                    },
                    {
                        "name": "Shantipriya Parida"
                    },
                    {
                        "name": "Dilip K. Prasad"
                    },
                    {
                        "name": "Ondřej Bojar"
                    }
                ],
                "author_detail": {
                    "name": "Ondřej Bojar"
                },
                "author": "Ondřej Bojar",
                "arxiv_comment": "This paper has been accepted for presentation at the RANLP 2025\n  conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11310v1",
                "updated": "2025-08-15T08:27:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    27,
                    58,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T08:27:58Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    27,
                    58,
                    4,
                    227,
                    0
                ],
                "title": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced\n  Benchmark for Automatic Survey Generation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced\n  Benchmark for Automatic Survey Generation Systems"
                },
                "summary": "The growing interest in automatic survey generation (ASG), a task that\ntraditionally required considerable time and effort, has been spurred by recent\nadvances in large language models (LLMs). With advancements in\nretrieval-augmented generation (RAG) and the rising popularity of multi-agent\nsystems (MASs), synthesizing academic surveys using LLMs has become a viable\napproach, thereby elevating the need for robust evaluation methods in this\ndomain. However, existing evaluation methods suffer from several limitations,\nincluding biased metrics, a lack of human preference, and an over-reliance on\nLLMs-as-judges. To address these challenges, we propose SGSimEval, a\ncomprehensive benchmark for Survey Generation with Similarity-Enhanced\nEvaluation that evaluates automatic survey generation systems by integrating\nassessments of the outline, content, and references, and also combines\nLLM-based scoring with quantitative metrics to provide a multifaceted\nevaluation framework. In SGSimEval, we also introduce human preference metrics\nthat emphasize both inherent quality and similarity to humans. Extensive\nexperiments reveal that current ASG systems demonstrate human-comparable\nsuperiority in outline generation, while showing significant room for\nimprovement in content and reference generation, and our evaluation metrics\nmaintain strong consistency with human assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in automatic survey generation (ASG), a task that\ntraditionally required considerable time and effort, has been spurred by recent\nadvances in large language models (LLMs). With advancements in\nretrieval-augmented generation (RAG) and the rising popularity of multi-agent\nsystems (MASs), synthesizing academic surveys using LLMs has become a viable\napproach, thereby elevating the need for robust evaluation methods in this\ndomain. However, existing evaluation methods suffer from several limitations,\nincluding biased metrics, a lack of human preference, and an over-reliance on\nLLMs-as-judges. To address these challenges, we propose SGSimEval, a\ncomprehensive benchmark for Survey Generation with Similarity-Enhanced\nEvaluation that evaluates automatic survey generation systems by integrating\nassessments of the outline, content, and references, and also combines\nLLM-based scoring with quantitative metrics to provide a multifaceted\nevaluation framework. In SGSimEval, we also introduce human preference metrics\nthat emphasize both inherent quality and similarity to humans. Extensive\nexperiments reveal that current ASG systems demonstrate human-comparable\nsuperiority in outline generation, while showing significant room for\nimprovement in content and reference generation, and our evaluation metrics\nmaintain strong consistency with human assessments."
                },
                "authors": [
                    {
                        "name": "Beichen Guo"
                    },
                    {
                        "name": "Zhiyuan Wen"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Ruosong Yang"
                    },
                    {
                        "name": "Jiaxing Shen"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxing Shen"
                },
                "author": "Jiaxing Shen",
                "arxiv_comment": "Accepted to The 21st International Conference on Advanced Data Mining\n  and Applications (ADMA2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11305v1",
                "updated": "2025-08-15T08:20:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    20,
                    9,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T08:20:09Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    20,
                    9,
                    4,
                    227,
                    0
                ],
                "title": "Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and\n  Reasoning"
                },
                "summary": "Logging code is written by developers to capture system runtime behavior and\nplays a vital role in debugging, performance analysis, and system monitoring.\nHowever, defects in logging code can undermine the usefulness of logs and lead\nto misinterpretations. Although prior work has identified several logging\ndefect patterns and provided valuable insights into logging practices, these\nstudies often focus on a narrow range of defect patterns derived from limited\nsources (e.g., commit histories) and lack a systematic and comprehensive\nanalysis. Moreover, large language models (LLMs) have demonstrated promising\ngeneralization and reasoning capabilities across a variety of code-related\ntasks, yet their potential for detecting logging code defects remains largely\nunexplored.\n  In this paper, we derive a comprehensive taxonomy of logging code defects,\nwhich encompasses seven logging code defect patterns with 14 detailed\nscenarios. We further construct a benchmark dataset, \\dataset, consisting of\n164 developer-verified real-world logging defects. Then we propose an automated\nframework that leverages various prompting strategies and contextual\ninformation to evaluate LLMs' capability in detecting and reasoning logging\ncode defects. Experimental results reveal that LLMs generally struggle to\naccurately detect and reason logging code defects based on the source code\nonly. However, incorporating proper knowledge (e.g., detailed scenarios of\ndefect patterns) can lead to 10.9\\% improvement in detection accuracy. Overall,\nour findings provide actionable guidance for practitioners to avoid common\ndefect patterns and establish a foundation for improving LLM-based reasoning in\nlogging code defect detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logging code is written by developers to capture system runtime behavior and\nplays a vital role in debugging, performance analysis, and system monitoring.\nHowever, defects in logging code can undermine the usefulness of logs and lead\nto misinterpretations. Although prior work has identified several logging\ndefect patterns and provided valuable insights into logging practices, these\nstudies often focus on a narrow range of defect patterns derived from limited\nsources (e.g., commit histories) and lack a systematic and comprehensive\nanalysis. Moreover, large language models (LLMs) have demonstrated promising\ngeneralization and reasoning capabilities across a variety of code-related\ntasks, yet their potential for detecting logging code defects remains largely\nunexplored.\n  In this paper, we derive a comprehensive taxonomy of logging code defects,\nwhich encompasses seven logging code defect patterns with 14 detailed\nscenarios. We further construct a benchmark dataset, \\dataset, consisting of\n164 developer-verified real-world logging defects. Then we propose an automated\nframework that leverages various prompting strategies and contextual\ninformation to evaluate LLMs' capability in detecting and reasoning logging\ncode defects. Experimental results reveal that LLMs generally struggle to\naccurately detect and reason logging code defects based on the source code\nonly. However, incorporating proper knowledge (e.g., detailed scenarios of\ndefect patterns) can lead to 10.9\\% improvement in detection accuracy. Overall,\nour findings provide actionable guidance for practitioners to avoid common\ndefect patterns and establish a foundation for improving LLM-based reasoning in\nlogging code defect detection."
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Zhenhao Li"
                    },
                    {
                        "name": "Zishuo Ding"
                    }
                ],
                "author_detail": {
                    "name": "Zishuo Ding"
                },
                "author": "Zishuo Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11736v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11736v2",
                "updated": "2025-08-15T08:12:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    12,
                    16,
                    4,
                    227,
                    0
                ],
                "published": "2024-12-16T12:57:19Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    57,
                    19,
                    0,
                    351,
                    0
                ],
                "title": "Personalized LLM for Generating Customized Responses to the Same Query\n  from Different Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized LLM for Generating Customized Responses to the Same Query\n  from Different Users"
                },
                "summary": "Existing work on large language model (LLM) personalization assigned\ndifferent responding roles to LLMs, but overlooked the diversity of queriers.\nIn this work, we propose a new form of querier-aware LLM personalization,\ngenerating different responses even for the same query from different queriers.\nWe design a dual-tower model architecture with a cross-querier general encoder\nand a querier-specific encoder. We further apply contrastive learning with\nmulti-view augmentation, pulling close the dialogue representations of the same\nquerier, while pulling apart those of different queriers. To mitigate the\nimpact of query diversity on querier-contrastive learning, we cluster the\ndialogues based on query similarity and restrict the scope of contrastive\nlearning within each cluster. To address the lack of datasets designed for\nquerier-aware personalization, we also build a multi-querier dataset from\nEnglish and Chinese scripts, as well as WeChat records, called MQDialog,\ncontaining 173 queriers and 12 responders. Extensive evaluations demonstrate\nthat our design significantly improves the quality of personalized response\ngeneration, achieving relative improvement of 8.4% to 48.7% in ROUGE-L scores\nand winning rates ranging from 54% to 82% compared with various baseline\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing work on large language model (LLM) personalization assigned\ndifferent responding roles to LLMs, but overlooked the diversity of queriers.\nIn this work, we propose a new form of querier-aware LLM personalization,\ngenerating different responses even for the same query from different queriers.\nWe design a dual-tower model architecture with a cross-querier general encoder\nand a querier-specific encoder. We further apply contrastive learning with\nmulti-view augmentation, pulling close the dialogue representations of the same\nquerier, while pulling apart those of different queriers. To mitigate the\nimpact of query diversity on querier-contrastive learning, we cluster the\ndialogues based on query similarity and restrict the scope of contrastive\nlearning within each cluster. To address the lack of datasets designed for\nquerier-aware personalization, we also build a multi-querier dataset from\nEnglish and Chinese scripts, as well as WeChat records, called MQDialog,\ncontaining 173 queriers and 12 responders. Extensive evaluations demonstrate\nthat our design significantly improves the quality of personalized response\ngeneration, achieving relative improvement of 8.4% to 48.7% in ROUGE-L scores\nand winning rates ranging from 54% to 82% compared with various baseline\nmethods."
                },
                "authors": [
                    {
                        "name": "Hang Zeng"
                    },
                    {
                        "name": "Chaoyue Niu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Accepted by CIKM'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11736v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11736v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10016v2",
                "updated": "2025-08-15T08:09:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    9,
                    53,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-06T16:17:29Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    17,
                    29,
                    2,
                    218,
                    0
                ],
                "title": "Training-Free Multimodal Large Language Model Orchestration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Multimodal Large Language Model Orchestration"
                },
                "summary": "Different Multimodal Large Language Models (MLLMs) cannot be integrated into\na unified multimodal input-output system directly. In previous work, training\nhas been considered as an inevitable component due to challenges in modal\nalignment, Text-to-Speech efficiency and other integration issues. In this\npaper, we introduce Multimodal Large Language Model Orchestration, an effective\napproach for creating interactive multimodal AI systems without additional\ntraining. MLLM Orchestration leverages the inherent reasoning capabilities of\nlarge language models to coordinate specialized models through explicit\nworkflows, enabling natural multimodal interactions while maintaining\nmodularity, improving interpretability, and significantly enhancing\ncomputational efficiency. Our orchestration framework is built upon three key\ninnovations: (1) a central controller LLM that analyzes user inputs and\ndynamically routes tasks to appropriate specialized models through carefully\ndesigned agents; (2) a parallel Text-to-Speech architecture that enables true\nfull-duplex interaction with seamless interruption handling and natural\nconversational flow; and (3) a cross-modal memory integration system that\nmaintains coherent context across modalities through intelligent information\nsynthesis and retrieval, selectively avoiding unnecessary modality calls in\ncertain scenarios to improve response speed. Extensive evaluations demonstrate\nthat MLLM Orchestration achieves comprehensive multimodal capabilities without\nadditional training, performance improvements of up to 7.8% over traditional\njointly-trained approaches on standard benchmarks, reduced latency by 10.3%,\nand significantly enhanced interpretability through explicit orchestration\nprocesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Different Multimodal Large Language Models (MLLMs) cannot be integrated into\na unified multimodal input-output system directly. In previous work, training\nhas been considered as an inevitable component due to challenges in modal\nalignment, Text-to-Speech efficiency and other integration issues. In this\npaper, we introduce Multimodal Large Language Model Orchestration, an effective\napproach for creating interactive multimodal AI systems without additional\ntraining. MLLM Orchestration leverages the inherent reasoning capabilities of\nlarge language models to coordinate specialized models through explicit\nworkflows, enabling natural multimodal interactions while maintaining\nmodularity, improving interpretability, and significantly enhancing\ncomputational efficiency. Our orchestration framework is built upon three key\ninnovations: (1) a central controller LLM that analyzes user inputs and\ndynamically routes tasks to appropriate specialized models through carefully\ndesigned agents; (2) a parallel Text-to-Speech architecture that enables true\nfull-duplex interaction with seamless interruption handling and natural\nconversational flow; and (3) a cross-modal memory integration system that\nmaintains coherent context across modalities through intelligent information\nsynthesis and retrieval, selectively avoiding unnecessary modality calls in\ncertain scenarios to improve response speed. Extensive evaluations demonstrate\nthat MLLM Orchestration achieves comprehensive multimodal capabilities without\nadditional training, performance improvements of up to 7.8% over traditional\njointly-trained approaches on standard benchmarks, reduced latency by 10.3%,\nand significantly enhanced interpretability through explicit orchestration\nprocesses."
                },
                "authors": [
                    {
                        "name": "Tianyu Xie"
                    },
                    {
                        "name": "Yuhang Wu"
                    },
                    {
                        "name": "Yongdong Luo"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Xiawu Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Xiawu Zheng"
                },
                "author": "Xiawu Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18932v2",
                "updated": "2025-08-15T08:08:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    8,
                    8,
                    0,
                    4,
                    227,
                    0
                ],
                "published": "2025-07-25T03:58:07Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    3,
                    58,
                    7,
                    4,
                    206,
                    0
                ],
                "title": "MMESGBench: Pioneering Multimodal Understanding and Complex Reasoning\n  Benchmark for ESG Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMESGBench: Pioneering Multimodal Understanding and Complex Reasoning\n  Benchmark for ESG Tasks"
                },
                "summary": "Environmental, Social, and Governance (ESG) reports are essential for\nevaluating sustainability practices, ensuring regulatory compliance, and\npromoting financial transparency. However, these documents are often lengthy,\nstructurally diverse, and multimodal, comprising dense text, structured tables,\ncomplex figures, and layout-dependent semantics. Existing AI systems often\nstruggle to perform reliable document-level reasoning in such settings, and no\ndedicated benchmark currently exists in ESG domain. To fill the gap, we\nintroduce \\textbf{MMESGBench}, a first-of-its-kind benchmark dataset targeted\nto evaluate multimodal understanding and complex reasoning across structurally\ndiverse and multi-source ESG documents. This dataset is constructed via a\nhuman-AI collaborative, multi-stage pipeline. First, a multimodal LLM generates\ncandidate question-answer (QA) pairs by jointly interpreting rich textual,\ntabular, and visual information from layout-aware document pages. Second, an\nLLM verifies the semantic accuracy, completeness, and reasoning complexity of\neach QA pair. This automated process is followed by an expert-in-the-loop\nvalidation, where domain specialists validate and calibrate QA pairs to ensure\nquality, relevance, and diversity. MMESGBench comprises 933 validated QA pairs\nderived from 45 ESG documents, spanning across seven distinct document types\nand three major ESG source categories. Questions are categorized as\nsingle-page, cross-page, or unanswerable, with each accompanied by fine-grained\nmultimodal evidence. Initial experiments validate that multimodal and\nretrieval-augmented models substantially outperform text-only baselines,\nparticularly on visually grounded and cross-page tasks. MMESGBench is publicly\navailable as an open-source dataset at\nhttps://github.com/Zhanglei1103/MMESGBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environmental, Social, and Governance (ESG) reports are essential for\nevaluating sustainability practices, ensuring regulatory compliance, and\npromoting financial transparency. However, these documents are often lengthy,\nstructurally diverse, and multimodal, comprising dense text, structured tables,\ncomplex figures, and layout-dependent semantics. Existing AI systems often\nstruggle to perform reliable document-level reasoning in such settings, and no\ndedicated benchmark currently exists in ESG domain. To fill the gap, we\nintroduce \\textbf{MMESGBench}, a first-of-its-kind benchmark dataset targeted\nto evaluate multimodal understanding and complex reasoning across structurally\ndiverse and multi-source ESG documents. This dataset is constructed via a\nhuman-AI collaborative, multi-stage pipeline. First, a multimodal LLM generates\ncandidate question-answer (QA) pairs by jointly interpreting rich textual,\ntabular, and visual information from layout-aware document pages. Second, an\nLLM verifies the semantic accuracy, completeness, and reasoning complexity of\neach QA pair. This automated process is followed by an expert-in-the-loop\nvalidation, where domain specialists validate and calibrate QA pairs to ensure\nquality, relevance, and diversity. MMESGBench comprises 933 validated QA pairs\nderived from 45 ESG documents, spanning across seven distinct document types\nand three major ESG source categories. Questions are categorized as\nsingle-page, cross-page, or unanswerable, with each accompanied by fine-grained\nmultimodal evidence. Initial experiments validate that multimodal and\nretrieval-augmented models substantially outperform text-only baselines,\nparticularly on visually grounded and cross-page tasks. MMESGBench is publicly\navailable as an open-source dataset at\nhttps://github.com/Zhanglei1103/MMESGBench."
                },
                "authors": [
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Chaoyue He"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Chunyan Miao"
                    }
                ],
                "author_detail": {
                    "name": "Chunyan Miao"
                },
                "author": "Chunyan Miao",
                "arxiv_comment": "Accepted at ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11291v1",
                "updated": "2025-08-15T07:55:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:55:05Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "title": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks"
                },
                "summary": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks."
                },
                "authors": [
                    {
                        "name": "Rui Bao"
                    },
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Chen"
                },
                "author": "Zhiyong Chen",
                "arxiv_comment": "accepted by IEEE/CIC ICCC workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11290v1",
                "updated": "2025-08-15T07:54:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    54,
                    42,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:54:42Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    54,
                    42,
                    4,
                    227,
                    0
                ],
                "title": "SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through\n  Task-Specific Trajectory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through\n  Task-Specific Trajectory"
                },
                "summary": "LLMs increasingly exhibit over-refusal behavior, where safety mechanisms\ncause models to reject benign instructions that superficially resemble harmful\ncontent. This phenomena diminishes utility in production applications that\nrepeatedly rely on common prompt templates or applications that frequently rely\non LLMs for specific tasks (e.g. sentiment analysis, language translation).\nThrough comprehensive evaluation, we demonstrate that LLMs still tend to refuse\nresponses to harmful instructions when those instructions are reframed to\nappear as benign tasks. Our mechanistic analysis reveal that LLMs follow\ndistinct \"constellation\" patterns in embedding space as representations\ntraverse layers, with each task maintaining consistent trajectories that shift\npredictably between refusal and non-refusal cases. We introduce\nSafeConstellations, an inference-time trajectory-shifting approach that tracks\ntask-specific trajectory patterns and guides representations toward non-refusal\npathways. By selectively guiding model behavior only on tasks prone to\nover-refusal, and by preserving general model behavior, our method reduces\nover-refusal rates by up to 73% with minimal impact on utility-offering a\nprincipled approach to mitigating over-refusals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs increasingly exhibit over-refusal behavior, where safety mechanisms\ncause models to reject benign instructions that superficially resemble harmful\ncontent. This phenomena diminishes utility in production applications that\nrepeatedly rely on common prompt templates or applications that frequently rely\non LLMs for specific tasks (e.g. sentiment analysis, language translation).\nThrough comprehensive evaluation, we demonstrate that LLMs still tend to refuse\nresponses to harmful instructions when those instructions are reframed to\nappear as benign tasks. Our mechanistic analysis reveal that LLMs follow\ndistinct \"constellation\" patterns in embedding space as representations\ntraverse layers, with each task maintaining consistent trajectories that shift\npredictably between refusal and non-refusal cases. We introduce\nSafeConstellations, an inference-time trajectory-shifting approach that tracks\ntask-specific trajectory patterns and guides representations toward non-refusal\npathways. By selectively guiding model behavior only on tasks prone to\nover-refusal, and by preserving general model behavior, our method reduces\nover-refusal rates by up to 73% with minimal impact on utility-offering a\nprincipled approach to mitigating over-refusals."
                },
                "authors": [
                    {
                        "name": "Utsav Maskey"
                    },
                    {
                        "name": "Sumit Yadav"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11287v1",
                "updated": "2025-08-15T07:49:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    49,
                    22,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:49:22Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    49,
                    22,
                    4,
                    227,
                    0
                ],
                "title": "CSGO: Generalized Optimization for Cold Start in Wireless Collaborative\n  Edge LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSGO: Generalized Optimization for Cold Start in Wireless Collaborative\n  Edge LLM Systems"
                },
                "summary": "While deploying large language models on edge devices promises low-latency\nand privacy-preserving AI services, it is hindered by limited device resources.\nAlthough pipeline parallelism facilitates distributed inference, existing\napproaches often ignore the cold-start latency caused by on-demand model\nloading. In this paper, we propose a latency-aware scheduling framework that\noverlaps model loading with computation and communication to minimize total\ninference latency. Based on device and model parameters, the framework\ndynamically adjusts layer partitioning and allocation to effectively hide\nloading time, thereby eliminating as many idle periods as possible. We\nformulate the problem as a Mixed-Integer Non-Linear Program and design an\nefficient dynamic programming algorithm to optimize model partitioning and\ndevice assignment. Experimental results show that the proposed method\nsignificantly reduces cold-start latency compared to baseline strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While deploying large language models on edge devices promises low-latency\nand privacy-preserving AI services, it is hindered by limited device resources.\nAlthough pipeline parallelism facilitates distributed inference, existing\napproaches often ignore the cold-start latency caused by on-demand model\nloading. In this paper, we propose a latency-aware scheduling framework that\noverlaps model loading with computation and communication to minimize total\ninference latency. Based on device and model parameters, the framework\ndynamically adjusts layer partitioning and allocation to effectively hide\nloading time, thereby eliminating as many idle periods as possible. We\nformulate the problem as a Mixed-Integer Non-Linear Program and design an\nefficient dynamic programming algorithm to optimize model partitioning and\ndevice assignment. Experimental results show that the proposed method\nsignificantly reduces cold-start latency compared to baseline strategies."
                },
                "authors": [
                    {
                        "name": "Xuran Liu"
                    },
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Rui Bao"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Shuguang Cui"
                    }
                ],
                "author_detail": {
                    "name": "Shuguang Cui"
                },
                "author": "Shuguang Cui",
                "arxiv_comment": "submitted to Journal of Communications and Information Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11285v1",
                "updated": "2025-08-15T07:47:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    47,
                    10,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:47:10Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    47,
                    10,
                    4,
                    227,
                    0
                ],
                "title": "AI in Mental Health: Emotional and Sentiment Analysis of Large Language\n  Models' Responses to Depression, Anxiety, and Stress Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI in Mental Health: Emotional and Sentiment Analysis of Large Language\n  Models' Responses to Depression, Anxiety, and Stress Queries"
                },
                "summary": "Depression, anxiety, and stress are widespread mental health concerns that\nincreasingly drive individuals to seek information from Large Language Models\n(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini\nPro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty\npragmatic questions about depression, anxiety, and stress when those questions\nare framed for six user profiles (baseline, woman, man, young, old, and\nuniversity student). The models generated 2,880 answers, which we scored for\nsentiment and emotions using state-of-the-art tools. Our analysis revealed that\noptimism, fear, and sadness dominated the emotional landscape across all\noutputs, with neutral sentiment maintaining consistently high values.\nGratitude, joy, and trust appeared at moderate levels, while emotions such as\nanger, disgust, and love were rarely expressed. The choice of LLM significantly\ninfluenced emotional expression patterns. Mixtral exhibited the highest levels\nof negative emotions including disapproval, annoyance, and sadness, while Llama\ndemonstrated the most optimistic and joyful responses. The type of mental\nhealth condition dramatically shaped emotional responses: anxiety prompts\nelicited extraordinarily high fear scores (0.974), depression prompts generated\nelevated sadness (0.686) and the highest negative sentiment, while\nstress-related queries produced the most optimistic responses (0.755) with\nelevated joy and trust. In contrast, demographic framing of queries produced\nonly marginal variations in emotional tone. Statistical analyses confirmed\nsignificant model-specific and condition-specific differences, while\ndemographic influences remained minimal. These findings highlight the critical\nimportance of model selection in mental health applications, as each LLM\nexhibits a distinct emotional signature that could significantly impact user\nexperience and outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depression, anxiety, and stress are widespread mental health concerns that\nincreasingly drive individuals to seek information from Large Language Models\n(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini\nPro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty\npragmatic questions about depression, anxiety, and stress when those questions\nare framed for six user profiles (baseline, woman, man, young, old, and\nuniversity student). The models generated 2,880 answers, which we scored for\nsentiment and emotions using state-of-the-art tools. Our analysis revealed that\noptimism, fear, and sadness dominated the emotional landscape across all\noutputs, with neutral sentiment maintaining consistently high values.\nGratitude, joy, and trust appeared at moderate levels, while emotions such as\nanger, disgust, and love were rarely expressed. The choice of LLM significantly\ninfluenced emotional expression patterns. Mixtral exhibited the highest levels\nof negative emotions including disapproval, annoyance, and sadness, while Llama\ndemonstrated the most optimistic and joyful responses. The type of mental\nhealth condition dramatically shaped emotional responses: anxiety prompts\nelicited extraordinarily high fear scores (0.974), depression prompts generated\nelevated sadness (0.686) and the highest negative sentiment, while\nstress-related queries produced the most optimistic responses (0.755) with\nelevated joy and trust. In contrast, demographic framing of queries produced\nonly marginal variations in emotional tone. Statistical analyses confirmed\nsignificant model-specific and condition-specific differences, while\ndemographic influences remained minimal. These findings highlight the critical\nimportance of model selection in mental health applications, as each LLM\nexhibits a distinct emotional signature that could significantly impact user\nexperience and outcomes."
                },
                "authors": [
                    {
                        "name": "Arya VarastehNezhad"
                    },
                    {
                        "name": "Reza Tavasoli"
                    },
                    {
                        "name": "Soroush Elyasi"
                    },
                    {
                        "name": "MohammadHossein LotfiNia"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11281v1",
                "updated": "2025-08-15T07:40:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    40,
                    41,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:40:41Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    40,
                    41,
                    4,
                    227,
                    0
                ],
                "title": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT\n  Fine-Tuning for French Toxicity Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT\n  Fine-Tuning for French Toxicity Detection"
                },
                "summary": "Detecting toxic content using language models is crucial yet challenging.\nWhile substantial progress has been made in English, toxicity detection in\nFrench remains underdeveloped, primarily due to the lack of culturally\nrelevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new\npublic benchmark of 53,622 French online comments, constructed via a\nsemi-automated annotation pipeline that reduces manual labeling to only 10%\nthrough high-confidence LLM-based pre-annotation and human verification. Then,\nwe benchmark a broad range of models and uncover a counterintuitive insight:\nSmall Language Models (SLMs) outperform many larger models in robustness and\ngeneralization under the toxicity detection task. Motivated by this finding, we\npropose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic\nweighted loss that progressively emphasizes the model's final decision,\nsignificantly improving faithfulness. Our fine-tuned 4B model achieves\nstate-of-the-art performance, improving its F1 score by 13% over its baseline\nand outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a\ncross-lingual toxicity benchmark demonstrates strong multilingual ability,\nsuggesting that our methodology can be effectively extended to other languages\nand safety-critical classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting toxic content using language models is crucial yet challenging.\nWhile substantial progress has been made in English, toxicity detection in\nFrench remains underdeveloped, primarily due to the lack of culturally\nrelevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new\npublic benchmark of 53,622 French online comments, constructed via a\nsemi-automated annotation pipeline that reduces manual labeling to only 10%\nthrough high-confidence LLM-based pre-annotation and human verification. Then,\nwe benchmark a broad range of models and uncover a counterintuitive insight:\nSmall Language Models (SLMs) outperform many larger models in robustness and\ngeneralization under the toxicity detection task. Motivated by this finding, we\npropose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic\nweighted loss that progressively emphasizes the model's final decision,\nsignificantly improving faithfulness. Our fine-tuned 4B model achieves\nstate-of-the-art performance, improving its F1 score by 13% over its baseline\nand outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a\ncross-lingual toxicity benchmark demonstrates strong multilingual ability,\nsuggesting that our methodology can be effectively extended to other languages\nand safety-critical classification tasks."
                },
                "authors": [
                    {
                        "name": "Axel Delaval"
                    },
                    {
                        "name": "Shujian Yang"
                    },
                    {
                        "name": "Haicheng Wang"
                    },
                    {
                        "name": "Han Qiu"
                    },
                    {
                        "name": "Jialiang Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jialiang Lu"
                },
                "author": "Jialiang Lu",
                "arxiv_comment": "14 pages, 5 figures, 8 tables. This paper introduces TOXIFRENCH, a\n  new large-scale benchmark for French toxicity detection, and proposes a\n  Chain-of-Thought (CoT) fine-tuning method with a dynamic weighted loss. The\n  resulting fine-tuned 4B parameter model, ToxiFrench, achieves\n  state-of-the-art performance, outperforming larger models like GPT-4o",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01212v2",
                "updated": "2025-08-15T07:40:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    40,
                    14,
                    4,
                    227,
                    0
                ],
                "published": "2025-01-02T11:41:43Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    41,
                    43,
                    3,
                    2,
                    0
                ],
                "title": "Towards Consumer-Grade Cybersickness Prediction: Multi-Model Alignment\n  for Real-Time Vision-Only Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Consumer-Grade Cybersickness Prediction: Multi-Model Alignment\n  for Real-Time Vision-Only Inference"
                },
                "summary": "Cybersickness remains a major obstacle to the widespread adoption of\nimmersive virtual reality (VR), particularly in consumer-grade environments.\nWhile prior methods rely on invasive signals such as electroencephalography\n(EEG) for high predictive accuracy, these approaches require specialized\nhardware and are impractical for real-world applications. In this work, we\npropose a scalable, deployable framework for personalized cybersickness\nprediction leveraging only non-invasive signals readily available from\ncommercial VR headsets, including head motion, eye tracking, and physiological\nresponses. Our model employs a modality-specific graph neural network enhanced\nwith a Difference Attention Module to extract temporal-spatial embeddings\ncapturing dynamic changes across modalities. A cross-modal alignment module\njointly trains the video encoder to learn personalized traits by aligning video\nfeatures with sensor-derived representations. Consequently, the model\naccurately predicts individual cybersickness using only video input during\ninference. Experimental results show our model achieves 88.4\\% accuracy,\nclosely matching EEG-based approaches (89.16\\%), while reducing deployment\ncomplexity. With an average inference latency of 90ms, our framework supports\nreal-time applications, ideal for integration into consumer-grade VR platforms\nwithout compromising personalization or performance. The code will be relesed\nat https://github.com/U235-Aurora/PTGNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cybersickness remains a major obstacle to the widespread adoption of\nimmersive virtual reality (VR), particularly in consumer-grade environments.\nWhile prior methods rely on invasive signals such as electroencephalography\n(EEG) for high predictive accuracy, these approaches require specialized\nhardware and are impractical for real-world applications. In this work, we\npropose a scalable, deployable framework for personalized cybersickness\nprediction leveraging only non-invasive signals readily available from\ncommercial VR headsets, including head motion, eye tracking, and physiological\nresponses. Our model employs a modality-specific graph neural network enhanced\nwith a Difference Attention Module to extract temporal-spatial embeddings\ncapturing dynamic changes across modalities. A cross-modal alignment module\njointly trains the video encoder to learn personalized traits by aligning video\nfeatures with sensor-derived representations. Consequently, the model\naccurately predicts individual cybersickness using only video input during\ninference. Experimental results show our model achieves 88.4\\% accuracy,\nclosely matching EEG-based approaches (89.16\\%), while reducing deployment\ncomplexity. With an average inference latency of 90ms, our framework supports\nreal-time applications, ideal for integration into consumer-grade VR platforms\nwithout compromising personalization or performance. The code will be relesed\nat https://github.com/U235-Aurora/PTGNN."
                },
                "authors": [
                    {
                        "name": "Yitong Zhu"
                    },
                    {
                        "name": "Zhuowen Liang"
                    },
                    {
                        "name": "Yiming Wu"
                    },
                    {
                        "name": "Tangyao Li"
                    },
                    {
                        "name": "Yuyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuyang Wang"
                },
                "author": "Yuyang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11280v1",
                "updated": "2025-08-15T07:37:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    37,
                    12,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:37:12Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    37,
                    12,
                    4,
                    227,
                    0
                ],
                "title": "LETToT: Label-Free Evaluation of Large Language Models On Tourism Using\n  Expert Tree-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LETToT: Label-Free Evaluation of Large Language Models On Tourism Using\n  Expert Tree-of-Thought"
                },
                "summary": "Evaluating large language models (LLMs) in specific domain like tourism\nremains challenging due to the prohibitive cost of annotated benchmarks and\npersistent issues like hallucinations. We propose $\\textbf{L}$able-Free\n$\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert\n$\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that\nleverages expert-derived reasoning structures-instead of labeled data-to access\nLLMs in tourism. First, we iteratively refine and validate hierarchical ToT\ncomponents through alignment with generic quality dimensions and expert\nfeedback. Results demonstrate the effectiveness of our systematically optimized\nexpert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we\napply LETToT's optimized expert ToT to evaluate models of varying scales\n(32B-671B parameters), revealing: (1) Scaling laws persist in specialized\ndomains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,\nDeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit\nreasoning architectures outperform counterparts in accuracy and conciseness\n($p<0.05$). Our work established a scalable, label-free paradigm for\ndomain-specific LLM evaluation, offering a robust alternative to conventional\nannotated benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) in specific domain like tourism\nremains challenging due to the prohibitive cost of annotated benchmarks and\npersistent issues like hallucinations. We propose $\\textbf{L}$able-Free\n$\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert\n$\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that\nleverages expert-derived reasoning structures-instead of labeled data-to access\nLLMs in tourism. First, we iteratively refine and validate hierarchical ToT\ncomponents through alignment with generic quality dimensions and expert\nfeedback. Results demonstrate the effectiveness of our systematically optimized\nexpert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we\napply LETToT's optimized expert ToT to evaluate models of varying scales\n(32B-671B parameters), revealing: (1) Scaling laws persist in specialized\ndomains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,\nDeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit\nreasoning architectures outperform counterparts in accuracy and conciseness\n($p<0.05$). Our work established a scalable, label-free paradigm for\ndomain-specific LLM evaluation, offering a robust alternative to conventional\nannotated benchmarks."
                },
                "authors": [
                    {
                        "name": "Ruiyan Qi"
                    },
                    {
                        "name": "Congding Wen"
                    },
                    {
                        "name": "Weibo Zhou"
                    },
                    {
                        "name": "Shangsong Liang"
                    },
                    {
                        "name": "Lingbo Li"
                    }
                ],
                "author_detail": {
                    "name": "Lingbo Li"
                },
                "author": "Lingbo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11278v1",
                "updated": "2025-08-15T07:29:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    29,
                    46,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:29:46Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    29,
                    46,
                    4,
                    227,
                    0
                ],
                "title": "Is General-Purpose AI Reasoning Sensitive to Data-Induced Cognitive\n  Biases? Dynamic Benchmarking on Typical Software Engineering Dilemmas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is General-Purpose AI Reasoning Sensitive to Data-Induced Cognitive\n  Biases? Dynamic Benchmarking on Typical Software Engineering Dilemmas"
                },
                "summary": "Human cognitive biases in software engineering can lead to costly errors.\nWhile general-purpose AI (GPAI) systems may help mitigate these biases due to\ntheir non-human nature, their training on human-generated data raises a\ncritical question: Do GPAI systems themselves exhibit cognitive biases?\n  To investigate this, we present the first dynamic benchmarking framework to\nevaluate data-induced cognitive biases in GPAI within software engineering\nworkflows. Starting with a seed set of 16 hand-crafted realistic tasks, each\nfeaturing one of 8 cognitive biases (e.g., anchoring, framing) and\ncorresponding unbiased variants, we test whether bias-inducing linguistic cues\nunrelated to task logic can lead GPAI systems from correct to incorrect\nconclusions.\n  To scale the benchmark and ensure realism, we develop an on-demand\naugmentation pipeline relying on GPAI systems to generate task variants that\npreserve bias-inducing cues while varying surface details. This pipeline\nensures correctness (88--99% on average, according to human evaluation),\npromotes diversity, and controls reasoning complexity by leveraging\nProlog-based reasoning and LLM-as-a-judge validation. It also verifies that the\nembedded biases are both harmful and undetectable by logic-based, unbiased\nreasoners.\n  We evaluate leading GPAI systems (GPT, LLaMA, DeepSeek) and find a consistent\ntendency to rely on shallow linguistic heuristics over deep reasoning. All\nsystems exhibit cognitive biases (ranging from 5.9% to 35% across types), with\nbias sensitivity increasing sharply with task complexity (up to 49%),\nhighlighting critical risks in real-world software engineering deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognitive biases in software engineering can lead to costly errors.\nWhile general-purpose AI (GPAI) systems may help mitigate these biases due to\ntheir non-human nature, their training on human-generated data raises a\ncritical question: Do GPAI systems themselves exhibit cognitive biases?\n  To investigate this, we present the first dynamic benchmarking framework to\nevaluate data-induced cognitive biases in GPAI within software engineering\nworkflows. Starting with a seed set of 16 hand-crafted realistic tasks, each\nfeaturing one of 8 cognitive biases (e.g., anchoring, framing) and\ncorresponding unbiased variants, we test whether bias-inducing linguistic cues\nunrelated to task logic can lead GPAI systems from correct to incorrect\nconclusions.\n  To scale the benchmark and ensure realism, we develop an on-demand\naugmentation pipeline relying on GPAI systems to generate task variants that\npreserve bias-inducing cues while varying surface details. This pipeline\nensures correctness (88--99% on average, according to human evaluation),\npromotes diversity, and controls reasoning complexity by leveraging\nProlog-based reasoning and LLM-as-a-judge validation. It also verifies that the\nembedded biases are both harmful and undetectable by logic-based, unbiased\nreasoners.\n  We evaluate leading GPAI systems (GPT, LLaMA, DeepSeek) and find a consistent\ntendency to rely on shallow linguistic heuristics over deep reasoning. All\nsystems exhibit cognitive biases (ranging from 5.9% to 35% across types), with\nbias sensitivity increasing sharply with task complexity (up to 49%),\nhighlighting critical risks in real-world software engineering deployments."
                },
                "authors": [
                    {
                        "name": "Francesco Sovrano"
                    },
                    {
                        "name": "Gabriele Dominici"
                    },
                    {
                        "name": "Rita Sevastjanova"
                    },
                    {
                        "name": "Alessandra Stramiglio"
                    },
                    {
                        "name": "Alberto Bacchelli"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Bacchelli"
                },
                "author": "Alberto Bacchelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11277v1",
                "updated": "2025-08-15T07:29:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    29,
                    42,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:29:42Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    29,
                    42,
                    4,
                    227,
                    0
                ],
                "title": "Probing the Representational Power of Sparse Autoencoders in Vision\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Representational Power of Sparse Autoencoders in Vision\n  Models"
                },
                "summary": "Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting\nthe hidden states of large language models (LLMs). By learning to reconstruct\nactivations from a sparse bottleneck layer, SAEs discover interpretable\nfeatures from the high-dimensional internal representations of LLMs. Despite\ntheir popularity with language models, SAEs remain understudied in the visual\ndomain. In this work, we provide an extensive evaluation the representational\npower of SAEs for vision models using a broad range of image-based tasks. Our\nexperimental results demonstrate that SAE features are semantically meaningful,\nimprove out-of-distribution generalization, and enable controllable generation\nacross three vision model architectures: vision embedding models, multi-modal\nLMMs and diffusion models. In vision embedding models, we find that learned SAE\nfeatures can be used for OOD detection and provide evidence that they recover\nthe ontological structure of the underlying model. For diffusion models, we\ndemonstrate that SAEs enable semantic steering through text encoder\nmanipulation and develop an automated pipeline for discovering\nhuman-interpretable attributes. Finally, we conduct exploratory experiments on\nmulti-modal LLMs, finding evidence that SAE features reveal shared\nrepresentations across vision and language modalities. Our study provides a\nfoundation for SAE evaluation in vision models, highlighting their strong\npotential improving interpretability, generalization, and steerability in the\nvisual domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting\nthe hidden states of large language models (LLMs). By learning to reconstruct\nactivations from a sparse bottleneck layer, SAEs discover interpretable\nfeatures from the high-dimensional internal representations of LLMs. Despite\ntheir popularity with language models, SAEs remain understudied in the visual\ndomain. In this work, we provide an extensive evaluation the representational\npower of SAEs for vision models using a broad range of image-based tasks. Our\nexperimental results demonstrate that SAE features are semantically meaningful,\nimprove out-of-distribution generalization, and enable controllable generation\nacross three vision model architectures: vision embedding models, multi-modal\nLMMs and diffusion models. In vision embedding models, we find that learned SAE\nfeatures can be used for OOD detection and provide evidence that they recover\nthe ontological structure of the underlying model. For diffusion models, we\ndemonstrate that SAEs enable semantic steering through text encoder\nmanipulation and develop an automated pipeline for discovering\nhuman-interpretable attributes. Finally, we conduct exploratory experiments on\nmulti-modal LLMs, finding evidence that SAE features reveal shared\nrepresentations across vision and language modalities. Our study provides a\nfoundation for SAE evaluation in vision models, highlighting their strong\npotential improving interpretability, generalization, and steerability in the\nvisual domain."
                },
                "authors": [
                    {
                        "name": "Matthew Lyle Olson"
                    },
                    {
                        "name": "Musashi Hinck"
                    },
                    {
                        "name": "Neale Ratzlaff"
                    },
                    {
                        "name": "Changbai Li"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Vasudev Lal"
                    },
                    {
                        "name": "Shao-Yen Tseng"
                    }
                ],
                "author_detail": {
                    "name": "Shao-Yen Tseng"
                },
                "author": "Shao-Yen Tseng",
                "arxiv_comment": "ICCV 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07356v2",
                "updated": "2025-08-15T07:28:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    28,
                    28,
                    4,
                    227,
                    0
                ],
                "published": "2025-07-10T00:42:59Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    0,
                    42,
                    59,
                    3,
                    191,
                    0
                ],
                "title": "UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid\n  Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid\n  Robots"
                },
                "summary": "Achieving expressive and generalizable whole-body motion control is essential\nfor deploying humanoid robots in real-world environments. In this work, we\npropose UniTracker, a three-stage training framework that enables robust and\nscalable motion tracking across a wide range of human behaviors. In the first\nstage, we train a teacher policy with privileged observations to generate\nhigh-quality actions. In the second stage, we introduce a Conditional\nVariational Autoencoder (CVAE) to model a universal student policy that can be\ndeployed directly on real hardware. The CVAE structure allows the policy to\nlearn a global latent representation of motion, enhancing generalization to\nunseen behaviors and addressing the limitations of standard MLP-based policies\nunder partial observations. Unlike pure MLPs that suffer from drift in global\nattributes like orientation, our CVAE-student policy incorporates global intent\nduring training by aligning a partial-observation prior to the full-observation\nencoder. In the third stage, we introduce a fast adaptation module that\nfine-tunes the universal policy on harder motion sequences that are difficult\nto track directly. This adaptation can be performed both for single sequences\nand in batch mode, further showcasing the flexibility and scalability of our\napproach. We evaluate UniTracker in both simulation and real-world settings\nusing a Unitree G1 humanoid, demonstrating strong performance in motion\ndiversity, tracking accuracy, and deployment robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving expressive and generalizable whole-body motion control is essential\nfor deploying humanoid robots in real-world environments. In this work, we\npropose UniTracker, a three-stage training framework that enables robust and\nscalable motion tracking across a wide range of human behaviors. In the first\nstage, we train a teacher policy with privileged observations to generate\nhigh-quality actions. In the second stage, we introduce a Conditional\nVariational Autoencoder (CVAE) to model a universal student policy that can be\ndeployed directly on real hardware. The CVAE structure allows the policy to\nlearn a global latent representation of motion, enhancing generalization to\nunseen behaviors and addressing the limitations of standard MLP-based policies\nunder partial observations. Unlike pure MLPs that suffer from drift in global\nattributes like orientation, our CVAE-student policy incorporates global intent\nduring training by aligning a partial-observation prior to the full-observation\nencoder. In the third stage, we introduce a fast adaptation module that\nfine-tunes the universal policy on harder motion sequences that are difficult\nto track directly. This adaptation can be performed both for single sequences\nand in batch mode, further showcasing the flexibility and scalability of our\napproach. We evaluate UniTracker in both simulation and real-world settings\nusing a Unitree G1 humanoid, demonstrating strong performance in motion\ndiversity, tracking accuracy, and deployment robustness."
                },
                "authors": [
                    {
                        "name": "Kangning Yin"
                    },
                    {
                        "name": "Weishuai Zeng"
                    },
                    {
                        "name": "Ke Fan"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Zheng Tian"
                    },
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "three-stage universal motion tracker for humanoid robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11269v1",
                "updated": "2025-08-15T07:08:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    8,
                    46,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:08:46Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    8,
                    46,
                    4,
                    227,
                    0
                ],
                "title": "Inference performance evaluation for LLMs on edge devices with a novel\n  benchmarking framework and metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference performance evaluation for LLMs on edge devices with a novel\n  benchmarking framework and metric"
                },
                "summary": "With the significant success achieved by large language models (LLMs) like\nLLaMA, edge computing-based LLM inference services for mobile and PC are in\nhigh demand for data privacy. However, different edge platforms have different\nhardware characteristics and the large demand for memory capacity and bandwidth\nmakes it very challenging to deploy and benchmark LLMs on edge devices. In this\npaper, we introduce a benchmarking tool named ELIB (edge LLM inference\nbenchmarking) to evaluate LLM inference performance of different edge\nplatforms, and propose a novel metric named MBU to indicate the percentage of\nthe theoretically efficient use of available memory bandwidth for a specific\nmodel running on edge hardware to optimize memory usage. We deploy ELIB on\nthree edge platforms and benchmark using five quantized models to optimize MBU\nin combination with other metrics such as FLOPS, throughput, latency and\naccuracy. And we analyze the results to derive the key factors, constraints,\nunpredictability in optimizing MBU that can guide deploying LLMs on more edge\nplatforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the significant success achieved by large language models (LLMs) like\nLLaMA, edge computing-based LLM inference services for mobile and PC are in\nhigh demand for data privacy. However, different edge platforms have different\nhardware characteristics and the large demand for memory capacity and bandwidth\nmakes it very challenging to deploy and benchmark LLMs on edge devices. In this\npaper, we introduce a benchmarking tool named ELIB (edge LLM inference\nbenchmarking) to evaluate LLM inference performance of different edge\nplatforms, and propose a novel metric named MBU to indicate the percentage of\nthe theoretically efficient use of available memory bandwidth for a specific\nmodel running on edge hardware to optimize memory usage. We deploy ELIB on\nthree edge platforms and benchmark using five quantized models to optimize MBU\nin combination with other metrics such as FLOPS, throughput, latency and\naccuracy. And we analyze the results to derive the key factors, constraints,\nunpredictability in optimizing MBU that can guide deploying LLMs on more edge\nplatforms."
                },
                "authors": [
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Cong Tian"
                    },
                    {
                        "name": "Zixuan He"
                    },
                    {
                        "name": "Bin Yu"
                    },
                    {
                        "name": "Yepang Liu"
                    },
                    {
                        "name": "Jialun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jialun Cao"
                },
                "author": "Jialun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11260v1",
                "updated": "2025-08-15T06:53:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:53:28Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?"
                },
                "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages."
                },
                "authors": [
                    {
                        "name": "Mukund Choudhary"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Gaurja Aeron"
                    },
                    {
                        "name": "Antara Raaghavi Bhattacharya"
                    },
                    {
                        "name": "Dang Khoa Dang Dinh"
                    },
                    {
                        "name": "Ikhlasul Akmal Hanif"
                    },
                    {
                        "name": "Daria Kotova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11258v1",
                "updated": "2025-08-15T06:50:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    50,
                    29,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:50:29Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    50,
                    29,
                    4,
                    227,
                    0
                ],
                "title": "Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed\n  LLMs via Post-Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed\n  LLMs via Post-Processing"
                },
                "summary": "Instruction fine-tuned large language models (LLMs) enable a simple zero-shot\nor few-shot prompting paradigm, also known as in-context learning, for building\nprediction models. This convenience, combined with continued advances in LLM\ncapability, has the potential to drive their adoption across a broad range of\ndomains, including high-stakes applications where group fairness -- preventing\ndisparate impacts across demographic groups -- is essential. The majority of\nexisting approaches to enforcing group fairness on LLM-based classifiers rely\non traditional fair algorithms applied via model fine-tuning or head-tuning on\nfinal-layer embeddings, but they are no longer applicable to closed-weight LLMs\nunder the in-context learning setting, which include some of the most capable\ncommercial models today, such as GPT-4, Gemini, and Claude. In this paper, we\npropose a framework for deriving fair classifiers from closed-weight LLMs via\nprompting: the LLM is treated as a feature extractor, and features are elicited\nfrom its probabilistic predictions (e.g., token log probabilities) using\nprompts strategically designed for the specified fairness criterion to obtain\nsufficient statistics for fair classification; a fair algorithm is then applied\nto these features to train a lightweight fair classifier in a post-hoc manner.\nExperiments on five datasets, including three tabular ones, demonstrate strong\naccuracy-fairness tradeoffs for the classifiers derived by our framework from\nboth open-weight and closed-weight LLMs; in particular, our framework is\ndata-efficient and outperforms fair classifiers trained on LLM embeddings\n(i.e., head-tuning) or from scratch on raw tabular features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction fine-tuned large language models (LLMs) enable a simple zero-shot\nor few-shot prompting paradigm, also known as in-context learning, for building\nprediction models. This convenience, combined with continued advances in LLM\ncapability, has the potential to drive their adoption across a broad range of\ndomains, including high-stakes applications where group fairness -- preventing\ndisparate impacts across demographic groups -- is essential. The majority of\nexisting approaches to enforcing group fairness on LLM-based classifiers rely\non traditional fair algorithms applied via model fine-tuning or head-tuning on\nfinal-layer embeddings, but they are no longer applicable to closed-weight LLMs\nunder the in-context learning setting, which include some of the most capable\ncommercial models today, such as GPT-4, Gemini, and Claude. In this paper, we\npropose a framework for deriving fair classifiers from closed-weight LLMs via\nprompting: the LLM is treated as a feature extractor, and features are elicited\nfrom its probabilistic predictions (e.g., token log probabilities) using\nprompts strategically designed for the specified fairness criterion to obtain\nsufficient statistics for fair classification; a fair algorithm is then applied\nto these features to train a lightweight fair classifier in a post-hoc manner.\nExperiments on five datasets, including three tabular ones, demonstrate strong\naccuracy-fairness tradeoffs for the classifiers derived by our framework from\nboth open-weight and closed-weight LLMs; in particular, our framework is\ndata-efficient and outperforms fair classifiers trained on LLM embeddings\n(i.e., head-tuning) or from scratch on raw tabular features."
                },
                "authors": [
                    {
                        "name": "Ruicheng Xian"
                    },
                    {
                        "name": "Yuxuan Wan"
                    },
                    {
                        "name": "Han Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Han Zhao"
                },
                "author": "Han Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11257v1",
                "updated": "2025-08-15T06:46:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    46,
                    50,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:46:50Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    46,
                    50,
                    4,
                    227,
                    0
                ],
                "title": "Hallucination in LLM-Based Code Generation: An Automotive Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination in LLM-Based Code Generation: An Automotive Case Study"
                },
                "summary": "Large Language Models (LLMs) have shown significant potential in automating\ncode generation tasks offering new opportunities across software engineering\ndomains. However, their practical application remains limited due to\nhallucinations - outputs that appear plausible but are factually incorrect,\nunverifiable or nonsensical. This paper investigates hallucination phenomena in\nthe context of code generation with a specific focus on the automotive domain.\nA case study is presented that evaluates multiple code LLMs for three different\nprompting complexities ranging from a minimal one-liner prompt to a prompt with\nCovesa Vehicle Signal Specifications (VSS) as additional context and finally to\na prompt with an additional code skeleton. The evaluation reveals a high\nfrequency of syntax violations, invalid reference errors and API knowledge\nconflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the\nevaluated models, only GPT-4.1 and GPT-4o were able to produce a correct\nsolution when given the most context-rich prompt. Simpler prompting strategies\nfailed to yield a working result, even after multiple refinement iterations.\nThese findings highlight the need for effective mitigation techniques to ensure\nthe safe and reliable use of LLM generated code, especially in safety-critical\ndomains such as automotive software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant potential in automating\ncode generation tasks offering new opportunities across software engineering\ndomains. However, their practical application remains limited due to\nhallucinations - outputs that appear plausible but are factually incorrect,\nunverifiable or nonsensical. This paper investigates hallucination phenomena in\nthe context of code generation with a specific focus on the automotive domain.\nA case study is presented that evaluates multiple code LLMs for three different\nprompting complexities ranging from a minimal one-liner prompt to a prompt with\nCovesa Vehicle Signal Specifications (VSS) as additional context and finally to\na prompt with an additional code skeleton. The evaluation reveals a high\nfrequency of syntax violations, invalid reference errors and API knowledge\nconflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the\nevaluated models, only GPT-4.1 and GPT-4o were able to produce a correct\nsolution when given the most context-rich prompt. Simpler prompting strategies\nfailed to yield a working result, even after multiple refinement iterations.\nThese findings highlight the need for effective mitigation techniques to ensure\nthe safe and reliable use of LLM generated code, especially in safety-critical\ndomains such as automotive software systems."
                },
                "authors": [
                    {
                        "name": "Marc Pavel"
                    },
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Lukasz Mazur"
                    },
                    {
                        "name": "Vahid Zolfaghari"
                    },
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08684v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08684v3",
                "updated": "2025-08-15T06:37:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    37,
                    14,
                    4,
                    227,
                    0
                ],
                "published": "2023-12-14T06:46:35Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    6,
                    46,
                    35,
                    3,
                    348,
                    0
                ],
                "title": "A Computationally Efficient Maximum A Posteriori Sequence Estimation via\n  Stein Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Computationally Efficient Maximum A Posteriori Sequence Estimation via\n  Stein Variational Inference"
                },
                "summary": "State estimation in robotic systems presents significant challenges,\nparticularly due to the prevalence of multimodal posterior distributions in\nreal-world scenarios. One effective strategy for handling such complexity is to\ncompute maximum a posteriori (MAP) sequences over a discretized or sampled\nstate space, which enables a concise representation of the most likely state\ntrajectory. However, this approach often incurs substantial computational\ncosts, especially in high-dimensional settings. In this article, we propose a\nnovel MAP sequence estimation method, \\textsf{Stein-MAP-Seq}, which effectively\naddresses multimodality while substantially reducing computational and memory\noverhead. Our key contribution is a sequential variational inference framework\nthat captures temporal dependencies in dynamical system models and integrates\nStein variational gradient descent (SVGD) into a Viterbi-style dynamic\nprogramming algorithm, enabling computationally efficient MAP sequence\nestimation. \\textsf{Stein-MAP-Seq} achieves a computational complexity of\n$\\mathcal{O}(M^2)$, where $M$ is the number of particles, in contrast to the\n$\\mathcal{O}(N^2)$ complexity of conventional MAP sequence estimators, with $N\n\\gg M$. Furthermore, the method inherits SVGD's parallelism, enabling efficient\ncomputation for real-time deployment on GPU-equipped autonomous systems. We\nvalidate the proposed method in various multimodal scenarios, including those\narising from nonlinear dynamics with ambiguous observations, unknown data\nassociations, and temporary unobservability, demonstrating substantial\nimprovements in estimation accuracy and robustness to multimodality over\nexisting approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State estimation in robotic systems presents significant challenges,\nparticularly due to the prevalence of multimodal posterior distributions in\nreal-world scenarios. One effective strategy for handling such complexity is to\ncompute maximum a posteriori (MAP) sequences over a discretized or sampled\nstate space, which enables a concise representation of the most likely state\ntrajectory. However, this approach often incurs substantial computational\ncosts, especially in high-dimensional settings. In this article, we propose a\nnovel MAP sequence estimation method, \\textsf{Stein-MAP-Seq}, which effectively\naddresses multimodality while substantially reducing computational and memory\noverhead. Our key contribution is a sequential variational inference framework\nthat captures temporal dependencies in dynamical system models and integrates\nStein variational gradient descent (SVGD) into a Viterbi-style dynamic\nprogramming algorithm, enabling computationally efficient MAP sequence\nestimation. \\textsf{Stein-MAP-Seq} achieves a computational complexity of\n$\\mathcal{O}(M^2)$, where $M$ is the number of particles, in contrast to the\n$\\mathcal{O}(N^2)$ complexity of conventional MAP sequence estimators, with $N\n\\gg M$. Furthermore, the method inherits SVGD's parallelism, enabling efficient\ncomputation for real-time deployment on GPU-equipped autonomous systems. We\nvalidate the proposed method in various multimodal scenarios, including those\narising from nonlinear dynamics with ambiguous observations, unknown data\nassociations, and temporary unobservability, demonstrating substantial\nimprovements in estimation accuracy and robustness to multimodality over\nexisting approaches."
                },
                "authors": [
                    {
                        "name": "Min-Won Seo"
                    },
                    {
                        "name": "Solmaz S. Kia"
                    }
                ],
                "author_detail": {
                    "name": "Solmaz S. Kia"
                },
                "author": "Solmaz S. Kia",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08684v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08684v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11247v1",
                "updated": "2025-08-15T06:36:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    36,
                    13,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:36:13Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    36,
                    13,
                    4,
                    227,
                    0
                ],
                "title": "Cross-Granularity Hypergraph Retrieval-Augmented Generation for\n  Multi-hop Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Granularity Hypergraph Retrieval-Augmented Generation for\n  Multi-hop Question Answering"
                },
                "summary": "Multi-hop question answering (MHQA) requires integrating knowledge scattered\nacross multiple passages to derive the correct answer. Traditional\nretrieval-augmented generation (RAG) methods primarily focus on coarse-grained\ntextual semantic similarity and ignore structural associations among dispersed\nknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods\naddress this by leveraging knowledge graphs (KGs) to capture structural\nassociations, but they tend to overly rely on structural information and\nfine-grained word- or phrase-level retrieval, resulting in an underutilization\nof textual semantics. In this paper, we propose a novel RAG approach called\nHGRAG for MHQA that achieves cross-granularity integration of structural and\nsemantic information via hypergraphs. Structurally, we construct an entity\nhypergraph where fine-grained entities serve as nodes and coarse-grained\npassages as hyperedges, and establish knowledge association through shared\nentities. Semantically, we design a hypergraph retrieval method that integrates\nfine-grained entity similarity and coarse-grained passage similarity via\nhypergraph diffusion. Finally, we employ a retrieval enhancement module, which\nfurther refines the retrieved results both semantically and structurally, to\nobtain the most relevant passages as context for answer generation with the\nLLM. Experimental results on benchmark datasets demonstrate that our approach\noutperforms state-of-the-art methods in QA performance, and achieves a\n6$\\times$ speedup in retrieval efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop question answering (MHQA) requires integrating knowledge scattered\nacross multiple passages to derive the correct answer. Traditional\nretrieval-augmented generation (RAG) methods primarily focus on coarse-grained\ntextual semantic similarity and ignore structural associations among dispersed\nknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods\naddress this by leveraging knowledge graphs (KGs) to capture structural\nassociations, but they tend to overly rely on structural information and\nfine-grained word- or phrase-level retrieval, resulting in an underutilization\nof textual semantics. In this paper, we propose a novel RAG approach called\nHGRAG for MHQA that achieves cross-granularity integration of structural and\nsemantic information via hypergraphs. Structurally, we construct an entity\nhypergraph where fine-grained entities serve as nodes and coarse-grained\npassages as hyperedges, and establish knowledge association through shared\nentities. Semantically, we design a hypergraph retrieval method that integrates\nfine-grained entity similarity and coarse-grained passage similarity via\nhypergraph diffusion. Finally, we employ a retrieval enhancement module, which\nfurther refines the retrieved results both semantically and structurally, to\nobtain the most relevant passages as context for answer generation with the\nLLM. Experimental results on benchmark datasets demonstrate that our approach\noutperforms state-of-the-art methods in QA performance, and achieves a\n6$\\times$ speedup in retrieval efficiency."
                },
                "authors": [
                    {
                        "name": "Changjian Wang"
                    },
                    {
                        "name": "Weihong Deng"
                    },
                    {
                        "name": "Weili Guan"
                    },
                    {
                        "name": "Quan Lu"
                    },
                    {
                        "name": "Ning Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Ning Jiang"
                },
                "author": "Ning Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10557v2",
                "updated": "2025-08-15T06:20:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    20,
                    9,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-14T11:55:21Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    11,
                    55,
                    21,
                    3,
                    226,
                    0
                ],
                "title": "PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D\n  Perception Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D\n  Perception Tasks"
                },
                "summary": "Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)\nrepresent two mainstream model quantization approaches. However, PTQ often\nleads to unacceptable performance degradation in quantized models, while QAT\nimposes substantial GPU memory requirements and extended training time due to\nweight fine-tuning. In this paper, we propose PTQAT, a novel general hybrid\nquantization algorithm for the efficient deployment of 3D perception networks.\nTo address the speed accuracy trade-off between PTQ and QAT, our method selects\ncritical layers for QAT fine-tuning and performs PTQ on the remaining layers.\nContrary to intuition, fine-tuning the layers with smaller output discrepancies\nbefore and after quantization, rather than those with larger discrepancies,\nactually leads to greater improvements in the model's quantization accuracy.\nThis means we better compensate for quantization errors during their\npropagation, rather than addressing them at the point where they occur. The\nproposed PTQAT achieves similar performance to QAT with more efficiency by\nfreezing nearly 50% of quantifiable layers. Additionally, PTQAT is a universal\nquantization method that supports various quantization bit widths (4 bits) as\nwell as different model architectures, including CNNs and Transformers. The\nexperimental results on nuScenes across diverse 3D perception tasks, including\nobject detection, semantic segmentation, and occupancy prediction, show that\nour method consistently outperforms QAT-only baselines. Notably, it achieves\n0.2%-0.9% NDS and 0.3%-1.0% mAP gains in object detection, 0.3%-2.0% mIoU gains\nin semantic segmentation and occupancy prediction while fine-tuning fewer\nweights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)\nrepresent two mainstream model quantization approaches. However, PTQ often\nleads to unacceptable performance degradation in quantized models, while QAT\nimposes substantial GPU memory requirements and extended training time due to\nweight fine-tuning. In this paper, we propose PTQAT, a novel general hybrid\nquantization algorithm for the efficient deployment of 3D perception networks.\nTo address the speed accuracy trade-off between PTQ and QAT, our method selects\ncritical layers for QAT fine-tuning and performs PTQ on the remaining layers.\nContrary to intuition, fine-tuning the layers with smaller output discrepancies\nbefore and after quantization, rather than those with larger discrepancies,\nactually leads to greater improvements in the model's quantization accuracy.\nThis means we better compensate for quantization errors during their\npropagation, rather than addressing them at the point where they occur. The\nproposed PTQAT achieves similar performance to QAT with more efficiency by\nfreezing nearly 50% of quantifiable layers. Additionally, PTQAT is a universal\nquantization method that supports various quantization bit widths (4 bits) as\nwell as different model architectures, including CNNs and Transformers. The\nexperimental results on nuScenes across diverse 3D perception tasks, including\nobject detection, semantic segmentation, and occupancy prediction, show that\nour method consistently outperforms QAT-only baselines. Notably, it achieves\n0.2%-0.9% NDS and 0.3%-1.0% mAP gains in object detection, 0.3%-2.0% mIoU gains\nin semantic segmentation and occupancy prediction while fine-tuning fewer\nweights."
                },
                "authors": [
                    {
                        "name": "Xinhao Wang"
                    },
                    {
                        "name": "Zhiwei Lin"
                    },
                    {
                        "name": "Zhongyu Xia"
                    },
                    {
                        "name": "Yongtao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yongtao Wang"
                },
                "author": "Yongtao Wang",
                "arxiv_comment": "8 pages, Accepted by ICCVW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10450v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10450v3",
                "updated": "2025-08-15T05:34:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    5,
                    34,
                    6,
                    4,
                    227,
                    0
                ],
                "published": "2024-06-15T00:07:44Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    0,
                    7,
                    44,
                    5,
                    167,
                    0
                ],
                "title": "TokenRec: Learning to Tokenize ID for LLM-based Generative\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenRec: Learning to Tokenize ID for LLM-based Generative\n  Recommendation"
                },
                "summary": "There is a growing interest in utilizing large-scale language models (LLMs)\nto advance next-generation Recommender Systems (RecSys), driven by their\noutstanding language understanding and in-context learning capabilities. In\nthis scenario, tokenizing (i.e., indexing) users and items becomes essential\nfor ensuring a seamless alignment of LLMs with recommendations. While several\nstudies have made progress in representing users and items through textual\ncontents or latent representations, challenges remain in efficiently capturing\nhigh-order collaborative knowledge into discrete tokens that are compatible\nwith LLMs. Additionally, the majority of existing tokenization approaches often\nface difficulties in generalizing effectively to new/unseen users or items that\nwere not in the training corpus. To address these challenges, we propose a\nnovel framework called TokenRec, which introduces not only an effective ID\ntokenization strategy but also an efficient retrieval paradigm for LLM-based\nrecommendations. Specifically, our tokenization strategy, Masked\nVector-Quantized (MQ) Tokenizer, involves quantizing the masked user/item\nrepresentations learned from collaborative filtering into discrete tokens, thus\nachieving a smooth incorporation of high-order collaborative knowledge and a\ngeneralizable tokenization of users and items for LLM-based RecSys. Meanwhile,\nour generative retrieval paradigm is designed to efficiently recommend top-$K$\nitems for users to eliminate the need for the time-consuming auto-regressive\ndecoding and beam search processes used by LLMs, thus significantly reducing\ninference time. Comprehensive experiments validate the effectiveness of the\nproposed methods, demonstrating that TokenRec outperforms competitive\nbenchmarks, including both traditional recommender systems and emerging\nLLM-based recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing interest in utilizing large-scale language models (LLMs)\nto advance next-generation Recommender Systems (RecSys), driven by their\noutstanding language understanding and in-context learning capabilities. In\nthis scenario, tokenizing (i.e., indexing) users and items becomes essential\nfor ensuring a seamless alignment of LLMs with recommendations. While several\nstudies have made progress in representing users and items through textual\ncontents or latent representations, challenges remain in efficiently capturing\nhigh-order collaborative knowledge into discrete tokens that are compatible\nwith LLMs. Additionally, the majority of existing tokenization approaches often\nface difficulties in generalizing effectively to new/unseen users or items that\nwere not in the training corpus. To address these challenges, we propose a\nnovel framework called TokenRec, which introduces not only an effective ID\ntokenization strategy but also an efficient retrieval paradigm for LLM-based\nrecommendations. Specifically, our tokenization strategy, Masked\nVector-Quantized (MQ) Tokenizer, involves quantizing the masked user/item\nrepresentations learned from collaborative filtering into discrete tokens, thus\nachieving a smooth incorporation of high-order collaborative knowledge and a\ngeneralizable tokenization of users and items for LLM-based RecSys. Meanwhile,\nour generative retrieval paradigm is designed to efficiently recommend top-$K$\nitems for users to eliminate the need for the time-consuming auto-regressive\ndecoding and beam search processes used by LLMs, thus significantly reducing\ninference time. Comprehensive experiments validate the effectiveness of the\nproposed methods, demonstrating that TokenRec outperforms competitive\nbenchmarks, including both traditional recommender systems and emerging\nLLM-based recommender systems."
                },
                "authors": [
                    {
                        "name": "Haohao Qu"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Zihuai Zhao"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "Accepted by IEEE TKDE. Codes and data are available at\n  https://github.com/Quhaoh233/TokenRec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10450v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10450v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.14749v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.14749v2",
                "updated": "2025-08-15T05:22:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    5,
                    22,
                    27,
                    4,
                    227,
                    0
                ],
                "published": "2023-11-23T10:14:23Z",
                "published_parsed": [
                    2023,
                    11,
                    23,
                    10,
                    14,
                    23,
                    3,
                    327,
                    0
                ],
                "title": "Compositional Zero-shot Learning via Progressive Language-based\n  Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Zero-shot Learning via Progressive Language-based\n  Observations"
                },
                "summary": "Compositional zero-shot learning aims to recognize unseen state-object\ncompositions by leveraging known primitives (state and object) during training.\nHowever, effectively modeling interactions between primitives and generalizing\nknowledge to novel compositions remains a perennial challenge. There are two\nkey factors: object-conditioned and state-conditioned variance, i.e., the\nappearance of states (or objects) can vary significantly when combined with\ndifferent objects (or states). For instance, the state \"old\" can signify a\nvintage design for a \"car\" or an advanced age for a \"cat\". In this paper, we\nargue that these variances can be mitigated by predicting composition\ncategories based on pre-observed primitive. To this end, we propose Progressive\nLanguage-based Observations (PLO), which can dynamically determine a better\nobservation order of primitives. These observations comprise a series of\nconcepts or languages that allow the model to understand image content in a\nstep-by-step manner. Specifically, PLO adopts pre-trained vision-language\nmodels (VLMs) to empower the model with observation capabilities. We further\ndevise two variants: 1) PLO-VLM: a two-step method, where a pre-observing\nclassifier dynamically determines the observation order of two primitives. 2)\nPLO-LLM: a multi-step scheme, which utilizes large language models (LLMs) to\ncraft composition-specific prompts for step-by-step observing. Extensive\nablations on three challenging datasets demonstrate the superiority of PLO\ncompared with state-of-the-art methods, affirming its abilities in\ncompositional recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional zero-shot learning aims to recognize unseen state-object\ncompositions by leveraging known primitives (state and object) during training.\nHowever, effectively modeling interactions between primitives and generalizing\nknowledge to novel compositions remains a perennial challenge. There are two\nkey factors: object-conditioned and state-conditioned variance, i.e., the\nappearance of states (or objects) can vary significantly when combined with\ndifferent objects (or states). For instance, the state \"old\" can signify a\nvintage design for a \"car\" or an advanced age for a \"cat\". In this paper, we\nargue that these variances can be mitigated by predicting composition\ncategories based on pre-observed primitive. To this end, we propose Progressive\nLanguage-based Observations (PLO), which can dynamically determine a better\nobservation order of primitives. These observations comprise a series of\nconcepts or languages that allow the model to understand image content in a\nstep-by-step manner. Specifically, PLO adopts pre-trained vision-language\nmodels (VLMs) to empower the model with observation capabilities. We further\ndevise two variants: 1) PLO-VLM: a two-step method, where a pre-observing\nclassifier dynamically determines the observation order of two primitives. 2)\nPLO-LLM: a multi-step scheme, which utilizes large language models (LLMs) to\ncraft composition-specific prompts for step-by-step observing. Extensive\nablations on three challenging datasets demonstrate the superiority of PLO\ncompared with state-of-the-art methods, affirming its abilities in\ncompositional recognition."
                },
                "authors": [
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Guikun Chen"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.14749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.14749v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09666v2",
                "updated": "2025-08-15T05:10:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    5,
                    10,
                    11,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-13T09:56:08Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    9,
                    56,
                    8,
                    2,
                    225,
                    0
                ],
                "title": "Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought\n  Distillation"
                },
                "summary": "Previous chain-of-thought (CoT) distillation methods primarily focused on\nenhancing the reasoning capabilities of Small Language Models (SLMs) by\nutilizing high-quality rationales generated by powerful Large Language Models\n(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM\nsafety brought by the training, which are revealed in this study. Although\nthere are works on safety alignment that fine-tune language models or\nmanipulate model weights to defend against harmful inputs, they require extra\ncomputation or annotated data, and probably impact the reasoning ability of\nSLMs. In this paper, we investigate how to maintain the safety of SLMs during\nthe CoT distillation process. Specifically, we propose a safe distillation\nmethod, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing\ntwo modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the\nmagnitude of model weight changes to optimize the model weights in the\nneighboring space near the initial weight distribution. Low-Entropy Masking\nmasks low-entropy tokens, which are regarded as unnecessary learning targets,\nto exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,\nLlama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,\nAGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety\nof SLMs and comparably improves their reasoning capability compared to existing\ndistillation methods. Furthermore, our ablation study presents the\neffectiveness of Slow Tuning and Low-Entropy Masking, with the former\nmaintaining the model's safety in the early stage and the latter prolonging the\nsafe training epochs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous chain-of-thought (CoT) distillation methods primarily focused on\nenhancing the reasoning capabilities of Small Language Models (SLMs) by\nutilizing high-quality rationales generated by powerful Large Language Models\n(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM\nsafety brought by the training, which are revealed in this study. Although\nthere are works on safety alignment that fine-tune language models or\nmanipulate model weights to defend against harmful inputs, they require extra\ncomputation or annotated data, and probably impact the reasoning ability of\nSLMs. In this paper, we investigate how to maintain the safety of SLMs during\nthe CoT distillation process. Specifically, we propose a safe distillation\nmethod, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing\ntwo modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the\nmagnitude of model weight changes to optimize the model weights in the\nneighboring space near the initial weight distribution. Low-Entropy Masking\nmasks low-entropy tokens, which are regarded as unnecessary learning targets,\nto exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,\nLlama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,\nAGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety\nof SLMs and comparably improves their reasoning capability compared to existing\ndistillation methods. Furthermore, our ablation study presents the\neffectiveness of Slow Tuning and Low-Entropy Masking, with the former\nmaintaining the model's safety in the early stage and the latter prolonging the\nsafe training epochs."
                },
                "authors": [
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Qingyue Yuan"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11222v1",
                "updated": "2025-08-15T05:03:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    5,
                    3,
                    26,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T05:03:26Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    5,
                    3,
                    26,
                    4,
                    227,
                    0
                ],
                "title": "ORFuzz: Fuzzing the \"Other Side\" of LLM Safety -- Testing Over-Refusal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORFuzz: Fuzzing the \"Other Side\" of LLM Safety -- Testing Over-Refusal"
                },
                "summary": "Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously\nrejecting benign queries due to overly conservative safety measures - a\ncritical functional flaw that undermines their reliability and usability.\nCurrent methods for testing this behavior are demonstrably inadequate,\nsuffering from flawed benchmarks and limited test generation capabilities, as\nhighlighted by our empirical user study. To the best of our knowledge, this\npaper introduces the first evolutionary testing framework, ORFuzz, for the\nsystematic detection and analysis of LLM over-refusals. ORFuzz uniquely\nintegrates three core components: (1) safety category-aware seed selection for\ncomprehensive test coverage, (2) adaptive mutator optimization using reasoning\nLLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge\nmodel validated to accurately reflect user perception of toxicity and refusal.\nOur extensive evaluations demonstrate that ORFuzz generates diverse, validated\nover-refusal instances at a rate (6.98% average) more than double that of\nleading baselines, effectively uncovering vulnerabilities. Furthermore,\nORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly\ntransferable test cases that achieves a superior 63.56% average over-refusal\nrate across 10 diverse LLMs, significantly outperforming existing datasets.\nORFuzz and ORFuzzSet provide a robust automated testing framework and a\nvaluable community resource, paving the way for developing more reliable and\ntrustworthy LLM-based software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously\nrejecting benign queries due to overly conservative safety measures - a\ncritical functional flaw that undermines their reliability and usability.\nCurrent methods for testing this behavior are demonstrably inadequate,\nsuffering from flawed benchmarks and limited test generation capabilities, as\nhighlighted by our empirical user study. To the best of our knowledge, this\npaper introduces the first evolutionary testing framework, ORFuzz, for the\nsystematic detection and analysis of LLM over-refusals. ORFuzz uniquely\nintegrates three core components: (1) safety category-aware seed selection for\ncomprehensive test coverage, (2) adaptive mutator optimization using reasoning\nLLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge\nmodel validated to accurately reflect user perception of toxicity and refusal.\nOur extensive evaluations demonstrate that ORFuzz generates diverse, validated\nover-refusal instances at a rate (6.98% average) more than double that of\nleading baselines, effectively uncovering vulnerabilities. Furthermore,\nORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly\ntransferable test cases that achieves a superior 63.56% average over-refusal\nrate across 10 diverse LLMs, significantly outperforming existing datasets.\nORFuzz and ORFuzzSet provide a robust automated testing framework and a\nvaluable community resource, paving the way for developing more reliable and\ntrustworthy LLM-based software systems."
                },
                "authors": [
                    {
                        "name": "Haonan Zhang"
                    },
                    {
                        "name": "Dongxia Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Kexin Chen"
                    },
                    {
                        "name": "Jiashui Wang"
                    },
                    {
                        "name": "Xinlei Ying"
                    },
                    {
                        "name": "Long Liu"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01671v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01671v3",
                "updated": "2025-08-15T05:01:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    5,
                    1,
                    34,
                    4,
                    227,
                    0
                ],
                "published": "2024-10-02T15:39:55Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    39,
                    55,
                    2,
                    276,
                    0
                ],
                "title": "Bridging Context Gaps: Leveraging Coreference Resolution for Long\n  Contextual Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Context Gaps: Leveraging Coreference Resolution for Long\n  Contextual Understanding"
                },
                "summary": "Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing; however, they still face difficulties when tasked with\nunderstanding lengthy contexts and executing effective question answering.\nThese challenges often arise due to the complexity and ambiguity present in\nlonger texts. To enhance the performance of LLMs in such scenarios, we\nintroduce the Long Question Coreference Adaptation (LQCA) method. This\ninnovative framework focuses on coreference resolution tailored to long\ncontexts, allowing the model to identify and manage references effectively. The\nLQCA method encompasses four key steps: resolving coreferences within\nsub-documents, computing the distances between mentions, defining a\nrepresentative mention for coreference, and answering questions through mention\nreplacement. By processing information systematically, the framework provides\neasier-to-handle partitions for LLMs, promoting better understanding.\nExperimental evaluations on a range of LLMs and datasets have yielded positive\nresults, with a notable improvements on OpenAI-o1-mini and GPT-4o models,\nhighlighting the effectiveness of leveraging coreference resolution to bridge\ncontext gaps in question answering. Our code is public at\nhttps://github.com/OceannTwT/LQCA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing; however, they still face difficulties when tasked with\nunderstanding lengthy contexts and executing effective question answering.\nThese challenges often arise due to the complexity and ambiguity present in\nlonger texts. To enhance the performance of LLMs in such scenarios, we\nintroduce the Long Question Coreference Adaptation (LQCA) method. This\ninnovative framework focuses on coreference resolution tailored to long\ncontexts, allowing the model to identify and manage references effectively. The\nLQCA method encompasses four key steps: resolving coreferences within\nsub-documents, computing the distances between mentions, defining a\nrepresentative mention for coreference, and answering questions through mention\nreplacement. By processing information systematically, the framework provides\neasier-to-handle partitions for LLMs, promoting better understanding.\nExperimental evaluations on a range of LLMs and datasets have yielded positive\nresults, with a notable improvements on OpenAI-o1-mini and GPT-4o models,\nhighlighting the effectiveness of leveraging coreference resolution to bridge\ncontext gaps in question answering. Our code is public at\nhttps://github.com/OceannTwT/LQCA."
                },
                "authors": [
                    {
                        "name": "Yanming Liu"
                    },
                    {
                        "name": "Xinyue Peng"
                    },
                    {
                        "name": "Jiannan Cao"
                    },
                    {
                        "name": "Yanxin Shen"
                    },
                    {
                        "name": "Tianyu Du"
                    },
                    {
                        "name": "Sheng Cheng"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Xuhong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuhong Zhang"
                },
                "author": "Xuhong Zhang",
                "arxiv_comment": "ICLR 2025 camera ready version, with second updated metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01671v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01671v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03807v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03807v4",
                "updated": "2025-08-15T04:55:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    55,
                    16,
                    4,
                    227,
                    0
                ],
                "published": "2024-06-06T07:30:14Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    7,
                    30,
                    14,
                    3,
                    158,
                    0
                ],
                "title": "Tool-Planner: Task Planning with Clusters across Multiple Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-Planner: Task Planning with Clusters across Multiple Tools"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional reasoning\ncapabilities, enabling them to solve various complex problems. Recently, this\nability has been applied to the paradigm of tool learning. Tool learning\ninvolves providing examples of tool usage and their corresponding functions,\nallowing LLMs to formulate plans and demonstrate the process of invoking and\nexecuting each tool. LLMs can address tasks that they cannot complete\nindependently, thereby enhancing their potential across different tasks.\nHowever, this approach faces two key challenges. First, redundant error\ncorrection leads to unstable planning and long execution time. Additionally,\ndesigning a correct plan among multiple tools is also a challenge in tool\nlearning. To address these issues, we propose Tool-Planner, a task-processing\nframework based on toolkits. Tool-Planner groups tools based on the API\nfunctions with the same function into a toolkit and allows LLMs to implement\nplanning across the various toolkits. When a tool error occurs, the language\nmodel can reselect and adjust tools based on the toolkit. Experiments show that\nour approach demonstrates a high pass and win rate across different datasets\nand optimizes the planning scheme for tool learning in models such as GPT-4 and\nClaude 3, showcasing the potential of our method. Our code is public at\nhttps://github.com/OceannTwT/Tool-Planner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional reasoning\ncapabilities, enabling them to solve various complex problems. Recently, this\nability has been applied to the paradigm of tool learning. Tool learning\ninvolves providing examples of tool usage and their corresponding functions,\nallowing LLMs to formulate plans and demonstrate the process of invoking and\nexecuting each tool. LLMs can address tasks that they cannot complete\nindependently, thereby enhancing their potential across different tasks.\nHowever, this approach faces two key challenges. First, redundant error\ncorrection leads to unstable planning and long execution time. Additionally,\ndesigning a correct plan among multiple tools is also a challenge in tool\nlearning. To address these issues, we propose Tool-Planner, a task-processing\nframework based on toolkits. Tool-Planner groups tools based on the API\nfunctions with the same function into a toolkit and allows LLMs to implement\nplanning across the various toolkits. When a tool error occurs, the language\nmodel can reselect and adjust tools based on the toolkit. Experiments show that\nour approach demonstrates a high pass and win rate across different datasets\nand optimizes the planning scheme for tool learning in models such as GPT-4 and\nClaude 3, showcasing the potential of our method. Our code is public at\nhttps://github.com/OceannTwT/Tool-Planner"
                },
                "authors": [
                    {
                        "name": "Yanming Liu"
                    },
                    {
                        "name": "Xinyue Peng"
                    },
                    {
                        "name": "Jiannan Cao"
                    },
                    {
                        "name": "Yuwei Zhang"
                    },
                    {
                        "name": "Xuhong Zhang"
                    },
                    {
                        "name": "Sheng Cheng"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Tianyu Du"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Du"
                },
                "author": "Tianyu Du",
                "arxiv_comment": "ICLR 2025 Camera Ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03807v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03807v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11218v1",
                "updated": "2025-08-15T04:50:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    50,
                    27,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T04:50:27Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    50,
                    27,
                    4,
                    227,
                    0
                ],
                "title": "A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian\n  Re-Identification in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian\n  Re-Identification in Autonomous Driving"
                },
                "summary": "Re-Identification (ReID) is a critical technology in intelligent perception\nsystems, especially within autonomous driving, where onboard cameras must\nidentify pedestrians across views and time in real-time to support safe\nnavigation and trajectory prediction. However, the presence of uncertain or\nmissing input modalities--such as RGB, infrared, sketches, or textual\ndescriptions--poses significant challenges to conventional ReID approaches.\nWhile large-scale pre-trained models offer strong multimodal semantic modeling\ncapabilities, their computational overhead limits practical deployment in\nresource-constrained environments. To address these challenges, we propose a\nlightweight Uncertainty Modal Modeling (UMM) framework, which integrates a\nmultimodal token mapper, synthetic modality augmentation strategy, and\ncross-modal cue interactive learner. Together, these components enable unified\nfeature representation, mitigate the impact of missing modalities, and extract\ncomplementary information across different data types. Additionally, UMM\nleverages CLIP's vision-language alignment ability to fuse multimodal inputs\nefficiently without extensive finetuning. Experimental results demonstrate that\nUMM achieves strong robustness, generalization, and computational efficiency\nunder uncertain modality conditions, offering a scalable and practical solution\nfor pedestrian re-identification in autonomous driving scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-Identification (ReID) is a critical technology in intelligent perception\nsystems, especially within autonomous driving, where onboard cameras must\nidentify pedestrians across views and time in real-time to support safe\nnavigation and trajectory prediction. However, the presence of uncertain or\nmissing input modalities--such as RGB, infrared, sketches, or textual\ndescriptions--poses significant challenges to conventional ReID approaches.\nWhile large-scale pre-trained models offer strong multimodal semantic modeling\ncapabilities, their computational overhead limits practical deployment in\nresource-constrained environments. To address these challenges, we propose a\nlightweight Uncertainty Modal Modeling (UMM) framework, which integrates a\nmultimodal token mapper, synthetic modality augmentation strategy, and\ncross-modal cue interactive learner. Together, these components enable unified\nfeature representation, mitigate the impact of missing modalities, and extract\ncomplementary information across different data types. Additionally, UMM\nleverages CLIP's vision-language alignment ability to fuse multimodal inputs\nefficiently without extensive finetuning. Experimental results demonstrate that\nUMM achieves strong robustness, generalization, and computational efficiency\nunder uncertain modality conditions, offering a scalable and practical solution\nfor pedestrian re-identification in autonomous driving scenarios."
                },
                "authors": [
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Shuqi Wu"
                    },
                    {
                        "name": "Ning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ning Wang"
                },
                "author": "Ning Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11211v1",
                "updated": "2025-08-15T04:41:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    41,
                    5,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T04:41:05Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    41,
                    5,
                    4,
                    227,
                    0
                ],
                "title": "Efficient Image-to-Image Schrödinger Bridge for CT Field of View\n  Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Image-to-Image Schrödinger Bridge for CT Field of View\n  Extension"
                },
                "summary": "Computed tomography (CT) is a cornerstone imaging modality for non-invasive,\nhigh-resolution visualization of internal anatomical structures. However, when\nthe scanned object exceeds the scanner's field of view (FOV), projection data\nare truncated, resulting in incomplete reconstructions and pronounced artifacts\nnear FOV boundaries. Conventional reconstruction algorithms struggle to recover\naccurate anatomy from such data, limiting clinical reliability. Deep learning\napproaches have been explored for FOV extension, with diffusion generative\nmodels representing the latest advances in image synthesis. Yet, conventional\ndiffusion models are computationally demanding and slow at inference due to\ntheir iterative sampling process. To address these limitations, we propose an\nefficient CT FOV extension framework based on the image-to-image Schr\\\"odinger\nBridge (I$^2$SB) diffusion model. Unlike traditional diffusion models that\nsynthesize images from pure Gaussian noise, I$^2$SB learns a direct stochastic\nmapping between paired limited-FOV and extended-FOV images. This direct\ncorrespondence yields a more interpretable and traceable generative process,\nenhancing anatomical consistency and structural fidelity in reconstructions.\nI$^2$SB achieves superior quantitative performance, with root-mean-square error\n(RMSE) values of 49.8\\,HU on simulated noisy data and 152.0HU on real data,\noutperforming state-of-the-art diffusion models such as conditional denoising\ndiffusion probabilistic models (cDDPM) and patch-based diffusion methods.\nMoreover, its one-step inference enables reconstruction in just 0.19s per 2D\nslice, representing over a 700-fold speedup compared to cDDPM (135s) and\nsurpassing diffusionGAN (0.58s), the second fastest. This combination of\naccuracy and efficiency makes I$^2$SB highly suitable for real-time or clinical\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computed tomography (CT) is a cornerstone imaging modality for non-invasive,\nhigh-resolution visualization of internal anatomical structures. However, when\nthe scanned object exceeds the scanner's field of view (FOV), projection data\nare truncated, resulting in incomplete reconstructions and pronounced artifacts\nnear FOV boundaries. Conventional reconstruction algorithms struggle to recover\naccurate anatomy from such data, limiting clinical reliability. Deep learning\napproaches have been explored for FOV extension, with diffusion generative\nmodels representing the latest advances in image synthesis. Yet, conventional\ndiffusion models are computationally demanding and slow at inference due to\ntheir iterative sampling process. To address these limitations, we propose an\nefficient CT FOV extension framework based on the image-to-image Schr\\\"odinger\nBridge (I$^2$SB) diffusion model. Unlike traditional diffusion models that\nsynthesize images from pure Gaussian noise, I$^2$SB learns a direct stochastic\nmapping between paired limited-FOV and extended-FOV images. This direct\ncorrespondence yields a more interpretable and traceable generative process,\nenhancing anatomical consistency and structural fidelity in reconstructions.\nI$^2$SB achieves superior quantitative performance, with root-mean-square error\n(RMSE) values of 49.8\\,HU on simulated noisy data and 152.0HU on real data,\noutperforming state-of-the-art diffusion models such as conditional denoising\ndiffusion probabilistic models (cDDPM) and patch-based diffusion methods.\nMoreover, its one-step inference enables reconstruction in just 0.19s per 2D\nslice, representing over a 700-fold speedup compared to cDDPM (135s) and\nsurpassing diffusionGAN (0.58s), the second fastest. This combination of\naccuracy and efficiency makes I$^2$SB highly suitable for real-time or clinical\ndeployment."
                },
                "authors": [
                    {
                        "name": "Zhenhao Li"
                    },
                    {
                        "name": "Long Yang"
                    },
                    {
                        "name": "Xiaojie Yin"
                    },
                    {
                        "name": "Haijun Yu"
                    },
                    {
                        "name": "Jiazhou Wang"
                    },
                    {
                        "name": "Hongbin Han"
                    },
                    {
                        "name": "Weigang Hu"
                    },
                    {
                        "name": "Yixing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yixing Huang"
                },
                "author": "Yixing Huang",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07088v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07088v7",
                "updated": "2025-08-15T04:39:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    39,
                    5,
                    4,
                    227,
                    0
                ],
                "published": "2024-03-11T18:26:02Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    18,
                    26,
                    2,
                    0,
                    71,
                    0
                ],
                "title": "SPA: Towards A Computational Friendly Cloud-Base and On-Devices\n  Collaboration Seq2seq Personalized Generation with Casual Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPA: Towards A Computational Friendly Cloud-Base and On-Devices\n  Collaboration Seq2seq Personalized Generation with Casual Inference"
                },
                "summary": "Large language models(LLMs) have shown its outperforming ability on various\ntasks and question answering. However, LLMs require substantial memory storage\non low-resource devices. More critically, the computational speed on these\ndevices is also severely limited. In this paper, we propose SPA(Side Plugin\nAdaption), a lightweight architecture for fast on-devices inference on the\nconstraints of strict on-devices computation and memory constraints. Compared\nwith other on-devices seq2seq generation, SPA could make a fast and stable\ninference on low-resource constraints, allowing it to obtain cost effiency. Our\nmethod establish an interaction between a pretrained LLMs on-cloud and additive\nparameters on-devices, which could provide the knowledge on both pretrained\nLLMs and featured personal feature. Further more, SPA provides a framework to\nkeep feature-base parameters on low computational devices while leave the\nparameters containing general information on the high computational devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models(LLMs) have shown its outperforming ability on various\ntasks and question answering. However, LLMs require substantial memory storage\non low-resource devices. More critically, the computational speed on these\ndevices is also severely limited. In this paper, we propose SPA(Side Plugin\nAdaption), a lightweight architecture for fast on-devices inference on the\nconstraints of strict on-devices computation and memory constraints. Compared\nwith other on-devices seq2seq generation, SPA could make a fast and stable\ninference on low-resource constraints, allowing it to obtain cost effiency. Our\nmethod establish an interaction between a pretrained LLMs on-cloud and additive\nparameters on-devices, which could provide the knowledge on both pretrained\nLLMs and featured personal feature. Further more, SPA provides a framework to\nkeep feature-base parameters on low computational devices while leave the\nparameters containing general information on the high computational devices."
                },
                "authors": [
                    {
                        "name": "Yanming Liu"
                    },
                    {
                        "name": "Xinyue Peng"
                    },
                    {
                        "name": "Ningjing Sang"
                    },
                    {
                        "name": "Yafeng Yan"
                    },
                    {
                        "name": "Xiaolan Ke"
                    },
                    {
                        "name": "Zhiting Zheng"
                    },
                    {
                        "name": "Shaobo Liu"
                    },
                    {
                        "name": "Songhang Deng"
                    },
                    {
                        "name": "Jiannan Cao"
                    },
                    {
                        "name": "Le Dai"
                    },
                    {
                        "name": "Xingzu Liu"
                    },
                    {
                        "name": "Ruilin Nong"
                    },
                    {
                        "name": "Weihao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weihao Liu"
                },
                "author": "Weihao Liu",
                "arxiv_comment": "Update for details",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07088v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07088v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v2",
                "updated": "2025-08-15T04:27:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    27,
                    30,
                    4,
                    227,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10318v2",
                "updated": "2025-08-15T04:13:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    13,
                    31,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-14T03:46:24Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    3,
                    46,
                    24,
                    3,
                    226,
                    0
                ],
                "title": "Quantifying the Value of Seismic Structural Health Monitoring for\n  post-earthquake recovery of electric power system in terms of resilience\n  enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the Value of Seismic Structural Health Monitoring for\n  post-earthquake recovery of electric power system in terms of resilience\n  enhancement"
                },
                "summary": "Post-earthquake recovery of electric power networks (EPNs) is critical to\ncommunity resilience. Traditional recovery processes often rely on prolonged\nand imprecise manual inspections for damage diagnosis, leading to suboptimal\nrepair prioritization and extended service disruptions. Seismic Structural\nHealth Monitoring (SSHM) offers the potential to expedite recovery by enabling\nmore accurate and timely damage assessment. However, SSHM deployment incurs\ncosts, and its system-level resilience benefit remains underexplored. This\nstudy proposes a probabilistic simulation framework to quantify the value of\nSSHM for enhancing EPN resilience. The framework includes seismic damage\nmodeling based on network configuration, hazard intensity, fragility functions,\nand damage-functionality mappings, combined with recovery simulations\nincorporating resource constraints, repair and transfer durations. System\nfunctionality is evaluated using graph-based island detection and optimal power\nflow analysis. Resilience is quantified via the Lack of Resilience (LoR) metric\nderived from the functionality restoration curve. SSHM is incorporated by\naltering the quality of damage information used in repair scheduling. Different\nmonitoring scenarios (e.g., no-SSHM baseline, partial SSHM, full SSHM with\nvarious accuracies) are modeled using confusion matrices to simulate damage\nmisclassification. Results show that improved damage awareness via SSHM\nsignificantly accelerates recovery and reduces LoR by up to 21%. This work\nsupports evidence-based decisions for SSHM deployment in critical\ninfrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-earthquake recovery of electric power networks (EPNs) is critical to\ncommunity resilience. Traditional recovery processes often rely on prolonged\nand imprecise manual inspections for damage diagnosis, leading to suboptimal\nrepair prioritization and extended service disruptions. Seismic Structural\nHealth Monitoring (SSHM) offers the potential to expedite recovery by enabling\nmore accurate and timely damage assessment. However, SSHM deployment incurs\ncosts, and its system-level resilience benefit remains underexplored. This\nstudy proposes a probabilistic simulation framework to quantify the value of\nSSHM for enhancing EPN resilience. The framework includes seismic damage\nmodeling based on network configuration, hazard intensity, fragility functions,\nand damage-functionality mappings, combined with recovery simulations\nincorporating resource constraints, repair and transfer durations. System\nfunctionality is evaluated using graph-based island detection and optimal power\nflow analysis. Resilience is quantified via the Lack of Resilience (LoR) metric\nderived from the functionality restoration curve. SSHM is incorporated by\naltering the quality of damage information used in repair scheduling. Different\nmonitoring scenarios (e.g., no-SSHM baseline, partial SSHM, full SSHM with\nvarious accuracies) are modeled using confusion matrices to simulate damage\nmisclassification. Results show that improved damage awareness via SSHM\nsignificantly accelerates recovery and reduces LoR by up to 21%. This work\nsupports evidence-based decisions for SSHM deployment in critical\ninfrastructure."
                },
                "authors": [
                    {
                        "name": "Huangbin Liang"
                    },
                    {
                        "name": "Beatriz Moya"
                    },
                    {
                        "name": "Francisco Chinesta"
                    },
                    {
                        "name": "Eleni Chatzi"
                    }
                ],
                "author_detail": {
                    "name": "Eleni Chatzi"
                },
                "author": "Eleni Chatzi",
                "arxiv_comment": "21 pages. 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90B25, 90C35, 62P30, 93C95",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11196v1",
                "updated": "2025-08-15T04:06:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    6,
                    40,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T04:06:40Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    6,
                    40,
                    4,
                    227,
                    0
                ],
                "title": "UAV-VL-R1: Generalizing Vision-Language Models via Supervised\n  Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV-VL-R1: Generalizing Vision-Language Models via Supervised\n  Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning"
                },
                "summary": "Recent advances in vision-language models (VLMs) have demonstrated strong\ngeneralization in natural image tasks. However, their performance often\ndegrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features\nhigh resolution, complex spatial semantics, and strict real-time constraints.\nThese challenges limit the applicability of general-purpose VLMs to structured\naerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a\nlightweight VLM explicitly designed for aerial visual reasoning. It is trained\nusing a hybrid method that combines supervised fine-tuning (SFT) and\nmulti-stage reinforcement learning (RL). We leverage the group relative policy\noptimization (GRPO) algorithm to promote structured and interpretable reasoning\nthrough rule-guided rewards and intra-group policy alignment. To support model\ntraining and evaluation, we introduce a high-resolution visual question\nanswering dataset named HRVQA-VL, which consists of 50,019 annotated samples\ncovering eight UAV-relevant reasoning tasks, including object counting,\ntransportation recognition, and spatial scene inference. Experimental results\nshow that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the\nQwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which\nis 36x larger, on multiple tasks. Ablation studies reveal that while SFT\nimproves semantic alignment, it may reduce reasoning diversity in mathematical\ntasks. GRPO-based RL compensates for this limitation by enhancing logical\nflexibility and the robustness of inference. Additionally, UAV-VL-R1 requires\nonly 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with\nINT8, supporting real-time deployment on resource-constrained UAV platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language models (VLMs) have demonstrated strong\ngeneralization in natural image tasks. However, their performance often\ndegrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features\nhigh resolution, complex spatial semantics, and strict real-time constraints.\nThese challenges limit the applicability of general-purpose VLMs to structured\naerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a\nlightweight VLM explicitly designed for aerial visual reasoning. It is trained\nusing a hybrid method that combines supervised fine-tuning (SFT) and\nmulti-stage reinforcement learning (RL). We leverage the group relative policy\noptimization (GRPO) algorithm to promote structured and interpretable reasoning\nthrough rule-guided rewards and intra-group policy alignment. To support model\ntraining and evaluation, we introduce a high-resolution visual question\nanswering dataset named HRVQA-VL, which consists of 50,019 annotated samples\ncovering eight UAV-relevant reasoning tasks, including object counting,\ntransportation recognition, and spatial scene inference. Experimental results\nshow that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the\nQwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which\nis 36x larger, on multiple tasks. Ablation studies reveal that while SFT\nimproves semantic alignment, it may reduce reasoning diversity in mathematical\ntasks. GRPO-based RL compensates for this limitation by enhancing logical\nflexibility and the robustness of inference. Additionally, UAV-VL-R1 requires\nonly 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with\nINT8, supporting real-time deployment on resource-constrained UAV platforms."
                },
                "authors": [
                    {
                        "name": "Jiajin Guan"
                    },
                    {
                        "name": "Haibo Mei"
                    },
                    {
                        "name": "Bonan Zhang"
                    },
                    {
                        "name": "Dan Liu"
                    },
                    {
                        "name": "Yuanshuang Fu"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "arxiv_affiliation": "School of Aeronautics and Astronautics, University of Electronic Science and Technology of China, Chengdu, China",
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09023v2",
                "updated": "2025-08-15T03:52:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    3,
                    52,
                    9,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-12T15:38:10Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    38,
                    10,
                    1,
                    224,
                    0
                ],
                "title": "E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and\n  Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and\n  Efficiency"
                },
                "summary": "SQL query rewriting aims to reformulate a query into a more efficient form\nwhile preserving equivalence. Most existing methods rely on predefined rewrite\nrules. However, such rule-based approaches face fundamental limitations: (1)\nfixed rule sets generalize poorly to novel query patterns and struggle with\ncomplex queries; (2) a wide range of effective rewriting strategies cannot be\nfully captured by declarative rules. To overcome these issues, we propose using\nlarge language models (LLMs) to generate rewrites. LLMs can capture complex\nstrategies, such as evaluation reordering and CTE rewriting. Despite this\npotential, directly applying LLMs often results in performance regressions or\nnon-equivalent rewrites due to a lack of execution awareness and semantic\ngrounding. To address these challenges, We present E3-Rewrite, an LLM-based SQL\nrewriting framework that produces executable, equivalent, and efficient\nqueries. It integrates two core components: a context construction module and a\nreinforcement learning framework. First, the context module leverages execution\nplans and retrieved demonstrations to build bottleneck-aware prompts that guide\ninference-time rewriting. Second, we design a reward function targeting\nexecutability, equivalence, and efficiency, evaluated via syntax checks,\nequivalence verification, and cost estimation. Third, to ensure stable\nmulti-objective learning, we adopt a staged curriculum that first emphasizes\nexecutability and equivalence, then gradually incorporates efficiency. Across\nmultiple SQL benchmarks, our experiments demonstrate that E3-Rewrite can\nshorten query execution time by as much as 25.6% relative to leading baselines,\nwhile also producing up to 24.4% more rewrites that meet strict equivalence\ncriteria. These gains extend to challenging query patterns that prior\napproaches could not effectively optimize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL query rewriting aims to reformulate a query into a more efficient form\nwhile preserving equivalence. Most existing methods rely on predefined rewrite\nrules. However, such rule-based approaches face fundamental limitations: (1)\nfixed rule sets generalize poorly to novel query patterns and struggle with\ncomplex queries; (2) a wide range of effective rewriting strategies cannot be\nfully captured by declarative rules. To overcome these issues, we propose using\nlarge language models (LLMs) to generate rewrites. LLMs can capture complex\nstrategies, such as evaluation reordering and CTE rewriting. Despite this\npotential, directly applying LLMs often results in performance regressions or\nnon-equivalent rewrites due to a lack of execution awareness and semantic\ngrounding. To address these challenges, We present E3-Rewrite, an LLM-based SQL\nrewriting framework that produces executable, equivalent, and efficient\nqueries. It integrates two core components: a context construction module and a\nreinforcement learning framework. First, the context module leverages execution\nplans and retrieved demonstrations to build bottleneck-aware prompts that guide\ninference-time rewriting. Second, we design a reward function targeting\nexecutability, equivalence, and efficiency, evaluated via syntax checks,\nequivalence verification, and cost estimation. Third, to ensure stable\nmulti-objective learning, we adopt a staged curriculum that first emphasizes\nexecutability and equivalence, then gradually incorporates efficiency. Across\nmultiple SQL benchmarks, our experiments demonstrate that E3-Rewrite can\nshorten query execution time by as much as 25.6% relative to leading baselines,\nwhile also producing up to 24.4% more rewrites that meet strict equivalence\ncriteria. These gains extend to challenging query patterns that prior\napproaches could not effectively optimize."
                },
                "authors": [
                    {
                        "name": "Dongjie Xu"
                    },
                    {
                        "name": "Yue Cui"
                    },
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Qingzhi Ma"
                    },
                    {
                        "name": "Hanghui Guo"
                    },
                    {
                        "name": "Jiaming Li"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Ruiyuan Zhang"
                    },
                    {
                        "name": "Shimin Di"
                    },
                    {
                        "name": "Jia Zhu"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Jiajie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajie Xu"
                },
                "author": "Jiajie Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11189v1",
                "updated": "2025-08-15T03:46:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    3,
                    46,
                    46,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T03:46:46Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    3,
                    46,
                    46,
                    4,
                    227,
                    0
                ],
                "title": "Novel Parasitic Dual-Scale Modeling for Efficient and Accurate\n  Multilingual Speech Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Parasitic Dual-Scale Modeling for Efficient and Accurate\n  Multilingual Speech Translation"
                },
                "summary": "Recent advancements in speech-to-text translation have led to the development\nof multilingual models capable of handling multiple language pairs\nsimultaneously. However, these unified models often suffer from large parameter\nsizes, making it challenging to balance inference efficiency and performance,\nparticularly in local deployment scenarios. We propose an innovative Parasitic\nDual-Scale Approach, which combines an enhanced speculative sampling method\nwith model compression and knowledge distillation techniques. Building on the\nWhisper Medium model, we enhance it for multilingual speech translation into\nwhisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art\n(SOTA) performance across six popular languages with improved inference\nefficiency. KVSPN enables a 40\\% speedup with no BLEU score degradation.\nCombined with distillation methods, it represents a 2.6$\\times$ speedup over\nthe original Whisper Medium with superior performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in speech-to-text translation have led to the development\nof multilingual models capable of handling multiple language pairs\nsimultaneously. However, these unified models often suffer from large parameter\nsizes, making it challenging to balance inference efficiency and performance,\nparticularly in local deployment scenarios. We propose an innovative Parasitic\nDual-Scale Approach, which combines an enhanced speculative sampling method\nwith model compression and knowledge distillation techniques. Building on the\nWhisper Medium model, we enhance it for multilingual speech translation into\nwhisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art\n(SOTA) performance across six popular languages with improved inference\nefficiency. KVSPN enables a 40\\% speedup with no BLEU score degradation.\nCombined with distillation methods, it represents a 2.6$\\times$ speedup over\nthe original Whisper Medium with superior performance."
                },
                "authors": [
                    {
                        "name": "Chenyang Le"
                    },
                    {
                        "name": "Yinfeng Xia"
                    },
                    {
                        "name": "Huiyan Li"
                    },
                    {
                        "name": "Manhong Wang"
                    },
                    {
                        "name": "Yutao Sun"
                    },
                    {
                        "name": "Xingyang Ma"
                    },
                    {
                        "name": "Yanmin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Yanmin Qian"
                },
                "author": "Yanmin Qian",
                "arxiv_comment": "Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18013v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18013v2",
                "updated": "2025-08-15T03:28:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    3,
                    28,
                    39,
                    4,
                    227,
                    0
                ],
                "published": "2024-02-28T03:16:44Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    3,
                    16,
                    44,
                    2,
                    59,
                    0
                ],
                "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems"
                },
                "summary": "This survey provides a comprehensive review of research on multi-turn\ndialogue systems, with a particular focus on multi-turn dialogue systems based\non large language models (LLMs). This paper aims to (a) give a summary of\nexisting LLMs and approaches for adapting LLMs to downstream tasks; (b)\nelaborate recent advances in multi-turn dialogue systems, covering both\nLLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,\nalong with datasets and evaluation metrics; (c) discuss some future emphasis\nand recent research problems arising from the development of LLMs and the\nincreasing demands on multi-turn dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey provides a comprehensive review of research on multi-turn\ndialogue systems, with a particular focus on multi-turn dialogue systems based\non large language models (LLMs). This paper aims to (a) give a summary of\nexisting LLMs and approaches for adapting LLMs to downstream tasks; (b)\nelaborate recent advances in multi-turn dialogue systems, covering both\nLLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,\nalong with datasets and evaluation metrics; (c) discuss some future emphasis\nand recent research problems arising from the development of LLMs and the\nincreasing demands on multi-turn dialogue systems."
                },
                "authors": [
                    {
                        "name": "Zihao Yi"
                    },
                    {
                        "name": "Jiarui Ouyang"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Yuwen Liu"
                    },
                    {
                        "name": "Tianhao Liao"
                    },
                    {
                        "name": "Haohao Luo"
                    },
                    {
                        "name": "Ying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shen"
                },
                "author": "Ying Shen",
                "arxiv_comment": "35 pages, 10 figures, ACM Computing Surveys",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18013v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18013v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11184v1",
                "updated": "2025-08-15T03:20:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    3,
                    20,
                    37,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T03:20:37Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    3,
                    20,
                    37,
                    4,
                    227,
                    0
                ],
                "title": "Personalized Distractor Generation via MCTS-Guided Reasoning\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Distractor Generation via MCTS-Guided Reasoning\n  Reconstruction"
                },
                "summary": "Distractors, incorrect but plausible answer choices in multiple-choice\nquestions (MCQs), play a critical role in educational assessment by diagnosing\nstudent misconceptions. Recent work has leveraged large language models (LLMs)\nto generate shared, group-level distractors by learning common error patterns\nacross large student populations. However, such distractors often fail to\ncapture the diverse reasoning errors of individual students, limiting their\ndiagnostic effectiveness. To address this limitation, we introduce the task of\npersonalized distractor generation, which aims to generate tailored distractors\nbased on individual misconceptions inferred from each student's past\nquestion-answering (QA) records, ensuring every student receives options that\neffectively exposes their specific reasoning errors. While promising, this task\nis challenging because each student typically has only a few QA records, which\noften lack the student's underlying reasoning processes, making training-based\ngroup-level approaches infeasible. To overcome this, we propose a training-free\ntwo-stage framework. In the first stage, we construct a student-specific\nmisconception prototype by applying Monte Carlo Tree Search (MCTS) to recover\nthe student's reasoning trajectories from past incorrect answers. In the second\nstage, this prototype guides the simulation of the student's reasoning on new\nquestions, enabling the generation of personalized distractors that align with\nthe student's recurring misconceptions. Experiments show that our approach\nachieves the best performance in generating plausible, personalized distractors\nfor 140 students, and also effectively generalizes to group-level settings,\nhighlighting its robustness and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distractors, incorrect but plausible answer choices in multiple-choice\nquestions (MCQs), play a critical role in educational assessment by diagnosing\nstudent misconceptions. Recent work has leveraged large language models (LLMs)\nto generate shared, group-level distractors by learning common error patterns\nacross large student populations. However, such distractors often fail to\ncapture the diverse reasoning errors of individual students, limiting their\ndiagnostic effectiveness. To address this limitation, we introduce the task of\npersonalized distractor generation, which aims to generate tailored distractors\nbased on individual misconceptions inferred from each student's past\nquestion-answering (QA) records, ensuring every student receives options that\neffectively exposes their specific reasoning errors. While promising, this task\nis challenging because each student typically has only a few QA records, which\noften lack the student's underlying reasoning processes, making training-based\ngroup-level approaches infeasible. To overcome this, we propose a training-free\ntwo-stage framework. In the first stage, we construct a student-specific\nmisconception prototype by applying Monte Carlo Tree Search (MCTS) to recover\nthe student's reasoning trajectories from past incorrect answers. In the second\nstage, this prototype guides the simulation of the student's reasoning on new\nquestions, enabling the generation of personalized distractors that align with\nthe student's recurring misconceptions. Experiments show that our approach\nachieves the best performance in generating plausible, personalized distractors\nfor 140 students, and also effectively generalizes to group-level settings,\nhighlighting its robustness and adaptability."
                },
                "authors": [
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Jingyuan Chen"
                    },
                    {
                        "name": "Wang Lin"
                    },
                    {
                        "name": "Jian Zhan"
                    },
                    {
                        "name": "Mengze Li"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00344v2",
                "updated": "2025-08-15T03:07:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    3,
                    7,
                    52,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-01T06:17:11Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    6,
                    17,
                    11,
                    4,
                    213,
                    0
                ],
                "title": "PilotRL: Training Language Model Agents via Global Planning-Guided\n  Progressive Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PilotRL: Training Language Model Agents via Global Planning-Guided\n  Progressive Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable advancements in tackling\nagent-oriented tasks. Despite their potential, existing work faces challenges\nwhen deploying LLMs in agent-based environments. The widely adopted agent\nparadigm ReAct centers on integrating single-step reasoning with immediate\naction execution, which limits its effectiveness in complex tasks requiring\nlong-term strategic planning. Furthermore, the coordination between the planner\nand executor during problem-solving is also a critical factor to consider in\nagent design. Additionally, current approaches predominantly rely on supervised\nfine-tuning, which often leads models to memorize established task completion\ntrajectories, thereby restricting their generalization ability when confronted\nwith novel problem contexts. To address these challenges, we introduce an\nadaptive global plan-based agent paradigm AdaPlan, aiming to synergize\nhigh-level explicit guidance with execution to support effective long-horizon\ndecision-making. Based on the proposed paradigm, we further put forward\nPilotRL, a global planning-guided training framework for LLM agents driven by\nprogressive reinforcement learning. We first develop the model's ability to\nfollow explicit guidance from global plans when addressing agent tasks.\nSubsequently, based on this foundation, we focus on optimizing the quality of\ngenerated plans. Finally, we conduct joint optimization of the model's planning\nand execution coordination. Experiments indicate that PilotRL could achieve\nstate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing\nclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%\ncomparing to GPT-4o-mini at a comparable parameter scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable advancements in tackling\nagent-oriented tasks. Despite their potential, existing work faces challenges\nwhen deploying LLMs in agent-based environments. The widely adopted agent\nparadigm ReAct centers on integrating single-step reasoning with immediate\naction execution, which limits its effectiveness in complex tasks requiring\nlong-term strategic planning. Furthermore, the coordination between the planner\nand executor during problem-solving is also a critical factor to consider in\nagent design. Additionally, current approaches predominantly rely on supervised\nfine-tuning, which often leads models to memorize established task completion\ntrajectories, thereby restricting their generalization ability when confronted\nwith novel problem contexts. To address these challenges, we introduce an\nadaptive global plan-based agent paradigm AdaPlan, aiming to synergize\nhigh-level explicit guidance with execution to support effective long-horizon\ndecision-making. Based on the proposed paradigm, we further put forward\nPilotRL, a global planning-guided training framework for LLM agents driven by\nprogressive reinforcement learning. We first develop the model's ability to\nfollow explicit guidance from global plans when addressing agent tasks.\nSubsequently, based on this foundation, we focus on optimizing the quality of\ngenerated plans. Finally, we conduct joint optimization of the model's planning\nand execution coordination. Experiments indicate that PilotRL could achieve\nstate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing\nclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%\ncomparing to GPT-4o-mini at a comparable parameter scale."
                },
                "authors": [
                    {
                        "name": "Keer Lu"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Huang Leng"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]