[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.17635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v2",
                "updated": "2025-03-06T06:39:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    6,
                    39,
                    56,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought"
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Camera ready version for NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01801v2",
                "updated": "2025-03-05T20:36:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    20,
                    36,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T18:32:31Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    32,
                    31,
                    0,
                    62,
                    0
                ],
                "title": "TUNA: Tuning Unstable and Noisy Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUNA: Tuning Unstable and Noisy Cloud Applications"
                },
                "summary": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies."
                },
                "authors": [
                    {
                        "name": "Johannes Freischuetz"
                    },
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Brian Kroth"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_doi": "10.1145/3689031.3717480",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3717480",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 20 figures, EuroSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03751v1",
                "updated": "2025-03-05T18:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control"
                },
                "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/"
                },
                "authors": [
                    {
                        "name": "Xuanchi Ren"
                    },
                    {
                        "name": "Tianchang Shen"
                    },
                    {
                        "name": "Jiahui Huang"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Merlin Nimier-David"
                    },
                    {
                        "name": "Thomas Müller"
                    },
                    {
                        "name": "Alexander Keller"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Jun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Gao"
                },
                "author": "Jun Gao",
                "arxiv_comment": "To appear in CVPR 2025. Website:\n  https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v1",
                "updated": "2025-03-05T17:59:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Video tokenizers, which transform videos into compact latent representations,\nare key to video generation. Existing video tokenizers are based on the VAE\narchitecture and follow a paradigm where an encoder compresses videos into\ncompact latents, and a deterministic decoder reconstructs the original videos\nfrom these latents. In this paper, we propose a novel\n\\underline{\\textbf{C}}onditioned \\underline{\\textbf{D}}iffusion-based video\n\\underline{\\textbf{T}}okenizer entitled \\textbf{\\ourmethod}, which departs from\nprevious methods by replacing the deterministic decoder with a 3D causal\ndiffusion model. The reverse diffusion generative process of the decoder is\nconditioned on the latent representations derived via the encoder. With a\nfeature caching and sampling acceleration, the framework efficiently\nreconstructs high-fidelity videos of arbitrary lengths. Results show that\n{\\ourmethod} achieves state-of-the-art performance in video reconstruction\ntasks using just a single-step sampling. Even a smaller version of {\\ourmethod}\nstill achieves reconstruction results on par with the top two baselines.\nFurthermore, the latent video generation model trained using {\\ourmethod} also\nshows superior performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video tokenizers, which transform videos into compact latent representations,\nare key to video generation. Existing video tokenizers are based on the VAE\narchitecture and follow a paradigm where an encoder compresses videos into\ncompact latents, and a deterministic decoder reconstructs the original videos\nfrom these latents. In this paper, we propose a novel\n\\underline{\\textbf{C}}onditioned \\underline{\\textbf{D}}iffusion-based video\n\\underline{\\textbf{T}}okenizer entitled \\textbf{\\ourmethod}, which departs from\nprevious methods by replacing the deterministic decoder with a 3D causal\ndiffusion model. The reverse diffusion generative process of the decoder is\nconditioned on the latent representations derived via the encoder. With a\nfeature caching and sampling acceleration, the framework efficiently\nreconstructs high-fidelity videos of arbitrary lengths. Results show that\n{\\ourmethod} achieves state-of-the-art performance in video reconstruction\ntasks using just a single-step sampling. Even a smaller version of {\\ourmethod}\nstill achieves reconstruction results on par with the top two baselines.\nFurthermore, the latent video generation model trained using {\\ourmethod} also\nshows superior performance."
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v3",
                "updated": "2025-03-05T14:43:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "arxiv_comment": "Will add a lemma in the proof of Theorem 5.3 to make the statement\n  and proof more rigorous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07714v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07714v5",
                "updated": "2025-03-05T07:39:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    39,
                    3,
                    2,
                    64,
                    0
                ],
                "published": "2024-03-12T14:57:40Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    14,
                    57,
                    40,
                    1,
                    72,
                    0
                ],
                "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system."
                },
                "authors": [
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shihao Liang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07714v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07714v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v1",
                "updated": "2025-03-05T04:54:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v1",
                "updated": "2025-03-04T19:51:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02812v1",
                "updated": "2025-03-04T17:37:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:37:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression"
                },
                "summary": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM."
                },
                "authors": [
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Éric de la Clergerie"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02758v1",
                "updated": "2025-03-04T16:21:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:21:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Efficient and Optimal No-Regret Caching under Partial Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Optimal No-Regret Caching under Partial Observation"
                },
                "summary": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces."
                },
                "authors": [
                    {
                        "name": "Younes Ben Mazziane"
                    },
                    {
                        "name": "Francescomaria Faticanti"
                    },
                    {
                        "name": "Sara Alouf"
                    },
                    {
                        "name": "Giovanni Neglia"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Neglia"
                },
                "author": "Giovanni Neglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03157v2",
                "updated": "2025-03-04T13:01:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2024-07-03T14:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    34,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "Let the Code LLM Edit Itself When You Edit the Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Code LLM Edit Itself When You Edit the Code"
                },
                "summary": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhenyu He"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shengjie Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Di He"
                    }
                ],
                "author_detail": {
                    "name": "Di He"
                },
                "author": "Di He",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02508v1",
                "updated": "2025-03-04T11:19:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:19:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "Q&C: When Quantization Meets Cache in Efficient Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q&C: When Quantization Meets Cache in Efficient Image Generation"
                },
                "summary": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache."
                },
                "authors": [
                    {
                        "name": "Xin Ding"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02504v1",
                "updated": "2025-03-04T11:15:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:15:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects"
                },
                "summary": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time."
                },
                "authors": [
                    {
                        "name": "Emese Sziklay"
                    },
                    {
                        "name": "Tamás Jursonovics"
                    }
                ],
                "author_detail": {
                    "name": "Tamás Jursonovics"
                },
                "author": "Tamás Jursonovics",
                "arxiv_comment": "13 pages, 7 figures, ICRIC 2023, Volume 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v1",
                "updated": "2025-03-04T08:41:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "draft paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v1",
                "updated": "2025-03-04T03:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05787v2",
                "updated": "2025-03-03T18:23:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    23,
                    47,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-08T18:57:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    57,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "RefreshKV: Updating Small KV Cache During Long-form Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefreshKV: Updating Small KV Cache During Long-form Generation"
                },
                "summary": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance."
                },
                "authors": [
                    {
                        "name": "Fangyuan Xu"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01586v1",
                "updated": "2025-03-03T14:26:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T14:26:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection"
                },
                "summary": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Sirui Song"
                    },
                    {
                        "name": "Boyang Liu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Senjie Jin"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01483v1",
                "updated": "2025-03-03T12:43:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T12:43:06Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "title": "KurTail : Kurtosis-based LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KurTail : Kurtosis-based LLM Quantization"
                },
                "summary": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadegh Akhondzadeh"
                    },
                    {
                        "name": "Aleksandar Bojchevski"
                    },
                    {
                        "name": "Evangelos Eleftheriou"
                    },
                    {
                        "name": "Martino Dazzi"
                    }
                ],
                "author_detail": {
                    "name": "Martino Dazzi"
                },
                "author": "Martino Dazzi",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01348v1",
                "updated": "2025-03-03T09:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:38:20Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "title": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension"
                },
                "summary": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems."
                },
                "authors": [
                    {
                        "name": "Hongguang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongguang Chen"
                },
                "author": "Hongguang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01330v1",
                "updated": "2025-03-03T09:12:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:12:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio."
                },
                "authors": [
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01323v1",
                "updated": "2025-03-03T09:04:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:04:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "CacheQuant: Comprehensively Accelerated Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheQuant: Comprehensively Accelerated Diffusion Models"
                },
                "summary": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant ."
                },
                "authors": [
                    {
                        "name": "Xuewen Liu"
                    },
                    {
                        "name": "Zhikai Li"
                    },
                    {
                        "name": "Qingyi Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyi Gu"
                },
                "author": "Qingyi Gu",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01281v1",
                "updated": "2025-03-03T08:06:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T08:06:55Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "title": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System"
                },
                "summary": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI."
                },
                "authors": [
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Yaobin Wang"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yingchen Song"
                    },
                    {
                        "name": "Huan Wu"
                    },
                    {
                        "name": "Qingfeng Wang"
                    },
                    {
                        "name": "Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Huang"
                },
                "author": "Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v2",
                "updated": "2025-03-03T05:49:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    49,
                    41,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00979v1",
                "updated": "2025-03-02T18:12:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T18:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs"
                },
                "summary": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ravi Ghadia"
                    },
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Prashant Nair"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v2",
                "updated": "2025-03-02T14:37:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    14,
                    37,
                    53,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00695v1",
                "updated": "2025-03-02T02:26:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T02:26:21Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "title": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis."
                },
                "authors": [
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Xu Lian"
                    },
                    {
                        "name": "Mathias Unberath"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Unberath"
                },
                "author": "Mathias Unberath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07295v2",
                "updated": "2025-03-02T01:39:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    1,
                    39,
                    57,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-09T16:21:38Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    21,
                    38,
                    2,
                    283,
                    0
                ],
                "title": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking"
                },
                "summary": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com."
                },
                "authors": [
                    {
                        "name": "Shubham Ugare"
                    },
                    {
                        "name": "Rohan Gumaste"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Gagandeep Singh"
                    },
                    {
                        "name": "Sasa Misailovic"
                    }
                ],
                "author_detail": {
                    "name": "Sasa Misailovic"
                },
                "author": "Sasa Misailovic",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00540v1",
                "updated": "2025-03-01T15:53:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T15:53:33Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "title": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval"
                },
                "summary": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models."
                },
                "authors": [
                    {
                        "name": "Shangzhe Di"
                    },
                    {
                        "name": "Zhelun Yu"
                    },
                    {
                        "name": "Guanghao Zhang"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Tao Zhong"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Bolin Li"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Fangxun Shu"
                    },
                    {
                        "name": "Hao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Jiang"
                },
                "author": "Hao Jiang",
                "arxiv_comment": "Accepted to ICLR 2025. Code: https://github.com/Becomebright/ReKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00392v1",
                "updated": "2025-03-01T07:56:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T07:56:42Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "title": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving"
                },
                "summary": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v6",
                "updated": "2025-03-01T05:43:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    5,
                    43,
                    19,
                    5,
                    60,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3706628.3708873",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706628.3708873",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.03058v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00323v1",
                "updated": "2025-03-01T03:20:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T03:20:30Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "title": "FLStore: Efficient Federated Learning Storage for non-training workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLStore: Efficient Federated Learning Storage for non-training workloads"
                },
                "summary": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable."
                },
                "authors": [
                    {
                        "name": "Ahmad Faraz Khan"
                    },
                    {
                        "name": "Samuel Fountain"
                    },
                    {
                        "name": "Ahmed M. Abdelmoniem"
                    },
                    {
                        "name": "Ali R. Butt"
                    },
                    {
                        "name": "Ali Anwar"
                    }
                ],
                "author_detail": {
                    "name": "Ali Anwar"
                },
                "author": "Ali Anwar",
                "arxiv_comment": "11 pages, 19 figures, 2 tables This paper has been accepted at the\n  The Eighth Annual Conference on Machine Learning and Systems (MLSys 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v4",
                "updated": "2025-02-28T18:04:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    4,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21117v1",
                "updated": "2025-02-28T14:54:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:54:35Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "title": "Distributed Data Access in Industrial Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Data Access in Industrial Edge Networks"
                },
                "summary": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication."
                },
                "authors": [
                    {
                        "name": "Theofanis P. Raptis"
                    },
                    {
                        "name": "Andrea Passarella"
                    },
                    {
                        "name": "Marco Conti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Conti"
                },
                "author": "Marco Conti",
                "arxiv_doi": "10.1109/JSAC.2020.2980917",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSAC.2020.2980917",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.21117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work was funded by the EC through the FoF-RIA Project AUTOWARE\n  (No. 723909)",
                "arxiv_journal_ref": "IEEE Journal on Selected Areas in Communications, vol. 38, no. 5,\n  pp. 915-927, May 2020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21079v1",
                "updated": "2025-02-28T14:11:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:11:20Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation"
                },
                "summary": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation."
                },
                "authors": [
                    {
                        "name": "Yifei Xia"
                    },
                    {
                        "name": "Suhan Ling"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v3",
                "updated": "2025-02-28T13:23:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    23,
                    56,
                    4,
                    59,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v3",
                "updated": "2025-02-28T13:08:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    8,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20812v1",
                "updated": "2025-02-28T07:56:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T07:56:37Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "title": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications."
                },
                "authors": [
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Yinglin Xie"
                    },
                    {
                        "name": "Zhao Liu"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Quanchen Zou"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v1",
                "updated": "2025-02-27T23:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%."
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "Mingyuan, Jize, and Haozhen contributed equally, while Minjia,\n  Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15896v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15896v3",
                "updated": "2025-02-27T21:50:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    50,
                    48,
                    3,
                    58,
                    0
                ],
                "published": "2023-12-26T06:16:12Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    6,
                    16,
                    12,
                    1,
                    360,
                    0
                ],
                "title": "WWW: What, When, Where to Compute-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WWW: What, When, Where to Compute-in-Memory"
                },
                "summary": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication."
                },
                "authors": [
                    {
                        "name": "Tanvi Sharma"
                    },
                    {
                        "name": "Mustafa Ali"
                    },
                    {
                        "name": "Indranil Chakraborty"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "added supplementary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15896v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15896v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20547v1",
                "updated": "2025-02-27T21:42:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T21:42:49Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "title": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches"
                },
                "summary": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers."
                },
                "authors": [
                    {
                        "name": "Aurore Poirier"
                    },
                    {
                        "name": "Erven Rohou"
                    },
                    {
                        "name": "Manuel Serrano"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Serrano"
                },
                "arxiv_affiliation": "Inria - University of Côte d'Azur, France",
                "author": "Manuel Serrano",
                "arxiv_doi": "10.22152/programming-journal.org/2026/10/6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.22152/programming-journal.org/2026/10/6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.20547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "The Art, Science, and Engineering of Programming, 2025, Vol. 10,\n  Issue 1, Article 6",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v1",
                "updated": "2025-02-27T17:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Inference with Retrieval-Augmented Speculative Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v2",
                "updated": "2025-02-27T15:29:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    29,
                    3,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v3",
                "updated": "2025-02-27T12:30:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    30,
                    43,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v2",
                "updated": "2025-02-27T12:15:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    15,
                    38,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uroš Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uroš Seljak"
                },
                "author": "Uroš Seljak",
                "arxiv_comment": "36 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16235v2",
                "updated": "2025-02-27T06:39:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    6,
                    39,
                    6,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-22T14:13:37Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Parallel Tree Search for Efficient LLM Reasoning"
                },
                "summary": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient."
                },
                "authors": [
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v4",
                "updated": "2025-02-27T03:22:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    3,
                    22,
                    41,
                    3,
                    58,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v3",
                "updated": "2025-02-26T11:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    47,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v3",
                "updated": "2025-02-26T10:49:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    49,
                    33,
                    2,
                    57,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "arxiv_comment": "Accepted by TMLR: https://openreview.net/forum?id=xXs2GKXPnH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v1",
                "updated": "2025-02-26T07:10:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v2",
                "updated": "2025-02-26T02:48:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    48,
                    22,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18755v1",
                "updated": "2025-02-26T02:16:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T02:16:46Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "title": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type"
                },
                "summary": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator."
                },
                "authors": [
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Haoyan Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Renyang Guan"
                    },
                    {
                        "name": "Zhendong Hua"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.02550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.02550v3",
                "updated": "2025-02-25T13:03:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    3,
                    44,
                    1,
                    56,
                    0
                ],
                "published": "2022-03-04T19:56:56Z",
                "published_parsed": [
                    2022,
                    3,
                    4,
                    19,
                    56,
                    56,
                    4,
                    63,
                    0
                ],
                "title": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications"
                },
                "summary": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation."
                },
                "authors": [
                    {
                        "name": "Jawad Haj Yahya"
                    },
                    {
                        "name": "Haris Volos"
                    },
                    {
                        "name": "Davide B. Bartolini"
                    },
                    {
                        "name": "Georgia Antoniou"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Kleovoulos Kalaitzidis"
                    },
                    {
                        "name": "Tom Rollet"
                    },
                    {
                        "name": "Zhirui Chen"
                    },
                    {
                        "name": "Ye Geng"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Yiannakis Sazeides"
                    }
                ],
                "author_detail": {
                    "name": "Yiannakis Sazeides"
                },
                "author": "Yiannakis Sazeides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.02550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.02550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18113v1",
                "updated": "2025-02-25T11:36:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:36:43Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "title": "Accelerating Graph Indexing for ANNS on Modern CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Graph Indexing for ANNS on Modern CPUs"
                },
                "summary": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance."
                },
                "authors": [
                    {
                        "name": "Mengzhao Wang"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yunjun Gao"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Wenchao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wenchao Zhou"
                },
                "author": "Wenchao Zhou",
                "arxiv_comment": "SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v2",
                "updated": "2025-02-25T09:42:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    42,
                    11,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v3",
                "updated": "2025-02-25T03:42:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    3,
                    42,
                    15,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "36 pages. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17606v1",
                "updated": "2025-02-24T19:48:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:48:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores"
                },
                "summary": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively."
                },
                "authors": [
                    {
                        "name": "Viraj Thakkar"
                    },
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Kenanya Keandra Adriel Prasetyo"
                    },
                    {
                        "name": "Raden Haryosatyo Wisjnunandono"
                    },
                    {
                        "name": "Achmad Imam Kistijantoro"
                    },
                    {
                        "name": "Reza Fuad Rachmadi"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v1",
                "updated": "2025-02-24T19:34:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v1",
                "updated": "2025-02-24T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification"
                },
                "summary": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01418v2",
                "updated": "2025-02-24T18:51:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    51,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-02T16:08:03Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    8,
                    3,
                    3,
                    123,
                    0
                ],
                "title": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version"
                },
                "summary": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system."
                },
                "authors": [
                    {
                        "name": "Libin Zhou"
                    },
                    {
                        "name": "Lu Xing"
                    },
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid. G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid. G. Aref"
                },
                "author": "Walid. G. Aref",
                "arxiv_comment": "technical report for our main paper GTX: A Write-Optimized Latch-free\n  Graph Data System with Transactional Support",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17398v1",
                "updated": "2025-02-24T18:26:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs"
                },
                "summary": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs."
                },
                "authors": [
                    {
                        "name": "Cyril Koenig"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v5",
                "updated": "2025-02-24T15:42:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    42,
                    59,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on GitHub\n  ^_^ Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17535v1",
                "updated": "2025-02-24T15:39:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:39:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?"
                },
                "summary": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods."
                },
                "authors": [
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v2",
                "updated": "2025-02-24T13:35:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    35,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v1",
                "updated": "2025-02-24T13:30:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Borui Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v1",
                "updated": "2025-02-24T06:33:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00022v1",
                "updated": "2025-02-24T02:57:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T02:57:51Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "title": "KVCrush: Key value cache size-reduction using similarity in\n  head-behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCrush: Key value cache size-reduction using similarity in\n  head-behaviour"
                },
                "summary": "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization."
                },
                "authors": [
                    {
                        "name": "Gopi Krishna Jha"
                    },
                    {
                        "name": "Sameh Gobriel"
                    },
                    {
                        "name": "Liubov Talamanova"
                    },
                    {
                        "name": "Alexander Kozlov"
                    },
                    {
                        "name": "Nilesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Nilesh Jain"
                },
                "author": "Nilesh Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13176v2",
                "updated": "2025-02-24T01:28:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    28,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-18T04:08:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference"
                },
                "summary": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels."
                },
                "authors": [
                    {
                        "name": "Ahmed Burak Gulhan"
                    },
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Mahmut Kandemir"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v2",
                "updated": "2025-02-23T19:48:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    19,
                    48,
                    12,
                    6,
                    54,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_doi": "10.1145/3701716.3715490",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715490",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.15605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, accepted by the Web Conference 2025 (WWW '25) as a short\n  paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16632v1",
                "updated": "2025-02-23T16:17:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "published": "2025-02-23T16:17:34Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "title": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G"
                },
                "summary": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G."
                },
                "authors": [
                    {
                        "name": "Xidong Mu"
                    },
                    {
                        "name": "Zhaolin Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu",
                "arxiv_doi": "10.1109/MNET.2024.3481293",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MNET.2024.3481293",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.16632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 figures, 8 pages, published in IEEE Network",
                "arxiv_journal_ref": "in IEEE Network, vol. 39, no. 1, pp. 47-55, Jan. 2025",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v3",
                "updated": "2025-02-23T11:52:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    11,
                    52,
                    45,
                    6,
                    54,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v4",
                "updated": "2025-02-23T03:27:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    3,
                    27,
                    1,
                    6,
                    54,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13502v2",
                "updated": "2025-02-22T22:32:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    22,
                    32,
                    8,
                    5,
                    53,
                    0
                ],
                "published": "2025-02-19T07:43:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference"
                },
                "summary": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache."
                },
                "authors": [
                    {
                        "name": "Burc Gokden"
                    }
                ],
                "author_detail": {
                    "name": "Burc Gokden"
                },
                "author": "Burc Gokden",
                "arxiv_comment": "15 pages, 1 figure, 12 tables, more ablation data included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15197v3",
                "updated": "2025-02-22T10:31:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    31,
                    51,
                    5,
                    53,
                    0
                ],
                "published": "2024-05-24T04:00:04Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    4,
                    0,
                    4,
                    4,
                    145,
                    0
                ],
                "title": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures"
                },
                "summary": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable."
                },
                "authors": [
                    {
                        "name": "Qiang Zou"
                    },
                    {
                        "name": "Yunzhu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunzhu Gao"
                },
                "author": "Yunzhu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v1",
                "updated": "2025-02-21T23:34:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15955v1",
                "updated": "2025-02-21T21:37:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T21:37:52Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "title": "Compression Barriers for Autoregressive Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression Barriers for Autoregressive Transformers"
                },
                "summary": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens."
                },
                "authors": [
                    {
                        "name": "Themistoklis Haris"
                    },
                    {
                        "name": "Krzysztof Onak"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Onak"
                },
                "author": "Krzysztof Onak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v2",
                "updated": "2025-02-21T13:35:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    35,
                    43,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17501v1",
                "updated": "2025-02-21T12:03:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:03:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "CoKV: Optimizing KV Cache Allocation via Cooperative Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoKV: Optimizing KV Cache Allocation via Cooperative Game"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models."
                },
                "authors": [
                    {
                        "name": "Qiheng Sun"
                    },
                    {
                        "name": "Hongwei Zhang"
                    },
                    {
                        "name": "Haocheng Xia"
                    },
                    {
                        "name": "Jiayao Zhang"
                    },
                    {
                        "name": "Jinfei Liu"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15304v1",
                "updated": "2025-02-21T08:55:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:55:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention"
                },
                "summary": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs."
                },
                "authors": [
                    {
                        "name": "Hong Yankun"
                    },
                    {
                        "name": "Li Xing"
                    },
                    {
                        "name": "Zhen Hui-Ling"
                    },
                    {
                        "name": "Yu Xianzhi"
                    },
                    {
                        "name": "Liu Wulong"
                    },
                    {
                        "name": "Yuan Mingxuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Mingxuan"
                },
                "author": "Yuan Mingxuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v1",
                "updated": "2025-02-21T04:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v2",
                "updated": "2025-02-20T23:28:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    23,
                    28,
                    1,
                    3,
                    51,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v1",
                "updated": "2025-02-20T22:24:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "More for Keys, Less for Values: Adaptive KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More for Keys, Less for Values: Adaptive KV Cache Quantization"
                },
                "summary": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant"
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Sixu Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v1",
                "updated": "2025-02-20T18:59:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v1",
                "updated": "2025-02-20T18:50:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14938v1",
                "updated": "2025-02-20T14:01:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:01:17Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "title": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models"
                },
                "summary": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments."
                },
                "authors": [
                    {
                        "name": "Miao Tao"
                    },
                    {
                        "name": "Yuanzhen Zhou"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yuchang Zhang"
                    },
                    {
                        "name": "Zhongling Su"
                    },
                    {
                        "name": "Linning Xu"
                    },
                    {
                        "name": "Zhenxiang Ma"
                    },
                    {
                        "name": "Rong Fu"
                    },
                    {
                        "name": "Hengjie Li"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14504v1",
                "updated": "2025-02-20T12:31:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:31:31Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Chenran Huang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Xiaoping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoping Zhang"
                },
                "author": "Xiaoping Zhang",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v2",
                "updated": "2025-02-20T12:14:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    14,
                    49,
                    3,
                    51,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v2",
                "updated": "2025-02-20T09:03:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    3,
                    5,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14347v1",
                "updated": "2025-02-20T08:00:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T08:00:25Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "title": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure"
                },
                "summary": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved."
                },
                "authors": [
                    {
                        "name": "Zheyu Wang"
                    },
                    {
                        "name": "Lingfei Wang"
                    },
                    {
                        "name": "King Yau Yip"
                    },
                    {
                        "name": "Ying Kit Tsui"
                    },
                    {
                        "name": "Tsz Fung Poon"
                    },
                    {
                        "name": "Wenyan Wang"
                    },
                    {
                        "name": "Chun Wai Tsang"
                    },
                    {
                        "name": "Shanmin Wang"
                    },
                    {
                        "name": "David Graf"
                    },
                    {
                        "name": "Alexandre Pourret"
                    },
                    {
                        "name": "Gabriel Seyfarth"
                    },
                    {
                        "name": "Georg Knebel"
                    },
                    {
                        "name": "Kwing To Lai"
                    },
                    {
                        "name": "Wing Chi Yu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Swee K. Goh"
                    }
                ],
                "author_detail": {
                    "name": "Swee K. Goh"
                },
                "author": "Swee K. Goh",
                "arxiv_comment": "10 pages, 5 figures. Advanced Science (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v1",
                "updated": "2025-02-20T07:10:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "We will release the code soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14307v1",
                "updated": "2025-02-20T06:42:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T06:42:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "μRL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "μRL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning"
                },
                "summary": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach."
                },
                "authors": [
                    {
                        "name": "M. Caner Tol"
                    },
                    {
                        "name": "Kemal Derya"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v4",
                "updated": "2025-02-20T06:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    7,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14280v1",
                "updated": "2025-02-20T05:41:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T05:41:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks."
                },
                "authors": [
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Sarathkrishna Swaminathan"
                    },
                    {
                        "name": "Georgios Kollias"
                    },
                    {
                        "name": "Elliot Nelson"
                    },
                    {
                        "name": "Khushbu Pahwa"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Igor Melnyk"
                    },
                    {
                        "name": "Matthew Riemer"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Riemer"
                },
                "author": "Matthew Riemer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14220v1",
                "updated": "2025-02-20T03:27:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T03:27:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table"
                },
                "summary": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Qingcai Jiang"
                    },
                    {
                        "name": "Buxin Tu"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v1",
                "updated": "2025-02-19T19:12:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v4",
                "updated": "2025-02-19T17:53:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    11,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v1",
                "updated": "2025-02-19T16:54:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v2",
                "updated": "2025-02-19T11:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v4",
                "updated": "2025-02-19T10:39:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    39,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ToCa is honored to be accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v1",
                "updated": "2025-02-19T09:30:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13542v1",
                "updated": "2025-02-19T08:50:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:50:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference"
                },
                "summary": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiaqi Tang"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.04724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04724v1",
                "updated": "2025-03-06T18:59:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    59,
                    38,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:59:38Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    59,
                    38,
                    3,
                    65,
                    0
                ],
                "title": "LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM"
                },
                "summary": "Recent advancements in speech-to-speech dialogue systems leverage LLMs for\nmultimodal interactions, yet they remain hindered by fine-tuning requirements,\nhigh computational overhead, and text-speech misalignment. Existing\nspeech-enabled LLMs often degrade conversational quality by modifying the LLM,\nthereby compromising its linguistic capabilities. In contrast, we propose\nLLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS\nsystem that generates high-quality speech with low latency, while fully\npreserving the capabilities of the base LLM. Our approach achieves a\nsignificantly lower Word Error Rate compared to speech-enabled LLMs, while\noperating at comparable latency and UTMOS score. By decoupling speech synthesis\nfrom LLM processing via a multi-queue token streaming system, LLMVoX supports\nseamless, infinite-length dialogues. Its plug-and-play design also facilitates\nextension to various tasks with different backbones. Furthermore, LLMVoX\ngeneralizes to new languages with only dataset adaptation, attaining a low\nCharacter Error Rate on an Arabic speech task. Additionally, we have integrated\nLLMVoX with a Vision-Language Model to create an omni-model with speech, text,\nand vision capabilities, without requiring additional multimodal training. Our\ncode base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in speech-to-speech dialogue systems leverage LLMs for\nmultimodal interactions, yet they remain hindered by fine-tuning requirements,\nhigh computational overhead, and text-speech misalignment. Existing\nspeech-enabled LLMs often degrade conversational quality by modifying the LLM,\nthereby compromising its linguistic capabilities. In contrast, we propose\nLLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS\nsystem that generates high-quality speech with low latency, while fully\npreserving the capabilities of the base LLM. Our approach achieves a\nsignificantly lower Word Error Rate compared to speech-enabled LLMs, while\noperating at comparable latency and UTMOS score. By decoupling speech synthesis\nfrom LLM processing via a multi-queue token streaming system, LLMVoX supports\nseamless, infinite-length dialogues. Its plug-and-play design also facilitates\nextension to various tasks with different backbones. Furthermore, LLMVoX\ngeneralizes to new languages with only dataset adaptation, attaining a low\nCharacter Error Rate on an Arabic speech task. Additionally, we have integrated\nLLMVoX with a Vision-Language Model to create an omni-model with speech, text,\nand vision capabilities, without requiring additional multimodal training. Our\ncode base and project page is available at https://mbzuai-oryx.github.io/LLMVoX ."
                },
                "authors": [
                    {
                        "name": "Sambal Shikhar"
                    },
                    {
                        "name": "Mohammed Irfan Kurpath"
                    },
                    {
                        "name": "Sahal Shaji Mullappilly"
                    },
                    {
                        "name": "Jean Lahoud"
                    },
                    {
                        "name": "Fahad Khan"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    }
                ],
                "author_detail": {
                    "name": "Hisham Cholakkal"
                },
                "author": "Hisham Cholakkal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04723v2",
                "updated": "2025-03-07T03:14:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    3,
                    14,
                    2,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-06T18:59:37Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    59,
                    37,
                    3,
                    65,
                    0
                ],
                "title": "Shifting Long-Context LLMs Research from Input to Output",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shifting Long-Context LLMs Research from Input to Output"
                },
                "summary": "Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Zhiqing Hu"
                    },
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Ming Shan Hee"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04722v1",
                "updated": "2025-03-06T18:59:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    59,
                    23,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:59:23Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    59,
                    23,
                    3,
                    65,
                    0
                ],
                "title": "Enough Coin Flips Can Make LLMs Act Bayesian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enough Coin Flips Can Make LLMs Act Bayesian"
                },
                "summary": "Large language models (LLMs) exhibit the ability to generalize given few-shot\nexamples in their input prompt, an emergent capability known as in-context\nlearning (ICL). We investigate whether LLMs utilize ICL to perform structured\nreasoning in ways that are consistent with a Bayesian framework or rely on\npattern matching. Using a controlled setting of biased coin flips, we find\nthat: (1) LLMs often possess biased priors, causing initial divergence in\nzero-shot settings, (2) in-context evidence outweighs explicit bias\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\ndeviations primarily due to miscalibrated priors rather than flawed updates,\nand (4) attention magnitude has negligible effect on Bayesian inference. With\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\npriors in a Bayesian manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit the ability to generalize given few-shot\nexamples in their input prompt, an emergent capability known as in-context\nlearning (ICL). We investigate whether LLMs utilize ICL to perform structured\nreasoning in ways that are consistent with a Bayesian framework or rely on\npattern matching. Using a controlled setting of biased coin flips, we find\nthat: (1) LLMs often possess biased priors, causing initial divergence in\nzero-shot settings, (2) in-context evidence outweighs explicit bias\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\ndeviations primarily due to miscalibrated priors rather than flawed updates,\nand (4) attention magnitude has negligible effect on Bayesian inference. With\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\npriors in a Bayesian manner."
                },
                "authors": [
                    {
                        "name": "Ritwik Gupta"
                    },
                    {
                        "name": "Rodolfo Corona"
                    },
                    {
                        "name": "Jiaxin Ge"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Dan Klein"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "David M. Chan"
                    }
                ],
                "author_detail": {
                    "name": "David M. Chan"
                },
                "author": "David M. Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04718v1",
                "updated": "2025-03-06T18:58:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    58,
                    45,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:58:45Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    58,
                    45,
                    3,
                    65,
                    0
                ],
                "title": "Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation"
                },
                "summary": "Scene flow estimation is a foundational task for many robotic applications,\nincluding robust dynamic object detection, automatic labeling, and sensor\nsynchronization. Two types of approaches to the problem have evolved: 1)\nSupervised and 2) optimization-based methods. Supervised methods are fast\nduring inference and achieve high-quality results, however, they are limited by\nthe need for large amounts of labeled training data and are susceptible to\ndomain gaps. In contrast, unsupervised test-time optimization methods do not\nface the problem of domain gaps but usually suffer from substantial runtime,\nexhibit artifacts, or fail to converge to the right solution. In this work, we\nmitigate several limitations of existing optimization-based methods. To this\nend, we 1) introduce a simple voxel grid-based model that improves over the\nstandard MLP-based formulation in multiple dimensions and 2) introduce a new\nmultiframe loss formulation. 3) We combine both contributions in our new\nmethod, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed only\nby EulerFlow among unsupervised methods while achieving comparable performance\nat a fraction of the computational cost. Floxels achieves a massive speedup of\nmore than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10\nminutes per sequence. Over the faster but low-quality baseline, NSFP, Floxels\nachieves a speedup of ~14x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene flow estimation is a foundational task for many robotic applications,\nincluding robust dynamic object detection, automatic labeling, and sensor\nsynchronization. Two types of approaches to the problem have evolved: 1)\nSupervised and 2) optimization-based methods. Supervised methods are fast\nduring inference and achieve high-quality results, however, they are limited by\nthe need for large amounts of labeled training data and are susceptible to\ndomain gaps. In contrast, unsupervised test-time optimization methods do not\nface the problem of domain gaps but usually suffer from substantial runtime,\nexhibit artifacts, or fail to converge to the right solution. In this work, we\nmitigate several limitations of existing optimization-based methods. To this\nend, we 1) introduce a simple voxel grid-based model that improves over the\nstandard MLP-based formulation in multiple dimensions and 2) introduce a new\nmultiframe loss formulation. 3) We combine both contributions in our new\nmethod, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed only\nby EulerFlow among unsupervised methods while achieving comparable performance\nat a fraction of the computational cost. Floxels achieves a massive speedup of\nmore than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10\nminutes per sequence. Over the faster but low-quality baseline, NSFP, Floxels\nachieves a speedup of ~14x."
                },
                "authors": [
                    {
                        "name": "David T. Hoffmann"
                    },
                    {
                        "name": "Syed Haseeb Raza"
                    },
                    {
                        "name": "Hanqiu Jiang"
                    },
                    {
                        "name": "Denis Tananaev"
                    },
                    {
                        "name": "Steffen Klingenhoefer"
                    },
                    {
                        "name": "Martin Meinke"
                    }
                ],
                "author_detail": {
                    "name": "Martin Meinke"
                },
                "author": "Martin Meinke",
                "arxiv_comment": "Accepted at CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04715v1",
                "updated": "2025-03-06T18:58:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    58,
                    29,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:58:29Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    58,
                    29,
                    3,
                    65,
                    0
                ],
                "title": "Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large\n  Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large\n  Language Model Pretraining"
                },
                "summary": "The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well-established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Through extensive empirical studies\ninvolving grid searches across diverse configurations, we discover universal\nscaling laws governing these hyperparameters: optimal learning rate follows a\npower-law relationship with both model parameters and data sizes, while optimal\nbatch size scales primarily with data sizes. Our analysis reveals a convex\noptimization landscape for hyperparameters under fixed models and data size\nconditions. This convexity implies an optimal hyperparameter plateau. We\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity. Its estimated values on the test set are merely 0.07\\% away from the\nglobally optimal LLM performance found via an exhaustive search. These laws\ndemonstrate remarkable robustness across variations in model sparsity, training\ndata distribution, and model shape. To our best known, this is the first work\nthat unifies different model shapes and structures, such as Mixture-of-Experts\nmodels and dense transformers, as well as establishes optimal hyperparameter\nscaling laws across diverse data distributions. This exhaustive optimization\nprocess demands substantial computational resources, utilizing nearly one\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\ntotal. To facilitate reproducibility and further research, we will\nprogressively release all loss measurements and model checkpoints through our\ndesignated repository https://step-law.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well-established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Through extensive empirical studies\ninvolving grid searches across diverse configurations, we discover universal\nscaling laws governing these hyperparameters: optimal learning rate follows a\npower-law relationship with both model parameters and data sizes, while optimal\nbatch size scales primarily with data sizes. Our analysis reveals a convex\noptimization landscape for hyperparameters under fixed models and data size\nconditions. This convexity implies an optimal hyperparameter plateau. We\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity. Its estimated values on the test set are merely 0.07\\% away from the\nglobally optimal LLM performance found via an exhaustive search. These laws\ndemonstrate remarkable robustness across variations in model sparsity, training\ndata distribution, and model shape. To our best known, this is the first work\nthat unifies different model shapes and structures, such as Mixture-of-Experts\nmodels and dense transformers, as well as establishes optimal hyperparameter\nscaling laws across diverse data distributions. This exhaustive optimization\nprocess demands substantial computational resources, utilizing nearly one\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\ntotal. To facilitate reproducibility and further research, we will\nprogressively release all loss measurements and model checkpoints through our\ndesignated repository https://step-law.github.io/"
                },
                "authors": [
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Wenzheng Zheng"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Yangshijie Xu"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11807v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11807v7",
                "updated": "2025-03-06T18:58:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    58,
                    23,
                    3,
                    65,
                    0
                ],
                "published": "2024-03-18T14:04:47Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    14,
                    4,
                    47,
                    0,
                    78,
                    0
                ],
                "title": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments"
                },
                "summary": "Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families,\nincluding GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by\nLLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental\nresults are publicly available at https://github.com/CUHK-ARISE/GAMABench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families,\nincluding GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by\nLLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental\nresults are publicly available at https://github.com/CUHK-ARISE/GAMABench."
                },
                "authors": [
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Eric John Li"
                    },
                    {
                        "name": "Man Ho Lam"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_comment": "Accepted to ICLR 2025; 11 pages of main text; 26 pages of appendices;\n  Included models: GPT-3.5-{0613, 1106, 0125}, GPT-4-0125, GPT-4o-0806,\n  Gemini-{1.0, 1.5)-Pro, LLaMA-3.1-{7, 70, 405}B, Mixtral-8x{7, 22}B,\n  Qwen-2-72B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11807v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11807v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04704v1",
                "updated": "2025-03-06T18:54:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    54,
                    32,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:54:32Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    54,
                    32,
                    3,
                    65,
                    0
                ],
                "title": "Universality of Layer-Level Entropy-Weighted Quantization Beyond Model\n  Architecture and Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universality of Layer-Level Entropy-Weighted Quantization Beyond Model\n  Architecture and Size"
                },
                "summary": "We present a novel approach to selective model quantization that transcends\nthe limitations of architecture-specific and size-dependent compression methods\nfor Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By\nanalyzing the entropy distribution across transformer blocks, EWQ determines\nwhich blocks can be safely quantized without causing significant performance\ndegradation, independent of model architecture or size. Our method outperforms\nuniform quantization approaches, maintaining Massive Multitask Language\nUnderstanding (MMLU) accuracy scores within 0.5% of unquantized models while\nreducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ\nacross multiple architectures-from 1.6B to 70B parameters-showcasing consistent\nimprovements in the quality-compression trade-off regardless of model scale or\narchitectural design. A surprising finding of EWQ is its ability to reduce\nperplexity compared to unquantized models, suggesting the presence of\nbeneficial regularization through selective precision reduction. This\nimprovement holds across different model families, indicating a fundamental\nrelationship between layer-level entropy and optimal precision requirements.\nAdditionally, we introduce FastEWQ, a rapid method for entropy distribution\nanalysis that eliminates the need for loading model weights. This technique\nleverages universal characteristics of entropy distribution that persist across\nvarious architectures and scales, enabling near-instantaneous quantization\ndecisions while maintaining 80% classification accuracy with full entropy\nanalysis. Our results demonstrate that effective quantization strategies can be\ndeveloped independently of specific architectural choices or model sizes,\nopening new possibilities for efficient LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to selective model quantization that transcends\nthe limitations of architecture-specific and size-dependent compression methods\nfor Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By\nanalyzing the entropy distribution across transformer blocks, EWQ determines\nwhich blocks can be safely quantized without causing significant performance\ndegradation, independent of model architecture or size. Our method outperforms\nuniform quantization approaches, maintaining Massive Multitask Language\nUnderstanding (MMLU) accuracy scores within 0.5% of unquantized models while\nreducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ\nacross multiple architectures-from 1.6B to 70B parameters-showcasing consistent\nimprovements in the quality-compression trade-off regardless of model scale or\narchitectural design. A surprising finding of EWQ is its ability to reduce\nperplexity compared to unquantized models, suggesting the presence of\nbeneficial regularization through selective precision reduction. This\nimprovement holds across different model families, indicating a fundamental\nrelationship between layer-level entropy and optimal precision requirements.\nAdditionally, we introduce FastEWQ, a rapid method for entropy distribution\nanalysis that eliminates the need for loading model weights. This technique\nleverages universal characteristics of entropy distribution that persist across\nvarious architectures and scales, enabling near-instantaneous quantization\ndecisions while maintaining 80% classification accuracy with full entropy\nanalysis. Our results demonstrate that effective quantization strategies can be\ndeveloped independently of specific architectural choices or model sizes,\nopening new possibilities for efficient LLM deployment."
                },
                "authors": [
                    {
                        "name": "Alireza Behtash"
                    },
                    {
                        "name": "Marijan Fofonjka"
                    },
                    {
                        "name": "Ethan Baird"
                    },
                    {
                        "name": "Tyler Mauer"
                    },
                    {
                        "name": "Hossein Moghimifam"
                    },
                    {
                        "name": "David Stout"
                    },
                    {
                        "name": "Joel Dennison"
                    }
                ],
                "author_detail": {
                    "name": "Joel Dennison"
                },
                "author": "Joel Dennison",
                "arxiv_comment": "29 pages, 7 figures, 14 tables; Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15037v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15037v4",
                "updated": "2025-03-06T18:50:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    50,
                    30,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-20T20:46:09Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    20,
                    46,
                    9,
                    3,
                    51,
                    0
                ],
                "title": "DEFT: Differentiable Branched Discrete Elastic Rods for Modeling\n  Furcated DLOs in Real-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEFT: Differentiable Branched Discrete Elastic Rods for Modeling\n  Furcated DLOs in Real-Time"
                },
                "summary": "Autonomous wire harness assembly requires robots to manipulate complex\nbranched cables with high precision and reliability. A key challenge in\nautomating this process is predicting how these flexible and branched\nstructures behave under manipulation. Without accurate predictions, it is\ndifficult for robots to reliably plan or execute assembly operations. While\nexisting research has made progress in modeling single-threaded Deformable\nLinear Objects (DLOs), extending these approaches to Branched Deformable Linear\nObjects (BDLOs) presents fundamental challenges. The junction points in BDLOs\ncreate complex force interactions and strain propagation patterns that cannot\nbe adequately captured by simply connecting multiple single-DLO models. To\naddress these challenges, this paper presents Differentiable discrete branched\nElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework\nthat combines a differentiable physics-based model with a learning framework\nto: 1) accurately model BDLO dynamics, including dynamic propagation at\njunction points and grasping in the middle of a BDLO, 2) achieve efficient\ncomputation for real-time inference, and 3) enable planning to demonstrate\ndexterous BDLO manipulation. A comprehensive series of real-world experiments\ndemonstrates DEFT's efficacy in terms of accuracy, computational speed, and\ngeneralizability compared to state-of-the-art alternatives. Project\npage:https://roahmlab.github.io/DEFT/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous wire harness assembly requires robots to manipulate complex\nbranched cables with high precision and reliability. A key challenge in\nautomating this process is predicting how these flexible and branched\nstructures behave under manipulation. Without accurate predictions, it is\ndifficult for robots to reliably plan or execute assembly operations. While\nexisting research has made progress in modeling single-threaded Deformable\nLinear Objects (DLOs), extending these approaches to Branched Deformable Linear\nObjects (BDLOs) presents fundamental challenges. The junction points in BDLOs\ncreate complex force interactions and strain propagation patterns that cannot\nbe adequately captured by simply connecting multiple single-DLO models. To\naddress these challenges, this paper presents Differentiable discrete branched\nElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework\nthat combines a differentiable physics-based model with a learning framework\nto: 1) accurately model BDLO dynamics, including dynamic propagation at\njunction points and grasping in the middle of a BDLO, 2) achieve efficient\ncomputation for real-time inference, and 3) enable planning to demonstrate\ndexterous BDLO manipulation. A comprehensive series of real-world experiments\ndemonstrates DEFT's efficacy in terms of accuracy, computational speed, and\ngeneralizability compared to state-of-the-art alternatives. Project\npage:https://roahmlab.github.io/DEFT/."
                },
                "authors": [
                    {
                        "name": "Yizhou Chen"
                    },
                    {
                        "name": "Xiaoyue Wu"
                    },
                    {
                        "name": "Yeheng Zong"
                    },
                    {
                        "name": "Anran Li"
                    },
                    {
                        "name": "Yuzhen Chen"
                    },
                    {
                        "name": "Julie Wu"
                    },
                    {
                        "name": "Bohao Zhang"
                    },
                    {
                        "name": "Ram Vasudevan"
                    }
                ],
                "author_detail": {
                    "name": "Ram Vasudevan"
                },
                "author": "Ram Vasudevan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15037v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15037v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04700v1",
                "updated": "2025-03-06T18:47:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    47,
                    30,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:47:30Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    47,
                    30,
                    3,
                    65,
                    0
                ],
                "title": "Inferring kilonova ejecta photospheric properties from early blackbody\n  spectra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring kilonova ejecta photospheric properties from early blackbody\n  spectra"
                },
                "summary": "We present simple analytic corrections to the standard blackbody fitting used\nfor early kilonova emission. We consider a spherical, relativistically\nexpanding shell that radiates thermally at a single temperature in its own rest\nframe. Due to relativistic effects, including Doppler boosting, time delay, and\ntemperature evolution -- the observed temperature is smeared across different\npolar angles by approximately $\\sim10\\%$. While the observed spectrum remains\nroughly consistent with a single-temperature blackbody, neglecting relativistic\neffects leads to significant systematic inaccuracies: the inferred photospheric\nvelocity and temperature are overestimated by up to $\\sim50\\%$ for mildly\nrelativistic velocities. By applying our analytic corrections, these deviations\nare reduced to within $10\\%$, even in cases where the photosphere is receding\nand cooling is considered. Applying our corrections to observed kilonovae\n(AT2017gfo and the thermal component of GRB211211A) reveals that standard\nblackbody fitting overestimated the inferred velocities and temperatures by\n$10\\%-40\\%$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present simple analytic corrections to the standard blackbody fitting used\nfor early kilonova emission. We consider a spherical, relativistically\nexpanding shell that radiates thermally at a single temperature in its own rest\nframe. Due to relativistic effects, including Doppler boosting, time delay, and\ntemperature evolution -- the observed temperature is smeared across different\npolar angles by approximately $\\sim10\\%$. While the observed spectrum remains\nroughly consistent with a single-temperature blackbody, neglecting relativistic\neffects leads to significant systematic inaccuracies: the inferred photospheric\nvelocity and temperature are overestimated by up to $\\sim50\\%$ for mildly\nrelativistic velocities. By applying our analytic corrections, these deviations\nare reduced to within $10\\%$, even in cases where the photosphere is receding\nand cooling is considered. Applying our corrections to observed kilonovae\n(AT2017gfo and the thermal component of GRB211211A) reveals that standard\nblackbody fitting overestimated the inferred velocities and temperatures by\n$10\\%-40\\%$."
                },
                "authors": [
                    {
                        "name": "Gilad Sadeh"
                    }
                ],
                "author_detail": {
                    "name": "Gilad Sadeh"
                },
                "author": "Gilad Sadeh",
                "arxiv_comment": "7 pages, 6 figures, submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04698v1",
                "updated": "2025-03-06T18:46:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    46,
                    10,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:46:10Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    46,
                    10,
                    3,
                    65,
                    0
                ],
                "title": "DEAL-YOLO: Drone-based Efficient Animal Localization using YOLO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEAL-YOLO: Drone-based Efficient Animal Localization using YOLO"
                },
                "summary": "Although advances in deep learning and aerial surveillance technology are\nimproving wildlife conservation efforts, complex and erratic environmental\nconditions still pose a problem, requiring innovative solutions for\ncost-effective small animal detection. This work introduces DEAL-YOLO, a novel\napproach that improves small object detection in Unmanned Aerial Vehicle (UAV)\nimages by using multi-objective loss functions like Wise IoU (WIoU) and\nNormalized Wasserstein Distance (NWD), which prioritize pixels near the centre\nof the bounding box, ensuring smoother localization and reducing abrupt\ndeviations. Additionally, the model is optimized through efficient feature\nextraction with Linear Deformable (LD) convolutions, enhancing accuracy while\nmaintaining computational efficiency. The Scaled Sequence Feature Fusion (SSFF)\nmodule enhances object detection by effectively capturing inter-scale\nrelationships, improving feature representation, and boosting metrics through\noptimized multiscale fusion. Comparison with baseline models reveals high\nefficacy with up to 69.5\\% fewer parameters compared to vanilla Yolov8-N,\nhighlighting the robustness of the proposed modifications. Through this\napproach, our paper aims to facilitate the detection of endangered species,\nanimal population analysis, habitat monitoring, biodiversity research, and\nvarious other applications that enrich wildlife conservation efforts. DEAL-YOLO\nemploys a two-stage inference paradigm for object detection, refining selected\nregions to improve localization and confidence. This approach enhances\nperformance, especially for small instances with low objectness scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although advances in deep learning and aerial surveillance technology are\nimproving wildlife conservation efforts, complex and erratic environmental\nconditions still pose a problem, requiring innovative solutions for\ncost-effective small animal detection. This work introduces DEAL-YOLO, a novel\napproach that improves small object detection in Unmanned Aerial Vehicle (UAV)\nimages by using multi-objective loss functions like Wise IoU (WIoU) and\nNormalized Wasserstein Distance (NWD), which prioritize pixels near the centre\nof the bounding box, ensuring smoother localization and reducing abrupt\ndeviations. Additionally, the model is optimized through efficient feature\nextraction with Linear Deformable (LD) convolutions, enhancing accuracy while\nmaintaining computational efficiency. The Scaled Sequence Feature Fusion (SSFF)\nmodule enhances object detection by effectively capturing inter-scale\nrelationships, improving feature representation, and boosting metrics through\noptimized multiscale fusion. Comparison with baseline models reveals high\nefficacy with up to 69.5\\% fewer parameters compared to vanilla Yolov8-N,\nhighlighting the robustness of the proposed modifications. Through this\napproach, our paper aims to facilitate the detection of endangered species,\nanimal population analysis, habitat monitoring, biodiversity research, and\nvarious other applications that enrich wildlife conservation efforts. DEAL-YOLO\nemploys a two-stage inference paradigm for object detection, refining selected\nregions to improve localization and confidence. This approach enhances\nperformance, especially for small instances with low objectness scores."
                },
                "authors": [
                    {
                        "name": "Aditya Prashant Naidu"
                    },
                    {
                        "name": "Hem Gosalia"
                    },
                    {
                        "name": "Ishaan Gakhar"
                    },
                    {
                        "name": "Shaurya Singh Rathore"
                    },
                    {
                        "name": "Krish Didwania"
                    },
                    {
                        "name": "Ujjwal Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ujjwal Verma"
                },
                "author": "Ujjwal Verma",
                "arxiv_comment": "Accepted as a Poster at the ML4RS Workshop at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04693v1",
                "updated": "2025-03-06T18:40:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    40,
                    0,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:40:00Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    40,
                    0,
                    3,
                    65,
                    0
                ],
                "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to\n  Forgetting Targets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to\n  Forgetting Targets"
                },
                "summary": "Large Language Models (LLMs) inevitably acquire harmful information during\ntraining on massive datasets. LLM unlearning aims to eliminate the influence of\nsuch harmful information while maintaining the model's overall performance.\nExisting unlearning methods, represented by gradient ascent-based approaches,\nprimarily focus on forgetting target data while overlooking the crucial impact\nof logically related knowledge on the effectiveness of unlearning. In this\npaper, through both theoretical and experimental analyses, we first demonstrate\nthat a key reason for the suboptimal unlearning performance is that models can\nreconstruct the target content through reasoning with logically related\nknowledge. To address this issue, we propose Unlearning Improvement via\nParameter Extrapolation (UIPE), a method that removes knowledge highly\ncorrelated with the forgetting targets. Experimental results show that UIPE\nsignificantly enhances the performance of various mainstream LLM unlearning\nmethods on the TOFU benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) inevitably acquire harmful information during\ntraining on massive datasets. LLM unlearning aims to eliminate the influence of\nsuch harmful information while maintaining the model's overall performance.\nExisting unlearning methods, represented by gradient ascent-based approaches,\nprimarily focus on forgetting target data while overlooking the crucial impact\nof logically related knowledge on the effectiveness of unlearning. In this\npaper, through both theoretical and experimental analyses, we first demonstrate\nthat a key reason for the suboptimal unlearning performance is that models can\nreconstruct the target content through reasoning with logically related\nknowledge. To address this issue, we propose Unlearning Improvement via\nParameter Extrapolation (UIPE), a method that removes knowledge highly\ncorrelated with the forgetting targets. Experimental results show that UIPE\nsignificantly enhances the performance of various mainstream LLM unlearning\nmethods on the TOFU benchmark."
                },
                "authors": [
                    {
                        "name": "Wenyu Wang"
                    },
                    {
                        "name": "Mengqi Zhang"
                    },
                    {
                        "name": "Xiaotian Ye"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Pengjie Ren"
                    }
                ],
                "author_detail": {
                    "name": "Pengjie Ren"
                },
                "author": "Pengjie Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04691v1",
                "updated": "2025-03-06T18:35:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    35,
                    39,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:35:39Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    35,
                    39,
                    3,
                    65,
                    0
                ],
                "title": "Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases"
                },
                "summary": "The latest reasoning-enhanced large language models (reasoning LLMs), such as\nDeepSeek-R1 and OpenAI-o3, have demonstrated remarkable success. However, the\napplication of such reasoning enhancements to the highly professional medical\ndomain has not been clearly evaluated, particularly regarding with not only\nassessing the final generation but also examining the quality of their\nreasoning processes. In this study, we present MedR-Bench, a reasoning-focused\nmedical evaluation benchmark comprising 1,453 structured patient cases with\nreasoning references mined from case reports. Our benchmark spans 13 body\nsystems and 10 specialty disorders, encompassing both common and rare diseases.\nIn our evaluation, we introduce a versatile framework consisting of three\ncritical clinical stages: assessment recommendation, diagnostic\ndecision-making, and treatment planning, comprehensively capturing the LLMs'\nperformance across the entire patient journey in healthcare. For metrics, we\npropose a novel agentic system, Reasoning Evaluator, designed to automate and\nobjectively quantify free-text reasoning responses in a scalable manner from\nthe perspectives of efficiency, factuality, and completeness by dynamically\nsearching and performing cross-referencing checks. As a result, we assess five\nstate-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and\nothers. Our results reveal that current LLMs can handle relatively simple\ndiagnostic tasks with sufficient critical assessment results, achieving\naccuracy generally over 85%. However, they still struggle with more complex\ntasks, such as assessment recommendation and treatment planning. In reasoning,\ntheir reasoning processes are generally reliable, with factuality scores\nexceeding 90%, though they often omit critical reasoning steps. Our study\nclearly reveals further development directions for current clinical LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The latest reasoning-enhanced large language models (reasoning LLMs), such as\nDeepSeek-R1 and OpenAI-o3, have demonstrated remarkable success. However, the\napplication of such reasoning enhancements to the highly professional medical\ndomain has not been clearly evaluated, particularly regarding with not only\nassessing the final generation but also examining the quality of their\nreasoning processes. In this study, we present MedR-Bench, a reasoning-focused\nmedical evaluation benchmark comprising 1,453 structured patient cases with\nreasoning references mined from case reports. Our benchmark spans 13 body\nsystems and 10 specialty disorders, encompassing both common and rare diseases.\nIn our evaluation, we introduce a versatile framework consisting of three\ncritical clinical stages: assessment recommendation, diagnostic\ndecision-making, and treatment planning, comprehensively capturing the LLMs'\nperformance across the entire patient journey in healthcare. For metrics, we\npropose a novel agentic system, Reasoning Evaluator, designed to automate and\nobjectively quantify free-text reasoning responses in a scalable manner from\nthe perspectives of efficiency, factuality, and completeness by dynamically\nsearching and performing cross-referencing checks. As a result, we assess five\nstate-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and\nothers. Our results reveal that current LLMs can handle relatively simple\ndiagnostic tasks with sufficient critical assessment results, achieving\naccuracy generally over 85%. However, they still struggle with more complex\ntasks, such as assessment recommendation and treatment planning. In reasoning,\ntheir reasoning processes are generally reliable, with factuality scores\nexceeding 90%, though they often omit critical reasoning steps. Our study\nclearly reveals further development directions for current clinical LLMs."
                },
                "authors": [
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Shuyu Liu"
                    },
                    {
                        "name": "Weike Zhao"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02664v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02664v2",
                "updated": "2025-03-06T18:31:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    31,
                    51,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-04T19:07:29Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    19,
                    7,
                    29,
                    1,
                    35,
                    0
                ],
                "title": "Differentiable Composite Neural Signed Distance Fields for Robot\n  Navigation in Dynamic Indoor Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Composite Neural Signed Distance Fields for Robot\n  Navigation in Dynamic Indoor Environments"
                },
                "summary": "Neural Signed Distance Fields (SDFs) provide a differentiable environment\nrepresentation to readily obtain collision checks and well-defined gradients\nfor robot navigation tasks. However, updating neural SDFs as the scene evolves\nentails re-training, which is tedious, time consuming, and inefficient, making\nit unsuitable for robot navigation with limited field-of-view in dynamic\nenvironments. Towards this objective, we propose a compositional framework of\nneural SDFs to solve robot navigation in indoor environments using only an\nonboard RGB-D sensor. Our framework embodies a dual mode procedure for\ntrajectory optimization, with different modes using complementary methods of\nmodeling collision costs and collision avoidance gradients. The primary stage\nqueries the robot body's SDF, swept along the route to goal, at the obstacle\npoint cloud, enabling swift local optimization of trajectories. The secondary\nstage infers the visible scene's SDF by aligning and composing the SDF\nrepresentations of its constituents, providing better informed costs and\ngradients for trajectory optimization. The dual mode procedure combines the\nbest of both stages, achieving a success rate of 98%, 14.4% higher than\nbaseline with comparable amortized plan time on iGibson 2.0. We also\ndemonstrate its effectiveness in adapting to real-world indoor scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Signed Distance Fields (SDFs) provide a differentiable environment\nrepresentation to readily obtain collision checks and well-defined gradients\nfor robot navigation tasks. However, updating neural SDFs as the scene evolves\nentails re-training, which is tedious, time consuming, and inefficient, making\nit unsuitable for robot navigation with limited field-of-view in dynamic\nenvironments. Towards this objective, we propose a compositional framework of\nneural SDFs to solve robot navigation in indoor environments using only an\nonboard RGB-D sensor. Our framework embodies a dual mode procedure for\ntrajectory optimization, with different modes using complementary methods of\nmodeling collision costs and collision avoidance gradients. The primary stage\nqueries the robot body's SDF, swept along the route to goal, at the obstacle\npoint cloud, enabling swift local optimization of trajectories. The secondary\nstage infers the visible scene's SDF by aligning and composing the SDF\nrepresentations of its constituents, providing better informed costs and\ngradients for trajectory optimization. The dual mode procedure combines the\nbest of both stages, achieving a success rate of 98%, 14.4% higher than\nbaseline with comparable amortized plan time on iGibson 2.0. We also\ndemonstrate its effectiveness in adapting to real-world indoor scenarios."
                },
                "authors": [
                    {
                        "name": "S. Talha Bukhari"
                    },
                    {
                        "name": "Daniel Lawson"
                    },
                    {
                        "name": "Ahmed H. Qureshi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed H. Qureshi"
                },
                "author": "Ahmed H. Qureshi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02664v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02664v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02800v2",
                "updated": "2025-03-06T18:30:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    30,
                    45,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-04T17:20:43Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    20,
                    43,
                    1,
                    63,
                    0
                ],
                "title": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration"
                },
                "summary": "Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7% to 89.1% on the real-world dataset. By allowing for the enriching of\ninput series data with semantics, RAAD-LLM incorporates multimodal capabilities\nthat facilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7% to 89.1% on the real-world dataset. By allowing for the enriching of\ninput series data with semantics, RAAD-LLM incorporates multimodal capabilities\nthat facilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries."
                },
                "authors": [
                    {
                        "name": "Alicia Russell-Gilbert"
                    },
                    {
                        "name": "Sudip Mittal"
                    },
                    {
                        "name": "Shahram Rahimi"
                    },
                    {
                        "name": "Maria Seale"
                    },
                    {
                        "name": "Joseph Jabour"
                    },
                    {
                        "name": "Thomas Arnold"
                    },
                    {
                        "name": "Joshua Church"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Church"
                },
                "author": "Joshua Church",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2411.00914",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "1.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04685v1",
                "updated": "2025-03-06T18:27:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    27,
                    41,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:27:41Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    27,
                    41,
                    3,
                    65,
                    0
                ],
                "title": "DIMSUM: Discourse in Mathematical Reasoning as a Supervision Module",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIMSUM: Discourse in Mathematical Reasoning as a Supervision Module"
                },
                "summary": "We look at reasoning on GSM8k, a dataset of short texts presenting primary\nschool, math problems. We find, with Mirzadeh et al. (2024), that current LLM\nprogress on the data set may not be explained by better reasoning but by\nexposure to a broader pretraining data distribution. We then introduce a novel\ninformation source for helping models with less data or inferior training\nreason better: discourse structure. We show that discourse structure improves\nperformance for models like Llama2 13b by up to 160%. Even for models that have\nmost likely memorized the data set, adding discourse structural information to\nthe model still improves predictions and dramatically improves large model\nperformance on out of distribution examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We look at reasoning on GSM8k, a dataset of short texts presenting primary\nschool, math problems. We find, with Mirzadeh et al. (2024), that current LLM\nprogress on the data set may not be explained by better reasoning but by\nexposure to a broader pretraining data distribution. We then introduce a novel\ninformation source for helping models with less data or inferior training\nreason better: discourse structure. We show that discourse structure improves\nperformance for models like Llama2 13b by up to 160%. Even for models that have\nmost likely memorized the data set, adding discourse structural information to\nthe model still improves predictions and dramatically improves large model\nperformance on out of distribution examples."
                },
                "authors": [
                    {
                        "name": "Krish Sharma"
                    },
                    {
                        "name": "Niyar R Barman"
                    },
                    {
                        "name": "Nicholas Asher"
                    },
                    {
                        "name": "Akshay Chaturvedi"
                    }
                ],
                "author_detail": {
                    "name": "Akshay Chaturvedi"
                },
                "author": "Akshay Chaturvedi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04680v1",
                "updated": "2025-03-06T18:22:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    22,
                    46,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:22:46Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    22,
                    46,
                    3,
                    65,
                    0
                ],
                "title": "Matrix Factorization for Inferring Associations and Missing Links",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix Factorization for Inferring Associations and Missing Links"
                },
                "summary": "Missing link prediction is a method for network analysis, with applications\nin recommender systems, biology, social sciences, cybersecurity, information\nretrieval, and Artificial Intelligence (AI) reasoning in Knowledge Graphs.\nMissing link prediction identifies unseen but potentially existing connections\nin a network by analyzing the observed patterns and relationships. In\nproliferation detection, this supports efforts to identify and characterize\nattempts by state and non-state actors to acquire nuclear weapons or associated\ntechnology - a notoriously challenging but vital mission for global security.\nDimensionality reduction techniques like Non-Negative Matrix Factorization\n(NMF) and Logistic Matrix Factorization (LMF) are effective but require\nselection of the matrix rank parameter, that is, of the number of hidden\nfeatures, k, to avoid over/under-fitting. We introduce novel Weighted (WNMFk),\nBoolean (BNMFk), and Recommender (RNMFk) matrix factorization methods, along\nwith ensemble variants incorporating logistic factorization, for link\nprediction. Our methods integrate automatic model determination for rank\nestimation by evaluating stability and accuracy using a modified bootstrap\nmethodology and uncertainty quantification (UQ), assessing prediction\nreliability under random perturbations. We incorporate Otsu threshold selection\nand k-means clustering for Boolean matrix factorization, comparing them to\ncoordinate descent-based Boolean thresholding. Our experiments highlight the\nimpact of rank k selection, evaluate model performance under varying test-set\nsizes, and demonstrate the benefits of UQ for reliable predictions using\nabstention. We validate our methods on three synthetic datasets (Boolean and\nuniformly distributed) and benchmark them against LMF and symmetric LMF\n(symLMF) on five real-world protein-protein interaction networks, showcasing an\nimproved prediction performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing link prediction is a method for network analysis, with applications\nin recommender systems, biology, social sciences, cybersecurity, information\nretrieval, and Artificial Intelligence (AI) reasoning in Knowledge Graphs.\nMissing link prediction identifies unseen but potentially existing connections\nin a network by analyzing the observed patterns and relationships. In\nproliferation detection, this supports efforts to identify and characterize\nattempts by state and non-state actors to acquire nuclear weapons or associated\ntechnology - a notoriously challenging but vital mission for global security.\nDimensionality reduction techniques like Non-Negative Matrix Factorization\n(NMF) and Logistic Matrix Factorization (LMF) are effective but require\nselection of the matrix rank parameter, that is, of the number of hidden\nfeatures, k, to avoid over/under-fitting. We introduce novel Weighted (WNMFk),\nBoolean (BNMFk), and Recommender (RNMFk) matrix factorization methods, along\nwith ensemble variants incorporating logistic factorization, for link\nprediction. Our methods integrate automatic model determination for rank\nestimation by evaluating stability and accuracy using a modified bootstrap\nmethodology and uncertainty quantification (UQ), assessing prediction\nreliability under random perturbations. We incorporate Otsu threshold selection\nand k-means clustering for Boolean matrix factorization, comparing them to\ncoordinate descent-based Boolean thresholding. Our experiments highlight the\nimpact of rank k selection, evaluate model performance under varying test-set\nsizes, and demonstrate the benefits of UQ for reliable predictions using\nabstention. We validate our methods on three synthetic datasets (Boolean and\nuniformly distributed) and benchmark them against LMF and symmetric LMF\n(symLMF) on five real-world protein-protein interaction networks, showcasing an\nimproved prediction performance."
                },
                "authors": [
                    {
                        "name": "Ryan Barron"
                    },
                    {
                        "name": "Maksim E. Eren"
                    },
                    {
                        "name": "Duc P. Truong"
                    },
                    {
                        "name": "Cynthia Matuszek"
                    },
                    {
                        "name": "James Wendelberger"
                    },
                    {
                        "name": "Mary F. Dorn"
                    },
                    {
                        "name": "Boian Alexandrov"
                    }
                ],
                "author_detail": {
                    "name": "Boian Alexandrov"
                },
                "author": "Boian Alexandrov",
                "arxiv_comment": "35 pages, 14 figures, 3 tables, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04679v1",
                "updated": "2025-03-06T18:22:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    22,
                    29,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:22:29Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    22,
                    29,
                    3,
                    65,
                    0
                ],
                "title": "Multi-Agent Inverse Q-Learning from Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Inverse Q-Learning from Demonstrations"
                },
                "summary": "When reward functions are hand-designed, deep reinforcement learning\nalgorithms often suffer from reward misspecification, causing them to learn\nsuboptimal policies in terms of the intended task objectives. In the\nsingle-agent case, inverse reinforcement learning (IRL) techniques attempt to\naddress this issue by inferring the reward function from expert demonstrations.\nHowever, in multi-agent problems, misalignment between the learned and true\nobjectives is exacerbated due to increased environment non-stationarity and\nvariance that scales with multiple agents. As such, in multi-agent general-sum\ngames, multi-agent IRL algorithms have difficulty balancing cooperative and\ncompetitive objectives. To address these issues, we propose Multi-Agent\nMarginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficient\nframework for multi-agent IRL. For each agent, MAMQL learns a critic\nmarginalized over the other agents' policies, allowing for a well-motivated use\nof Boltzmann policies in the multi-agent context. We identify a connection\nbetween optimal marginalized critics and single-agent soft-Q IRL, allowing us\nto apply a direct, simple optimization criterion from the single-agent domain.\nAcross our experiments on three different simulated domains, MAMQL\nsignificantly outperforms previous multi-agent methods in average reward,\nsample efficiency, and reward recovery by often more than 2-5x. We make our\ncode available at https://sites.google.com/view/mamql .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When reward functions are hand-designed, deep reinforcement learning\nalgorithms often suffer from reward misspecification, causing them to learn\nsuboptimal policies in terms of the intended task objectives. In the\nsingle-agent case, inverse reinforcement learning (IRL) techniques attempt to\naddress this issue by inferring the reward function from expert demonstrations.\nHowever, in multi-agent problems, misalignment between the learned and true\nobjectives is exacerbated due to increased environment non-stationarity and\nvariance that scales with multiple agents. As such, in multi-agent general-sum\ngames, multi-agent IRL algorithms have difficulty balancing cooperative and\ncompetitive objectives. To address these issues, we propose Multi-Agent\nMarginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficient\nframework for multi-agent IRL. For each agent, MAMQL learns a critic\nmarginalized over the other agents' policies, allowing for a well-motivated use\nof Boltzmann policies in the multi-agent context. We identify a connection\nbetween optimal marginalized critics and single-agent soft-Q IRL, allowing us\nto apply a direct, simple optimization criterion from the single-agent domain.\nAcross our experiments on three different simulated domains, MAMQL\nsignificantly outperforms previous multi-agent methods in average reward,\nsample efficiency, and reward recovery by often more than 2-5x. We make our\ncode available at https://sites.google.com/view/mamql ."
                },
                "authors": [
                    {
                        "name": "Nathaniel Haynam"
                    },
                    {
                        "name": "Adam Khoja"
                    },
                    {
                        "name": "Dhruv Kumar"
                    },
                    {
                        "name": "Vivek Myers"
                    },
                    {
                        "name": "Erdem Bıyık"
                    }
                ],
                "author_detail": {
                    "name": "Erdem Bıyık"
                },
                "author": "Erdem Bıyık",
                "arxiv_comment": "8 pages, 4 figures, 2 tables. Published at the International\n  Conference on Robotics and Automation (ICRA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10065v2",
                "updated": "2025-03-06T18:17:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    17,
                    2,
                    3,
                    65,
                    0
                ],
                "published": "2024-02-15T16:30:55Z",
                "published_parsed": [
                    2024,
                    2,
                    15,
                    16,
                    30,
                    55,
                    3,
                    46,
                    0
                ],
                "title": "Some Targets Are Harder to Identify than Others: Quantifying the\n  Target-dependent Membership Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some Targets Are Harder to Identify than Others: Quantifying the\n  Target-dependent Membership Leakage"
                },
                "summary": "In a Membership Inference (MI) game, an attacker tries to infer whether a\ntarget point was included or not in the input of an algorithm. Existing works\nshow that some target points are easier to identify, while others are harder.\nThis paper explains the target-dependent hardness of membership attacks by\nstudying the powers of the optimal attacks in a fixed-target MI game. We\ncharacterise the optimal advantage and trade-off functions of attacks against\nthe empirical mean in terms of the Mahalanobis distance between the target\npoint and the data-generating distribution. We further derive the impacts of\ntwo privacy defences, i.e. adding Gaussian noise and sub-sampling, and that of\ntarget misspecification on optimal attacks. As by-products of our novel\nanalysis of the Likelihood Ratio (LR) test, we provide a new covariance attack\nwhich generalises and improves the scalar product attack. Also, we propose a\nnew optimal canary-choosing strategy for auditing privacy in the white-box\nfederated learning setting. Our experiments validate that the Mahalanobis score\nexplains the hardness of fixed-target MI games.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a Membership Inference (MI) game, an attacker tries to infer whether a\ntarget point was included or not in the input of an algorithm. Existing works\nshow that some target points are easier to identify, while others are harder.\nThis paper explains the target-dependent hardness of membership attacks by\nstudying the powers of the optimal attacks in a fixed-target MI game. We\ncharacterise the optimal advantage and trade-off functions of attacks against\nthe empirical mean in terms of the Mahalanobis distance between the target\npoint and the data-generating distribution. We further derive the impacts of\ntwo privacy defences, i.e. adding Gaussian noise and sub-sampling, and that of\ntarget misspecification on optimal attacks. As by-products of our novel\nanalysis of the Likelihood Ratio (LR) test, we provide a new covariance attack\nwhich generalises and improves the scalar product attack. Also, we propose a\nnew optimal canary-choosing strategy for auditing privacy in the white-box\nfederated learning setting. Our experiments validate that the Mahalanobis score\nexplains the hardness of fixed-target MI games."
                },
                "authors": [
                    {
                        "name": "Achraf Azize"
                    },
                    {
                        "name": "Debabrota Basu"
                    }
                ],
                "author_detail": {
                    "name": "Debabrota Basu"
                },
                "author": "Debabrota Basu",
                "arxiv_comment": "Appears in AISTATS 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04675v1",
                "updated": "2025-03-06T18:12:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    12,
                    33,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:12:33Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    12,
                    33,
                    3,
                    65,
                    0
                ],
                "title": "LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable\n  User Satisfaction Estimation in Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable\n  User Satisfaction Estimation in Dialogue"
                },
                "summary": "Understanding user satisfaction with conversational systems, known as User\nSatisfaction Estimation (USE), is essential for assessing dialogue quality and\nenhancing user experiences. However, existing methods for USE face challenges\ndue to limited understanding of underlying reasons for user dissatisfaction and\nthe high costs of annotating user intentions. To address these challenges, we\npropose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction\nEstimation), an interpretable framework for effective user satisfaction\nprediction. PRAISE operates through three key modules. The Strategy Planner\ndevelops strategies, which are natural language criteria for classifying user\nsatisfaction. The Feature Retriever then incorporates knowledge on user\nsatisfaction from Large Language Models (LLMs) and retrieves relevance features\nfrom utterances. Finally, the Score Analyzer evaluates strategy predictions and\nclassifies user satisfaction. Experimental results demonstrate that PRAISE\nachieves state-of-the-art performance on three benchmarks for the USE task.\nBeyond its superior performance, PRAISE offers additional benefits. It enhances\ninterpretability by providing instance-level explanations through effective\nalignment of utterances with strategies. Moreover, PRAISE operates more\nefficiently than existing approaches by eliminating the need for LLMs during\nthe inference phase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding user satisfaction with conversational systems, known as User\nSatisfaction Estimation (USE), is essential for assessing dialogue quality and\nenhancing user experiences. However, existing methods for USE face challenges\ndue to limited understanding of underlying reasons for user dissatisfaction and\nthe high costs of annotating user intentions. To address these challenges, we\npropose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction\nEstimation), an interpretable framework for effective user satisfaction\nprediction. PRAISE operates through three key modules. The Strategy Planner\ndevelops strategies, which are natural language criteria for classifying user\nsatisfaction. The Feature Retriever then incorporates knowledge on user\nsatisfaction from Large Language Models (LLMs) and retrieves relevance features\nfrom utterances. Finally, the Score Analyzer evaluates strategy predictions and\nclassifies user satisfaction. Experimental results demonstrate that PRAISE\nachieves state-of-the-art performance on three benchmarks for the USE task.\nBeyond its superior performance, PRAISE offers additional benefits. It enhances\ninterpretability by providing instance-level explanations through effective\nalignment of utterances with strategies. Moreover, PRAISE operates more\nefficiently than existing approaches by eliminating the need for LLMs during\nthe inference phase."
                },
                "authors": [
                    {
                        "name": "Sangyeop Kim"
                    },
                    {
                        "name": "Sohhyung Park"
                    },
                    {
                        "name": "Jaewon Jung"
                    },
                    {
                        "name": "Jinseok Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "arxiv_comment": "Accepted by NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02067v2",
                "updated": "2025-03-06T18:09:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    9,
                    38,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-04T07:32:39Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    32,
                    39,
                    1,
                    35,
                    0
                ],
                "title": "AdaptBot: Combining LLM with Knowledge Graphs and Human Input for\n  Generic-to-Specific Task Decomposition and Knowledge Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptBot: Combining LLM with Knowledge Graphs and Human Input for\n  Generic-to-Specific Task Decomposition and Knowledge Refinement"
                },
                "summary": "An embodied agent assisting humans is often asked to complete new tasks, and\nthere may not be sufficient time or labeled examples to train the agent to\nperform these new tasks. Large Language Models (LLMs) trained on considerable\nknowledge across many domains can be used to predict a sequence of abstract\nactions for completing such tasks, although the agent may not be able to\nexecute this sequence due to task-, agent-, or domain-specific constraints. Our\nframework addresses these challenges by leveraging the generic predictions\nprovided by LLM and the prior domain knowledge encoded in a Knowledge Graph\n(KG), enabling an agent to quickly adapt to new tasks. The robot also solicits\nand uses human input as needed to refine its existing knowledge. Based on\nexperimental evaluation in the context of cooking and cleaning tasks in\nsimulation domains, we demonstrate that the interplay between LLM, KG, and\nhuman input leads to substantial performance gains compared with just using the\nLLM. Project website{\\S}: https://sssshivvvv.github.io/adaptbot/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An embodied agent assisting humans is often asked to complete new tasks, and\nthere may not be sufficient time or labeled examples to train the agent to\nperform these new tasks. Large Language Models (LLMs) trained on considerable\nknowledge across many domains can be used to predict a sequence of abstract\nactions for completing such tasks, although the agent may not be able to\nexecute this sequence due to task-, agent-, or domain-specific constraints. Our\nframework addresses these challenges by leveraging the generic predictions\nprovided by LLM and the prior domain knowledge encoded in a Knowledge Graph\n(KG), enabling an agent to quickly adapt to new tasks. The robot also solicits\nand uses human input as needed to refine its existing knowledge. Based on\nexperimental evaluation in the context of cooking and cleaning tasks in\nsimulation domains, we demonstrate that the interplay between LLM, KG, and\nhuman input leads to substantial performance gains compared with just using the\nLLM. Project website{\\S}: https://sssshivvvv.github.io/adaptbot/"
                },
                "authors": [
                    {
                        "name": "Shivam Singh"
                    },
                    {
                        "name": "Karthik Swaminathan"
                    },
                    {
                        "name": "Nabanita Dash"
                    },
                    {
                        "name": "Ramandeep Singh"
                    },
                    {
                        "name": "Snehasis Banerjee"
                    },
                    {
                        "name": "Mohan Sridharan"
                    },
                    {
                        "name": "Madhava Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Madhava Krishna"
                },
                "author": "Madhava Krishna",
                "arxiv_comment": "Accepted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16600v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16600v4",
                "updated": "2025-03-06T17:56:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    56,
                    40,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-23T15:00:53Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    15,
                    0,
                    53,
                    6,
                    54,
                    0
                ],
                "title": "Diagnosing Moral Reasoning Acquisition in Language Models: Pragmatics\n  and Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnosing Moral Reasoning Acquisition in Language Models: Pragmatics\n  and Generalization"
                },
                "summary": "Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs."
                },
                "authors": [
                    {
                        "name": "Guangliang Liu"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Xitong Zhang"
                    },
                    {
                        "name": "Kristen Marie Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Kristen Marie Johnson"
                },
                "author": "Kristen Marie Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16600v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16600v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00799v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00799v6",
                "updated": "2025-03-06T17:43:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    43,
                    10,
                    3,
                    65,
                    0
                ],
                "published": "2024-06-02T16:53:21Z",
                "published_parsed": [
                    2024,
                    6,
                    2,
                    16,
                    53,
                    21,
                    6,
                    154,
                    0
                ],
                "title": "Get my drift? Catching LLM Task Drift with Activation Deltas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Get my drift? Catching LLM Task Drift with Activation Deltas"
                },
                "summary": "LLMs are commonly used in retrieval-augmented applications to execute user\ninstructions based on data from external sources. For example, modern search\nengines use LLMs to answer queries based on relevant search results; email\nplugins summarize emails by processing their content through an LLM. However,\nthe potentially untrusted provenance of these data sources can lead to prompt\ninjection attacks, where the LLM is manipulated by natural language\ninstructions embedded in the external data, causing it to deviate from the\nuser's original instruction(s). We define this deviation as task drift. Task\ndrift is a significant concern as it allows attackers to exfiltrate data or\ninfluence the LLM's output for other users. We study LLM activations as a\nsolution to detect task drift, showing that activation deltas - the difference\nin activations before and after processing external data - are strongly\ncorrelated with this phenomenon. Through two probing methods, we demonstrate\nthat a simple linear classifier can detect drift with near-perfect ROC AUC on\nan out-of-distribution test set. We evaluate these methods by making minimal\nassumptions about how users' tasks, system prompts, and attacks can be phrased.\nWe observe that this approach generalizes surprisingly well to unseen task\ndomains, such as prompt injections, jailbreaks, and malicious instructions,\nwithout being trained on any of these attacks. Interestingly, the fact that\nthis solution does not require any modifications to the LLM (e.g.,\nfine-tuning), as well as its compatibility with existing meta-prompting\nsolutions, makes it cost-efficient and easy to deploy. To encourage further\nresearch on activation-based task inspection, decoding, and interpretability,\nwe release our large-scale TaskTracker toolkit, featuring a dataset of over\n500K instances, representations from six SoTA language models, and a suite of\ninspection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are commonly used in retrieval-augmented applications to execute user\ninstructions based on data from external sources. For example, modern search\nengines use LLMs to answer queries based on relevant search results; email\nplugins summarize emails by processing their content through an LLM. However,\nthe potentially untrusted provenance of these data sources can lead to prompt\ninjection attacks, where the LLM is manipulated by natural language\ninstructions embedded in the external data, causing it to deviate from the\nuser's original instruction(s). We define this deviation as task drift. Task\ndrift is a significant concern as it allows attackers to exfiltrate data or\ninfluence the LLM's output for other users. We study LLM activations as a\nsolution to detect task drift, showing that activation deltas - the difference\nin activations before and after processing external data - are strongly\ncorrelated with this phenomenon. Through two probing methods, we demonstrate\nthat a simple linear classifier can detect drift with near-perfect ROC AUC on\nan out-of-distribution test set. We evaluate these methods by making minimal\nassumptions about how users' tasks, system prompts, and attacks can be phrased.\nWe observe that this approach generalizes surprisingly well to unseen task\ndomains, such as prompt injections, jailbreaks, and malicious instructions,\nwithout being trained on any of these attacks. Interestingly, the fact that\nthis solution does not require any modifications to the LLM (e.g.,\nfine-tuning), as well as its compatibility with existing meta-prompting\nsolutions, makes it cost-efficient and easy to deploy. To encourage further\nresearch on activation-based task inspection, decoding, and interpretability,\nwe release our large-scale TaskTracker toolkit, featuring a dataset of over\n500K instances, representations from six SoTA language models, and a suite of\ninspection tools."
                },
                "authors": [
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Aideen Fay"
                    },
                    {
                        "name": "Giovanni Cherubin"
                    },
                    {
                        "name": "Ahmed Salem"
                    },
                    {
                        "name": "Mario Fritz"
                    },
                    {
                        "name": "Andrew Paverd"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Paverd"
                },
                "author": "Andrew Paverd",
                "arxiv_comment": "SaTML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00799v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00799v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.12041v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.12041v4",
                "updated": "2025-03-06T17:40:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    40,
                    23,
                    3,
                    65,
                    0
                ],
                "published": "2022-12-22T21:23:51Z",
                "published_parsed": [
                    2022,
                    12,
                    22,
                    21,
                    23,
                    51,
                    3,
                    356,
                    0
                ],
                "title": "Estimating network-mediated causal effects via principal components\n  network regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating network-mediated causal effects via principal components\n  network regression"
                },
                "summary": "We develop a method to decompose causal effects on a social network into an\nindirect effect mediated by the network, and a direct effect independent of the\nsocial network. To handle the complexity of network structures, we assume that\nlatent social groups act as causal mediators. We develop principal components\nnetwork regression models to differentiate the social effect from the\nnon-social effect. Fitting the regression models is as simple as principal\ncomponents analysis followed by ordinary least squares estimation. We prove\nasymptotic theory for regression coefficients from this procedure and show that\nit is widely applicable, allowing for a variety of distributions on the\nregression errors and network edges. We carefully characterize the\ncounterfactual assumptions necessary to use the regression models for causal\ninference, and show that current approaches to causal network regression may\nresult in over-control bias. The method is very general, so that it is\napplicable to many types of structured data beyond social networks, such as\ntext, areal data, psychometrics, images and omics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a method to decompose causal effects on a social network into an\nindirect effect mediated by the network, and a direct effect independent of the\nsocial network. To handle the complexity of network structures, we assume that\nlatent social groups act as causal mediators. We develop principal components\nnetwork regression models to differentiate the social effect from the\nnon-social effect. Fitting the regression models is as simple as principal\ncomponents analysis followed by ordinary least squares estimation. We prove\nasymptotic theory for regression coefficients from this procedure and show that\nit is widely applicable, allowing for a variety of distributions on the\nregression errors and network edges. We carefully characterize the\ncounterfactual assumptions necessary to use the regression models for causal\ninference, and show that current approaches to causal network regression may\nresult in over-control bias. The method is very general, so that it is\napplicable to many types of structured data beyond social networks, such as\ntext, areal data, psychometrics, images and omics."
                },
                "authors": [
                    {
                        "name": "Alex Hayes"
                    },
                    {
                        "name": "Mark M. Fredrickson"
                    },
                    {
                        "name": "Keith Levin"
                    }
                ],
                "author_detail": {
                    "name": "Keith Levin"
                },
                "author": "Keith Levin",
                "arxiv_comment": "Updating to match published JMLR version",
                "arxiv_journal_ref": "Journal of Machine Learning Research 26 (2025): 1-99",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.12041v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.12041v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04650v1",
                "updated": "2025-03-06T17:39:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    39,
                    12,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:39:12Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    39,
                    12,
                    3,
                    65,
                    0
                ],
                "title": "Joint Masked Reconstruction and Contrastive Learning for Mining\n  Interactions Between Proteins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Masked Reconstruction and Contrastive Learning for Mining\n  Interactions Between Proteins"
                },
                "summary": "Protein-protein interaction (PPI) prediction is an instrumental means in\nelucidating the mechanisms underlying cellular operations, holding significant\npractical implications for the realms of pharmaceutical development and\nclinical treatment. Presently, the majority of research methods primarily\nconcentrate on the analysis of amino acid sequences, while investigations\npredicated on protein structures remain in the nascent stages of exploration.\nDespite the emergence of several structure-based algorithms in recent years,\nthese are still confronted with inherent challenges: (1) the extraction of\nintrinsic structural information of proteins typically necessitates the\nexpenditure of substantial computational resources; (2) these models are overly\nreliant on seen protein data, struggling to effectively unearth interaction\ncues between unknown proteins. To further propel advancements in this domain,\nthis paper introduces a novel PPI prediction method jointing masked\nreconstruction and contrastive learning, termed JmcPPI. This methodology\ndissects the PPI prediction task into two distinct phases: during the residue\nstructure encoding phase, JmcPPI devises two feature reconstruction tasks and\nemploys graph attention mechanism to capture structural information between\nresidues; during the protein interaction inference phase, JmcPPI perturbs the\noriginal PPI graph and employs a multi-graph contrastive learning strategy to\nthoroughly mine extrinsic interaction information of novel proteins. Extensive\nexperiments conducted on three widely utilized PPI datasets demonstrate that\nJmcPPI surpasses existing optimal baseline models across various data partition\nschemes. The associated code can be accessed via\nhttps://github.com/lijfrank-open/JmcPPI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein-protein interaction (PPI) prediction is an instrumental means in\nelucidating the mechanisms underlying cellular operations, holding significant\npractical implications for the realms of pharmaceutical development and\nclinical treatment. Presently, the majority of research methods primarily\nconcentrate on the analysis of amino acid sequences, while investigations\npredicated on protein structures remain in the nascent stages of exploration.\nDespite the emergence of several structure-based algorithms in recent years,\nthese are still confronted with inherent challenges: (1) the extraction of\nintrinsic structural information of proteins typically necessitates the\nexpenditure of substantial computational resources; (2) these models are overly\nreliant on seen protein data, struggling to effectively unearth interaction\ncues between unknown proteins. To further propel advancements in this domain,\nthis paper introduces a novel PPI prediction method jointing masked\nreconstruction and contrastive learning, termed JmcPPI. This methodology\ndissects the PPI prediction task into two distinct phases: during the residue\nstructure encoding phase, JmcPPI devises two feature reconstruction tasks and\nemploys graph attention mechanism to capture structural information between\nresidues; during the protein interaction inference phase, JmcPPI perturbs the\noriginal PPI graph and employs a multi-graph contrastive learning strategy to\nthoroughly mine extrinsic interaction information of novel proteins. Extensive\nexperiments conducted on three widely utilized PPI datasets demonstrate that\nJmcPPI surpasses existing optimal baseline models across various data partition\nschemes. The associated code can be accessed via\nhttps://github.com/lijfrank-open/JmcPPI."
                },
                "authors": [
                    {
                        "name": "Jiang Li"
                    },
                    {
                        "name": "Xiaoping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoping Wang"
                },
                "author": "Xiaoping Wang",
                "arxiv_comment": "Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14281v2",
                "updated": "2025-03-06T17:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    38,
                    20,
                    3,
                    65,
                    0
                ],
                "published": "2024-09-22T00:56:42Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    0,
                    56,
                    42,
                    6,
                    266,
                    0
                ],
                "title": "Creative Writers' Attitudes on Writing as Training Data for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative Writers' Attitudes on Writing as Training Data for Large\n  Language Models"
                },
                "summary": "The use of creative writing as training data for large language models (LLMs)\nis highly contentious and many writers have expressed outrage at the use of\ntheir work without consent or compensation. In this paper, we seek to\nunderstand how creative writers reason about the real or hypothetical use of\ntheir writing as training data. We interviewed 33 writers with variation across\ngenre, method of publishing, degree of professionalization, and attitudes\ntoward and engagement with LLMs. We report on core principles that writers\nexpress (support of the creative chain, respect for writers and writing, and\nthe human element of creativity) and how these principles can be at odds with\ntheir realistic expectations of the world (a lack of control, industry-scale\nimpacts, and interpretation of scale). Collectively these findings demonstrate\nthat writers have a nuanced understanding of LLMs and are more concerned with\npower imbalances than the technology itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of creative writing as training data for large language models (LLMs)\nis highly contentious and many writers have expressed outrage at the use of\ntheir work without consent or compensation. In this paper, we seek to\nunderstand how creative writers reason about the real or hypothetical use of\ntheir writing as training data. We interviewed 33 writers with variation across\ngenre, method of publishing, degree of professionalization, and attitudes\ntoward and engagement with LLMs. We report on core principles that writers\nexpress (support of the creative chain, respect for writers and writing, and\nthe human element of creativity) and how these principles can be at odds with\ntheir realistic expectations of the world (a lack of control, industry-scale\nimpacts, and interpretation of scale). Collectively these findings demonstrate\nthat writers have a nuanced understanding of LLMs and are more concerned with\npower imbalances than the technology itself."
                },
                "authors": [
                    {
                        "name": "Katy Ilonka Gero"
                    },
                    {
                        "name": "Meera Desai"
                    },
                    {
                        "name": "Carly Schnitzler"
                    },
                    {
                        "name": "Nayun Eom"
                    },
                    {
                        "name": "Jack Cushman"
                    },
                    {
                        "name": "Elena L. Glassman"
                    }
                ],
                "author_detail": {
                    "name": "Elena L. Glassman"
                },
                "author": "Elena L. Glassman",
                "arxiv_doi": "10.1145/3706598.3713287",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713287",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.14281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "CHI 25",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04649v1",
                "updated": "2025-03-06T17:35:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    35,
                    37,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:35:37Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    35,
                    37,
                    3,
                    65,
                    0
                ],
                "title": "Transferable Foundation Models for Geometric Tasks on Point Cloud\n  Representations: Geometric Neural Operators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable Foundation Models for Geometric Tasks on Point Cloud\n  Representations: Geometric Neural Operators"
                },
                "summary": "We introduce methods for obtaining pretrained Geometric Neural Operators\n(GNPs) that can serve as basal foundation models for use in obtaining geometric\nfeatures. These can be used within data processing pipelines for machine\nlearning tasks and numerical methods. We show how our GNPs can be trained to\nlearn robust latent representations for the differential geometry of\npoint-clouds to provide estimates of metric, curvature, and other shape-related\nfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimate\nthe geometric properties of surfaces of arbitrary shape and topologies with\nrobustness in the presence of noise, (ii) to approximate solutions of geometric\npartial differential equations (PDEs) on manifolds, and (iii) to solve\nequations for shape deformations such as curvature driven flows. We also\nrelease a package of the codes and weights for using our pre-trained GNPs for\nprocessing point cloud representations. This allows for incorporating our\npre-trained GNPs as components for reuse within existing and new data\nprocessing pipelines. The GNPs also can be used as part of numerical solvers\ninvolving geometry or as part of methods for performing inference and other\ngeometric tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce methods for obtaining pretrained Geometric Neural Operators\n(GNPs) that can serve as basal foundation models for use in obtaining geometric\nfeatures. These can be used within data processing pipelines for machine\nlearning tasks and numerical methods. We show how our GNPs can be trained to\nlearn robust latent representations for the differential geometry of\npoint-clouds to provide estimates of metric, curvature, and other shape-related\nfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimate\nthe geometric properties of surfaces of arbitrary shape and topologies with\nrobustness in the presence of noise, (ii) to approximate solutions of geometric\npartial differential equations (PDEs) on manifolds, and (iii) to solve\nequations for shape deformations such as curvature driven flows. We also\nrelease a package of the codes and weights for using our pre-trained GNPs for\nprocessing point cloud representations. This allows for incorporating our\npre-trained GNPs as components for reuse within existing and new data\nprocessing pipelines. The GNPs also can be used as part of numerical solvers\ninvolving geometry or as part of methods for performing inference and other\ngeometric tasks."
                },
                "authors": [
                    {
                        "name": "Blaine Quackenbush"
                    },
                    {
                        "name": "Paul J. Atzberger"
                    }
                ],
                "author_detail": {
                    "name": "Paul J. Atzberger"
                },
                "author": "Paul J. Atzberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04648v1",
                "updated": "2025-03-06T17:35:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    35,
                    6,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:35:06Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    35,
                    6,
                    3,
                    65,
                    0
                ],
                "title": "Assessing the performance of compartmental and renewal models for\n  learning $R_{t}$ using spatially heterogeneous epidemic simulations on real\n  geographies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the performance of compartmental and renewal models for\n  learning $R_{t}$ using spatially heterogeneous epidemic simulations on real\n  geographies"
                },
                "summary": "The time-varying reproduction number ($R_t$) gives an indication of the\ntrajectory of an infectious disease outbreak. Commonly used frameworks for\ninferring $R_t$ from epidemiological time series include those based on\ncompartmental models (such as the SEIR model) and renewal equation models.\nThese inference methods are usually validated using synthetic data generated\nfrom a simple model, often from the same class of model as the inference\nframework. However, in a real outbreak the transmission processes, and thus the\ninfection data collected, are much more complex. The performance of common\n$R_t$ inference methods on data with similar complexity to real world scenarios\nhas been subject to less comprehensive validation. We therefore propose\nevaluating these inference methods on outbreak data generated from a\nsophisticated, geographically accurate agent-based model. We illustrate this\nproposed method by generating synthetic data for two outbreaks in Northern\nIreland: one with minimal spatial heterogeneity, and one with additional\nheterogeneity. We find that the simple SEIR model struggles with the greater\nheterogeneity, while the renewal equation model demonstrates greater robustness\nto spatial heterogeneity, though is sensitive to the accuracy of the generation\ntime distribution used in inference. Our approach represents a principled way\nto benchmark epidemiological inference tools and is built upon an open-source\nsoftware platform for reproducible epidemic simulation and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The time-varying reproduction number ($R_t$) gives an indication of the\ntrajectory of an infectious disease outbreak. Commonly used frameworks for\ninferring $R_t$ from epidemiological time series include those based on\ncompartmental models (such as the SEIR model) and renewal equation models.\nThese inference methods are usually validated using synthetic data generated\nfrom a simple model, often from the same class of model as the inference\nframework. However, in a real outbreak the transmission processes, and thus the\ninfection data collected, are much more complex. The performance of common\n$R_t$ inference methods on data with similar complexity to real world scenarios\nhas been subject to less comprehensive validation. We therefore propose\nevaluating these inference methods on outbreak data generated from a\nsophisticated, geographically accurate agent-based model. We illustrate this\nproposed method by generating synthetic data for two outbreaks in Northern\nIreland: one with minimal spatial heterogeneity, and one with additional\nheterogeneity. We find that the simple SEIR model struggles with the greater\nheterogeneity, while the renewal equation model demonstrates greater robustness\nto spatial heterogeneity, though is sensitive to the accuracy of the generation\ntime distribution used in inference. Our approach represents a principled way\nto benchmark epidemiological inference tools and is built upon an open-source\nsoftware platform for reproducible epidemic simulation and inference."
                },
                "authors": [
                    {
                        "name": "Matthew Ghosh"
                    },
                    {
                        "name": "Yunli Qi"
                    },
                    {
                        "name": "Abbie Evans"
                    },
                    {
                        "name": "Tom Reed"
                    },
                    {
                        "name": "Lara Herriott"
                    },
                    {
                        "name": "Ioana Bouros"
                    },
                    {
                        "name": "Ben Lambert"
                    },
                    {
                        "name": "David J. Gavaghan"
                    },
                    {
                        "name": "Katherine M. Shepherd"
                    },
                    {
                        "name": "Richard Creswell"
                    },
                    {
                        "name": "Kit Gallagher"
                    }
                ],
                "author_detail": {
                    "name": "Kit Gallagher"
                },
                "author": "Kit Gallagher",
                "arxiv_comment": "48 pages, 4 figures, 7 supplementary figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04647v1",
                "updated": "2025-03-06T17:33:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    33,
                    1,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:33:01Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    33,
                    1,
                    3,
                    65,
                    0
                ],
                "title": "Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference\n  Alignment"
                },
                "summary": "Direct Preference Optimization (DPO) has become a prominent method for\naligning Large Language Models (LLMs) with human preferences. While DPO has\nenabled significant progress in aligning English LLMs, multilingual preference\nalignment is hampered by data scarcity. To address this, we propose a novel\napproach that $\\textit{captures}$ learned preferences from well-aligned English\nmodels by implicit rewards and $\\textit{transfers}$ them to other languages\nthrough iterative training. Specifically, we derive an implicit reward model\nfrom the logits of an English DPO-aligned model and its corresponding reference\nmodel. This reward model is then leveraged to annotate preference relations in\ncross-lingual instruction-following pairs, using English instructions to\nevaluate multilingual responses. The annotated data is subsequently used for\nmultilingual DPO fine-tuning, facilitating preference knowledge transfer from\nEnglish to other languages. Fine-tuning Llama3 for two iterations resulted in a\n12.72% average improvement in Win Rate and a 5.97% increase in Length Control\nWin Rate across all training languages on the X-AlpacaEval leaderboard. Our\nfindings demonstrate that leveraging existing English-aligned models can enable\nefficient and effective multilingual preference alignment, significantly\nreducing the need for extensive multilingual preference data. The code is\navailable at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has become a prominent method for\naligning Large Language Models (LLMs) with human preferences. While DPO has\nenabled significant progress in aligning English LLMs, multilingual preference\nalignment is hampered by data scarcity. To address this, we propose a novel\napproach that $\\textit{captures}$ learned preferences from well-aligned English\nmodels by implicit rewards and $\\textit{transfers}$ them to other languages\nthrough iterative training. Specifically, we derive an implicit reward model\nfrom the logits of an English DPO-aligned model and its corresponding reference\nmodel. This reward model is then leveraged to annotate preference relations in\ncross-lingual instruction-following pairs, using English instructions to\nevaluate multilingual responses. The annotated data is subsequently used for\nmultilingual DPO fine-tuning, facilitating preference knowledge transfer from\nEnglish to other languages. Fine-tuning Llama3 for two iterations resulted in a\n12.72% average improvement in Win Rate and a 5.97% increase in Length Control\nWin Rate across all training languages on the X-AlpacaEval leaderboard. Our\nfindings demonstrate that leveraging existing English-aligned models can enable\nefficient and effective multilingual preference alignment, significantly\nreducing the need for extensive multilingual preference data. The code is\navailable at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding"
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Chengqing Zong"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04645v1",
                "updated": "2025-03-06T17:32:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    32,
                    35,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    32,
                    35,
                    3,
                    65,
                    0
                ],
                "title": "Ultra-Low-Latency Edge Intelligent Sensing: A Source-Channel Tradeoff\n  and Its Application to Coding Rate Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-Low-Latency Edge Intelligent Sensing: A Source-Channel Tradeoff\n  and Its Application to Coding Rate Adaptation"
                },
                "summary": "The forthcoming sixth-generation (6G) mobile network is set to merge edge\nartificial intelligence (AI) and integrated sensing and communication (ISAC)\nextensively, giving rise to the new paradigm of edge intelligent sensing\n(EI-Sense). This paradigm leverages ubiquitous edge devices for environmental\nsensing and deploys AI algorithms at edge servers to interpret the observations\nvia remote inference on wirelessly uploaded features. A significant challenge\narises in designing EI-Sense systems for 6G mission-critical applications,\nwhich demand high performance under stringent latency constraints. To tackle\nthis challenge, we focus on the end-to-end (E2E) performance of EI-Sense and\ncharacterize a source-channel tradeoff that balances source distortion and\nchannel reliability. In this work, we establish a theoretical foundation for\nthe source-channel tradeoff by quantifying the effects of source coding on\nfeature discriminant gains and channel reliability on packet loss. Building on\nthis foundation, we design the coding rate control by optimizing the tradeoff\nto minimize the E2E sensing error probability, leading to a low-complexity\nalgorithm for ultra-low-latency EI-Sense. Finally, we validate our theoretical\nanalysis and proposed coding rate control algorithm through extensive\nexperiments on both synthetic and real datasets, demonstrating the sensing\nperformance gain of our approach with respect to traditional\nreliability-centric methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The forthcoming sixth-generation (6G) mobile network is set to merge edge\nartificial intelligence (AI) and integrated sensing and communication (ISAC)\nextensively, giving rise to the new paradigm of edge intelligent sensing\n(EI-Sense). This paradigm leverages ubiquitous edge devices for environmental\nsensing and deploys AI algorithms at edge servers to interpret the observations\nvia remote inference on wirelessly uploaded features. A significant challenge\narises in designing EI-Sense systems for 6G mission-critical applications,\nwhich demand high performance under stringent latency constraints. To tackle\nthis challenge, we focus on the end-to-end (E2E) performance of EI-Sense and\ncharacterize a source-channel tradeoff that balances source distortion and\nchannel reliability. In this work, we establish a theoretical foundation for\nthe source-channel tradeoff by quantifying the effects of source coding on\nfeature discriminant gains and channel reliability on packet loss. Building on\nthis foundation, we design the coding rate control by optimizing the tradeoff\nto minimize the E2E sensing error probability, leading to a low-complexity\nalgorithm for ultra-low-latency EI-Sense. Finally, we validate our theoretical\nanalysis and proposed coding rate control algorithm through extensive\nexperiments on both synthetic and real datasets, demonstrating the sensing\nperformance gain of our approach with respect to traditional\nreliability-centric methods."
                },
                "authors": [
                    {
                        "name": "Qunsong Zeng"
                    },
                    {
                        "name": "Jianhao Huang"
                    },
                    {
                        "name": "Zhanwei Wang"
                    },
                    {
                        "name": "Kaibin Huang"
                    },
                    {
                        "name": "Kin K. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Kin K. Leung"
                },
                "author": "Kin K. Leung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04644v1",
                "updated": "2025-03-06T17:32:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    32,
                    22,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:32:22Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    32,
                    22,
                    3,
                    65,
                    0
                ],
                "title": "IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in\n  Expert-Domain Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in\n  Expert-Domain Information Retrieval"
                },
                "summary": "We introduce IFIR, the first comprehensive benchmark designed to evaluate\ninstruction-following information retrieval (IR) in expert domains. IFIR\nincludes 2,426 high-quality examples and covers eight subsets across four\nspecialized domains: finance, law, healthcare, and science literature. Each\nsubset addresses one or more domain-specific retrieval tasks, replicating\nreal-world scenarios where customized instructions are critical. IFIR enables a\ndetailed analysis of instruction-following retrieval capabilities by\nincorporating instructions at different levels of complexity. We also propose a\nnovel LLM-based evaluation method to provide a more precise and reliable\nassessment of model performance in following instructions. Through extensive\nexperiments on 15 frontier retrieval models, including those based on LLMs, our\nresults reveal that current models face significant challenges in effectively\nfollowing complex, domain-specific instructions. We further provide in-depth\nanalyses to highlight these limitations, offering valuable insights to guide\nfuture advancements in retriever development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce IFIR, the first comprehensive benchmark designed to evaluate\ninstruction-following information retrieval (IR) in expert domains. IFIR\nincludes 2,426 high-quality examples and covers eight subsets across four\nspecialized domains: finance, law, healthcare, and science literature. Each\nsubset addresses one or more domain-specific retrieval tasks, replicating\nreal-world scenarios where customized instructions are critical. IFIR enables a\ndetailed analysis of instruction-following retrieval capabilities by\nincorporating instructions at different levels of complexity. We also propose a\nnovel LLM-based evaluation method to provide a more precise and reliable\nassessment of model performance in following instructions. Through extensive\nexperiments on 15 frontier retrieval models, including those based on LLMs, our\nresults reveal that current models face significant challenges in effectively\nfollowing complex, domain-specific instructions. We further provide in-depth\nanalyses to highlight these limitations, offering valuable insights to guide\nfuture advancements in retriever development."
                },
                "authors": [
                    {
                        "name": "Tingyu Song"
                    },
                    {
                        "name": "Guo Gan"
                    },
                    {
                        "name": "Mingsheng Shang"
                    },
                    {
                        "name": "Yilun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Zhao"
                },
                "author": "Yilun Zhao",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11047v2",
                "updated": "2025-03-06T17:28:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    28,
                    54,
                    3,
                    65,
                    0
                ],
                "published": "2024-09-17T10:13:09Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    13,
                    9,
                    1,
                    261,
                    0
                ],
                "title": "TacDiffusion: Force-domain Diffusion Policy for Precise Tactile\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TacDiffusion: Force-domain Diffusion Policy for Precise Tactile\n  Manipulation"
                },
                "summary": "Assembly is a crucial skill for robots in both modern manufacturing and\nservice robotics. However, mastering transferable insertion skills that can\nhandle a variety of high-precision assembly tasks remains a significant\nchallenge. This paper presents a novel framework that utilizes diffusion models\nto generate 6D wrench for high-precision tactile robotic insertion tasks. It\nlearns from demonstrations performed on a single task and achieves a zero-shot\ntransfer success rate of 95.7% across various novel high-precision tasks. Our\nmethod effectively inherits the self-adaptability demonstrated by our previous\nwork. In this framework, we address the frequency misalignment between the\ndiffusion policy and the real-time control loop with a dynamic system-based\nfilter, significantly improving the task success rate by 9.15%. Furthermore, we\nprovide a practical guideline regarding the trade-off between diffusion models'\ninference ability and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assembly is a crucial skill for robots in both modern manufacturing and\nservice robotics. However, mastering transferable insertion skills that can\nhandle a variety of high-precision assembly tasks remains a significant\nchallenge. This paper presents a novel framework that utilizes diffusion models\nto generate 6D wrench for high-precision tactile robotic insertion tasks. It\nlearns from demonstrations performed on a single task and achieves a zero-shot\ntransfer success rate of 95.7% across various novel high-precision tasks. Our\nmethod effectively inherits the self-adaptability demonstrated by our previous\nwork. In this framework, we address the frequency misalignment between the\ndiffusion policy and the real-time control loop with a dynamic system-based\nfilter, significantly improving the task success rate by 9.15%. Furthermore, we\nprovide a practical guideline regarding the trade-off between diffusion models'\ninference ability and speed."
                },
                "authors": [
                    {
                        "name": "Yansong Wu"
                    },
                    {
                        "name": "Zongxie Chen"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Lingyun Chen"
                    },
                    {
                        "name": "Liding Zhang"
                    },
                    {
                        "name": "Zhenshan Bing"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Sami Haddadin"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "arxiv_comment": "7 pages. Accepted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2202.00665v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2202.00665v4",
                "updated": "2025-03-06T17:24:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    24,
                    46,
                    3,
                    65,
                    0
                ],
                "published": "2022-02-01T18:58:33Z",
                "published_parsed": [
                    2022,
                    2,
                    1,
                    18,
                    58,
                    33,
                    1,
                    32,
                    0
                ],
                "title": "Tutorial on amortized optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tutorial on amortized optimization"
                },
                "summary": "Optimization is a ubiquitous modeling tool and is often deployed in settings\nwhich repeatedly solve similar instances of the same problem. Amortized\noptimization methods use learning to predict the solutions to problems in these\nsettings, exploiting the shared structure between similar problem instances.\nThese methods have been crucial in variational inference and reinforcement\nlearning and are capable of solving optimization problems many orders of\nmagnitudes times faster than traditional optimization methods that do not use\namortization. This tutorial presents an introduction to the amortized\noptimization foundations behind these advancements and overviews their\napplications in variational inference, sparse coding, gradient-based\nmeta-learning, control, reinforcement learning, convex optimization, optimal\ntransport, and deep equilibrium networks. The source code for this tutorial is\navailable at\nhttps://github.com/facebookresearch/amortized-optimization-tutorial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization is a ubiquitous modeling tool and is often deployed in settings\nwhich repeatedly solve similar instances of the same problem. Amortized\noptimization methods use learning to predict the solutions to problems in these\nsettings, exploiting the shared structure between similar problem instances.\nThese methods have been crucial in variational inference and reinforcement\nlearning and are capable of solving optimization problems many orders of\nmagnitudes times faster than traditional optimization methods that do not use\namortization. This tutorial presents an introduction to the amortized\noptimization foundations behind these advancements and overviews their\napplications in variational inference, sparse coding, gradient-based\nmeta-learning, control, reinforcement learning, convex optimization, optimal\ntransport, and deep equilibrium networks. The source code for this tutorial is\navailable at\nhttps://github.com/facebookresearch/amortized-optimization-tutorial."
                },
                "authors": [
                    {
                        "name": "Brandon Amos"
                    }
                ],
                "author_detail": {
                    "name": "Brandon Amos"
                },
                "author": "Brandon Amos",
                "arxiv_comment": "Foundations and Trends in Machine Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2202.00665v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2202.00665v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04636v1",
                "updated": "2025-03-06T17:24:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    24,
                    6,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:24:06Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    24,
                    6,
                    3,
                    65,
                    0
                ],
                "title": "Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models\n  via Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models\n  via Watermarking"
                },
                "summary": "As open-source large language models (LLMs) like Llama3 become more capable,\nit is crucial to develop watermarking techniques to detect their potential\nmisuse. Existing watermarking methods either add watermarks during LLM\ninference, which is unsuitable for open-source LLMs, or primarily target\nclassification LLMs rather than recent generative LLMs. Adapting these\nwatermarks to open-source LLMs for misuse detection remains an open challenge.\nThis work defines two misuse scenarios for open-source LLMs: intellectual\nproperty (IP) violation and LLM Usage Violation. Then, we explore the\napplication of inference-time watermark distillation and backdoor watermarking\nin these contexts. We propose comprehensive evaluation methods to assess the\nimpact of various real-world further fine-tuning scenarios on watermarks and\nthe effect of these watermarks on LLM performance. Our experiments reveal that\nbackdoor watermarking could effectively detect IP Violation, while\ninference-time watermark distillation is applicable in both scenarios but less\nrobust to further fine-tuning and has a more significant impact on LLM\nperformance compared to backdoor watermarking. Exploring more advanced\nwatermarking methods for open-source LLMs to detect their misuse should be an\nimportant future direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As open-source large language models (LLMs) like Llama3 become more capable,\nit is crucial to develop watermarking techniques to detect their potential\nmisuse. Existing watermarking methods either add watermarks during LLM\ninference, which is unsuitable for open-source LLMs, or primarily target\nclassification LLMs rather than recent generative LLMs. Adapting these\nwatermarks to open-source LLMs for misuse detection remains an open challenge.\nThis work defines two misuse scenarios for open-source LLMs: intellectual\nproperty (IP) violation and LLM Usage Violation. Then, we explore the\napplication of inference-time watermark distillation and backdoor watermarking\nin these contexts. We propose comprehensive evaluation methods to assess the\nimpact of various real-world further fine-tuning scenarios on watermarks and\nthe effect of these watermarks on LLM performance. Our experiments reveal that\nbackdoor watermarking could effectively detect IP Violation, while\ninference-time watermark distillation is applicable in both scenarios but less\nrobust to further fine-tuning and has a more significant impact on LLM\nperformance compared to backdoor watermarking. Exploring more advanced\nwatermarking methods for open-source LLMs to detect their misuse should be an\nimportant future direction."
                },
                "authors": [
                    {
                        "name": "Yijie Xu"
                    },
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Lijie Wen"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by the 1st Workshop on GenAI Watermarking, collocated with\n  ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04629v1",
                "updated": "2025-03-06T17:15:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    15,
                    48,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:15:48Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    15,
                    48,
                    3,
                    65,
                    0
                ],
                "title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing"
                },
                "summary": "Survey paper plays a crucial role in scientific research, especially given\nthe rapid growth of research publications. Recently, researchers have begun\nusing LLMs to automate survey generation for better efficiency. However, the\nquality gap between LLM-generated surveys and those written by human remains\nsignificant, particularly in terms of outline quality and citation accuracy. To\nclose these gaps, we introduce SurveyForge, which first generates the outline\nby analyzing the logical structure of human-written outlines and referring to\nthe retrieved domain-related articles. Subsequently, leveraging high-quality\npapers retrieved from memory by our scholar navigation agent, SurveyForge can\nautomatically generate and refine the content of the generated article.\nMoreover, to achieve a comprehensive evaluation, we construct SurveyBench,\nwhich includes 100 human-written survey papers for win-rate comparison and\nassesses AI-generated survey papers across three dimensions: reference,\noutline, and content quality. Experiments demonstrate that SurveyForge can\noutperform previous works such as AutoSurvey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey paper plays a crucial role in scientific research, especially given\nthe rapid growth of research publications. Recently, researchers have begun\nusing LLMs to automate survey generation for better efficiency. However, the\nquality gap between LLM-generated surveys and those written by human remains\nsignificant, particularly in terms of outline quality and citation accuracy. To\nclose these gaps, we introduce SurveyForge, which first generates the outline\nby analyzing the logical structure of human-written outlines and referring to\nthe retrieved domain-related articles. Subsequently, leveraging high-quality\npapers retrieved from memory by our scholar navigation agent, SurveyForge can\nautomatically generate and refine the content of the generated article.\nMoreover, to achieve a comprehensive evaluation, we construct SurveyBench,\nwhich includes 100 human-written survey papers for win-rate comparison and\nassesses AI-generated survey papers across three dimensions: reference,\noutline, and content quality. Experiments demonstrate that SurveyForge can\noutperform previous works such as AutoSurvey."
                },
                "authors": [
                    {
                        "name": "Xiangchao Yan"
                    },
                    {
                        "name": "Shiyang Feng"
                    },
                    {
                        "name": "Jiakang Yuan"
                    },
                    {
                        "name": "Renqiu Xia"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Lei Bai"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bai"
                },
                "author": "Lei Bai",
                "arxiv_comment": "Code and dataset are available for downloading at:\n  https://github.com/Alpha-Innovator/SurveyForge 22 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11919v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11919v3",
                "updated": "2025-03-06T17:12:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    12,
                    48,
                    3,
                    65,
                    0
                ],
                "published": "2024-09-18T12:32:25Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    12,
                    32,
                    25,
                    2,
                    262,
                    0
                ],
                "title": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language\n  Models for Referring Expression Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language\n  Models for Referring Expression Comprehension"
                },
                "summary": "Vision Language Models (VLMs) have demonstrated remarkable capabilities in\nvarious open-vocabulary tasks, yet their zero-shot performance lags behind\ntask-specific fine-tuned models, particularly in complex tasks like Referring\nExpression Comprehension (REC). Fine-tuning usually requires 'white-box' access\nto the model's architecture and weights, which is not always feasible due to\nproprietary or privacy concerns. In this work, we propose LLM-wrapper, a method\nfor 'black-box' adaptation of VLMs for the REC task using Large Language Models\n(LLMs). LLM-wrapper capitalizes on the reasoning abilities of LLMs, improved\nwith a light fine-tuning, to select the most relevant bounding box matching the\nreferring expression, from candidates generated by a zero-shot black-box VLM.\nOur approach offers several advantages: it enables the adaptation of\nclosed-source models without needing access to their internal workings, it is\nversatile as it works with any VLM, it transfers to new VLMs and datasets, and\nit allows for the adaptation of an ensemble of VLMs. We evaluate LLM-wrapper on\nmultiple datasets using different VLMs and LLMs, demonstrating significant\nperformance improvements and highlighting the versatility of our method. While\nLLM-wrapper is not meant to directly compete with standard white-box\nfine-tuning, it offers a practical and effective alternative for black-box VLM\nadaptation. Code and checkpoints are available at\nhttps://github.com/valeoai/LLM_wrapper .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have demonstrated remarkable capabilities in\nvarious open-vocabulary tasks, yet their zero-shot performance lags behind\ntask-specific fine-tuned models, particularly in complex tasks like Referring\nExpression Comprehension (REC). Fine-tuning usually requires 'white-box' access\nto the model's architecture and weights, which is not always feasible due to\nproprietary or privacy concerns. In this work, we propose LLM-wrapper, a method\nfor 'black-box' adaptation of VLMs for the REC task using Large Language Models\n(LLMs). LLM-wrapper capitalizes on the reasoning abilities of LLMs, improved\nwith a light fine-tuning, to select the most relevant bounding box matching the\nreferring expression, from candidates generated by a zero-shot black-box VLM.\nOur approach offers several advantages: it enables the adaptation of\nclosed-source models without needing access to their internal workings, it is\nversatile as it works with any VLM, it transfers to new VLMs and datasets, and\nit allows for the adaptation of an ensemble of VLMs. We evaluate LLM-wrapper on\nmultiple datasets using different VLMs and LLMs, demonstrating significant\nperformance improvements and highlighting the versatility of our method. While\nLLM-wrapper is not meant to directly compete with standard white-box\nfine-tuning, it offers a practical and effective alternative for black-box VLM\nadaptation. Code and checkpoints are available at\nhttps://github.com/valeoai/LLM_wrapper ."
                },
                "authors": [
                    {
                        "name": "Amaia Cardiel"
                    },
                    {
                        "name": "Eloi Zablocki"
                    },
                    {
                        "name": "Elias Ramzi"
                    },
                    {
                        "name": "Oriane Siméoni"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "LLM-wrapper (v3) is published as a conference paper at ICLR 2025. (v1\n  was presented at EVAL-FoMo workshop, ECCV 2024.)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11919v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11919v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04625v1",
                "updated": "2025-03-06T17:11:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    11,
                    51,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:11:51Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    11,
                    51,
                    3,
                    65,
                    0
                ],
                "title": "START: Self-taught Reasoner with Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "START: Self-taught Reasoner with Tools"
                },
                "summary": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have\ndemonstrated remarkable capabilities in complex reasoning tasks through the\nutilization of long Chain-of-thought (CoT). However, these models often suffer\nfrom hallucinations and inefficiencies due to their reliance solely on internal\nreasoning processes. In this paper, we introduce START (Self-Taught Reasoner\nwith Tools), a novel tool-integrated long CoT reasoning LLM that significantly\nenhances reasoning capabilities by leveraging external tools. Through code\nexecution, START is capable of performing complex computations, self-checking,\nexploring diverse methods, and self-debugging, thereby addressing the\nlimitations of LRMs. The core innovation of START lies in its self-learning\nframework, which comprises two key techniques: 1) Hint-infer: We demonstrate\nthat inserting artificially designed hints (e.g., ``Wait, maybe using Python\nhere is a good idea.'') during the inference process of a LRM effectively\nstimulates its ability to utilize external tools without the need for any\ndemonstration data. Hint-infer can also serve as a simple and effective\nsequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning\n(Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and\nmodifying the reasoning trajectories with tool invocation generated by a LRM\nvia Hint-infer, followed by fine-tuning the LRM. Through this framework, we\nhave fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA\n(GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the\ncompetition-level code benchmark (LiveCodeBench), START achieves accuracy rates\nof 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly\noutperforms the base QwQ-32B and achieves performance comparable to the\nstate-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary\nmodel o1-Preview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have\ndemonstrated remarkable capabilities in complex reasoning tasks through the\nutilization of long Chain-of-thought (CoT). However, these models often suffer\nfrom hallucinations and inefficiencies due to their reliance solely on internal\nreasoning processes. In this paper, we introduce START (Self-Taught Reasoner\nwith Tools), a novel tool-integrated long CoT reasoning LLM that significantly\nenhances reasoning capabilities by leveraging external tools. Through code\nexecution, START is capable of performing complex computations, self-checking,\nexploring diverse methods, and self-debugging, thereby addressing the\nlimitations of LRMs. The core innovation of START lies in its self-learning\nframework, which comprises two key techniques: 1) Hint-infer: We demonstrate\nthat inserting artificially designed hints (e.g., ``Wait, maybe using Python\nhere is a good idea.'') during the inference process of a LRM effectively\nstimulates its ability to utilize external tools without the need for any\ndemonstration data. Hint-infer can also serve as a simple and effective\nsequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning\n(Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and\nmodifying the reasoning trajectories with tool invocation generated by a LRM\nvia Hint-infer, followed by fine-tuning the LRM. Through this framework, we\nhave fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA\n(GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the\ncompetition-level code benchmark (LiveCodeBench), START achieves accuracy rates\nof 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly\noutperforms the base QwQ-32B and achieves performance comparable to the\nstate-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary\nmodel o1-Preview."
                },
                "authors": [
                    {
                        "name": "Chengpeng Li"
                    },
                    {
                        "name": "Mingfeng Xue"
                    },
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Dayiheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dayiheng Liu"
                },
                "author": "Dayiheng Liu",
                "arxiv_comment": "38 pages, 5 figures and 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04619v1",
                "updated": "2025-03-06T17:05:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    5,
                    33,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:05:33Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    5,
                    33,
                    3,
                    65,
                    0
                ],
                "title": "SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming\n  User Sentiment Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming\n  User Sentiment Modeling"
                },
                "summary": "User reviews on e-commerce platforms exhibit dynamic sentiment patterns\ndriven by temporal and contextual factors. Traditional sentiment analysis\nmethods focus on static reviews, failing to capture the evolving temporal\nrelationship between user sentiment rating and textual content. Sentiment\nanalysis on streaming reviews addresses this limitation by modeling and\npredicting the temporal evolution of user sentiments. However, it suffers from\ndata sparsity, manifesting in temporal, spatial, and combined forms. In this\npaper, we introduce SynGraph, a novel framework designed to address data\nsparsity in sentiment analysis on streaming reviews. SynGraph alleviates data\nsparsity by categorizing users into mid-tail, long-tail, and extreme scenarios\nand incorporating LLM-augmented enhancements within a dynamic graph-based\nstructure. Experiments on real-world datasets demonstrate its effectiveness in\naddressing sparsity and improving sentiment modeling in streaming reviews.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User reviews on e-commerce platforms exhibit dynamic sentiment patterns\ndriven by temporal and contextual factors. Traditional sentiment analysis\nmethods focus on static reviews, failing to capture the evolving temporal\nrelationship between user sentiment rating and textual content. Sentiment\nanalysis on streaming reviews addresses this limitation by modeling and\npredicting the temporal evolution of user sentiments. However, it suffers from\ndata sparsity, manifesting in temporal, spatial, and combined forms. In this\npaper, we introduce SynGraph, a novel framework designed to address data\nsparsity in sentiment analysis on streaming reviews. SynGraph alleviates data\nsparsity by categorizing users into mid-tail, long-tail, and extreme scenarios\nand incorporating LLM-augmented enhancements within a dynamic graph-based\nstructure. Experiments on real-world datasets demonstrate its effectiveness in\naddressing sparsity and improving sentiment modeling in streaming reviews."
                },
                "authors": [
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Qiyu Wei"
                    },
                    {
                        "name": "Yingjie Zhu"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Deyu Zhou"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou",
                "arxiv_comment": "18 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04618v1",
                "updated": "2025-03-06T17:03:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    3,
                    17,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:03:17Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    3,
                    17,
                    3,
                    65,
                    0
                ],
                "title": "Better Process Supervision with Bi-directional Rewarding Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Process Supervision with Bi-directional Rewarding Signals"
                },
                "summary": "Process supervision, i.e., evaluating each step, is critical for complex\nlarge language model (LLM) reasoning and test-time searching with increased\ninference compute. Existing approaches, represented by process reward models\n(PRMs), primarily focus on rewarding signals up to the current step, exhibiting\na one-directional nature and lacking a mechanism to model the distance to the\nfinal target. To address this problem, we draw inspiration from the A*\nalgorithm, which states that an effective supervisory signal should\nsimultaneously consider the incurred cost and the estimated cost for reaching\nthe target. Building on this key insight, we introduce BiRM, a novel process\nsupervision model that not only evaluates the correctness of previous steps but\nalso models the probability of future success. We conduct extensive experiments\non mathematical reasoning tasks and demonstrate that BiRM provides more precise\nevaluations of LLM reasoning steps, achieving an improvement of 3.1% on\nGaokao2023 over PRM under the Best-of-N sampling method. Besides, in\nsearch-based strategies, BiRM provides more comprehensive guidance and\noutperforms ORM by 5.0% and PRM by 3.8% respectively on MATH-500.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process supervision, i.e., evaluating each step, is critical for complex\nlarge language model (LLM) reasoning and test-time searching with increased\ninference compute. Existing approaches, represented by process reward models\n(PRMs), primarily focus on rewarding signals up to the current step, exhibiting\na one-directional nature and lacking a mechanism to model the distance to the\nfinal target. To address this problem, we draw inspiration from the A*\nalgorithm, which states that an effective supervisory signal should\nsimultaneously consider the incurred cost and the estimated cost for reaching\nthe target. Building on this key insight, we introduce BiRM, a novel process\nsupervision model that not only evaluates the correctness of previous steps but\nalso models the probability of future success. We conduct extensive experiments\non mathematical reasoning tasks and demonstrate that BiRM provides more precise\nevaluations of LLM reasoning steps, achieving an improvement of 3.1% on\nGaokao2023 over PRM under the Best-of-N sampling method. Besides, in\nsearch-based strategies, BiRM provides more comprehensive guidance and\noutperforms ORM by 5.0% and PRM by 3.8% respectively on MATH-500."
                },
                "authors": [
                    {
                        "name": "Wenxiang Chen"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Honglin Guo"
                    },
                    {
                        "name": "Boyang Hong"
                    },
                    {
                        "name": "Jiazheng Zhang"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Nijun Li"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04615v1",
                "updated": "2025-03-06T16:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    59,
                    18,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T16:59:18Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    59,
                    18,
                    3,
                    65,
                    0
                ],
                "title": "HalluCounter: Reference-free LLM Hallucination Detection in the Wild!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HalluCounter: Reference-free LLM Hallucination Detection in the Wild!"
                },
                "summary": "Response consistency-based, reference-free hallucination detection (RFHD)\nmethods do not depend on internal model states, such as generation\nprobabilities or gradients, which Grey-box models typically rely on but are\ninaccessible in closed-source LLMs. However, their inability to capture\nquery-response alignment patterns often results in lower detection accuracy.\nAdditionally, the lack of large-scale benchmark datasets spanning diverse\ndomains remains a challenge, as most existing datasets are limited in size and\nscope. To this end, we propose HalluCounter, a novel reference-free\nhallucination detection method that utilizes both response-response and\nquery-response consistency and alignment patterns. This enables the training of\na classifier that detects hallucinations and provides a confidence score and an\noptimal response for user queries. Furthermore, we introduce HalluCounterEval,\na benchmark dataset comprising both synthetically generated and human-curated\nsamples across multiple domains. Our method outperforms state-of-the-art\napproaches by a significant margin, achieving over 90\\% average confidence in\nhallucination detection across datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Response consistency-based, reference-free hallucination detection (RFHD)\nmethods do not depend on internal model states, such as generation\nprobabilities or gradients, which Grey-box models typically rely on but are\ninaccessible in closed-source LLMs. However, their inability to capture\nquery-response alignment patterns often results in lower detection accuracy.\nAdditionally, the lack of large-scale benchmark datasets spanning diverse\ndomains remains a challenge, as most existing datasets are limited in size and\nscope. To this end, we propose HalluCounter, a novel reference-free\nhallucination detection method that utilizes both response-response and\nquery-response consistency and alignment patterns. This enables the training of\na classifier that detects hallucinations and provides a confidence score and an\noptimal response for user queries. Furthermore, we introduce HalluCounterEval,\na benchmark dataset comprising both synthetically generated and human-curated\nsamples across multiple domains. Our method outperforms state-of-the-art\napproaches by a significant margin, achieving over 90\\% average confidence in\nhallucination detection across datasets."
                },
                "authors": [
                    {
                        "name": "Ashok Urlana"
                    },
                    {
                        "name": "Gopichand Kanumolu"
                    },
                    {
                        "name": "Charaka Vinayak Kumar"
                    },
                    {
                        "name": "Bala Mallikarjunarao Garlapati"
                    },
                    {
                        "name": "Rahul Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Mishra"
                },
                "author": "Rahul Mishra",
                "arxiv_comment": "30 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04614v1",
                "updated": "2025-03-06T16:59:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    59,
                    10,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T16:59:10Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    59,
                    10,
                    3,
                    65,
                    0
                ],
                "title": "Currernt Flow Mapping in Conducting Ferroelectric Domain Walls using\n  Scanning NV-Magnetometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currernt Flow Mapping in Conducting Ferroelectric Domain Walls using\n  Scanning NV-Magnetometry"
                },
                "summary": "The electrical conductivity of parallel plate capacitors, with ferroelectric\nlithium niobate as the dielectric layer, can be extensively and progressively\nmodified by the controlled injection of conducting domain walls. Domain\nwall-based memristor devices hence result. Microstructures, developed as a\nresult of partial switching, are complex and so simple models of equivalent\ncircuits, based on the collective action of all conducting domain wall channels\nacting identically and in parallel, may not be appropriate. Here, we directly\nmap the current density in ferroelectric domain wall memristors in-situ, by\nmapping Oersted fields, using nitrogen vacancy centre microscopy. Current\ndensity maps were found to directly correlate with the domain microstructure,\nrevealing that a strikingly small fraction of the total domain wall network is\nresponsible for the majority of the current flow. This insight forces a two\norder of magnitude correction to the carrier densities, previously inferred\nfrom standard scanning probe or macroscopic electrical characterisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electrical conductivity of parallel plate capacitors, with ferroelectric\nlithium niobate as the dielectric layer, can be extensively and progressively\nmodified by the controlled injection of conducting domain walls. Domain\nwall-based memristor devices hence result. Microstructures, developed as a\nresult of partial switching, are complex and so simple models of equivalent\ncircuits, based on the collective action of all conducting domain wall channels\nacting identically and in parallel, may not be appropriate. Here, we directly\nmap the current density in ferroelectric domain wall memristors in-situ, by\nmapping Oersted fields, using nitrogen vacancy centre microscopy. Current\ndensity maps were found to directly correlate with the domain microstructure,\nrevealing that a strikingly small fraction of the total domain wall network is\nresponsible for the majority of the current flow. This insight forces a two\norder of magnitude correction to the carrier densities, previously inferred\nfrom standard scanning probe or macroscopic electrical characterisation."
                },
                "authors": [
                    {
                        "name": "Conor J. McCluskey"
                    },
                    {
                        "name": "James Dalzell"
                    },
                    {
                        "name": "Amit Kumar"
                    },
                    {
                        "name": "J. Marty Gregg"
                    }
                ],
                "author_detail": {
                    "name": "J. Marty Gregg"
                },
                "author": "J. Marty Gregg",
                "arxiv_comment": "Main Text: 19 Pages, 4 Figures. Supplementary Information: 7 Pages, 3\n  Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04611v1",
                "updated": "2025-03-06T16:57:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    57,
                    26,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T16:57:26Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    57,
                    26,
                    3,
                    65,
                    0
                ],
                "title": "Towards Data-Efficient Language Models: A Child-Inspired Approach to\n  Language Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Data-Efficient Language Models: A Child-Inspired Approach to\n  Language Learning"
                },
                "summary": "In this work, we explain our approach employed in the BabyLM Challenge, which\nuses various methods of training language models (LMs) with significantly less\ndata compared to traditional large language models (LLMs) and are inspired by\nhow human children learn. While a human child is exposed to far less linguistic\ninput than an LLM, they still achieve remarkable language understanding and\ngeneration abilities. To this end, we develop a model trained on a curated\ndataset consisting of 10 million words, primarily sourced from child-directed\ntranscripts. The 2024 BabyLM Challenge initial dataset of 10M words is filtered\nto 8.5M. Next, it is supplemented with a randomly selected subset of TVR\ndataset consisting of 1.5M words of television dialogues. The latter dataset\nensures that similar to children, the model is also exposed to language through\nmedia. Furthermore, we reduce the vocabulary size to 32,000 tokens, aligning it\nwith the limited vocabulary of children in the early stages of language\nacquisition. We use curriculum learning and is able to match the baseline on\ncertain benchmarks while surpassing the baseline on others. Additionally,\nincorporating common LLM training datasets, such as MADLAD-400, degrades\nperformance. These findings underscore the importance of dataset selection,\nvocabulary scaling, and curriculum learning in creating more data-efficient\nlanguage models that better mimic human learning processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explain our approach employed in the BabyLM Challenge, which\nuses various methods of training language models (LMs) with significantly less\ndata compared to traditional large language models (LLMs) and are inspired by\nhow human children learn. While a human child is exposed to far less linguistic\ninput than an LLM, they still achieve remarkable language understanding and\ngeneration abilities. To this end, we develop a model trained on a curated\ndataset consisting of 10 million words, primarily sourced from child-directed\ntranscripts. The 2024 BabyLM Challenge initial dataset of 10M words is filtered\nto 8.5M. Next, it is supplemented with a randomly selected subset of TVR\ndataset consisting of 1.5M words of television dialogues. The latter dataset\nensures that similar to children, the model is also exposed to language through\nmedia. Furthermore, we reduce the vocabulary size to 32,000 tokens, aligning it\nwith the limited vocabulary of children in the early stages of language\nacquisition. We use curriculum learning and is able to match the baseline on\ncertain benchmarks while surpassing the baseline on others. Additionally,\nincorporating common LLM training datasets, such as MADLAD-400, degrades\nperformance. These findings underscore the importance of dataset selection,\nvocabulary scaling, and curriculum learning in creating more data-efficient\nlanguage models that better mimic human learning processes."
                },
                "authors": [
                    {
                        "name": "Mohammad Amin Ghanizadeh"
                    },
                    {
                        "name": "Mohammad Javad Dousti"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Javad Dousti"
                },
                "author": "Mohammad Javad Dousti",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04599v1",
                "updated": "2025-03-06T16:42:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    42,
                    10,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T16:42:10Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    42,
                    10,
                    3,
                    65,
                    0
                ],
                "title": "Power-Efficient Deceptive Wireless Beamforming Against Eavesdroppers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Power-Efficient Deceptive Wireless Beamforming Against Eavesdroppers"
                },
                "summary": "Eavesdroppers of wireless signals want to infer as much as possible regarding\nthe transmitter (Tx). Popular methods to minimize information leakage to the\neavesdropper include covert communication, directional modulation, and\nbeamforming with nulling. In this paper we do not attempt to prevent\ninformation leakage to the eavesdropper like the previous methods. Instead we\npropose to beamform the wireless signal at the Tx in such a way that it\nincorporates deceptive information. The beamformed orthogonal frequency\ndivision multiplexing (OFDM) signal includes a deceptive value for the Doppler\n(velocity) and range of the Tx. To design the optimal baseband waveform with\nthese characteristics, we define and solve an optimization problem for\npower-efficient deceptive wireless beamforming (DWB). The relaxed convex\nQuadratic Program (QP) is solved using a heuristic algorithm. Our simulation\nresults indicate that our DWB scheme can successfully inject deceptive\ninformation with low power consumption, while preserving the shape of the\ncreated beam.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eavesdroppers of wireless signals want to infer as much as possible regarding\nthe transmitter (Tx). Popular methods to minimize information leakage to the\neavesdropper include covert communication, directional modulation, and\nbeamforming with nulling. In this paper we do not attempt to prevent\ninformation leakage to the eavesdropper like the previous methods. Instead we\npropose to beamform the wireless signal at the Tx in such a way that it\nincorporates deceptive information. The beamformed orthogonal frequency\ndivision multiplexing (OFDM) signal includes a deceptive value for the Doppler\n(velocity) and range of the Tx. To design the optimal baseband waveform with\nthese characteristics, we define and solve an optimization problem for\npower-efficient deceptive wireless beamforming (DWB). The relaxed convex\nQuadratic Program (QP) is solved using a heuristic algorithm. Our simulation\nresults indicate that our DWB scheme can successfully inject deceptive\ninformation with low power consumption, while preserving the shape of the\ncreated beam."
                },
                "authors": [
                    {
                        "name": "Georgios Chrysanidis"
                    },
                    {
                        "name": "Antonios Argyriou"
                    },
                    {
                        "name": "Le-Nam Tran"
                    },
                    {
                        "name": "Yanming Zhang"
                    },
                    {
                        "name": "Yanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Liu"
                },
                "author": "Yanwei Liu",
                "arxiv_comment": "IEEE Wireless Communications and Networking Conference (WCNC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04598v1",
                "updated": "2025-03-06T16:40:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    40,
                    48,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T16:40:48Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    40,
                    48,
                    3,
                    65,
                    0
                ],
                "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization"
                },
                "summary": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the location of layer normalization. While\nPre-Norm structures facilitate easier training due to their more prominent\nidentity path, they often yield suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a straightforward yet\neffective hybrid normalization strategy that integrates the advantages of both\nPre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV\nnormalization within the attention mechanism and Post-Norm in the feed-forward\nnetwork (FFN) of each transformer block. This design not only stabilizes\ntraining but also enhances performance, particularly in the context of LLMs.\nComprehensive experiments in both dense and sparse architectures show that\nHybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches,\nachieving state-of-the-art results across various benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. %Code\nwill be made publicly available. Code is available at\nhttps://github.com/BryceZhuo/HybridNorm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the location of layer normalization. While\nPre-Norm structures facilitate easier training due to their more prominent\nidentity path, they often yield suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a straightforward yet\neffective hybrid normalization strategy that integrates the advantages of both\nPre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV\nnormalization within the attention mechanism and Post-Norm in the feed-forward\nnetwork (FFN) of each transformer block. This design not only stabilizes\ntraining but also enhances performance, particularly in the context of LLMs.\nComprehensive experiments in both dense and sparse architectures show that\nHybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches,\nachieving state-of-the-art results across various benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. %Code\nwill be made publicly available. Code is available at\nhttps://github.com/BryceZhuo/HybridNorm."
                },
                "authors": [
                    {
                        "name": "Zhijian Zhuo"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Xiaoqing Li"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Jinwen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jinwen Ma"
                },
                "author": "Jinwen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04596v1",
                "updated": "2025-03-06T16:38:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    38,
                    23,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T16:38:23Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    38,
                    23,
                    3,
                    65,
                    0
                ],
                "title": "The Next Frontier of LLM Applications: Open Ecosystems and Hardware\n  Synergy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Next Frontier of LLM Applications: Open Ecosystems and Hardware\n  Synergy"
                },
                "summary": "Large Language Model (LLM) applications, including LLM app stores and\nautonomous agents, are shaping the future of AI ecosystems. However, platform\nsilos, fragmented hardware integration, and the absence of standardized\ninterfaces limit scalability, interoperability, and resource efficiency. While\nLLM app stores democratize AI, their closed ecosystems restrict modular AI\nreuse and cross-platform portability. Meanwhile, agent-based frameworks offer\nflexibility but often lack seamless integration across diverse environments.\nThis paper envisions the future of LLM applications and proposes a three-layer\ndecoupled architecture grounded in software engineering principles such as\nlayered system design, service-oriented architectures, and hardware-software\nco-design. This architecture separates application logic, communication\nprotocols, and hardware execution, enhancing modularity, efficiency, and\ncross-platform compatibility. Beyond architecture, we highlight key security\nand privacy challenges for safe, scalable AI deployment and outline research\ndirections in software and security engineering. This vision aims to foster\nopen, secure, and interoperable LLM ecosystems, guiding future advancements in\nAI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) applications, including LLM app stores and\nautonomous agents, are shaping the future of AI ecosystems. However, platform\nsilos, fragmented hardware integration, and the absence of standardized\ninterfaces limit scalability, interoperability, and resource efficiency. While\nLLM app stores democratize AI, their closed ecosystems restrict modular AI\nreuse and cross-platform portability. Meanwhile, agent-based frameworks offer\nflexibility but often lack seamless integration across diverse environments.\nThis paper envisions the future of LLM applications and proposes a three-layer\ndecoupled architecture grounded in software engineering principles such as\nlayered system design, service-oriented architectures, and hardware-software\nco-design. This architecture separates application logic, communication\nprotocols, and hardware execution, enhancing modularity, efficiency, and\ncross-platform compatibility. Beyond architecture, we highlight key security\nand privacy challenges for safe, scalable AI deployment and outline research\ndirections in software and security engineering. This vision aims to foster\nopen, secure, and interoperable LLM ecosystems, guiding future advancements in\nAI applications."
                },
                "authors": [
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04593v1",
                "updated": "2025-03-06T16:33:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    33,
                    9,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T16:33:09Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    33,
                    9,
                    3,
                    65,
                    0
                ],
                "title": "Bayesian estimation of a multivariate TAR model when the noise process\n  distribution belongs to the class of Gaussian variance mixtures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian estimation of a multivariate TAR model when the noise process\n  distribution belongs to the class of Gaussian variance mixtures"
                },
                "summary": "A threshold autoregressive (TAR) model is a powerful tool for analyzing\nnonlinear multivariate time series, which includes special cases like\nself-exciting threshold autoregressive (SETAR) models and vector autoregressive\n(VAR) models. In this paper, estimation, inference, and forecasting using the\nBayesian approach are developed for multivariate TAR (MTAR) models considering\na flexible setup, under which the noise process behavior can be described using\nnot only the Gaussian distribution but also other distributions that belong to\nthe class of Gaussian variance mixtures, which includes Student-t, Slash,\nsymmetric hyperbolic, and contaminated normal distributions, which are also\nsymmetric but are more flexible and with heavier tails than the Gaussian one.\nInferences from MTAR models based on that kind of distribution may be less\naffected by extreme or outlying observations than those based on the Gaussian\none. All parameters in the MTAR model are included in the proposed MCMC-type\nalgorithm, except the number of regimes and the autoregressive orders, which\ncan be chosen using the Deviance Information Criterion (DIC) and/or the\nWatanabe-Akaike Information Criterion (WAIC). A library for the language and\nenvironment for statistical computing R was also developed to assess the\neffectiveness of the proposed methodology using simulation studies and analysis\nof two real multivariate time series.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A threshold autoregressive (TAR) model is a powerful tool for analyzing\nnonlinear multivariate time series, which includes special cases like\nself-exciting threshold autoregressive (SETAR) models and vector autoregressive\n(VAR) models. In this paper, estimation, inference, and forecasting using the\nBayesian approach are developed for multivariate TAR (MTAR) models considering\na flexible setup, under which the noise process behavior can be described using\nnot only the Gaussian distribution but also other distributions that belong to\nthe class of Gaussian variance mixtures, which includes Student-t, Slash,\nsymmetric hyperbolic, and contaminated normal distributions, which are also\nsymmetric but are more flexible and with heavier tails than the Gaussian one.\nInferences from MTAR models based on that kind of distribution may be less\naffected by extreme or outlying observations than those based on the Gaussian\none. All parameters in the MTAR model are included in the proposed MCMC-type\nalgorithm, except the number of regimes and the autoregressive orders, which\ncan be chosen using the Deviance Information Criterion (DIC) and/or the\nWatanabe-Akaike Information Criterion (WAIC). A library for the language and\nenvironment for statistical computing R was also developed to assess the\neffectiveness of the proposed methodology using simulation studies and analysis\nof two real multivariate time series."
                },
                "authors": [
                    {
                        "name": "L. H. Vanegas"
                    },
                    {
                        "name": "S. A. Calderón"
                    },
                    {
                        "name": "L. M. Rondón"
                    }
                ],
                "author_detail": {
                    "name": "L. M. Rondón"
                },
                "author": "L. M. Rondón",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04592v1",
                "updated": "2025-03-06T16:31:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    31,
                    34,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T16:31:34Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    31,
                    34,
                    3,
                    65,
                    0
                ],
                "title": "A Benchmark for Multi-Lingual Vision-Language Learning in Remote Sensing\n  Image Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Benchmark for Multi-Lingual Vision-Language Learning in Remote Sensing\n  Image Captioning"
                },
                "summary": "Remote Sensing Image Captioning (RSIC) is a cross-modal field bridging vision\nand language, aimed at automatically generating natural language descriptions\nof features and scenes in remote sensing imagery. Despite significant advances\nin developing sophisticated methods and large-scale datasets for training\nvision-language models (VLMs), two critical challenges persist: the scarcity of\nnon-English descriptive datasets and the lack of multilingual capability\nevaluation for models. These limitations fundamentally impede the progress and\npractical deployment of RSIC, particularly in the era of large VLMs. To address\nthese challenges, this paper presents several significant contributions to the\nfield. First, we introduce and analyze BRSIC (Bilingual Remote Sensing Image\nCaptioning), a comprehensive bilingual dataset that enriches three established\nEnglish RSIC datasets with Chinese descriptions, encompassing 13,634 images\npaired with 68,170 bilingual captions. Building upon this foundation, we\ndevelop a systematic evaluation framework that addresses the prevalent\ninconsistency in evaluation protocols, enabling rigorous assessment of model\nperformance through standardized retraining procedures on BRSIC. Furthermore,\nwe present an extensive empirical study of eight state-of-the-art large\nvision-language models (LVLMs), examining their capabilities across multiple\nparadigms including zero-shot inference, supervised fine-tuning, and\nmulti-lingual training. This comprehensive evaluation provides crucial insights\ninto the strengths and limitations of current LVLMs in handling multilingual\nremote sensing tasks. Additionally, our cross-dataset transfer experiments\nreveal interesting findings. The code and data will be available at\nhttps://github.com/mrazhou/BRSIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote Sensing Image Captioning (RSIC) is a cross-modal field bridging vision\nand language, aimed at automatically generating natural language descriptions\nof features and scenes in remote sensing imagery. Despite significant advances\nin developing sophisticated methods and large-scale datasets for training\nvision-language models (VLMs), two critical challenges persist: the scarcity of\nnon-English descriptive datasets and the lack of multilingual capability\nevaluation for models. These limitations fundamentally impede the progress and\npractical deployment of RSIC, particularly in the era of large VLMs. To address\nthese challenges, this paper presents several significant contributions to the\nfield. First, we introduce and analyze BRSIC (Bilingual Remote Sensing Image\nCaptioning), a comprehensive bilingual dataset that enriches three established\nEnglish RSIC datasets with Chinese descriptions, encompassing 13,634 images\npaired with 68,170 bilingual captions. Building upon this foundation, we\ndevelop a systematic evaluation framework that addresses the prevalent\ninconsistency in evaluation protocols, enabling rigorous assessment of model\nperformance through standardized retraining procedures on BRSIC. Furthermore,\nwe present an extensive empirical study of eight state-of-the-art large\nvision-language models (LVLMs), examining their capabilities across multiple\nparadigms including zero-shot inference, supervised fine-tuning, and\nmulti-lingual training. This comprehensive evaluation provides crucial insights\ninto the strengths and limitations of current LVLMs in handling multilingual\nremote sensing tasks. Additionally, our cross-dataset transfer experiments\nreveal interesting findings. The code and data will be available at\nhttps://github.com/mrazhou/BRSIC."
                },
                "authors": [
                    {
                        "name": "Qing Zhou"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Junyu Gao"
                    },
                    {
                        "name": "Weiping Ni"
                    },
                    {
                        "name": "Junzheng Wu"
                    },
                    {
                        "name": "Qi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Wang"
                },
                "author": "Qi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00053v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00053v3",
                "updated": "2025-03-06T16:28:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    28,
                    55,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-30T19:09:02Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    19,
                    9,
                    2,
                    2,
                    304,
                    0
                ],
                "title": "ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration"
                },
                "summary": "Large language models (LLMs) have demonstrated a remarkable ability to serve\nas general-purpose tools for various language-based tasks. Recent works have\ndemonstrated that the efficacy of such models can be improved through iterative\ndialog between multiple models. While these paradigms show promise in improving\nmodel efficacy, most works in this area treat collaboration as an emergent\nbehavior, rather than a learned behavior. In doing so, current multi-agent\nframeworks rely on collaborative behaviors to have been sufficiently trained\ninto off-the-shelf models. To address this limitation, we propose ACC-Collab,\nan Actor-Critic based learning framework to produce a two-agent team (an\nactor-agent and a critic-agent) specialized in collaboration. We demonstrate\nthat ACC-Collab outperforms SotA multi-agent techniques on a wide array of\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated a remarkable ability to serve\nas general-purpose tools for various language-based tasks. Recent works have\ndemonstrated that the efficacy of such models can be improved through iterative\ndialog between multiple models. While these paradigms show promise in improving\nmodel efficacy, most works in this area treat collaboration as an emergent\nbehavior, rather than a learned behavior. In doing so, current multi-agent\nframeworks rely on collaborative behaviors to have been sufficiently trained\ninto off-the-shelf models. To address this limitation, we propose ACC-Collab,\nan Actor-Critic based learning framework to produce a two-agent team (an\nactor-agent and a critic-agent) specialized in collaboration. We demonstrate\nthat ACC-Collab outperforms SotA multi-agent techniques on a wide array of\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Andrew Estornell"
                    },
                    {
                        "name": "Jean-Francois Ton"
                    },
                    {
                        "name": "Yuanshun Yao"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00053v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00053v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04588v1",
                "updated": "2025-03-06T16:26:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    26,
                    21,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T16:26:21Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    26,
                    21,
                    3,
                    65,
                    0
                ],
                "title": "Fiducial Inference for Random-Effects Calibration Models: Advancing\n  Reliable Quantification in Environmental Analytical Chemistry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fiducial Inference for Random-Effects Calibration Models: Advancing\n  Reliable Quantification in Environmental Analytical Chemistry"
                },
                "summary": "This article addresses calibration challenges in analytical chemistry by\nemploying a random-effects calibration curve model and its generalizations to\ncapture variability in analyte concentrations. The model is motivated by\nspecific issues in analytical chemistry, where measurement errors remain\nconstant at low concentrations but increase proportionally as concentrations\nrise. To account for this, the model permits the parameters of the calibration\ncurve, which relate instrument responses to true concentrations, to vary across\ndifferent laboratories, thereby reflecting real-world variability in\nmeasurement processes. Traditional large-sample interval estimation methods are\ninadequate for small samples, leading to the use of an alternative approach,\nnamely the fiducial approach. The calibration curve that accurately captures\nthe heteroscedastic nature of the data, results in more reliable estimates\nacross diverse laboratory conditions. It turns out that the fiducial approach,\nwhen used to construct a confidence interval for an unknown concentration,\nproduces a slightly wider width while achieving the desired coverage\nprobability. Applications considered include the determination of the presence\nof an analyte and the interval estimation of an unknown true analyte\nconcentration. The proposed method is demonstrated for both simulated and real\ninterlaboratory data, including examples involving copper and cadmium in\ndistilled water.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article addresses calibration challenges in analytical chemistry by\nemploying a random-effects calibration curve model and its generalizations to\ncapture variability in analyte concentrations. The model is motivated by\nspecific issues in analytical chemistry, where measurement errors remain\nconstant at low concentrations but increase proportionally as concentrations\nrise. To account for this, the model permits the parameters of the calibration\ncurve, which relate instrument responses to true concentrations, to vary across\ndifferent laboratories, thereby reflecting real-world variability in\nmeasurement processes. Traditional large-sample interval estimation methods are\ninadequate for small samples, leading to the use of an alternative approach,\nnamely the fiducial approach. The calibration curve that accurately captures\nthe heteroscedastic nature of the data, results in more reliable estimates\nacross diverse laboratory conditions. It turns out that the fiducial approach,\nwhen used to construct a confidence interval for an unknown concentration,\nproduces a slightly wider width while achieving the desired coverage\nprobability. Applications considered include the determination of the presence\nof an analyte and the interval estimation of an unknown true analyte\nconcentration. The proposed method is demonstrated for both simulated and real\ninterlaboratory data, including examples involving copper and cadmium in\ndistilled water."
                },
                "authors": [
                    {
                        "name": "Soumya Sahu"
                    },
                    {
                        "name": "Thomas Mathew"
                    },
                    {
                        "name": "Robert Gibbons"
                    },
                    {
                        "name": "Dulal K. Bhaumik"
                    }
                ],
                "author_detail": {
                    "name": "Dulal K. Bhaumik"
                },
                "author": "Dulal K. Bhaumik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17412v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17412v3",
                "updated": "2025-03-06T16:22:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    22,
                    22,
                    3,
                    65,
                    0
                ],
                "published": "2024-05-27T17:57:12Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    17,
                    57,
                    12,
                    0,
                    148,
                    0
                ],
                "title": "Towards One Model for Classical Dimensionality Reduction: A\n  Probabilistic Perspective on UMAP and t-SNE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards One Model for Classical Dimensionality Reduction: A\n  Probabilistic Perspective on UMAP and t-SNE"
                },
                "summary": "This paper shows that dimensionality reduction methods such as UMAP and\nt-SNE, can be approximately recast as MAP inference methods corresponding to a\nmodel introduced in ProbDR, that describes the graph Laplacian (an estimate of\nthe data precision matrix) using a Wishart distribution, with a mean given by a\nnon-linear covariance function evaluated on the latents. This interpretation\noffers deeper theoretical and semantic insights into such algorithms, by\nshowing that variances corresponding to these covariances are low (potentially\nmisspecified), and forging a connection to Gaussian process latent variable\nmodels by showing that well-known kernels can be used to describe covariances\nimplied by graph Laplacians. We also introduce tools with which similar\ndimensionality reduction methods can be studied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper shows that dimensionality reduction methods such as UMAP and\nt-SNE, can be approximately recast as MAP inference methods corresponding to a\nmodel introduced in ProbDR, that describes the graph Laplacian (an estimate of\nthe data precision matrix) using a Wishart distribution, with a mean given by a\nnon-linear covariance function evaluated on the latents. This interpretation\noffers deeper theoretical and semantic insights into such algorithms, by\nshowing that variances corresponding to these covariances are low (potentially\nmisspecified), and forging a connection to Gaussian process latent variable\nmodels by showing that well-known kernels can be used to describe covariances\nimplied by graph Laplacians. We also introduce tools with which similar\ndimensionality reduction methods can be studied."
                },
                "authors": [
                    {
                        "name": "Aditya Ravuri"
                    },
                    {
                        "name": "Neil D. Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Neil D. Lawrence"
                },
                "author": "Neil D. Lawrence",
                "arxiv_comment": "Updated preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17412v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17412v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02972v2",
                "updated": "2025-03-06T16:16:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    16,
                    7,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-04T19:57:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    57,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic\n  Templatisation and Orthographic Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic\n  Templatisation and Orthographic Obfuscation"
                },
                "summary": "Assessing the reasoning capabilities of large language models (LLMs) is\nsusceptible to overestimation due to data exposure of evaluation benchmarks. We\nintroduce a framework for producing linguistic reasoning problems that reduces\nthe effect of memorisation in model performance estimates and apply this\nframework to develop LINGOLY-TOO, a challenging benchmark for linguistic\nreasoning. By developing orthographic templates, we dynamically obfuscate the\nwriting systems of real languages to generate numerousquestion variations.\nThese variations preserve the reasoning steps required for each solution while\nreducing the likelihood of specific problem instances appearing in model\ntraining data. Our experiments demonstrate that frontier models, including\nClaud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning.\nOur analysis also shows that LLMs exhibit noticeable variance in accuracy\nacross permutations of the same problem, and on average perform better on\nquestions appearing in their original orthography. Our findings highlight the\nopaque nature of response generation in LLMs and provide evidence that prior\ndata exposure contributes to over estimating the reasoning capabilities of\nfrontier models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the reasoning capabilities of large language models (LLMs) is\nsusceptible to overestimation due to data exposure of evaluation benchmarks. We\nintroduce a framework for producing linguistic reasoning problems that reduces\nthe effect of memorisation in model performance estimates and apply this\nframework to develop LINGOLY-TOO, a challenging benchmark for linguistic\nreasoning. By developing orthographic templates, we dynamically obfuscate the\nwriting systems of real languages to generate numerousquestion variations.\nThese variations preserve the reasoning steps required for each solution while\nreducing the likelihood of specific problem instances appearing in model\ntraining data. Our experiments demonstrate that frontier models, including\nClaud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning.\nOur analysis also shows that LLMs exhibit noticeable variance in accuracy\nacross permutations of the same problem, and on average perform better on\nquestions appearing in their original orthography. Our findings highlight the\nopaque nature of response generation in LLMs and provide evidence that prior\ndata exposure contributes to over estimating the reasoning capabilities of\nfrontier models."
                },
                "authors": [
                    {
                        "name": "Jude Khouja"
                    },
                    {
                        "name": "Karolina Korgul"
                    },
                    {
                        "name": "Simi Hellsten"
                    },
                    {
                        "name": "Lingyi Yang"
                    },
                    {
                        "name": "Vlad Neacs"
                    },
                    {
                        "name": "Harry Mayne"
                    },
                    {
                        "name": "Ryan Kearns"
                    },
                    {
                        "name": "Andrew Bean"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17504v2",
                "updated": "2025-03-06T16:14:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    14,
                    45,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-21T19:22:10Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    19,
                    22,
                    10,
                    4,
                    52,
                    0
                ],
                "title": "Protein Large Language Models: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein Large Language Models: A Comprehensive Survey"
                },
                "summary": "Protein-specific large language models (Protein LLMs) are revolutionizing\nprotein science by enabling more efficient protein structure prediction,\nfunction annotation, and design. While existing surveys focus on specific\naspects or applications, this work provides the first comprehensive overview of\nProtein LLMs, covering their architectures, training datasets, evaluation\nmetrics, and diverse applications. Through a systematic analysis of over 100\narticles, we propose a structured taxonomy of state-of-the-art Protein LLMs,\nanalyze how they leverage large-scale protein sequence data for improved\naccuracy, and explore their potential in advancing protein engineering and\nbiomedical research. Additionally, we discuss key challenges and future\ndirections, positioning Protein LLMs as essential tools for scientific\ndiscovery in protein science. Resources are maintained at\nhttps://github.com/Yijia-Xiao/Protein-LLM-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein-specific large language models (Protein LLMs) are revolutionizing\nprotein science by enabling more efficient protein structure prediction,\nfunction annotation, and design. While existing surveys focus on specific\naspects or applications, this work provides the first comprehensive overview of\nProtein LLMs, covering their architectures, training datasets, evaluation\nmetrics, and diverse applications. Through a systematic analysis of over 100\narticles, we propose a structured taxonomy of state-of-the-art Protein LLMs,\nanalyze how they leverage large-scale protein sequence data for improved\naccuracy, and explore their potential in advancing protein engineering and\nbiomedical research. Additionally, we discuss key challenges and future\ndirections, positioning Protein LLMs as essential tools for scientific\ndiscovery in protein science. Resources are maintained at\nhttps://github.com/Yijia-Xiao/Protein-LLM-Survey."
                },
                "authors": [
                    {
                        "name": "Yijia Xiao"
                    },
                    {
                        "name": "Wanjia Zhao"
                    },
                    {
                        "name": "Junkai Zhang"
                    },
                    {
                        "name": "Yiqiao Jin"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Zhicheng Ren"
                    },
                    {
                        "name": "Renliang Sun"
                    },
                    {
                        "name": "Haixin Wang"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Pan Lu"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Yizhou Sun"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "24 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12464v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12464v9",
                "updated": "2025-03-06T16:13:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    13,
                    4,
                    3,
                    65,
                    0
                ],
                "published": "2024-04-18T18:48:50Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    18,
                    48,
                    50,
                    3,
                    109,
                    0
                ],
                "title": "NormAd: A Framework for Measuring the Cultural Adaptability of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NormAd: A Framework for Measuring the Cultural Adaptability of Large\n  Language Models"
                },
                "summary": "To be effectively and safely deployed to global user populations, large\nlanguage models (LLMs) may need to adapt outputs to user values and cultures,\nnot just know about them. We introduce NormAd, an evaluation framework to\nassess LLMs' cultural adaptability, specifically measuring their ability to\njudge social acceptability across varying levels of cultural norm specificity,\nfrom abstract values to explicit social norms. As an instantiation of our\nframework, we create NormAd-Eti, a benchmark of 2.6k situational descriptions\nrepresenting social-etiquette related cultural norms from 75 countries. Through\ncomprehensive experiments on NormAd-Eti, we find that LLMs struggle to\naccurately judge social acceptability across these varying degrees of cultural\ncontexts and show stronger adaptability to English-centric cultures over those\nfrom the Global South. Even in the simplest setting where the relevant social\nnorms are provided, the best LLMs' performance (< 82\\%) lags behind humans (>\n95\\%). In settings with abstract values and country information, model\nperformance drops substantially (< 60\\%), while human accuracy remains high (>\n90\\%). Furthermore, we find that models are better at recognizing socially\nacceptable versus unacceptable situations. Our findings showcase the current\npitfalls in socio-cultural reasoning of LLMs which hinder their adaptability\nfor global audiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To be effectively and safely deployed to global user populations, large\nlanguage models (LLMs) may need to adapt outputs to user values and cultures,\nnot just know about them. We introduce NormAd, an evaluation framework to\nassess LLMs' cultural adaptability, specifically measuring their ability to\njudge social acceptability across varying levels of cultural norm specificity,\nfrom abstract values to explicit social norms. As an instantiation of our\nframework, we create NormAd-Eti, a benchmark of 2.6k situational descriptions\nrepresenting social-etiquette related cultural norms from 75 countries. Through\ncomprehensive experiments on NormAd-Eti, we find that LLMs struggle to\naccurately judge social acceptability across these varying degrees of cultural\ncontexts and show stronger adaptability to English-centric cultures over those\nfrom the Global South. Even in the simplest setting where the relevant social\nnorms are provided, the best LLMs' performance (< 82\\%) lags behind humans (>\n95\\%). In settings with abstract values and country information, model\nperformance drops substantially (< 60\\%), while human accuracy remains high (>\n90\\%). Furthermore, we find that models are better at recognizing socially\nacceptable versus unacceptable situations. Our findings showcase the current\npitfalls in socio-cultural reasoning of LLMs which hinder their adaptability\nfor global audiences."
                },
                "authors": [
                    {
                        "name": "Abhinav Rao"
                    },
                    {
                        "name": "Akhila Yerukola"
                    },
                    {
                        "name": "Vishwa Shah"
                    },
                    {
                        "name": "Katharina Reinecke"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "Accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12464v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12464v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01804v2",
                "updated": "2025-03-06T16:07:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    7,
                    43,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-03T18:33:46Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    33,
                    46,
                    0,
                    62,
                    0
                ],
                "title": "$\\texttt{SEM-CTRL}$: Semantically Controlled Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\texttt{SEM-CTRL}$: Semantically Controlled Decoding"
                },
                "summary": "Ensuring both syntactic and semantic correctness in Large Language Model\n(LLM) outputs remains a significant challenge, despite being critical for\nreal-world deployment. In this paper, we introduce $\\texttt{SEM-CTRL}$, a\nunified approach that enforces rich context-sensitive constraints and task- and\ninstance-specific semantics directly on an LLM decoder. Our approach integrates\ntoken-level MCTS, which is guided by specific syntactic and semantic\nconstraints. The constraints over the desired outputs are expressed using\nAnswer Set Grammars -- a logic-based formalism that generalizes\ncontext-sensitive grammars while incorporating background knowledge to\nrepresent task-specific semantics. We show that our approach guarantees correct\ncompletions for any off-the-shelf LLM without the need for fine-tuning. We\nevaluate $\\texttt{SEM-CTRL}$ on a range of tasks, including synthetic grammar\nsynthesis, combinatorial reasoning, and planning. Our results demonstrate that\n$\\texttt{SEM-CTRL}$ allows small pre-trained LLMs to efficiently outperform\nlarger variants and state-of-the-art reasoning models (e.g., o1-preview) while\nsimultaneously guaranteeing solution correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring both syntactic and semantic correctness in Large Language Model\n(LLM) outputs remains a significant challenge, despite being critical for\nreal-world deployment. In this paper, we introduce $\\texttt{SEM-CTRL}$, a\nunified approach that enforces rich context-sensitive constraints and task- and\ninstance-specific semantics directly on an LLM decoder. Our approach integrates\ntoken-level MCTS, which is guided by specific syntactic and semantic\nconstraints. The constraints over the desired outputs are expressed using\nAnswer Set Grammars -- a logic-based formalism that generalizes\ncontext-sensitive grammars while incorporating background knowledge to\nrepresent task-specific semantics. We show that our approach guarantees correct\ncompletions for any off-the-shelf LLM without the need for fine-tuning. We\nevaluate $\\texttt{SEM-CTRL}$ on a range of tasks, including synthetic grammar\nsynthesis, combinatorial reasoning, and planning. Our results demonstrate that\n$\\texttt{SEM-CTRL}$ allows small pre-trained LLMs to efficiently outperform\nlarger variants and state-of-the-art reasoning models (e.g., o1-preview) while\nsimultaneously guaranteeing solution correctness."
                },
                "authors": [
                    {
                        "name": "Mohammad Albinhassan"
                    },
                    {
                        "name": "Pranava Madhyastha"
                    },
                    {
                        "name": "Alessandra Russo"
                    }
                ],
                "author_detail": {
                    "name": "Alessandra Russo"
                },
                "author": "Alessandra Russo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00153v2",
                "updated": "2025-03-06T15:50:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    50,
                    28,
                    3,
                    65,
                    0
                ],
                "published": "2024-09-30T18:52:53Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    52,
                    53,
                    0,
                    274,
                    0
                ],
                "title": "Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with\n  Gaussian Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with\n  Gaussian Distribution"
                },
                "summary": "Probing learned concepts in large language models (LLMs) is crucial for\nunderstanding how semantic knowledge is encoded internally. Training linear\nclassifiers on probing tasks is a principle approach to denote the vector of a\ncertain concept in the representation space. However, the single vector\nidentified for a concept varies with both data and training, making it less\nrobust and weakening its effectiveness in real-world applications. To address\nthis challenge, we propose an approach to approximate the subspace representing\na specific concept. Built on linear probing classifiers, we extend the concept\nvectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's\neffectiveness through measuring its faithfulness and plausibility across\nmultiple LLMs with different sizes and architectures. Additionally, we use\nrepresentation intervention tasks to showcase its efficacy in real-world\napplications such as emotion steering. Experimental results indicate that GCS\nconcept vectors have the potential to balance steering performance and\nmaintaining the fluency in natural language generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing learned concepts in large language models (LLMs) is crucial for\nunderstanding how semantic knowledge is encoded internally. Training linear\nclassifiers on probing tasks is a principle approach to denote the vector of a\ncertain concept in the representation space. However, the single vector\nidentified for a concept varies with both data and training, making it less\nrobust and weakening its effectiveness in real-world applications. To address\nthis challenge, we propose an approach to approximate the subspace representing\na specific concept. Built on linear probing classifiers, we extend the concept\nvectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's\neffectiveness through measuring its faithfulness and plausibility across\nmultiple LLMs with different sizes and architectures. Additionally, we use\nrepresentation intervention tasks to showcase its efficacy in real-world\napplications such as emotion steering. Experimental results indicate that GCS\nconcept vectors have the potential to balance steering performance and\nmaintaining the fluency in natural language generation tasks."
                },
                "authors": [
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Heng Zhao"
                    },
                    {
                        "name": "Bo Shen"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04557v1",
                "updated": "2025-03-06T15:49:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    49,
                    16,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T15:49:16Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    49,
                    16,
                    3,
                    65,
                    0
                ],
                "title": "Learning Generalizable Language-Conditioned Cloth Manipulation from Long\n  Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Generalizable Language-Conditioned Cloth Manipulation from Long\n  Demonstrations"
                },
                "summary": "Multi-step cloth manipulation is a challenging problem for robots due to the\nhigh-dimensional state spaces and the dynamics of cloth. Despite recent\nsignificant advances in end-to-end imitation learning for multi-step cloth\nmanipulation skills, these methods fail to generalize to unseen tasks. Our\ninsight in tackling the challenge of generalizable multi-step cloth\nmanipulation is decomposition. We propose a novel pipeline that autonomously\nlearns basic skills from long demonstrations and composes learned basic skills\nto generalize to unseen tasks. Specifically, our method first discovers and\nlearns basic skills from the existing long demonstration benchmark with the\ncommonsense knowledge of a large language model (LLM). Then, leveraging a\nhigh-level LLM-based task planner, these basic skills can be composed to\ncomplete unseen tasks. Experimental results demonstrate that our method\noutperforms baseline methods in learning multi-step cloth manipulation skills\nfor both seen and unseen tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-step cloth manipulation is a challenging problem for robots due to the\nhigh-dimensional state spaces and the dynamics of cloth. Despite recent\nsignificant advances in end-to-end imitation learning for multi-step cloth\nmanipulation skills, these methods fail to generalize to unseen tasks. Our\ninsight in tackling the challenge of generalizable multi-step cloth\nmanipulation is decomposition. We propose a novel pipeline that autonomously\nlearns basic skills from long demonstrations and composes learned basic skills\nto generalize to unseen tasks. Specifically, our method first discovers and\nlearns basic skills from the existing long demonstration benchmark with the\ncommonsense knowledge of a large language model (LLM). Then, leveraging a\nhigh-level LLM-based task planner, these basic skills can be composed to\ncomplete unseen tasks. Experimental results demonstrate that our method\noutperforms baseline methods in learning multi-step cloth manipulation skills\nfor both seen and unseen tasks."
                },
                "authors": [
                    {
                        "name": "Hanyi Zhao"
                    },
                    {
                        "name": "Jinxuan Zhu"
                    },
                    {
                        "name": "Zihao Yan"
                    },
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuhong Deng"
                    },
                    {
                        "name": "Xueqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xueqian Wang"
                },
                "author": "Xueqian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04556v1",
                "updated": "2025-03-06T15:47:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    47,
                    19,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T15:47:19Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    47,
                    19,
                    3,
                    65,
                    0
                ],
                "title": "Compositional Causal Reasoning Evaluation in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Causal Reasoning Evaluation in Language Models"
                },
                "summary": "Causal reasoning and compositional reasoning are two core aspirations in\ngenerative AI. Measuring the extent of these behaviors requires principled\nevaluation methods. We explore a unified perspective that considers both\nbehaviors simultaneously, termed compositional causal reasoning (CCR): the\nability to infer how causal measures compose and, equivalently, how causal\nquantities propagate through graphs. We instantiate a framework for the\nsystematic evaluation of CCR for the average treatment effect and the\nprobability of necessity and sufficiency. As proof of concept, we demonstrate\nthe design of CCR tasks for language models in the LLama, Phi, and GPT\nfamilies. On a math word problem, our framework revealed a range of\ntaxonomically distinct error patterns. Additionally, CCR errors increased with\nthe complexity of causal paths for all models except o1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal reasoning and compositional reasoning are two core aspirations in\ngenerative AI. Measuring the extent of these behaviors requires principled\nevaluation methods. We explore a unified perspective that considers both\nbehaviors simultaneously, termed compositional causal reasoning (CCR): the\nability to infer how causal measures compose and, equivalently, how causal\nquantities propagate through graphs. We instantiate a framework for the\nsystematic evaluation of CCR for the average treatment effect and the\nprobability of necessity and sufficiency. As proof of concept, we demonstrate\nthe design of CCR tasks for language models in the LLama, Phi, and GPT\nfamilies. On a math word problem, our framework revealed a range of\ntaxonomically distinct error patterns. Additionally, CCR errors increased with\nthe complexity of causal paths for all models except o1."
                },
                "authors": [
                    {
                        "name": "Jacqueline R. M. A. Maasch"
                    },
                    {
                        "name": "Alihan Hüyük"
                    },
                    {
                        "name": "Xinnuo Xu"
                    },
                    {
                        "name": "Aditya V. Nori"
                    },
                    {
                        "name": "Javier Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Javier Gonzalez"
                },
                "author": "Javier Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18691v2",
                "updated": "2025-03-06T15:47:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    47,
                    1,
                    3,
                    65,
                    0
                ],
                "published": "2024-07-26T12:16:53Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    12,
                    16,
                    53,
                    4,
                    208,
                    0
                ],
                "title": "Graph Neural Networks for Virtual Sensing in Complex Systems: Addressing\n  Heterogeneous Temporal Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks for Virtual Sensing in Complex Systems: Addressing\n  Heterogeneous Temporal Dynamics"
                },
                "summary": "Real-time condition monitoring is crucial for the reliable and efficient\noperation of complex systems. However, relying solely on physical sensors can\nbe limited due to their cost, placement constraints, or inability to directly\nmeasure certain critical parameters. Virtual sensing addresses these\nlimitations by leveraging readily available sensor data and system knowledge to\nestimate inaccessible parameters or infer system states. The increasing\ncomplexity of industrial systems necessitates deployments of sensors with\ndiverse modalities to provide a comprehensive understanding of system states.\nThese sensors capture data at varying frequencies to monitor both rapid and\nslowly varying system dynamics, as well as local and global state evolutions of\nthe systems. This leads to heterogeneous temporal dynamics, which, particularly\nunder varying operational end environmental conditions, pose a significant\nchallenge for accurate virtual sensing. To address this, we propose a\nHeterogeneous Temporal Graph Neural Network (HTGNN) framework. HTGNN explicitly\nmodels signals from diverse sensors and integrates operating conditions into\nthe model architecture. We evaluate HTGNN using two newly released datasets: a\nbearing dataset with diverse load conditions for bearing load prediction and a\nyear-long simulated dataset for predicting bridge live loads. Our results\ndemonstrate that HTGNN significantly outperforms established baseline methods\nin both tasks, particularly under highly varying operating conditions. These\nresults highlight HTGNN's potential as a robust and accurate virtual sensing\napproach for complex systems, paving the way for improved monitoring,\npredictive maintenance, and enhanced system performance. Our code and data are\navailable under https://github.com/EPFL-IMOS/htgnn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time condition monitoring is crucial for the reliable and efficient\noperation of complex systems. However, relying solely on physical sensors can\nbe limited due to their cost, placement constraints, or inability to directly\nmeasure certain critical parameters. Virtual sensing addresses these\nlimitations by leveraging readily available sensor data and system knowledge to\nestimate inaccessible parameters or infer system states. The increasing\ncomplexity of industrial systems necessitates deployments of sensors with\ndiverse modalities to provide a comprehensive understanding of system states.\nThese sensors capture data at varying frequencies to monitor both rapid and\nslowly varying system dynamics, as well as local and global state evolutions of\nthe systems. This leads to heterogeneous temporal dynamics, which, particularly\nunder varying operational end environmental conditions, pose a significant\nchallenge for accurate virtual sensing. To address this, we propose a\nHeterogeneous Temporal Graph Neural Network (HTGNN) framework. HTGNN explicitly\nmodels signals from diverse sensors and integrates operating conditions into\nthe model architecture. We evaluate HTGNN using two newly released datasets: a\nbearing dataset with diverse load conditions for bearing load prediction and a\nyear-long simulated dataset for predicting bridge live loads. Our results\ndemonstrate that HTGNN significantly outperforms established baseline methods\nin both tasks, particularly under highly varying operating conditions. These\nresults highlight HTGNN's potential as a robust and accurate virtual sensing\napproach for complex systems, paving the way for improved monitoring,\npredictive maintenance, and enhanced system performance. Our code and data are\navailable under https://github.com/EPFL-IMOS/htgnn."
                },
                "authors": [
                    {
                        "name": "Mengjie Zhao"
                    },
                    {
                        "name": "Cees Taal"
                    },
                    {
                        "name": "Stephan Baggerohr"
                    },
                    {
                        "name": "Olga Fink"
                    }
                ],
                "author_detail": {
                    "name": "Olga Fink"
                },
                "author": "Olga Fink",
                "arxiv_comment": "This paper extends our previous conference paper (Best Paper at\n  European Conference of the PHM Society 2024,\n  https://doi.org/10.36001/phme.2024.v8i1.3998). Accepted by Mechanical Systems\n  and Signal Processing (MSSP)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09990v2",
                "updated": "2025-03-06T15:38:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    38,
                    31,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-14T08:22:51Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    22,
                    51,
                    4,
                    45,
                    0
                ],
                "title": "X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from\n  Multi-Turn Jailbreaks without Compromising Usability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from\n  Multi-Turn Jailbreaks without Compromising Usability"
                },
                "summary": "Despite the rapid development of safety alignment techniques for LLMs,\ndefending against multi-turn jailbreaks is still a challenging task. In this\npaper, we conduct a comprehensive comparison, revealing that some existing\ndefense methods can improve the robustness of LLMs against multi-turn\njailbreaks but compromise usability, i.e., reducing general capabilities or\ncausing the over-refusal problem. From the perspective of mechanism\ninterpretability of LLMs, we discover that these methods fail to establish a\nboundary that exactly distinguishes safe and harmful feature representations.\nTherefore, boundary-safe representations close to harmful representations are\ninevitably disrupted, leading to a decline in usability. To address this issue,\nwe propose X-Boundary to push harmful representations away from boundary-safe\nrepresentations and obtain an exact distinction boundary. In this way, harmful\nrepresentations can be precisely erased without disrupting safe ones.\nExperimental results show that X-Boundary achieves state-of-the-art defense\nperformance against multi-turn jailbreaks, while reducing the over-refusal rate\nby about 20% and maintaining nearly complete general capability. Furthermore,\nwe theoretically prove and empirically verify that X-Boundary can accelerate\nthe convergence process during training. Please see our code at:\nhttps://github.com/AI45Lab/X-Boundary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rapid development of safety alignment techniques for LLMs,\ndefending against multi-turn jailbreaks is still a challenging task. In this\npaper, we conduct a comprehensive comparison, revealing that some existing\ndefense methods can improve the robustness of LLMs against multi-turn\njailbreaks but compromise usability, i.e., reducing general capabilities or\ncausing the over-refusal problem. From the perspective of mechanism\ninterpretability of LLMs, we discover that these methods fail to establish a\nboundary that exactly distinguishes safe and harmful feature representations.\nTherefore, boundary-safe representations close to harmful representations are\ninevitably disrupted, leading to a decline in usability. To address this issue,\nwe propose X-Boundary to push harmful representations away from boundary-safe\nrepresentations and obtain an exact distinction boundary. In this way, harmful\nrepresentations can be precisely erased without disrupting safe ones.\nExperimental results show that X-Boundary achieves state-of-the-art defense\nperformance against multi-turn jailbreaks, while reducing the over-refusal rate\nby about 20% and maintaining nearly complete general capability. Furthermore,\nwe theoretically prove and empirically verify that X-Boundary can accelerate\nthe convergence process during training. Please see our code at:\nhttps://github.com/AI45Lab/X-Boundary."
                },
                "authors": [
                    {
                        "name": "Xiaoya Lu"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Yi Yu"
                    },
                    {
                        "name": "Luxin Xu"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04554v1",
                "updated": "2025-03-06T15:37:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    37,
                    31,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T15:37:31Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    37,
                    31,
                    3,
                    65,
                    0
                ],
                "title": "Compositional Translation: A Novel LLM-based Approach for Low-resource\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Translation: A Novel LLM-based Approach for Low-resource\n  Machine Translation"
                },
                "summary": "The ability of generative large language models (LLMs) to perform in-context\nlearning has given rise to a large body of research into how best to prompt\nmodels for various natural language processing tasks. Machine Translation (MT)\nhas been shown to benefit from in-context examples, in particular when they are\nsemantically similar to the sentence to translate. In this paper, we propose a\nnew LLM-based translation paradigm, compositional translation, to replace naive\nfew-shot MT with similarity-based demonstrations. An LLM is used to decompose a\nsentence into simpler phrases, and then to translate each phrase with the help\nof retrieved demonstrations. Finally, the LLM is prompted to translate the\ninitial sentence with the help of the self-generated phrase-translation pairs.\nOur intuition is that this approach should improve translation because these\nshorter phrases should be intrinsically easier to translate and easier to match\nwith relevant examples. This is especially beneficial in low-resource\nscenarios, and more generally whenever the selection pool is small or out of\ndomain. We show that compositional translation boosts LLM translation\nperformance on a wide range of popular MT benchmarks, including FLORES 200,\nNTREX 128 and TICO-19. Code and outputs are available at\nhttps://github.com/ArmelRandy/compositional-translation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of generative large language models (LLMs) to perform in-context\nlearning has given rise to a large body of research into how best to prompt\nmodels for various natural language processing tasks. Machine Translation (MT)\nhas been shown to benefit from in-context examples, in particular when they are\nsemantically similar to the sentence to translate. In this paper, we propose a\nnew LLM-based translation paradigm, compositional translation, to replace naive\nfew-shot MT with similarity-based demonstrations. An LLM is used to decompose a\nsentence into simpler phrases, and then to translate each phrase with the help\nof retrieved demonstrations. Finally, the LLM is prompted to translate the\ninitial sentence with the help of the self-generated phrase-translation pairs.\nOur intuition is that this approach should improve translation because these\nshorter phrases should be intrinsically easier to translate and easier to match\nwith relevant examples. This is especially beneficial in low-resource\nscenarios, and more generally whenever the selection pool is small or out of\ndomain. We show that compositional translation boosts LLM translation\nperformance on a wide range of popular MT benchmarks, including FLORES 200,\nNTREX 128 and TICO-19. Code and outputs are available at\nhttps://github.com/ArmelRandy/compositional-translation"
                },
                "authors": [
                    {
                        "name": "Armel Zebaze"
                    },
                    {
                        "name": "Benoît Sagot"
                    },
                    {
                        "name": "Rachel Bawden"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Bawden"
                },
                "author": "Rachel Bawden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20984v2",
                "updated": "2025-03-06T15:36:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    36,
                    48,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-28T11:52:02Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    52,
                    2,
                    4,
                    59,
                    0
                ],
                "title": "UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models\n  for Multilingual Multimodal Idiomaticity Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models\n  for Multilingual Multimodal Idiomaticity Representation"
                },
                "summary": "SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL."
                },
                "authors": [
                    {
                        "name": "Thanet Markchom"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Liting Huang"
                    },
                    {
                        "name": "Huizhi Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huizhi Liang"
                },
                "author": "Huizhi Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04550v1",
                "updated": "2025-03-06T15:36:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    36,
                    6,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T15:36:06Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    36,
                    6,
                    3,
                    65,
                    0
                ],
                "title": "Benchmarking Reasoning Robustness in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Reasoning Robustness in Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs) in reasoning such\nas DeepSeek, we for the first time identify a key dilemma in reasoning\nrobustness and generalization: significant performance degradation on novel or\nincomplete data, suggesting a reliance on memorized patterns rather than\nsystematic reasoning. Our closer examination reveals four key unique\nlimitations underlying this issue:(1) Positional bias--models favor earlier\nqueries in multi-query inputs but answering the wrong one in the latter (e.g.,\nGPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction\nsensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series\nand by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical\nfragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from\n97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5\npercent); and (4) Memory dependence--models resort to guesswork when missing\ncritical data. These findings further highlight the reliance on heuristic\nrecall over rigorous logical inference, demonstrating challenges in reasoning\nrobustness. To comprehensively investigate these robustness challenges, this\npaper introduces a novel benchmark, termed as Math-RoB, that exploits\nhallucinations triggered by missing information to expose reasoning gaps. This\nis achieved by an instruction-based approach to generate diverse datasets that\nclosely resemble training distributions, facilitating a holistic robustness\nassessment and advancing the development of more robust reasoning frameworks.\nBad character(s) in field Abstract.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs) in reasoning such\nas DeepSeek, we for the first time identify a key dilemma in reasoning\nrobustness and generalization: significant performance degradation on novel or\nincomplete data, suggesting a reliance on memorized patterns rather than\nsystematic reasoning. Our closer examination reveals four key unique\nlimitations underlying this issue:(1) Positional bias--models favor earlier\nqueries in multi-query inputs but answering the wrong one in the latter (e.g.,\nGPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction\nsensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series\nand by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical\nfragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from\n97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5\npercent); and (4) Memory dependence--models resort to guesswork when missing\ncritical data. These findings further highlight the reliance on heuristic\nrecall over rigorous logical inference, demonstrating challenges in reasoning\nrobustness. To comprehensively investigate these robustness challenges, this\npaper introduces a novel benchmark, termed as Math-RoB, that exploits\nhallucinations triggered by missing information to expose reasoning gaps. This\nis achieved by an instruction-based approach to generate diverse datasets that\nclosely resemble training distributions, facilitating a holistic robustness\nassessment and advancing the development of more robust reasoning frameworks.\nBad character(s) in field Abstract."
                },
                "authors": [
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Xikun Zhang"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Wenjie Wu"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04548v1",
                "updated": "2025-03-06T15:34:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    34,
                    27,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T15:34:27Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    34,
                    27,
                    3,
                    65,
                    0
                ],
                "title": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models"
                },
                "summary": "In this report, we present the third technical report on the development of\nslow-thinking models as part of the STILL project. As the technical pathway\nbecomes clearer, scaling RL training has become a central technique for\nimplementing such reasoning models. We systematically experiment with and\ndocument the effects of various factors influencing RL training, conducting\nexperiments on both base models and fine-tuned models. Specifically, we\ndemonstrate that our RL training approach consistently improves the Qwen2.5-32B\nbase models, enhancing both response length and test accuracy. Furthermore, we\nshow that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already\nachieved a high performance level, it can be further refined through RL\ntraining, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we\nalso explore the use of tool manipulation, finding that it significantly boosts\nthe reasoning performance of large reasoning models. This approach achieves a\nremarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its\neffectiveness in enhancing model capabilities. We release our resources at the\nSTILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we present the third technical report on the development of\nslow-thinking models as part of the STILL project. As the technical pathway\nbecomes clearer, scaling RL training has become a central technique for\nimplementing such reasoning models. We systematically experiment with and\ndocument the effects of various factors influencing RL training, conducting\nexperiments on both base models and fine-tuned models. Specifically, we\ndemonstrate that our RL training approach consistently improves the Qwen2.5-32B\nbase models, enhancing both response length and test accuracy. Furthermore, we\nshow that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already\nachieved a high performance level, it can be further refined through RL\ntraining, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we\nalso explore the use of tool manipulation, finding that it significantly boosts\nthe reasoning performance of large reasoning models. This approach achieves a\nremarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its\neffectiveness in enhancing model capabilities. We release our resources at the\nSTILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs."
                },
                "authors": [
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Xu Miao"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Lei Fang"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Technical Report on Slow Thinking with LLMs: Part III",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07180v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07180v5",
                "updated": "2025-03-06T15:26:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    26,
                    56,
                    3,
                    65,
                    0
                ],
                "published": "2024-11-11T17:57:30Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    57,
                    30,
                    0,
                    316,
                    0
                ],
                "title": "Gumbel Counterfactual Generation From Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gumbel Counterfactual Generation From Language Models"
                },
                "summary": "Understanding and manipulating the causal generation mechanisms in language\nmodels is essential for controlling their behavior. Previous work has primarily\nrelied on techniques such as representation surgery -- e.g., model ablations or\nmanipulation of linear subspaces tied to specific concepts -- to\n\\emph{intervene} on these models. To understand the impact of interventions\nprecisely, it is useful to examine \\emph{counterfactuals} -- e.g., how a given\nsentence would have appeared had it been generated by the model following a\nspecific intervention. We highlight that counterfactual reasoning is\nconceptually distinct from interventions, as articulated in Pearl's causal\nhierarchy. Based on this observation, we propose a framework for generating\ntrue string counterfactuals by reformulating language models as a structural\nequation model using the Gumbel-max trick, which we called Gumbel\ncounterfactual generation. This reformulation allows us to model the joint\ndistribution over original strings and their counterfactuals resulting from the\nsame instantiation of the sampling noise. We develop an algorithm based on\nhindsight Gumbel sampling that allows us to infer the latent noise variables\nand generate counterfactuals of observed strings. Our experiments demonstrate\nthat the approach produces meaningful counterfactuals while at the same time\nshowing that commonly used intervention techniques have considerable undesired\nside effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and manipulating the causal generation mechanisms in language\nmodels is essential for controlling their behavior. Previous work has primarily\nrelied on techniques such as representation surgery -- e.g., model ablations or\nmanipulation of linear subspaces tied to specific concepts -- to\n\\emph{intervene} on these models. To understand the impact of interventions\nprecisely, it is useful to examine \\emph{counterfactuals} -- e.g., how a given\nsentence would have appeared had it been generated by the model following a\nspecific intervention. We highlight that counterfactual reasoning is\nconceptually distinct from interventions, as articulated in Pearl's causal\nhierarchy. Based on this observation, we propose a framework for generating\ntrue string counterfactuals by reformulating language models as a structural\nequation model using the Gumbel-max trick, which we called Gumbel\ncounterfactual generation. This reformulation allows us to model the joint\ndistribution over original strings and their counterfactuals resulting from the\nsame instantiation of the sampling noise. We develop an algorithm based on\nhindsight Gumbel sampling that allows us to infer the latent noise variables\nand generate counterfactuals of observed strings. Our experiments demonstrate\nthat the approach produces meaningful counterfactuals while at the same time\nshowing that commonly used intervention techniques have considerable undesired\nside effects."
                },
                "authors": [
                    {
                        "name": "Shauli Ravfogel"
                    },
                    {
                        "name": "Anej Svete"
                    },
                    {
                        "name": "Vésteinn Snæbjarnarson"
                    },
                    {
                        "name": "Ryan Cotterell"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Cotterell"
                },
                "author": "Ryan Cotterell",
                "arxiv_comment": "Accepted in ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07180v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07180v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00362v2",
                "updated": "2025-03-06T15:23:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    23,
                    54,
                    3,
                    65,
                    0
                ],
                "published": "2024-08-01T08:03:19Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    8,
                    3,
                    19,
                    3,
                    214,
                    0
                ],
                "title": "Measuring the speed of gravity and the cosmic expansion with time delays\n  between gravity and light from binary neutron stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the speed of gravity and the cosmic expansion with time delays\n  between gravity and light from binary neutron stars"
                },
                "summary": "The first observation of a gravitational wave (GW) and a short gamma-ray\nburst (sGRB) emitted by the same binary neutron star (BNS) merger officially\nopened the field of GW multimessenger astronomy. In this paper, we define and\naddress $\\textit{lagging sirens}$, a new class of multimessenger BNSs for which\nassociated GWs and sGRBs are observed without the identification of their host\ngalaxy. We propose a new methodology to use the observed time delay of these\nsources to constrain the speed of gravity that is, the propagation speed of\ngravitational waves, the Hubble constant and the prompt time delay distribution\nbetween GWs and sGRBs, even though a direct redshift estimation from the host\ngalaxy is unavailable. Our method exploits the intrinsic relation between GWs\nand sGRBs observed and prompt time delays to obtain a statistical redshift\nmeasure for the cosmological sources. We show that this technique can be used\nto infer the Hubble constant at the $10\\%$~level of precision with\nfuture-generation GW detectors such as the Einstein Telescope and only 100\nobservations of this kind. The novel procedure that we propose has systematics\nthat differ completely from the ones of previous GW methods for cosmology.\nAdditionally, we demonstrate for the first time that the speed of gravity and\nthe distribution of the prompt time delays between GWs and sGRBs can be\ninferred conjointly with less than 10 sources even with current GW detector\nsensitivities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The first observation of a gravitational wave (GW) and a short gamma-ray\nburst (sGRB) emitted by the same binary neutron star (BNS) merger officially\nopened the field of GW multimessenger astronomy. In this paper, we define and\naddress $\\textit{lagging sirens}$, a new class of multimessenger BNSs for which\nassociated GWs and sGRBs are observed without the identification of their host\ngalaxy. We propose a new methodology to use the observed time delay of these\nsources to constrain the speed of gravity that is, the propagation speed of\ngravitational waves, the Hubble constant and the prompt time delay distribution\nbetween GWs and sGRBs, even though a direct redshift estimation from the host\ngalaxy is unavailable. Our method exploits the intrinsic relation between GWs\nand sGRBs observed and prompt time delays to obtain a statistical redshift\nmeasure for the cosmological sources. We show that this technique can be used\nto infer the Hubble constant at the $10\\%$~level of precision with\nfuture-generation GW detectors such as the Einstein Telescope and only 100\nobservations of this kind. The novel procedure that we propose has systematics\nthat differ completely from the ones of previous GW methods for cosmology.\nAdditionally, we demonstrate for the first time that the speed of gravity and\nthe distribution of the prompt time delays between GWs and sGRBs can be\ninferred conjointly with less than 10 sources even with current GW detector\nsensitivities."
                },
                "authors": [
                    {
                        "name": "Leonardo Iampieri"
                    },
                    {
                        "name": "Simone Mastrogiovanni"
                    },
                    {
                        "name": "Francesco Pannarale"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pannarale"
                },
                "author": "Francesco Pannarale",
                "arxiv_doi": "10.1103/PhysRevD.111.023533",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.023533",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.00362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 11 figures",
                "arxiv_journal_ref": "Phys. Rev. D 111, 023533 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04531v1",
                "updated": "2025-03-06T15:19:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    19,
                    38,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T15:19:38Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    19,
                    38,
                    3,
                    65,
                    0
                ],
                "title": "MIRI-LRS spectrum of a cold exoplanet around a white dwarf: water,\n  ammonia, and methane measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRI-LRS spectrum of a cold exoplanet around a white dwarf: water,\n  ammonia, and methane measurements"
                },
                "summary": "The study of the atmosphere of exoplanets orbiting white dwarfs is a largely\nunexplored field. With WD\\,0806-661\\,b, we present the first deep dive into the\natmospheric physics and chemistry of a cold exoplanet around a white dwarf. We\nobserved WD 0806-661 b using JWST's Mid-InfraRed Instrument Low-Resolution\nSpectrometer (MIRI-LRS), covering the wavelength range from 5 -- 12~$\\mu\n\\rm{m}$, and the Imager, providing us with 12.8, 15, 18 and 21\\,$\\mu$m\nphotometric measurements. We carried the data reduction of those datasets,\ntackling second-order effects to ensure a reliable retrieval analysis. Using\nthe \\textsc{TauREx} retrieval code, we inferred the pressure-temperature\nstructure, atmospheric chemistry, mass, and radius of the planet. The spectrum\nof WD 0806-661 b is shaped by molecular absorption of water, ammonia, and\nmethane, consistent with a cold Jupiter atmosphere, allowing us to retrieve\ntheir abundances. From the mixing ratio of water, ammonia and methane we derive\n$\\rm{C/O} = 0.34 \\pm 0.06$, $\\rm{C/N} = 14.4 ^{+2.5}_{-1.8}$ and $\\rm{N/O} =\n0.023 \\pm 0.004$ and the ratio of detected metals as proxy for metallicity. We\nalso derive upper limits for the abundance of CO and $\\rm{CO_2}$\n($1.2\\cdot10^{-6} \\rm{\\,and\\,} 1.6\\cdot10^{-7}$ respectively), which were not\ndetected by our retrieval models. While our interpretation of WD\\,0806-661\\,b's\natmosphere is mostly consistent with our theoretical understanding, some\nresults -- such as the lack of evidence for water clouds, an apparent increase\nin the mixing ratio of ammonia at low pressure, or the retrieved mass at odds\nwith the supposed age -- remain surprising and require follow-up observational\nand theoretical studies to be confirmed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of the atmosphere of exoplanets orbiting white dwarfs is a largely\nunexplored field. With WD\\,0806-661\\,b, we present the first deep dive into the\natmospheric physics and chemistry of a cold exoplanet around a white dwarf. We\nobserved WD 0806-661 b using JWST's Mid-InfraRed Instrument Low-Resolution\nSpectrometer (MIRI-LRS), covering the wavelength range from 5 -- 12~$\\mu\n\\rm{m}$, and the Imager, providing us with 12.8, 15, 18 and 21\\,$\\mu$m\nphotometric measurements. We carried the data reduction of those datasets,\ntackling second-order effects to ensure a reliable retrieval analysis. Using\nthe \\textsc{TauREx} retrieval code, we inferred the pressure-temperature\nstructure, atmospheric chemistry, mass, and radius of the planet. The spectrum\nof WD 0806-661 b is shaped by molecular absorption of water, ammonia, and\nmethane, consistent with a cold Jupiter atmosphere, allowing us to retrieve\ntheir abundances. From the mixing ratio of water, ammonia and methane we derive\n$\\rm{C/O} = 0.34 \\pm 0.06$, $\\rm{C/N} = 14.4 ^{+2.5}_{-1.8}$ and $\\rm{N/O} =\n0.023 \\pm 0.004$ and the ratio of detected metals as proxy for metallicity. We\nalso derive upper limits for the abundance of CO and $\\rm{CO_2}$\n($1.2\\cdot10^{-6} \\rm{\\,and\\,} 1.6\\cdot10^{-7}$ respectively), which were not\ndetected by our retrieval models. While our interpretation of WD\\,0806-661\\,b's\natmosphere is mostly consistent with our theoretical understanding, some\nresults -- such as the lack of evidence for water clouds, an apparent increase\nin the mixing ratio of ammonia at low pressure, or the retrieved mass at odds\nwith the supposed age -- remain surprising and require follow-up observational\nand theoretical studies to be confirmed."
                },
                "authors": [
                    {
                        "name": "Maël Voyer"
                    },
                    {
                        "name": "Quentin Changeat"
                    },
                    {
                        "name": "Pierre-Olivier Lagage"
                    },
                    {
                        "name": "Pascal Tremblin"
                    },
                    {
                        "name": "Rens Waters"
                    },
                    {
                        "name": "Manuel Güdel"
                    },
                    {
                        "name": "Thomas Henning"
                    },
                    {
                        "name": "Olivier Absil"
                    },
                    {
                        "name": "David Barrado"
                    },
                    {
                        "name": "Anthony Boccaletti"
                    },
                    {
                        "name": "Jeroen Bouwman"
                    },
                    {
                        "name": "Alain Coulais"
                    },
                    {
                        "name": "Leen Decin"
                    },
                    {
                        "name": "Adrian Glauser"
                    },
                    {
                        "name": "John Pye"
                    },
                    {
                        "name": "Alistair Glasse"
                    },
                    {
                        "name": "René Gastaud"
                    },
                    {
                        "name": "Sarah Kendrew"
                    },
                    {
                        "name": "Polychronis Patapis"
                    },
                    {
                        "name": "Daniel Rouan"
                    },
                    {
                        "name": "Ewine van Dishoeck"
                    },
                    {
                        "name": "Göran Östlin"
                    },
                    {
                        "name": "Tom Ray"
                    },
                    {
                        "name": "Gillian Wright"
                    }
                ],
                "author_detail": {
                    "name": "Gillian Wright"
                },
                "author": "Gillian Wright",
                "arxiv_comment": "Accepted for publication in ApJL in March 2025. 9 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04530v1",
                "updated": "2025-03-06T15:19:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    19,
                    17,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T15:19:17Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    19,
                    17,
                    3,
                    65,
                    0
                ],
                "title": "SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning"
                },
                "summary": "Large Language Models (LLMs) excel in reasoning but remain constrained by\ntheir Chain-of-Thought (CoT) approach, which struggles with complex tasks\nrequiring more nuanced topological reasoning. We introduce SOLAR, Scalable\nOptimization of Large-scale Architecture for Reasoning, a framework that\ndynamically optimizes various reasoning topologies to enhance accuracy and\nefficiency.\n  Our Topological Annotation Generation (TAG) system automates topological\ndataset creation and segmentation, improving post-training and evaluation.\nAdditionally, we propose Topological-Scaling, a reward-driven framework that\naligns training and inference scaling, equipping LLMs with adaptive, task-aware\nreasoning.\n  SOLAR achieves substantial gains on MATH and GSM8K: +5% accuracy with\nTopological Tuning, +9% with Topological Reward, and +10.02% with Hybrid\nScaling. It also reduces response length by over 5% for complex problems,\nlowering inference latency.\n  To foster the reward system, we train a multi-task Topological Reward Model\n(M-TRM), which autonomously selects the best reasoning topology and answer in a\nsingle pass, eliminating the need for training and inference on multiple\nsingle-task TRMs (S-TRMs), thus reducing both training cost and inference\nlatency. In addition, in terms of performance, M-TRM surpasses all S-TRMs,\nimproving accuracy by +10% and rank correlation by +9%.\n  To the best of our knowledge, SOLAR sets a new benchmark for scalable,\nhigh-precision LLM reasoning while introducing an automated annotation process\nand a dynamic reasoning topology competition mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in reasoning but remain constrained by\ntheir Chain-of-Thought (CoT) approach, which struggles with complex tasks\nrequiring more nuanced topological reasoning. We introduce SOLAR, Scalable\nOptimization of Large-scale Architecture for Reasoning, a framework that\ndynamically optimizes various reasoning topologies to enhance accuracy and\nefficiency.\n  Our Topological Annotation Generation (TAG) system automates topological\ndataset creation and segmentation, improving post-training and evaluation.\nAdditionally, we propose Topological-Scaling, a reward-driven framework that\naligns training and inference scaling, equipping LLMs with adaptive, task-aware\nreasoning.\n  SOLAR achieves substantial gains on MATH and GSM8K: +5% accuracy with\nTopological Tuning, +9% with Topological Reward, and +10.02% with Hybrid\nScaling. It also reduces response length by over 5% for complex problems,\nlowering inference latency.\n  To foster the reward system, we train a multi-task Topological Reward Model\n(M-TRM), which autonomously selects the best reasoning topology and answer in a\nsingle pass, eliminating the need for training and inference on multiple\nsingle-task TRMs (S-TRMs), thus reducing both training cost and inference\nlatency. In addition, in terms of performance, M-TRM surpasses all S-TRMs,\nimproving accuracy by +10% and rank correlation by +9%.\n  To the best of our knowledge, SOLAR sets a new benchmark for scalable,\nhigh-precision LLM reasoning while introducing an automated annotation process\nand a dynamic reasoning topology competition mechanism."
                },
                "authors": [
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Yinyi Luo"
                    },
                    {
                        "name": "Anudeep Bolimera"
                    },
                    {
                        "name": "Marios Savvides"
                    }
                ],
                "author_detail": {
                    "name": "Marios Savvides"
                },
                "author": "Marios Savvides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12580v2",
                "updated": "2025-03-06T15:14:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    14,
                    17,
                    3,
                    65,
                    0
                ],
                "published": "2024-11-19T15:47:12Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    47,
                    12,
                    1,
                    324,
                    0
                ],
                "title": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language\n  Models"
                },
                "summary": "The capabilities and limitations of Large Language Models have been sketched\nout in great detail in recent years, providing an intriguing yet conflicting\npicture. On the one hand, LLMs demonstrate a general ability to solve problems.\nOn the other hand, they show surprising reasoning gaps when compared to humans,\ncasting doubt on the robustness of their generalisation strategies. The sheer\nvolume of data used in the design of LLMs has precluded us from applying the\nmethod traditionally used to measure generalisation: train-test set separation.\nTo overcome this, we study what kind of generalisation strategies LLMs employ\nwhen performing reasoning tasks by investigating the pretraining data they rely\non. For two models of different sizes (7B and 35B) and 2.5B of their\npretraining tokens, we identify what documents influence the model outputs for\nthree simple mathematical reasoning tasks and contrast this to the data that\nare influential for answering factual questions. We find that, while the models\nrely on mostly distinct sets of data for each factual question, a document\noften has a similar influence across different reasoning questions within the\nsame task, indicating the presence of procedural knowledge. We further find\nthat the answers to factual questions often show up in the most influential\ndata. However, for reasoning questions the answers usually do not show up as\nhighly influential, nor do the answers to the intermediate reasoning steps.\nWhen we characterise the top ranked documents for the reasoning questions\nqualitatively, we confirm that the influential documents often contain\nprocedural knowledge, like demonstrating how to obtain a solution using\nformulae or code. Our findings indicate that the approach to reasoning the\nmodels use is unlike retrieval, and more like a generalisable strategy that\nsynthesises procedural knowledge from documents doing a similar form of\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities and limitations of Large Language Models have been sketched\nout in great detail in recent years, providing an intriguing yet conflicting\npicture. On the one hand, LLMs demonstrate a general ability to solve problems.\nOn the other hand, they show surprising reasoning gaps when compared to humans,\ncasting doubt on the robustness of their generalisation strategies. The sheer\nvolume of data used in the design of LLMs has precluded us from applying the\nmethod traditionally used to measure generalisation: train-test set separation.\nTo overcome this, we study what kind of generalisation strategies LLMs employ\nwhen performing reasoning tasks by investigating the pretraining data they rely\non. For two models of different sizes (7B and 35B) and 2.5B of their\npretraining tokens, we identify what documents influence the model outputs for\nthree simple mathematical reasoning tasks and contrast this to the data that\nare influential for answering factual questions. We find that, while the models\nrely on mostly distinct sets of data for each factual question, a document\noften has a similar influence across different reasoning questions within the\nsame task, indicating the presence of procedural knowledge. We further find\nthat the answers to factual questions often show up in the most influential\ndata. However, for reasoning questions the answers usually do not show up as\nhighly influential, nor do the answers to the intermediate reasoning steps.\nWhen we characterise the top ranked documents for the reasoning questions\nqualitatively, we confirm that the influential documents often contain\nprocedural knowledge, like demonstrating how to obtain a solution using\nformulae or code. Our findings indicate that the approach to reasoning the\nmodels use is unlike retrieval, and more like a generalisable strategy that\nsynthesises procedural knowledge from documents doing a similar form of\nreasoning."
                },
                "authors": [
                    {
                        "name": "Laura Ruis"
                    },
                    {
                        "name": "Maximilian Mozes"
                    },
                    {
                        "name": "Juhan Bae"
                    },
                    {
                        "name": "Siddhartha Rao Kamalakara"
                    },
                    {
                        "name": "Dwarak Talupuru"
                    },
                    {
                        "name": "Acyr Locatelli"
                    },
                    {
                        "name": "Robert Kirk"
                    },
                    {
                        "name": "Tim Rocktäschel"
                    },
                    {
                        "name": "Edward Grefenstette"
                    },
                    {
                        "name": "Max Bartolo"
                    }
                ],
                "author_detail": {
                    "name": "Max Bartolo"
                },
                "author": "Max Bartolo",
                "arxiv_comment": "Published at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04521v1",
                "updated": "2025-03-06T15:08:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    8,
                    31,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T15:08:31Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    8,
                    31,
                    3,
                    65,
                    0
                ],
                "title": "Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market"
                },
                "summary": "The convergence of edge computing and AI gives rise to Edge-AI, which enables\nthe deployment of real-time AI applications and services at the network edge.\nOne of the fundamental research issues in Edge-AI is edge inference\nacceleration, which aims to realize low-latency high-accuracy DNN inference\nservices by leveraging the fine-grained offloading of partitioned inference\ntasks from end devices to edge servers. However, existing research has yet to\nadopt a practical Edge-AI market perspective, which would systematically\nexplore the personalized inference needs of AI users (e.g., inference accuracy,\nlatency, and task complexity), the revenue incentives for AI service providers\nthat offer edge inference services, and multi-stakeholder governance within a\nmarket-oriented context. To bridge this gap, we propose an Auction-based Edge\nInference Pricing Mechanism (AERIA) for revenue maximization to tackle the\nmulti-dimensional optimization problem of DNN model partition, edge inference\npricing, and resource allocation. We investigate the multi-exit device-edge\nsynergistic inference scheme for on-demand DNN inference acceleration, and\nanalyse the auction dynamics amongst the AI service providers, AI users and\nedge infrastructure provider. Owing to the strategic mechanism design via\nrandomized consensus estimate and cost sharing techniques, the Edge-AI market\nattains several desirable properties, including competitiveness in revenue\nmaximization, incentive compatibility, and envy-freeness, which are crucial to\nmaintain the effectiveness, truthfulness, and fairness of our auction outcomes.\nThe extensive simulation experiments based on four representative DNN inference\nworkloads demonstrate that our AERIA mechanism significantly outperforms\nseveral state-of-the-art approaches in revenue maximization, demonstrating the\nefficacy of AERIA for on-demand DNN inference in the Edge-AI market.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The convergence of edge computing and AI gives rise to Edge-AI, which enables\nthe deployment of real-time AI applications and services at the network edge.\nOne of the fundamental research issues in Edge-AI is edge inference\nacceleration, which aims to realize low-latency high-accuracy DNN inference\nservices by leveraging the fine-grained offloading of partitioned inference\ntasks from end devices to edge servers. However, existing research has yet to\nadopt a practical Edge-AI market perspective, which would systematically\nexplore the personalized inference needs of AI users (e.g., inference accuracy,\nlatency, and task complexity), the revenue incentives for AI service providers\nthat offer edge inference services, and multi-stakeholder governance within a\nmarket-oriented context. To bridge this gap, we propose an Auction-based Edge\nInference Pricing Mechanism (AERIA) for revenue maximization to tackle the\nmulti-dimensional optimization problem of DNN model partition, edge inference\npricing, and resource allocation. We investigate the multi-exit device-edge\nsynergistic inference scheme for on-demand DNN inference acceleration, and\nanalyse the auction dynamics amongst the AI service providers, AI users and\nedge infrastructure provider. Owing to the strategic mechanism design via\nrandomized consensus estimate and cost sharing techniques, the Edge-AI market\nattains several desirable properties, including competitiveness in revenue\nmaximization, incentive compatibility, and envy-freeness, which are crucial to\nmaintain the effectiveness, truthfulness, and fairness of our auction outcomes.\nThe extensive simulation experiments based on four representative DNN inference\nworkloads demonstrate that our AERIA mechanism significantly outperforms\nseveral state-of-the-art approaches in revenue maximization, demonstrating the\nefficacy of AERIA for on-demand DNN inference in the Edge-AI market."
                },
                "authors": [
                    {
                        "name": "Songyuan Li"
                    },
                    {
                        "name": "Jia Hu"
                    },
                    {
                        "name": "Geyong Min"
                    },
                    {
                        "name": "Haojun Huang"
                    },
                    {
                        "name": "Jiwei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jiwei Huang"
                },
                "author": "Jiwei Huang",
                "arxiv_comment": "Index Terms: Edge-AI, DNN Inference Offloading, Resource Management,\n  Dynamic Pricing, Auction Mechanism",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05874v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05874v2",
                "updated": "2025-03-06T15:02:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    2,
                    33,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-09T12:23:40Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    12,
                    23,
                    40,
                    6,
                    40,
                    0
                ],
                "title": "MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor\n  Scene Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor\n  Scene Generation"
                },
                "summary": "Controllable 3D scene generation has extensive applications in virtual\nreality and interior design, where the generated scenes should exhibit high\nlevels of realism and controllability in terms of geometry. Scene graphs\nprovide a suitable data representation that facilitates these applications.\nHowever, current graph-based methods for scene generation are constrained to\ntext-based inputs and exhibit insufficient adaptability to flexible user\ninputs, hindering the ability to precisely control object geometry. To address\nthis issue, we propose MMGDreamer, a dual-branch diffusion model for scene\ngeneration that incorporates a novel Mixed-Modality Graph, visual enhancement\nmodule, and relation predictor. The mixed-modality graph allows object nodes to\nintegrate textual and visual modalities, with optional relationships between\nnodes. It enhances adaptability to flexible user inputs and enables meticulous\ncontrol over the geometry of objects in the generated scenes. The visual\nenhancement module enriches the visual fidelity of text-only nodes by\nconstructing visual representations using text embeddings. Furthermore, our\nrelation predictor leverages node representations to infer absent relationships\nbetween nodes, resulting in more coherent scene layouts. Extensive experimental\nresults demonstrate that MMGDreamer exhibits superior control of object\ngeometry, achieving state-of-the-art scene generation performance. Project\npage: https://yangzhifeio.github.io/project/MMGDreamer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable 3D scene generation has extensive applications in virtual\nreality and interior design, where the generated scenes should exhibit high\nlevels of realism and controllability in terms of geometry. Scene graphs\nprovide a suitable data representation that facilitates these applications.\nHowever, current graph-based methods for scene generation are constrained to\ntext-based inputs and exhibit insufficient adaptability to flexible user\ninputs, hindering the ability to precisely control object geometry. To address\nthis issue, we propose MMGDreamer, a dual-branch diffusion model for scene\ngeneration that incorporates a novel Mixed-Modality Graph, visual enhancement\nmodule, and relation predictor. The mixed-modality graph allows object nodes to\nintegrate textual and visual modalities, with optional relationships between\nnodes. It enhances adaptability to flexible user inputs and enables meticulous\ncontrol over the geometry of objects in the generated scenes. The visual\nenhancement module enriches the visual fidelity of text-only nodes by\nconstructing visual representations using text embeddings. Furthermore, our\nrelation predictor leverages node representations to infer absent relationships\nbetween nodes, resulting in more coherent scene layouts. Extensive experimental\nresults demonstrate that MMGDreamer exhibits superior control of object\ngeometry, achieving state-of-the-art scene generation performance. Project\npage: https://yangzhifeio.github.io/project/MMGDreamer."
                },
                "authors": [
                    {
                        "name": "Zhifei Yang"
                    },
                    {
                        "name": "Keyang Lu"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Jiaxing Qi"
                    },
                    {
                        "name": "Hanqi Jiang"
                    },
                    {
                        "name": "Ruifei Ma"
                    },
                    {
                        "name": "Shenglin Yin"
                    },
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Mingzhe Xing"
                    },
                    {
                        "name": "Zhen Xiao"
                    },
                    {
                        "name": "Jieyi Long"
                    },
                    {
                        "name": "Xiangde Liu"
                    },
                    {
                        "name": "Guangyao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangyao Zhai"
                },
                "author": "Guangyao Zhai",
                "arxiv_comment": "Accepted by AAAI 2025 Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05874v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05874v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06639v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06639v4",
                "updated": "2025-03-06T14:54:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    54,
                    1,
                    3,
                    65,
                    0
                ],
                "published": "2024-07-09T08:07:39Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    8,
                    7,
                    39,
                    1,
                    191,
                    0
                ],
                "title": "Learning Li-ion battery health and degradation modes from data with\n  aging-aware circuit models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Li-ion battery health and degradation modes from data with\n  aging-aware circuit models"
                },
                "summary": "Non-invasive estimation of Li-ion battery state-of-health from operational\ndata is valuable for battery applications, but remains challenging. Pure\nmodel-based methods may suffer from inaccuracy and long-term instability of\nparameter estimates, whereas pure data-driven methods rely heavily on training\ndata quality and quantity, causing lack of generality when extrapolating to\nunseen cases. We apply an aging-aware equivalent circuit model for health\nestimation, combining the flexibility of data-driven techniques within a\nmodel-based approach. A simplified electrical model with voltage source and\nresistor incorporates Gaussian process regression to learn capacity fade over\ntime and also the dependence of resistance on operating conditions and time.\nThe approach was validated against two datasets and shown to give accurate\nperformance with less than 1% relative root mean square error (RMSE) in\ncapacity and less than 2% mean absolute percentage error (MAPE). Critically, we\nshow that the open circuit voltage versus state-of-charge function must be\naccurately known, and any inaccuracies or changes in this over time strongly\ninfluence the inferred resistance. However, this feature (or bug) may also be\nused to estimate in operando differential voltage curves from operational data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-invasive estimation of Li-ion battery state-of-health from operational\ndata is valuable for battery applications, but remains challenging. Pure\nmodel-based methods may suffer from inaccuracy and long-term instability of\nparameter estimates, whereas pure data-driven methods rely heavily on training\ndata quality and quantity, causing lack of generality when extrapolating to\nunseen cases. We apply an aging-aware equivalent circuit model for health\nestimation, combining the flexibility of data-driven techniques within a\nmodel-based approach. A simplified electrical model with voltage source and\nresistor incorporates Gaussian process regression to learn capacity fade over\ntime and also the dependence of resistance on operating conditions and time.\nThe approach was validated against two datasets and shown to give accurate\nperformance with less than 1% relative root mean square error (RMSE) in\ncapacity and less than 2% mean absolute percentage error (MAPE). Critically, we\nshow that the open circuit voltage versus state-of-charge function must be\naccurately known, and any inaccuracies or changes in this over time strongly\ninfluence the inferred resistance. However, this feature (or bug) may also be\nused to estimate in operando differential voltage curves from operational data."
                },
                "authors": [
                    {
                        "name": "Zihao Zhou"
                    },
                    {
                        "name": "Antti Aitio"
                    },
                    {
                        "name": "David Howey"
                    }
                ],
                "author_detail": {
                    "name": "David Howey"
                },
                "author": "David Howey",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06639v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06639v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04497v1",
                "updated": "2025-03-06T14:45:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    45,
                    38,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:45:38Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    45,
                    38,
                    3,
                    65,
                    0
                ],
                "title": "Precoder Learning for Weighted Sum Rate Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precoder Learning for Weighted Sum Rate Maximization"
                },
                "summary": "Weighted sum rate maximization (WSRM) for precoder optimization effectively\nbalances performance and fairness among users. Recent studies have demonstrated\nthe potential of deep learning in precoder optimization for sum rate\nmaximization. However, the WSRM problem necessitates a redesign of neural\nnetwork architectures to incorporate user weights into the input. In this\npaper, we propose a novel deep neural network (DNN) to learn the precoder for\nWSRM. Compared to existing DNNs, the proposed DNN leverage the joint unitary\nand permutation equivariant property inherent in the optimal precoding policy,\neffectively enhancing learning performance while reducing training complexity.\nSimulation results demonstrate that the proposed method significantly\noutperforms baseline learning methods in terms of both learning and\ngeneralization performance while maintaining low training and inference\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weighted sum rate maximization (WSRM) for precoder optimization effectively\nbalances performance and fairness among users. Recent studies have demonstrated\nthe potential of deep learning in precoder optimization for sum rate\nmaximization. However, the WSRM problem necessitates a redesign of neural\nnetwork architectures to incorporate user weights into the input. In this\npaper, we propose a novel deep neural network (DNN) to learn the precoder for\nWSRM. Compared to existing DNNs, the proposed DNN leverage the joint unitary\nand permutation equivariant property inherent in the optimal precoding policy,\neffectively enhancing learning performance while reducing training complexity.\nSimulation results demonstrate that the proposed method significantly\noutperforms baseline learning methods in terms of both learning and\ngeneralization performance while maintaining low training and inference\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Mingyu Deng"
                    },
                    {
                        "name": "Shengqian Han"
                    }
                ],
                "author_detail": {
                    "name": "Shengqian Han"
                },
                "author": "Shengqian Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04491v1",
                "updated": "2025-03-06T14:39:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    39,
                    22,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:39:22Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    39,
                    22,
                    3,
                    65,
                    0
                ],
                "title": "A Spatiotemporal, Quasi-experimental Causal Inference Approach to\n  Characterize the Effects of Global Plastic Waste Export and Burning on Air\n  Quality Using Remotely Sensed Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Spatiotemporal, Quasi-experimental Causal Inference Approach to\n  Characterize the Effects of Global Plastic Waste Export and Burning on Air\n  Quality Using Remotely Sensed Data"
                },
                "summary": "Open burning of plastic waste may pose a significant threat to global health\nby degrading air quality, but quantitative research on this problem -- crucial\nfor policy making -- has previously been stunted by lack of data. Critically,\nmany low- and middle-income countries, where open burning is of greatest\nconcern, have little to no air quality monitoring. Here, we propose an\napproach, at the intersection of modern causal inference and environmental data\nscience, to leverage remotely sensed data products combined with spatiotemporal\ncausal analytic techniques to evaluate the impact of large-scale plastic waste\npolicies on air quality. Throughout, we use the case study of Indonesia before\nand after 2018, when China halted its import of plastic waste, resulting in\ndiversion of this massive waste stream to other countries in the East Asia &\nPacific region, including Indonesia. We tailor cutting-edge statistical methods\nto this setting, estimating effects of the increase in plastic waste imports on\nfine particulate matter near waste dump sites in Indonesia and allowing effects\nto vary as a function of the site's proximity to ports (from which\ninternational plastic waste enters the country), which serves as an induced\ncontinuous exposure or \"dose\" of treatment. We observe a statistically\nsignificant increase in monthly fine particulate matter concentrations near\ndump sites after China's ban took effect (2018-2019) compared to concentrations\nexpected under business-as-usual (2012-2017), with increases ranging from\n0.76--1.72$\\mu$g/m$^3$ (15--34\\% of the World Health Organization's recommended\nlimit for exposure on an annual basis) depending on the site's port proximity,\nat sites with port proximity above the 20th quantile. Sites with lower port\nproximity had smaller and not statistically significant effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open burning of plastic waste may pose a significant threat to global health\nby degrading air quality, but quantitative research on this problem -- crucial\nfor policy making -- has previously been stunted by lack of data. Critically,\nmany low- and middle-income countries, where open burning is of greatest\nconcern, have little to no air quality monitoring. Here, we propose an\napproach, at the intersection of modern causal inference and environmental data\nscience, to leverage remotely sensed data products combined with spatiotemporal\ncausal analytic techniques to evaluate the impact of large-scale plastic waste\npolicies on air quality. Throughout, we use the case study of Indonesia before\nand after 2018, when China halted its import of plastic waste, resulting in\ndiversion of this massive waste stream to other countries in the East Asia &\nPacific region, including Indonesia. We tailor cutting-edge statistical methods\nto this setting, estimating effects of the increase in plastic waste imports on\nfine particulate matter near waste dump sites in Indonesia and allowing effects\nto vary as a function of the site's proximity to ports (from which\ninternational plastic waste enters the country), which serves as an induced\ncontinuous exposure or \"dose\" of treatment. We observe a statistically\nsignificant increase in monthly fine particulate matter concentrations near\ndump sites after China's ban took effect (2018-2019) compared to concentrations\nexpected under business-as-usual (2012-2017), with increases ranging from\n0.76--1.72$\\mu$g/m$^3$ (15--34\\% of the World Health Organization's recommended\nlimit for exposure on an annual basis) depending on the site's port proximity,\nat sites with port proximity above the 20th quantile. Sites with lower port\nproximity had smaller and not statistically significant effects."
                },
                "authors": [
                    {
                        "name": "Ellen M. Considine"
                    },
                    {
                        "name": "Rachel C. Nethery"
                    }
                ],
                "author_detail": {
                    "name": "Rachel C. Nethery"
                },
                "author": "Rachel C. Nethery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04490v1",
                "updated": "2025-03-06T14:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    38,
                    20,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:38:20Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    38,
                    20,
                    3,
                    65,
                    0
                ],
                "title": "Large Language Models in Bioinformatics: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models in Bioinformatics: A Survey"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing bioinformatics, enabling\nadvanced analysis of DNA, RNA, proteins, and single-cell data. This survey\nprovides a systematic review of recent advancements, focusing on genomic\nsequence modeling, RNA structure prediction, protein function inference, and\nsingle-cell transcriptomics. Meanwhile, we also discuss several key challenges,\nincluding data scarcity, computational complexity, and cross-omics integration,\nand explore future directions such as multimodal learning, hybrid AI models,\nand clinical applications. By offering a comprehensive perspective, this paper\nunderscores the transformative potential of LLMs in driving innovations in\nbioinformatics and precision medicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing bioinformatics, enabling\nadvanced analysis of DNA, RNA, proteins, and single-cell data. This survey\nprovides a systematic review of recent advancements, focusing on genomic\nsequence modeling, RNA structure prediction, protein function inference, and\nsingle-cell transcriptomics. Meanwhile, we also discuss several key challenges,\nincluding data scarcity, computational complexity, and cross-omics integration,\nand explore future directions such as multimodal learning, hybrid AI models,\nand clinical applications. By offering a comprehensive perspective, this paper\nunderscores the transformative potential of LLMs in driving innovations in\nbioinformatics and precision medicine."
                },
                "authors": [
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Zikang Wang"
                    },
                    {
                        "name": "Jiyue Jiang"
                    },
                    {
                        "name": "Pengan Chen"
                    },
                    {
                        "name": "Xiangyu Shi"
                    },
                    {
                        "name": "Yu Li"
                    }
                ],
                "author_detail": {
                    "name": "Yu Li"
                },
                "author": "Yu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03222v2",
                "updated": "2025-03-06T14:32:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    32,
                    49,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-05T06:32:49Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    6,
                    32,
                    49,
                    2,
                    64,
                    0
                ],
                "title": "Mocap-2-to-3: Lifting 2D Diffusion-Based Pretrained Models for 3D Motion\n  Capture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mocap-2-to-3: Lifting 2D Diffusion-Based Pretrained Models for 3D Motion\n  Capture"
                },
                "summary": "Recovering absolute poses in the world coordinate system from monocular views\npresents significant challenges. Two primary issues arise in this context.\nFirstly, existing methods rely on 3D motion data for training, which requires\ncollection in limited environments. Acquiring such 3D labels for new actions in\na timely manner is impractical, severely restricting the model's generalization\ncapabilities. In contrast, 2D poses are far more accessible and easier to\nobtain. Secondly, estimating a person's absolute position in metric space from\na single viewpoint is inherently more complex. To address these challenges, we\nintroduce Mocap-2-to-3, a novel framework that decomposes intricate 3D motions\ninto 2D poses, leveraging 2D data to enhance 3D motion reconstruction in\ndiverse scenarios and accurately predict absolute positions in the world\ncoordinate system. We initially pretrain a single-view diffusion model with\nextensive 2D data, followed by fine-tuning a multi-view diffusion model for\nview consistency using publicly available 3D data. This strategy facilitates\nthe effective use of large-scale 2D data. Additionally, we propose an\ninnovative human motion representation that decouples local actions from global\nmovements and encodes geometric priors of the ground, ensuring the generative\nmodel learns accurate motion priors from 2D data. During inference, this allows\nfor the gradual recovery of global movements, resulting in more plausible\npositioning. We evaluate our model's performance on real-world datasets,\ndemonstrating superior accuracy in motion and absolute human positioning\ncompared to state-of-the-art methods, along with enhanced generalization and\nscalability. Our code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering absolute poses in the world coordinate system from monocular views\npresents significant challenges. Two primary issues arise in this context.\nFirstly, existing methods rely on 3D motion data for training, which requires\ncollection in limited environments. Acquiring such 3D labels for new actions in\na timely manner is impractical, severely restricting the model's generalization\ncapabilities. In contrast, 2D poses are far more accessible and easier to\nobtain. Secondly, estimating a person's absolute position in metric space from\na single viewpoint is inherently more complex. To address these challenges, we\nintroduce Mocap-2-to-3, a novel framework that decomposes intricate 3D motions\ninto 2D poses, leveraging 2D data to enhance 3D motion reconstruction in\ndiverse scenarios and accurately predict absolute positions in the world\ncoordinate system. We initially pretrain a single-view diffusion model with\nextensive 2D data, followed by fine-tuning a multi-view diffusion model for\nview consistency using publicly available 3D data. This strategy facilitates\nthe effective use of large-scale 2D data. Additionally, we propose an\ninnovative human motion representation that decouples local actions from global\nmovements and encodes geometric priors of the ground, ensuring the generative\nmodel learns accurate motion priors from 2D data. During inference, this allows\nfor the gradual recovery of global movements, resulting in more plausible\npositioning. We evaluate our model's performance on real-world datasets,\ndemonstrating superior accuracy in motion and absolute human positioning\ncompared to state-of-the-art methods, along with enhanced generalization and\nscalability. Our code will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Zhumei Wang"
                    },
                    {
                        "name": "Zechen Hu"
                    },
                    {
                        "name": "Ruoxi Guo"
                    },
                    {
                        "name": "Huaijin Pi"
                    },
                    {
                        "name": "Ziyong Feng"
                    },
                    {
                        "name": "Sida Peng"
                    },
                    {
                        "name": "Xiaowei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Zhou"
                },
                "author": "Xiaowei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04483v1",
                "updated": "2025-03-06T14:32:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    32,
                    0,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:32:00Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    32,
                    0,
                    3,
                    65,
                    0
                ],
                "title": "InfoSEM: A Deep Generative Model with Informative Priors for Gene\n  Regulatory Network Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfoSEM: A Deep Generative Model with Informative Priors for Gene\n  Regulatory Network Inference"
                },
                "summary": "Inferring Gene Regulatory Networks (GRNs) from gene expression data is\ncrucial for understanding biological processes. While supervised models are\nreported to achieve high performance for this task, they rely on costly ground\ntruth (GT) labels and risk learning gene-specific biases, such as class\nimbalances of GT interactions, rather than true regulatory mechanisms. To\naddress these issues, we introduce InfoSEM, an unsupervised generative model\nthat leverages textual gene embeddings as informative priors, improving GRN\ninference without GT labels. InfoSEM can also integrate GT labels as an\nadditional prior when available, avoiding biases and further enhancing\nperformance. Additionally, we propose a biologically motivated benchmarking\nframework that better reflects real-world applications such as biomarker\ndiscovery and reveals learned biases of existing supervised methods. InfoSEM\noutperforms existing models by 38.5% across four datasets using textual\nembeddings prior and further boosts performance by 11.1% when integrating\nlabeled data as priors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Gene Regulatory Networks (GRNs) from gene expression data is\ncrucial for understanding biological processes. While supervised models are\nreported to achieve high performance for this task, they rely on costly ground\ntruth (GT) labels and risk learning gene-specific biases, such as class\nimbalances of GT interactions, rather than true regulatory mechanisms. To\naddress these issues, we introduce InfoSEM, an unsupervised generative model\nthat leverages textual gene embeddings as informative priors, improving GRN\ninference without GT labels. InfoSEM can also integrate GT labels as an\nadditional prior when available, avoiding biases and further enhancing\nperformance. Additionally, we propose a biologically motivated benchmarking\nframework that better reflects real-world applications such as biomarker\ndiscovery and reveals learned biases of existing supervised methods. InfoSEM\noutperforms existing models by 38.5% across four datasets using textual\nembeddings prior and further boosts performance by 11.1% when integrating\nlabeled data as priors."
                },
                "authors": [
                    {
                        "name": "Tianyu Cui"
                    },
                    {
                        "name": "Song-Jun Xu"
                    },
                    {
                        "name": "Artem Moskalev"
                    },
                    {
                        "name": "Shuwei Li"
                    },
                    {
                        "name": "Tommaso Mansi"
                    },
                    {
                        "name": "Mangal Prakash"
                    },
                    {
                        "name": "Rui Liao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Liao"
                },
                "author": "Rui Liao",
                "arxiv_comment": "ICLR 2025 AI4NA Oral, ICLR 2025 MLGenX Spotlight, ICLR 2025 LMRL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04480v1",
                "updated": "2025-03-06T14:30:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    30,
                    15,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:30:15Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    30,
                    15,
                    3,
                    65,
                    0
                ],
                "title": "Poisoning Bayesian Inference via Data Deletion and Replication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poisoning Bayesian Inference via Data Deletion and Replication"
                },
                "summary": "Research in adversarial machine learning (AML) has shown that statistical\nmodels are vulnerable to maliciously altered data. However, despite advances in\nBayesian machine learning models, most AML research remains concentrated on\nclassical techniques. Therefore, we focus on extending the white-box model\npoisoning paradigm to attack generic Bayesian inference, highlighting its\nvulnerability in adversarial contexts. A suite of attacks are developed that\nallow an attacker to steer the Bayesian posterior toward a target distribution\nthrough the strategic deletion and replication of true observations, even when\nonly sampling access to the posterior is available. Analytic properties of\nthese algorithms are proven and their performance is empirically examined in\nboth synthetic and real-world scenarios. With relatively little effort, the\nattacker is able to substantively alter the Bayesian's beliefs and, by\naccepting more risk, they can mold these beliefs to their will. By carefully\nconstructing the adversarial posterior, surgical poisoning is achieved such\nthat only targeted inferences are corrupted and others are minimally disturbed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research in adversarial machine learning (AML) has shown that statistical\nmodels are vulnerable to maliciously altered data. However, despite advances in\nBayesian machine learning models, most AML research remains concentrated on\nclassical techniques. Therefore, we focus on extending the white-box model\npoisoning paradigm to attack generic Bayesian inference, highlighting its\nvulnerability in adversarial contexts. A suite of attacks are developed that\nallow an attacker to steer the Bayesian posterior toward a target distribution\nthrough the strategic deletion and replication of true observations, even when\nonly sampling access to the posterior is available. Analytic properties of\nthese algorithms are proven and their performance is empirically examined in\nboth synthetic and real-world scenarios. With relatively little effort, the\nattacker is able to substantively alter the Bayesian's beliefs and, by\naccepting more risk, they can mold these beliefs to their will. By carefully\nconstructing the adversarial posterior, surgical poisoning is achieved such\nthat only targeted inferences are corrupted and others are minimally disturbed."
                },
                "authors": [
                    {
                        "name": "Matthieu Carreau"
                    },
                    {
                        "name": "Roi Naveiro"
                    },
                    {
                        "name": "William N. Caballero"
                    }
                ],
                "author_detail": {
                    "name": "William N. Caballero"
                },
                "author": "William N. Caballero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04479v1",
                "updated": "2025-03-06T14:29:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    29,
                    52,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:29:52Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    29,
                    52,
                    3,
                    65,
                    0
                ],
                "title": "ToolFuzz -- Automated Agent Tool Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolFuzz -- Automated Agent Tool Testing"
                },
                "summary": "Large Language Model (LLM) Agents leverage the advanced reasoning\ncapabilities of LLMs in real-world applications. To interface with an\nenvironment, these agents often rely on tools, such as web search or database\nAPIs. As the agent provides the LLM with tool documentation along the user\nquery, the completeness and correctness of this documentation is critical.\nHowever, tool documentation is often over-, under-, or ill-specified, impeding\nthe agent's accuracy. Standard software testing approaches struggle to identify\nthese errors as they are expressed in natural language. Thus, despite its\nimportance, there currently exists no automated method to test the tool\ndocumentation for agents. To address this issue, we present ToolFuzz, the first\nmethod for automated testing of tool documentations. ToolFuzz is designed to\ndiscover two types of errors: (1) user queries leading to tool runtime errors\nand (2) user queries that lead to incorrect agent responses. ToolFuzz can\ngenerate a large and diverse set of natural inputs, effectively finding tool\ndescription errors at a low false positive rate. Further, we present two\nstraightforward prompt-engineering approaches. We evaluate all three tool\ntesting approaches on 32 common LangChain tools and 35 newly created custom\ntools and 2 novel benchmarks to further strengthen the assessment. We find that\nmany publicly available tools suffer from underspecification. Specifically, we\nshow that ToolFuzz identifies 20x more erroneous inputs compared to the\nprompt-engineering approaches, making it a key component for building reliable\nAI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) Agents leverage the advanced reasoning\ncapabilities of LLMs in real-world applications. To interface with an\nenvironment, these agents often rely on tools, such as web search or database\nAPIs. As the agent provides the LLM with tool documentation along the user\nquery, the completeness and correctness of this documentation is critical.\nHowever, tool documentation is often over-, under-, or ill-specified, impeding\nthe agent's accuracy. Standard software testing approaches struggle to identify\nthese errors as they are expressed in natural language. Thus, despite its\nimportance, there currently exists no automated method to test the tool\ndocumentation for agents. To address this issue, we present ToolFuzz, the first\nmethod for automated testing of tool documentations. ToolFuzz is designed to\ndiscover two types of errors: (1) user queries leading to tool runtime errors\nand (2) user queries that lead to incorrect agent responses. ToolFuzz can\ngenerate a large and diverse set of natural inputs, effectively finding tool\ndescription errors at a low false positive rate. Further, we present two\nstraightforward prompt-engineering approaches. We evaluate all three tool\ntesting approaches on 32 common LangChain tools and 35 newly created custom\ntools and 2 novel benchmarks to further strengthen the assessment. We find that\nmany publicly available tools suffer from underspecification. Specifically, we\nshow that ToolFuzz identifies 20x more erroneous inputs compared to the\nprompt-engineering approaches, making it a key component for building reliable\nAI agents."
                },
                "authors": [
                    {
                        "name": "Ivan Milev"
                    },
                    {
                        "name": "Mislav Balunović"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04474v1",
                "updated": "2025-03-06T14:24:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    24,
                    12,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:24:12Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    24,
                    12,
                    3,
                    65,
                    0
                ],
                "title": "Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges"
                },
                "summary": "Large Language Model (LLM) based judges form the underpinnings of key safety\nevaluation processes such as offline benchmarking, automated red-teaming, and\nonline guardrailing. This widespread requirement raises the crucial question:\ncan we trust the evaluations of these evaluators? In this paper, we highlight\ntwo critical challenges that are typically overlooked: (i) evaluations in the\nwild where factors like prompt sensitivity and distribution shifts can affect\nperformance and (ii) adversarial attacks that target the judge. We highlight\nthe importance of these through a study of commonly used safety judges, showing\nthat small changes such as the style of the model output can lead to jumps of\nup to 0.24 in the false negative rate on the same dataset, whereas adversarial\nattacks on the model generation can fool some judges into misclassifying 100%\nof harmful generations as safe ones. These findings reveal gaps in commonly\nused meta-evaluation benchmarks and weaknesses in the robustness of current LLM\njudges, indicating that low attack success under certain judges could create a\nfalse sense of security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based judges form the underpinnings of key safety\nevaluation processes such as offline benchmarking, automated red-teaming, and\nonline guardrailing. This widespread requirement raises the crucial question:\ncan we trust the evaluations of these evaluators? In this paper, we highlight\ntwo critical challenges that are typically overlooked: (i) evaluations in the\nwild where factors like prompt sensitivity and distribution shifts can affect\nperformance and (ii) adversarial attacks that target the judge. We highlight\nthe importance of these through a study of commonly used safety judges, showing\nthat small changes such as the style of the model output can lead to jumps of\nup to 0.24 in the false negative rate on the same dataset, whereas adversarial\nattacks on the model generation can fool some judges into misclassifying 100%\nof harmful generations as safe ones. These findings reveal gaps in commonly\nused meta-evaluation benchmarks and weaknesses in the robustness of current LLM\njudges, indicating that low attack success under certain judges could create a\nfalse sense of security."
                },
                "authors": [
                    {
                        "name": "Francisco Eiras"
                    },
                    {
                        "name": "Eliott Zemour"
                    },
                    {
                        "name": "Eric Lin"
                    },
                    {
                        "name": "Vaikkunth Mugunthan"
                    }
                ],
                "author_detail": {
                    "name": "Vaikkunth Mugunthan"
                },
                "author": "Vaikkunth Mugunthan",
                "arxiv_comment": "Accepted to the ICBINB Workshop at ICLR'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08440v2",
                "updated": "2025-03-06T14:16:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    16,
                    47,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-12T14:30:57Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    30,
                    57,
                    2,
                    43,
                    0
                ],
                "title": "Scenario Analysis with Multivariate Bayesian Machine Learning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scenario Analysis with Multivariate Bayesian Machine Learning Models"
                },
                "summary": "We present an econometric framework that adapts tools for scenario analysis,\nsuch as variants of conditional forecasts and impulse response functions, for\nuse with dynamic nonparametric multivariate models. We demonstrate the utility\nof our approach with simulated data and three real-world applications: (1)\nscenario-based conditional forecasts aligned with Federal Reserve stress test\nassumptions, measuring (2) macroeconomic risk under varying financial\nconditions, and (3) asymmetric effects of US-based financial shocks and their\ninternational spillovers. Our results indicate the importance of nonlinearities\nand asymmetries in dynamic relationships between macroeconomic and financial\nvariables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an econometric framework that adapts tools for scenario analysis,\nsuch as variants of conditional forecasts and impulse response functions, for\nuse with dynamic nonparametric multivariate models. We demonstrate the utility\nof our approach with simulated data and three real-world applications: (1)\nscenario-based conditional forecasts aligned with Federal Reserve stress test\nassumptions, measuring (2) macroeconomic risk under varying financial\nconditions, and (3) asymmetric effects of US-based financial shocks and their\ninternational spillovers. Our results indicate the importance of nonlinearities\nand asymmetries in dynamic relationships between macroeconomic and financial\nvariables."
                },
                "authors": [
                    {
                        "name": "Michael Pfarrhofer"
                    },
                    {
                        "name": "Anna Stelzer"
                    }
                ],
                "author_detail": {
                    "name": "Anna Stelzer"
                },
                "author": "Anna Stelzer",
                "arxiv_comment": "Keywords: conditional forecast, generalized impulse response\n  function, Bayesian additive regression trees, nonlinearities, structural\n  inference; JEL: C11, C32, C53, C54",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04463v1",
                "updated": "2025-03-06T14:15:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    15,
                    7,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:15:07Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    15,
                    7,
                    3,
                    65,
                    0
                ],
                "title": "Guiding LLMs to Generate High-Fidelity and High-Quality Counterfactual\n  Explanations for Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding LLMs to Generate High-Fidelity and High-Quality Counterfactual\n  Explanations for Text Classification"
                },
                "summary": "The need for interpretability in deep learning has driven interest in\ncounterfactual explanations, which identify minimal changes to an instance that\nchange a model's prediction. Current counterfactual (CF) generation methods\nrequire task-specific fine-tuning and produce low-quality text. Large Language\nModels (LLMs), though effective for high-quality text generation, struggle with\nlabel-flipping counterfactuals (i.e., counterfactuals that change the\nprediction) without fine-tuning. We introduce two simple classifier-guided\napproaches to support counterfactual generation by LLMs, eliminating the need\nfor fine-tuning while preserving the strengths of LLMs. Despite their\nsimplicity, our methods outperform state-of-the-art counterfactual generation\nmethods and are effective across different LLMs, highlighting the benefits of\nguiding counterfactual generation by LLMs with classifier information. We\nfurther show that data augmentation by our generated CFs can improve a\nclassifier's robustness. Our analysis reveals a critical issue in\ncounterfactual generation by LLMs: LLMs rely on parametric knowledge rather\nthan faithfully following the classifier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for interpretability in deep learning has driven interest in\ncounterfactual explanations, which identify minimal changes to an instance that\nchange a model's prediction. Current counterfactual (CF) generation methods\nrequire task-specific fine-tuning and produce low-quality text. Large Language\nModels (LLMs), though effective for high-quality text generation, struggle with\nlabel-flipping counterfactuals (i.e., counterfactuals that change the\nprediction) without fine-tuning. We introduce two simple classifier-guided\napproaches to support counterfactual generation by LLMs, eliminating the need\nfor fine-tuning while preserving the strengths of LLMs. Despite their\nsimplicity, our methods outperform state-of-the-art counterfactual generation\nmethods and are effective across different LLMs, highlighting the benefits of\nguiding counterfactual generation by LLMs with classifier information. We\nfurther show that data augmentation by our generated CFs can improve a\nclassifier's robustness. Our analysis reveals a critical issue in\ncounterfactual generation by LLMs: LLMs rely on parametric knowledge rather\nthan faithfully following the classifier."
                },
                "authors": [
                    {
                        "name": "Van Bach Nguyen"
                    },
                    {
                        "name": "Christin Seifert"
                    },
                    {
                        "name": "Jörg Schlötterer"
                    }
                ],
                "author_detail": {
                    "name": "Jörg Schlötterer"
                },
                "author": "Jörg Schlötterer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04457v1",
                "updated": "2025-03-06T14:11:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    11,
                    0,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:11:00Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    11,
                    0,
                    3,
                    65,
                    0
                ],
                "title": "TPC: Cross-Temporal Prediction Connection for Vision-Language Model\n  Hallucination Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPC: Cross-Temporal Prediction Connection for Vision-Language Model\n  Hallucination Reduction"
                },
                "summary": "Vision-language models (VLMs) have achieved remarkable advancements,\ncapitalizing on the impressive capabilities of large language models (LLMs)\nacross diverse tasks. Despite this, a critical challenge known as hallucination\noccurs when models overconfidently describe objects or attributes absent from\nthe image, a problem exacerbated by the tendency of VLMs to rely on linguistic\npriors. This limitation reduces model reliability in high-stakes applications.\nIn this work, we have observed the characteristic of logits' continuity\nconsistency enhancement and introduced a straightforward and efficient method,\nCross-Temporal Prediction Connection (TPC), designed to enhance the semantic\nconsistency of logits by connecting them temporally across timesteps. TPC\namplifies information flow and improves coherence, effectively reducing\nhallucination. Extensive experiments show that TPC surpasses existing\nrepresentatives, delivering superior performance in both accuracy and\nefficiency while maintaining robustness in open-ended text generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) have achieved remarkable advancements,\ncapitalizing on the impressive capabilities of large language models (LLMs)\nacross diverse tasks. Despite this, a critical challenge known as hallucination\noccurs when models overconfidently describe objects or attributes absent from\nthe image, a problem exacerbated by the tendency of VLMs to rely on linguistic\npriors. This limitation reduces model reliability in high-stakes applications.\nIn this work, we have observed the characteristic of logits' continuity\nconsistency enhancement and introduced a straightforward and efficient method,\nCross-Temporal Prediction Connection (TPC), designed to enhance the semantic\nconsistency of logits by connecting them temporally across timesteps. TPC\namplifies information flow and improves coherence, effectively reducing\nhallucination. Extensive experiments show that TPC surpasses existing\nrepresentatives, delivering superior performance in both accuracy and\nefficiency while maintaining robustness in open-ended text generation tasks."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Weiwei Fu"
                    },
                    {
                        "name": "Yang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhou"
                },
                "author": "Yang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13728v2",
                "updated": "2025-03-06T14:07:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    7,
                    57,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-19T13:54:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    54,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Secure Federated Data Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Federated Data Distillation"
                },
                "summary": "Dataset Distillation (DD) is a powerful technique for reducing large datasets\ninto compact, representative synthetic datasets, accelerating Machine Learning\ntraining. However, traditional DD methods operate in a centralized manner,\nwhich poses significant privacy threats and reduces its applicability. To\nmitigate these risks, we propose a Secure Federated Data Distillation (SFDD)\nframework to decentralize the distillation process while preserving privacy.\nUnlike existing Federated Distillation techniques that focus on training global\nmodels with distilled knowledge, our approach aims to produce a distilled\ndataset without exposing local contributions. We leverage the\ngradient-matching-based distillation method, adapting it for a distributed\nsetting where clients contribute to the distillation process without sharing\nraw data. The central aggregator iteratively refines a synthetic dataset by\nintegrating client-side updates while ensuring data confidentiality. To make\nour approach resilient to inference attacks perpetrated by the server that\ncould exploit gradient updates to reconstruct private data, we create an\noptimized Local Differential Privacy approach, called LDPO-RLD. Furthermore, we\nassess the framework's resilience against malicious clients executing backdoor\nattacks (such as Doorping) and demonstrate robustness under the assumption of a\nsufficient number of participating clients. Our experimental results\ndemonstrate the effectiveness of SFDD and that the proposed defense concretely\nmitigates the identified vulnerabilities, with minimal impact on the\nperformance of the distilled dataset. By addressing the interplay between\nprivacy and federation in dataset distillation, this work advances the field of\nprivacy-preserving Machine Learning making our SFDD framework a viable solution\nfor sensitive data-sharing applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset Distillation (DD) is a powerful technique for reducing large datasets\ninto compact, representative synthetic datasets, accelerating Machine Learning\ntraining. However, traditional DD methods operate in a centralized manner,\nwhich poses significant privacy threats and reduces its applicability. To\nmitigate these risks, we propose a Secure Federated Data Distillation (SFDD)\nframework to decentralize the distillation process while preserving privacy.\nUnlike existing Federated Distillation techniques that focus on training global\nmodels with distilled knowledge, our approach aims to produce a distilled\ndataset without exposing local contributions. We leverage the\ngradient-matching-based distillation method, adapting it for a distributed\nsetting where clients contribute to the distillation process without sharing\nraw data. The central aggregator iteratively refines a synthetic dataset by\nintegrating client-side updates while ensuring data confidentiality. To make\nour approach resilient to inference attacks perpetrated by the server that\ncould exploit gradient updates to reconstruct private data, we create an\noptimized Local Differential Privacy approach, called LDPO-RLD. Furthermore, we\nassess the framework's resilience against malicious clients executing backdoor\nattacks (such as Doorping) and demonstrate robustness under the assumption of a\nsufficient number of participating clients. Our experimental results\ndemonstrate the effectiveness of SFDD and that the proposed defense concretely\nmitigates the identified vulnerabilities, with minimal impact on the\nperformance of the distilled dataset. By addressing the interplay between\nprivacy and federation in dataset distillation, this work advances the field of\nprivacy-preserving Machine Learning making our SFDD framework a viable solution\nfor sensitive data-sharing applications."
                },
                "authors": [
                    {
                        "name": "Marco Arazzi"
                    },
                    {
                        "name": "Mert Cihangiroglu"
                    },
                    {
                        "name": "Serena Nicolazzo"
                    },
                    {
                        "name": "Antonino Nocera"
                    }
                ],
                "author_detail": {
                    "name": "Antonino Nocera"
                },
                "author": "Antonino Nocera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04452v1",
                "updated": "2025-03-06T14:06:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    6,
                    35,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:06:35Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    6,
                    35,
                    3,
                    65,
                    0
                ],
                "title": "A lightweight model FDM-YOLO for small target improvement based on\n  YOLOv8",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A lightweight model FDM-YOLO for small target improvement based on\n  YOLOv8"
                },
                "summary": "Small targets are particularly difficult to detect due to their low pixel\ncount, complex backgrounds, and varying shooting angles, which make it hard for\nmodels to extract effective features. While some large-scale models offer high\naccuracy, their long inference times make them unsuitable for real-time\ndeployment on edge devices. On the other hand, models designed for low\ncomputational power often suffer from poor detection accuracy. This paper\nfocuses on small target detection and explores methods for object detection\nunder low computational constraints. Building on the YOLOv8 model, we propose a\nnew network architecture called FDM-YOLO. Our research includes the following\nkey contributions: We introduce FDM-YOLO by analyzing the output of the YOLOv8\ndetection head. We add a highresolution layer and remove the large target\ndetection layer to better handle small targets. Based on PConv, we propose a\nlightweight network structure called Fast-C2f, which is integrated into the PAN\nmodule of the model. To mitigate the accuracy loss caused by model\nlightweighting, we employ dynamic upsampling (Dysample) and a lightweight EMA\nattention mechanism.The FDM-YOLO model was validated on the Visdrone dataset,\nachieving a 38% reduction in parameter count and improving the Map0.5 score\nfrom 38.4% to 42.5%, all while maintaining nearly the same inference speed.\nThis demonstrates the effectiveness of our approach in balancing accuracy and\nefficiency for edge device deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small targets are particularly difficult to detect due to their low pixel\ncount, complex backgrounds, and varying shooting angles, which make it hard for\nmodels to extract effective features. While some large-scale models offer high\naccuracy, their long inference times make them unsuitable for real-time\ndeployment on edge devices. On the other hand, models designed for low\ncomputational power often suffer from poor detection accuracy. This paper\nfocuses on small target detection and explores methods for object detection\nunder low computational constraints. Building on the YOLOv8 model, we propose a\nnew network architecture called FDM-YOLO. Our research includes the following\nkey contributions: We introduce FDM-YOLO by analyzing the output of the YOLOv8\ndetection head. We add a highresolution layer and remove the large target\ndetection layer to better handle small targets. Based on PConv, we propose a\nlightweight network structure called Fast-C2f, which is integrated into the PAN\nmodule of the model. To mitigate the accuracy loss caused by model\nlightweighting, we employ dynamic upsampling (Dysample) and a lightweight EMA\nattention mechanism.The FDM-YOLO model was validated on the Visdrone dataset,\nachieving a 38% reduction in parameter count and improving the Map0.5 score\nfrom 38.4% to 42.5%, all while maintaining nearly the same inference speed.\nThis demonstrates the effectiveness of our approach in balancing accuracy and\nefficiency for edge device deployment."
                },
                "authors": [
                    {
                        "name": "Xuerui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuerui Zhang"
                },
                "author": "Xuerui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04444v1",
                "updated": "2025-03-06T14:00:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    0,
                    59,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:00:59Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    0,
                    59,
                    3,
                    65,
                    0
                ],
                "title": "ToFu: Visual Tokens Reduction via Fusion for Multi-modal, Multi-patch,\n  Multi-image Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToFu: Visual Tokens Reduction via Fusion for Multi-modal, Multi-patch,\n  Multi-image Task"
                },
                "summary": "Large Multimodal Models (LMMs) are powerful tools that are capable of\nreasoning and understanding multimodal information beyond text and language.\nDespite their entrenched impact, the development of LMMs is hindered by the\nhigher computational requirements compared to their unimodal counterparts. One\nof the main causes of this is the large amount of tokens needed to encode the\nvisual input, which is especially evident for multi-image multimodal tasks.\nRecent approaches to reduce visual tokens depend on the visual encoder\narchitecture, require fine-tuning the LLM to maintain the performance, and only\nconsider single-image scenarios. To address these limitations, we propose ToFu,\na visual encoder-agnostic, training-free Token Fusion strategy that combines\nredundant visual tokens of LMMs for high-resolution, multi-image, tasks. The\ncore intuition behind our method is straightforward yet effective: preserve\ndistinctive tokens while combining similar ones. We achieve this by\nsequentially examining visual tokens and deciding whether to merge them with\nothers or keep them as separate entities. We validate our approach on the\nwell-established LLaVA-Interleave Bench, which covers challenging multi-image\ntasks. In addition, we push to the extreme our method by testing it on a\nnewly-created benchmark, ComPairs, focused on multi-image comparisons where a\nlarger amount of images and visual tokens are inputted to the LMMs. Our\nextensive analysis, considering several LMM architectures, demonstrates the\nbenefits of our approach both in terms of efficiency and performance gain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) are powerful tools that are capable of\nreasoning and understanding multimodal information beyond text and language.\nDespite their entrenched impact, the development of LMMs is hindered by the\nhigher computational requirements compared to their unimodal counterparts. One\nof the main causes of this is the large amount of tokens needed to encode the\nvisual input, which is especially evident for multi-image multimodal tasks.\nRecent approaches to reduce visual tokens depend on the visual encoder\narchitecture, require fine-tuning the LLM to maintain the performance, and only\nconsider single-image scenarios. To address these limitations, we propose ToFu,\na visual encoder-agnostic, training-free Token Fusion strategy that combines\nredundant visual tokens of LMMs for high-resolution, multi-image, tasks. The\ncore intuition behind our method is straightforward yet effective: preserve\ndistinctive tokens while combining similar ones. We achieve this by\nsequentially examining visual tokens and deciding whether to merge them with\nothers or keep them as separate entities. We validate our approach on the\nwell-established LLaVA-Interleave Bench, which covers challenging multi-image\ntasks. In addition, we push to the extreme our method by testing it on a\nnewly-created benchmark, ComPairs, focused on multi-image comparisons where a\nlarger amount of images and visual tokens are inputted to the LMMs. Our\nextensive analysis, considering several LMM architectures, demonstrates the\nbenefits of our approach both in terms of efficiency and performance gain."
                },
                "authors": [
                    {
                        "name": "Vittorio Pippi"
                    },
                    {
                        "name": "Matthieu Guillaumin"
                    },
                    {
                        "name": "Silvia Cascianelli"
                    },
                    {
                        "name": "Rita Cucchiara"
                    },
                    {
                        "name": "Maximilian Jaritz"
                    },
                    {
                        "name": "Loris Bazzani"
                    }
                ],
                "author_detail": {
                    "name": "Loris Bazzani"
                },
                "author": "Loris Bazzani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04441v1",
                "updated": "2025-03-06T13:56:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    56,
                    48,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T13:56:48Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    56,
                    48,
                    3,
                    65,
                    0
                ],
                "title": "EvidMTL: Evidential Multi-Task Learning for Uncertainty-Aware Semantic\n  Surface Mapping from Monocular RGB Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvidMTL: Evidential Multi-Task Learning for Uncertainty-Aware Semantic\n  Surface Mapping from Monocular RGB Images"
                },
                "summary": "For scene understanding in unstructured environments, an accurate and\nuncertainty-aware metric-semantic mapping is required to enable informed action\nselection by autonomous systems.Existing mapping methods often suffer from\noverconfident semantic predictions, and sparse and noisy depth sensing, leading\nto inconsistent map representations. In this paper, we therefore introduce\nEvidMTL, a multi-task learning framework that uses evidential heads for depth\nestimation and semantic segmentation, enabling uncertainty-aware inference from\nmonocular RGB images. To enable uncertainty-calibrated evidential multi-task\nlearning, we propose a novel evidential depth loss function that jointly\noptimizes the belief strength of the depth prediction in conjunction with\nevidential segmentation loss. Building on this, we present EvidKimera, an\nuncertainty-aware semantic surface mapping framework, which uses evidential\ndepth and semantics prediction for improved 3D metric-semantic consistency. We\ntrain and evaluate EvidMTL on the NYUDepthV2 and assess its zero-shot\nperformance on ScanNetV2, demonstrating superior uncertainty estimation\ncompared to conventional approaches while maintaining comparable depth\nestimation and semantic segmentation. In zero-shot mapping tests on ScanNetV2,\nEvidKimera outperforms Kimera in semantic surface mapping accuracy and\nconsistency, highlighting the benefits of uncertainty-aware mapping and\nunderscoring its potential for real-world robotic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For scene understanding in unstructured environments, an accurate and\nuncertainty-aware metric-semantic mapping is required to enable informed action\nselection by autonomous systems.Existing mapping methods often suffer from\noverconfident semantic predictions, and sparse and noisy depth sensing, leading\nto inconsistent map representations. In this paper, we therefore introduce\nEvidMTL, a multi-task learning framework that uses evidential heads for depth\nestimation and semantic segmentation, enabling uncertainty-aware inference from\nmonocular RGB images. To enable uncertainty-calibrated evidential multi-task\nlearning, we propose a novel evidential depth loss function that jointly\noptimizes the belief strength of the depth prediction in conjunction with\nevidential segmentation loss. Building on this, we present EvidKimera, an\nuncertainty-aware semantic surface mapping framework, which uses evidential\ndepth and semantics prediction for improved 3D metric-semantic consistency. We\ntrain and evaluate EvidMTL on the NYUDepthV2 and assess its zero-shot\nperformance on ScanNetV2, demonstrating superior uncertainty estimation\ncompared to conventional approaches while maintaining comparable depth\nestimation and semantic segmentation. In zero-shot mapping tests on ScanNetV2,\nEvidKimera outperforms Kimera in semantic surface mapping accuracy and\nconsistency, highlighting the benefits of uncertainty-aware mapping and\nunderscoring its potential for real-world robotic applications."
                },
                "authors": [
                    {
                        "name": "Rohit Menon"
                    },
                    {
                        "name": "Nils Dengler"
                    },
                    {
                        "name": "Sicong Pan"
                    },
                    {
                        "name": "Gokul Krishna Chenchani"
                    },
                    {
                        "name": "Maren Bennewitz"
                    }
                ],
                "author_detail": {
                    "name": "Maren Bennewitz"
                },
                "author": "Maren Bennewitz",
                "arxiv_comment": "Submitted to IROS 2025 Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03272v2",
                "updated": "2025-03-06T13:49:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    49,
                    46,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-05T08:52:55Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    8,
                    52,
                    55,
                    2,
                    64,
                    0
                ],
                "title": "Towards Effective and Sparse Adversarial Attack on Spiking Neural\n  Networks via Breaking Invisible Surrogate Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Effective and Sparse Adversarial Attack on Spiking Neural\n  Networks via Breaking Invisible Surrogate Gradients"
                },
                "summary": "Spiking neural networks (SNNs) have shown their competence in handling\nspatial-temporal event-based data with low energy consumption. Similar to\nconventional artificial neural networks (ANNs), SNNs are also vulnerable to\ngradient-based adversarial attacks, wherein gradients are calculated by\nspatial-temporal back-propagation (STBP) and surrogate gradients (SGs).\nHowever, the SGs may be invisible for an inference-only model as they do not\ninfluence the inference results, and current gradient-based attacks are\nineffective for binary dynamic images captured by the dynamic vision sensor\n(DVS). While some approaches addressed the issue of invisible SGs through\nuniversal SGs, their SGs lack a correlation with the victim model, resulting in\nsub-optimal performance. Moreover, the imperceptibility of existing SNN-based\nbinary attacks is still insufficient. In this paper, we introduce an innovative\npotential-dependent surrogate gradient (PDSG) method to establish a robust\nconnection between the SG and the model, thereby enhancing the adaptability of\nadversarial attacks across various models with invisible SGs. Additionally, we\npropose the sparse dynamic attack (SDA) to effectively attack binary dynamic\nimages. Utilizing a generation-reduction paradigm, SDA can fully optimize the\nsparsity of adversarial perturbations. Experimental results demonstrate that\nour PDSG and SDA outperform state-of-the-art SNN-based attacks across various\nmodels and datasets. Specifically, our PDSG achieves 100% attack success rate\non ImageNet, and our SDA obtains 82% attack success rate by modifying only\n0.24% of the pixels on CIFAR10DVS. The code is available at\nhttps://github.com/ryime/PDSG-SDA .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking neural networks (SNNs) have shown their competence in handling\nspatial-temporal event-based data with low energy consumption. Similar to\nconventional artificial neural networks (ANNs), SNNs are also vulnerable to\ngradient-based adversarial attacks, wherein gradients are calculated by\nspatial-temporal back-propagation (STBP) and surrogate gradients (SGs).\nHowever, the SGs may be invisible for an inference-only model as they do not\ninfluence the inference results, and current gradient-based attacks are\nineffective for binary dynamic images captured by the dynamic vision sensor\n(DVS). While some approaches addressed the issue of invisible SGs through\nuniversal SGs, their SGs lack a correlation with the victim model, resulting in\nsub-optimal performance. Moreover, the imperceptibility of existing SNN-based\nbinary attacks is still insufficient. In this paper, we introduce an innovative\npotential-dependent surrogate gradient (PDSG) method to establish a robust\nconnection between the SG and the model, thereby enhancing the adaptability of\nadversarial attacks across various models with invisible SGs. Additionally, we\npropose the sparse dynamic attack (SDA) to effectively attack binary dynamic\nimages. Utilizing a generation-reduction paradigm, SDA can fully optimize the\nsparsity of adversarial perturbations. Experimental results demonstrate that\nour PDSG and SDA outperform state-of-the-art SNN-based attacks across various\nmodels and datasets. Specifically, our PDSG achieves 100% attack success rate\non ImageNet, and our SDA obtains 82% attack success rate by modifying only\n0.24% of the pixels on CIFAR10DVS. The code is available at\nhttps://github.com/ryime/PDSG-SDA ."
                },
                "authors": [
                    {
                        "name": "Li Lun"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Qinglong Ni"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Dunshan Yu"
                    },
                    {
                        "name": "Xiaoxin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Cui"
                },
                "author": "Xiaoxin Cui",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04426v1",
                "updated": "2025-03-06T13:35:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    35,
                    59,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T13:35:59Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    35,
                    59,
                    3,
                    65,
                    0
                ],
                "title": "FORTALESA: Fault-Tolerant Reconfigurable Systolic Array for DNN\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FORTALESA: Fault-Tolerant Reconfigurable Systolic Array for DNN\n  Inference"
                },
                "summary": "The emergence of Deep Neural Networks (DNNs) in mission- and safety-critical\napplications brings their reliability to the front. High performance demands of\nDNNs require the use of specialized hardware accelerators. Systolic array\narchitecture is widely used in DNN accelerators due to its parallelism and\nregular structure. This work presents a run-time reconfigurable systolic array\narchitecture with three execution modes and four implementation options. All\nfour implementations are evaluated in terms of resource utilization,\nthroughput, and fault tolerance improvement. The proposed architecture is used\nfor reliability enhancement of DNN inference on systolic array through\nheterogeneous mapping of different network layers to different execution modes.\nThe approach is supported by a novel reliability assessment method based on\nfault propagation analysis. It is used for the exploration of the appropriate\nexecution mode-layer mapping for DNN inference. The proposed architecture\nefficiently protects registers and MAC units of systolic array PEs from\ntransient and permanent faults. The reconfigurability feature enables a speedup\nof up to $3\\times$, depending on layer vulnerability. Furthermore, it requires\n$6\\times$ less resources compared to static redundancy and $2.5\\times$ less\nresources compared to the previously proposed solution for transient faults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Deep Neural Networks (DNNs) in mission- and safety-critical\napplications brings their reliability to the front. High performance demands of\nDNNs require the use of specialized hardware accelerators. Systolic array\narchitecture is widely used in DNN accelerators due to its parallelism and\nregular structure. This work presents a run-time reconfigurable systolic array\narchitecture with three execution modes and four implementation options. All\nfour implementations are evaluated in terms of resource utilization,\nthroughput, and fault tolerance improvement. The proposed architecture is used\nfor reliability enhancement of DNN inference on systolic array through\nheterogeneous mapping of different network layers to different execution modes.\nThe approach is supported by a novel reliability assessment method based on\nfault propagation analysis. It is used for the exploration of the appropriate\nexecution mode-layer mapping for DNN inference. The proposed architecture\nefficiently protects registers and MAC units of systolic array PEs from\ntransient and permanent faults. The reconfigurability feature enables a speedup\nof up to $3\\times$, depending on layer vulnerability. Furthermore, it requires\n$6\\times$ less resources compared to static redundancy and $2.5\\times$ less\nresources compared to the previously proposed solution for transient faults."
                },
                "authors": [
                    {
                        "name": "Natalia Cherezova"
                    },
                    {
                        "name": "Artur Jutman"
                    },
                    {
                        "name": "Maksim Jenihhin"
                    }
                ],
                "author_detail": {
                    "name": "Maksim Jenihhin"
                },
                "author": "Maksim Jenihhin",
                "arxiv_comment": "11 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.07978v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.07978v4",
                "updated": "2025-03-06T13:29:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    29,
                    24,
                    3,
                    65,
                    0
                ],
                "published": "2023-11-14T08:10:14Z",
                "published_parsed": [
                    2023,
                    11,
                    14,
                    8,
                    10,
                    14,
                    1,
                    318,
                    0
                ],
                "title": "AfroBench: How Good are Large Language Models on African Languages?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AfroBench: How Good are Large Language Models on African Languages?"
                },
                "summary": "Large-scale multilingual evaluations, such as MEGA, often include only a\nhandful of African languages due to the scarcity of high-quality evaluation\ndata and the limited discoverability of existing African datasets. This lack of\nrepresentation hinders comprehensive LLM evaluation across a diverse range of\nlanguages and tasks. To address these challenges, we introduce AfroBench -- a\nmulti-task benchmark for evaluating the performance of LLMs across 64 African\nlanguages, 15 tasks and 22 datasets. AfroBench consists of nine natural\nlanguage understanding datasets, six text generation datasets, six knowledge\nand question answering tasks, and one mathematical reasoning task. We present\nresults comparing the performance of prompting LLMs to fine-tuned baselines\nbased on BERT and T5-style models. Our results suggest large gaps in\nperformance between high-resource languages, such as English, and African\nlanguages across most tasks; but performance also varies based on the\navailability of monolingual data resources. Our findings confirm that\nperformance on African languages continues to remain a hurdle for current LLMs,\nunderscoring the need for additional efforts to close this gap.\n  https://mcgill-nlp.github.io/AfroBench/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale multilingual evaluations, such as MEGA, often include only a\nhandful of African languages due to the scarcity of high-quality evaluation\ndata and the limited discoverability of existing African datasets. This lack of\nrepresentation hinders comprehensive LLM evaluation across a diverse range of\nlanguages and tasks. To address these challenges, we introduce AfroBench -- a\nmulti-task benchmark for evaluating the performance of LLMs across 64 African\nlanguages, 15 tasks and 22 datasets. AfroBench consists of nine natural\nlanguage understanding datasets, six text generation datasets, six knowledge\nand question answering tasks, and one mathematical reasoning task. We present\nresults comparing the performance of prompting LLMs to fine-tuned baselines\nbased on BERT and T5-style models. Our results suggest large gaps in\nperformance between high-resource languages, such as English, and African\nlanguages across most tasks; but performance also varies based on the\navailability of monolingual data resources. Our findings confirm that\nperformance on African languages continues to remain a hurdle for current LLMs,\nunderscoring the need for additional efforts to close this gap.\n  https://mcgill-nlp.github.io/AfroBench/"
                },
                "authors": [
                    {
                        "name": "Jessica Ojo"
                    },
                    {
                        "name": "Odunayo Ogundepo"
                    },
                    {
                        "name": "Akintunde Oladipo"
                    },
                    {
                        "name": "Kelechi Ogueji"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Pontus Stenetorp"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    }
                ],
                "author_detail": {
                    "name": "David Ifeoluwa Adelani"
                },
                "author": "David Ifeoluwa Adelani",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.07978v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.07978v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04418v1",
                "updated": "2025-03-06T13:21:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    21,
                    38,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T13:21:38Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    21,
                    38,
                    3,
                    65,
                    0
                ],
                "title": "AOLO: Analysis and Optimization For Low-Carbon Oriented Wireless Large\n  Language Model Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AOLO: Analysis and Optimization For Low-Carbon Oriented Wireless Large\n  Language Model Services"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to their\nwidespread adoption and large-scale deployment across various domains. However,\ntheir environmental impact, particularly during inference, has become a growing\nconcern due to their substantial energy consumption and carbon footprint.\nExisting research has focused on inference computation alone, overlooking the\nanalysis and optimization of carbon footprint in network-aided LLM service\nsystems. To address this gap, we propose AOLO, a framework for analysis and\noptimization for low-carbon oriented wireless LLM services. AOLO introduces a\ncomprehensive carbon footprint model that quantifies greenhouse gas emissions\nacross the entire LLM service chain, including computational inference and\nwireless communication. Furthermore, we formulate an optimization problem aimed\nat minimizing the overall carbon footprint, which is solved through joint\noptimization of inference outputs and transmit power under\nquality-of-experience and system performance constraints. To achieve this joint\noptimization, we leverage the energy efficiency of spiking neural networks\n(SNNs) by adopting SNN as the actor network and propose a low-carbon-oriented\noptimization algorithm, i.e., SNN-based deep reinforcement learning (SDRL).\nComprehensive simulations demonstrate that SDRL algorithm significantly reduces\noverall carbon footprint, achieving an 18.77% reduction compared to the\nbenchmark soft actor-critic, highlighting its potential for enabling more\nsustainable LLM inference services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to their\nwidespread adoption and large-scale deployment across various domains. However,\ntheir environmental impact, particularly during inference, has become a growing\nconcern due to their substantial energy consumption and carbon footprint.\nExisting research has focused on inference computation alone, overlooking the\nanalysis and optimization of carbon footprint in network-aided LLM service\nsystems. To address this gap, we propose AOLO, a framework for analysis and\noptimization for low-carbon oriented wireless LLM services. AOLO introduces a\ncomprehensive carbon footprint model that quantifies greenhouse gas emissions\nacross the entire LLM service chain, including computational inference and\nwireless communication. Furthermore, we formulate an optimization problem aimed\nat minimizing the overall carbon footprint, which is solved through joint\noptimization of inference outputs and transmit power under\nquality-of-experience and system performance constraints. To achieve this joint\noptimization, we leverage the energy efficiency of spiking neural networks\n(SNNs) by adopting SNN as the actor network and propose a low-carbon-oriented\noptimization algorithm, i.e., SNN-based deep reinforcement learning (SDRL).\nComprehensive simulations demonstrate that SDRL algorithm significantly reduces\noverall carbon footprint, achieving an 18.77% reduction compared to the\nbenchmark soft actor-critic, highlighting its potential for enabling more\nsustainable LLM inference services."
                },
                "authors": [
                    {
                        "name": "Xiaoqi Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Yuehong Gao"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04412v1",
                "updated": "2025-03-06T13:10:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    10,
                    40,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T13:10:40Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    10,
                    40,
                    3,
                    65,
                    0
                ],
                "title": "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive\n  Branching Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive\n  Branching Tree Search"
                },
                "summary": "Recent advances demonstrate that increasing inference-time computation can\nsignificantly boost the reasoning capabilities of large language models (LLMs).\nAlthough repeated sampling (i.e., generating multiple candidate outputs) is a\nhighly effective strategy, it does not leverage external feedback signals for\nrefinement, which are often available in tasks like coding. In this work, we\npropose $\\textit{Adaptive Branching Monte Carlo Tree Search (AB-MCTS)}$, a\nnovel inference-time framework that generalizes repeated sampling with\nprincipled multi-turn exploration and exploitation. At each node in the search\ntree, AB-MCTS dynamically decides whether to \"go wider\" by expanding new\ncandidate responses or \"go deeper\" by revisiting existing ones based on\nexternal feedback signals. We evaluate our method on complex coding and\nengineering tasks using frontier models. Empirical results show that AB-MCTS\nconsistently outperforms both repeated sampling and standard MCTS, underscoring\nthe importance of combining the response diversity of LLMs with multi-turn\nsolution refinement for effective inference-time scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances demonstrate that increasing inference-time computation can\nsignificantly boost the reasoning capabilities of large language models (LLMs).\nAlthough repeated sampling (i.e., generating multiple candidate outputs) is a\nhighly effective strategy, it does not leverage external feedback signals for\nrefinement, which are often available in tasks like coding. In this work, we\npropose $\\textit{Adaptive Branching Monte Carlo Tree Search (AB-MCTS)}$, a\nnovel inference-time framework that generalizes repeated sampling with\nprincipled multi-turn exploration and exploitation. At each node in the search\ntree, AB-MCTS dynamically decides whether to \"go wider\" by expanding new\ncandidate responses or \"go deeper\" by revisiting existing ones based on\nexternal feedback signals. We evaluate our method on complex coding and\nengineering tasks using frontier models. Empirical results show that AB-MCTS\nconsistently outperforms both repeated sampling and standard MCTS, underscoring\nthe importance of combining the response diversity of LLMs with multi-turn\nsolution refinement for effective inference-time scaling."
                },
                "authors": [
                    {
                        "name": "Kou Misaki"
                    },
                    {
                        "name": "Yuichi Inoue"
                    },
                    {
                        "name": "Yuki Imajuku"
                    },
                    {
                        "name": "So Kuroki"
                    },
                    {
                        "name": "Taishi Nakamura"
                    },
                    {
                        "name": "Takuya Akiba"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Akiba"
                },
                "author": "Takuya Akiba",
                "arxiv_comment": "To appear at ICLR 2025 Workshop on Foundation Models in the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12753v2",
                "updated": "2025-03-06T12:55:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    55,
                    25,
                    3,
                    65,
                    0
                ],
                "published": "2024-06-18T16:20:53Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    16,
                    20,
                    53,
                    1,
                    170,
                    0
                ],
                "title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for\n  Superintelligent AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for\n  Superintelligent AI"
                },
                "summary": "The evolution of Artificial Intelligence (AI) has been significantly\naccelerated by advancements in Large Language Models (LLMs) and Large\nMultimodal Models (LMMs), gradually showcasing potential cognitive reasoning\nabilities in problem-solving and scientific discovery (i.e., AI4Science) once\nexclusive to human intellect. To comprehensively evaluate current models'\nperformance in cognitive reasoning abilities, we introduce OlympicArena, which\nincludes 11,163 bilingual problems across both text-only and interleaved\ntext-image modalities. These challenges encompass a wide range of disciplines\nspanning seven fields and 62 international Olympic competitions, rigorously\nexamined for data leakage. We argue that the challenges in Olympic competition\nproblems are ideal for evaluating AI's cognitive reasoning due to their\ncomplexity and interdisciplinary nature, which are essential for tackling\ncomplex scientific challenges and facilitating discoveries. Beyond evaluating\nperformance across various disciplines using answer-only criteria, we conduct\ndetailed experiments and analyses from multiple perspectives. We delve into the\nmodels' cognitive reasoning abilities, their performance across different\nmodalities, and their outcomes in process-level evaluations, which are vital\nfor tasks requiring complex reasoning with lengthy solutions. Our extensive\nevaluations reveal that even advanced models like GPT-4o only achieve a 39.97%\noverall accuracy, illustrating current AI limitations in complex reasoning and\nmultimodal integration. Through the OlympicArena, we aim to advance AI towards\nsuperintelligence, equipping it to address more complex challenges in science\nand beyond. We also provide a comprehensive set of resources to support AI\nresearch, including a benchmark dataset, an open-source annotation platform, a\ndetailed evaluation tool, and a leaderboard with automatic submission features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of Artificial Intelligence (AI) has been significantly\naccelerated by advancements in Large Language Models (LLMs) and Large\nMultimodal Models (LMMs), gradually showcasing potential cognitive reasoning\nabilities in problem-solving and scientific discovery (i.e., AI4Science) once\nexclusive to human intellect. To comprehensively evaluate current models'\nperformance in cognitive reasoning abilities, we introduce OlympicArena, which\nincludes 11,163 bilingual problems across both text-only and interleaved\ntext-image modalities. These challenges encompass a wide range of disciplines\nspanning seven fields and 62 international Olympic competitions, rigorously\nexamined for data leakage. We argue that the challenges in Olympic competition\nproblems are ideal for evaluating AI's cognitive reasoning due to their\ncomplexity and interdisciplinary nature, which are essential for tackling\ncomplex scientific challenges and facilitating discoveries. Beyond evaluating\nperformance across various disciplines using answer-only criteria, we conduct\ndetailed experiments and analyses from multiple perspectives. We delve into the\nmodels' cognitive reasoning abilities, their performance across different\nmodalities, and their outcomes in process-level evaluations, which are vital\nfor tasks requiring complex reasoning with lengthy solutions. Our extensive\nevaluations reveal that even advanced models like GPT-4o only achieve a 39.97%\noverall accuracy, illustrating current AI limitations in complex reasoning and\nmultimodal integration. Through the OlympicArena, we aim to advance AI towards\nsuperintelligence, equipping it to address more complex challenges in science\nand beyond. We also provide a comprehensive set of resources to support AI\nresearch, including a benchmark dataset, an open-source annotation platform, a\ndetailed evaluation tool, and a leaderboard with automatic submission features."
                },
                "authors": [
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Zengzhi Wang"
                    },
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Xuefeng Li"
                    },
                    {
                        "name": "Haoyang Zou"
                    },
                    {
                        "name": "Ruijie Xu"
                    },
                    {
                        "name": "Run-Ze Fan"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Ethan Chern"
                    },
                    {
                        "name": "Yixin Ye"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Ting Wu"
                    },
                    {
                        "name": "Binjie Wang"
                    },
                    {
                        "name": "Shichao Sun"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Yiyuan Li"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Steffi Chern"
                    },
                    {
                        "name": "Yiwei Qin"
                    },
                    {
                        "name": "Yan Ma"
                    },
                    {
                        "name": "Jiadi Su"
                    },
                    {
                        "name": "Yixiu Liu"
                    },
                    {
                        "name": "Yuxiang Zheng"
                    },
                    {
                        "name": "Shaoting Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05569v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05569v3",
                "updated": "2025-03-06T12:54:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    54,
                    37,
                    3,
                    65,
                    0
                ],
                "published": "2024-04-08T14:43:13Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    14,
                    43,
                    13,
                    0,
                    99,
                    0
                ],
                "title": "360$^\\circ$REA: Towards A Reusable Experience Accumulation with\n  360° Assessment for Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "360$^\\circ$REA: Towards A Reusable Experience Accumulation with\n  360° Assessment for Multi-Agent System"
                },
                "summary": "Large language model agents have demonstrated remarkable advancements across\nvarious complex tasks. Recent works focus on optimizing the agent team or\nemploying self-reflection to iteratively solve complex tasks. Since these\nagents are all based on the same LLM, only conducting self-evaluation or\nremoving underperforming agents does not substantively enhance the capability\nof the agents. We argue that a comprehensive evaluation and accumulating\nexperience from evaluation feedback is an effective approach to improving\nsystem performance. In this paper, we propose Reusable Experience Accumulation\nwith 360$^\\circ$ Assessment (360$^\\circ$REA), a hierarchical multi-agent\nframework inspired by corporate organizational practices. The framework employs\na novel 360$^\\circ$ performance assessment method for multi-perspective\nperformance evaluation with fine-grained assessment. To enhance the capability\nof agents in addressing complex tasks, we introduce dual-level experience pool\nfor agents to accumulate experience through fine-grained assessment. Extensive\nexperiments on complex task datasets demonstrate the effectiveness of\n360$^\\circ$REA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model agents have demonstrated remarkable advancements across\nvarious complex tasks. Recent works focus on optimizing the agent team or\nemploying self-reflection to iteratively solve complex tasks. Since these\nagents are all based on the same LLM, only conducting self-evaluation or\nremoving underperforming agents does not substantively enhance the capability\nof the agents. We argue that a comprehensive evaluation and accumulating\nexperience from evaluation feedback is an effective approach to improving\nsystem performance. In this paper, we propose Reusable Experience Accumulation\nwith 360$^\\circ$ Assessment (360$^\\circ$REA), a hierarchical multi-agent\nframework inspired by corporate organizational practices. The framework employs\na novel 360$^\\circ$ performance assessment method for multi-perspective\nperformance evaluation with fine-grained assessment. To enhance the capability\nof agents in addressing complex tasks, we introduce dual-level experience pool\nfor agents to accumulate experience through fine-grained assessment. Extensive\nexperiments on complex task datasets demonstrate the effectiveness of\n360$^\\circ$REA."
                },
                "authors": [
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Chengrui Huang"
                    },
                    {
                        "name": "Quan Tu"
                    },
                    {
                        "name": "Zhiliang Tian"
                    },
                    {
                        "name": "Minlie Huang"
                    },
                    {
                        "name": "Shuo Shang"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Shang"
                },
                "author": "Shuo Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05569v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05569v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04398v1",
                "updated": "2025-03-06T12:52:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    52,
                    22,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T12:52:22Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    52,
                    22,
                    3,
                    65,
                    0
                ],
                "title": "Speculative MoE: Communication Efficient Parallel MoE Inference with\n  Speculative Token and Expert Pre-scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative MoE: Communication Efficient Parallel MoE Inference with\n  Speculative Token and Expert Pre-scheduling"
                },
                "summary": "MoE (Mixture of Experts) prevails as a neural architecture that can scale\nmodern transformer-based LLMs (Large Language Models) to unprecedented scales.\nNevertheless, large MoEs' great demands of computing power, memory capacity and\nmemory bandwidth make scalable serving a fundamental challenge and efficient\nparallel inference has become a requisite to attain adequate throughput under\nlatency constraints. DeepSpeed-MoE, one state-of-the-art MoE inference\nframework, adopts a 3D-parallel paradigm including EP (Expert Parallelism), TP\n(Tensor Parallel) and DP (Data Parallelism). However, our analysis shows\nDeepSpeed-MoE's inference efficiency is largely bottlenecked by EP, which is\nimplemented with costly all-to-all collectives to route token activation. Our\nwork aims to boost DeepSpeed-MoE by strategically reducing EP's communication\noverhead with a technique named Speculative MoE. Speculative MoE has two\nspeculative parallelization schemes, speculative token shuffling and\nspeculative expert grouping, which predict outstanding tokens' expert routing\npaths and pre-schedule tokens and experts across devices to losslessly trim\nEP's communication volume. Besides DeepSpeed-MoE, we also build Speculative MoE\ninto a prevailing MoE inference engine SGLang. Experiments show Speculative MoE\ncan significantly boost state-of-the-art MoE inference frameworks on fast\nhomogeneous and slow heterogeneous interconnects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE (Mixture of Experts) prevails as a neural architecture that can scale\nmodern transformer-based LLMs (Large Language Models) to unprecedented scales.\nNevertheless, large MoEs' great demands of computing power, memory capacity and\nmemory bandwidth make scalable serving a fundamental challenge and efficient\nparallel inference has become a requisite to attain adequate throughput under\nlatency constraints. DeepSpeed-MoE, one state-of-the-art MoE inference\nframework, adopts a 3D-parallel paradigm including EP (Expert Parallelism), TP\n(Tensor Parallel) and DP (Data Parallelism). However, our analysis shows\nDeepSpeed-MoE's inference efficiency is largely bottlenecked by EP, which is\nimplemented with costly all-to-all collectives to route token activation. Our\nwork aims to boost DeepSpeed-MoE by strategically reducing EP's communication\noverhead with a technique named Speculative MoE. Speculative MoE has two\nspeculative parallelization schemes, speculative token shuffling and\nspeculative expert grouping, which predict outstanding tokens' expert routing\npaths and pre-schedule tokens and experts across devices to losslessly trim\nEP's communication volume. Besides DeepSpeed-MoE, we also build Speculative MoE\ninto a prevailing MoE inference engine SGLang. Experiments show Speculative MoE\ncan significantly boost state-of-the-art MoE inference frameworks on fast\nhomogeneous and slow heterogeneous interconnects."
                },
                "authors": [
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Pengfei Zheng"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Zewei Xu"
                    },
                    {
                        "name": "Yunfei Du"
                    },
                    {
                        "name": "Zhengang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhengang Wang"
                },
                "author": "Zhengang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13483v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13483v3",
                "updated": "2025-03-06T12:51:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    51,
                    49,
                    3,
                    65,
                    0
                ],
                "published": "2025-01-23T08:57:02Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    8,
                    57,
                    2,
                    3,
                    23,
                    0
                ],
                "title": "Robust Amortized Bayesian Inference with Self-Consistency Losses on\n  Unlabeled Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Amortized Bayesian Inference with Self-Consistency Losses on\n  Unlabeled Data"
                },
                "summary": "Neural amortized Bayesian inference (ABI) can solve probabilistic inverse\nproblems orders of magnitude faster than classical methods. However, neural ABI\nis not yet sufficiently robust for widespread and safe applicability. In\nparticular, when performing inference on observations outside of the scope of\nthe simulated data seen during training, for example, because of model\nmisspecification, the posterior approximations are likely to become highly\nbiased. Due to the bad pre-asymptotic behavior of current neural posterior\nestimators in the out-of-simulation regime, the resulting estimation biases\ncannot be fixed in acceptable time by just simulating more training data. In\nthis proof-of-concept paper, we propose a semi-supervised approach that enables\ntraining not only on (labeled) simulated data generated from the model, but\nalso on unlabeled data originating from any source, including real-world data.\nTo achieve the latter, we exploit Bayesian self-consistency properties that can\nbe transformed into strictly proper losses without requiring knowledge of true\nparameter values, that is, without requiring data labels. The results of our\ninitial experiments show remarkable improvements in the robustness of ABI on\nout-of-simulation data. Even if the observed data is far away from both labeled\nand unlabeled training data, inference remains highly accurate. If our findings\nalso generalize to other scenarios and model classes, we believe that our new\nmethod represents a major breakthrough in neural ABI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural amortized Bayesian inference (ABI) can solve probabilistic inverse\nproblems orders of magnitude faster than classical methods. However, neural ABI\nis not yet sufficiently robust for widespread and safe applicability. In\nparticular, when performing inference on observations outside of the scope of\nthe simulated data seen during training, for example, because of model\nmisspecification, the posterior approximations are likely to become highly\nbiased. Due to the bad pre-asymptotic behavior of current neural posterior\nestimators in the out-of-simulation regime, the resulting estimation biases\ncannot be fixed in acceptable time by just simulating more training data. In\nthis proof-of-concept paper, we propose a semi-supervised approach that enables\ntraining not only on (labeled) simulated data generated from the model, but\nalso on unlabeled data originating from any source, including real-world data.\nTo achieve the latter, we exploit Bayesian self-consistency properties that can\nbe transformed into strictly proper losses without requiring knowledge of true\nparameter values, that is, without requiring data labels. The results of our\ninitial experiments show remarkable improvements in the robustness of ABI on\nout-of-simulation data. Even if the observed data is far away from both labeled\nand unlabeled training data, inference remains highly accurate. If our findings\nalso generalize to other scenarios and model classes, we believe that our new\nmethod represents a major breakthrough in neural ABI."
                },
                "authors": [
                    {
                        "name": "Aayush Mishra"
                    },
                    {
                        "name": "Daniel Habermann"
                    },
                    {
                        "name": "Marvin Schmitt"
                    },
                    {
                        "name": "Stefan T. Radev"
                    },
                    {
                        "name": "Paul-Christian Bürkner"
                    }
                ],
                "author_detail": {
                    "name": "Paul-Christian Bürkner"
                },
                "author": "Paul-Christian Bürkner",
                "arxiv_comment": "added acknowledgements",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13483v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13483v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04396v1",
                "updated": "2025-03-06T12:50:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    50,
                    14,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T12:50:14Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    50,
                    14,
                    3,
                    65,
                    0
                ],
                "title": "TableLoRA: Low-rank Adaptation on Table Structure Understanding for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableLoRA: Low-rank Adaptation on Table Structure Understanding for\n  Large Language Models"
                },
                "summary": "Tabular data are crucial in many fields and their understanding by large\nlanguage models (LLMs) under high parameter efficiency paradigm is important.\nHowever, directly applying parameter-efficient fine-tuning (PEFT) techniques to\ntabular tasks presents significant challenges, particularly in terms of better\ntable serialization and the representation of two-dimensional structured\ninformation within a one-dimensional sequence. To address this, we propose\nTableLoRA, a module designed to improve LLMs' understanding of table structure\nduring PEFT. It incorporates special tokens for serializing tables with special\ntoken encoder and uses 2D LoRA to encode low-rank information on cell\npositions. Experiments on four tabular-related datasets demonstrate that\nTableLoRA consistently outperforms vanilla LoRA and surpasses various table\nencoding methods tested in control experiments. These findings reveal that\nTableLoRA, as a table-specific LoRA, enhances the ability of LLMs to process\ntabular data effectively, especially in low-parameter settings, demonstrating\nits potential as a robust solution for handling table-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data are crucial in many fields and their understanding by large\nlanguage models (LLMs) under high parameter efficiency paradigm is important.\nHowever, directly applying parameter-efficient fine-tuning (PEFT) techniques to\ntabular tasks presents significant challenges, particularly in terms of better\ntable serialization and the representation of two-dimensional structured\ninformation within a one-dimensional sequence. To address this, we propose\nTableLoRA, a module designed to improve LLMs' understanding of table structure\nduring PEFT. It incorporates special tokens for serializing tables with special\ntoken encoder and uses 2D LoRA to encode low-rank information on cell\npositions. Experiments on four tabular-related datasets demonstrate that\nTableLoRA consistently outperforms vanilla LoRA and surpasses various table\nencoding methods tested in control experiments. These findings reveal that\nTableLoRA, as a table-specific LoRA, enhances the ability of LLMs to process\ntabular data effectively, especially in low-parameter settings, demonstrating\nits potential as a robust solution for handling table-related tasks."
                },
                "authors": [
                    {
                        "name": "Xinyi He"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Mengyu Zhou"
                    },
                    {
                        "name": "Yeye He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Shi Han"
                    },
                    {
                        "name": "Zejian Yuan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04395v1",
                "updated": "2025-03-06T12:47:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    47,
                    54,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T12:47:54Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    47,
                    54,
                    3,
                    65,
                    0
                ],
                "title": "Shaping Shared Languages: Human and Large Language Models' Inductive\n  Biases in Emergent Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shaping Shared Languages: Human and Large Language Models' Inductive\n  Biases in Emergent Communication"
                },
                "summary": "Languages are shaped by the inductive biases of their users. Using a\nclassical referential game, we investigate how artificial languages evolve when\noptimised for inductive biases in humans and large language models (LLMs) via\nHuman-Human, LLM-LLM and Human-LLM experiments. We show that referentially\ngrounded vocabularies emerge that enable reliable communication in all\nconditions, even when humans and LLMs collaborate. Comparisons between\nconditions reveal that languages optimised for LLMs subtly differ from those\noptimised for humans. Interestingly, interactions between humans and LLMs\nalleviate these differences and result in vocabularies which are more\nhuman-like than LLM-like. These findings advance our understanding of how\ninductive biases in LLMs play a role in the dynamic nature of human language\nand contribute to maintaining alignment in human and machine communication. In\nparticular, our work underscores the need to think of new methods that include\nhuman interaction in the training processes of LLMs, and shows that using\ncommunicative success as a reward signal can be a fruitful, novel direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Languages are shaped by the inductive biases of their users. Using a\nclassical referential game, we investigate how artificial languages evolve when\noptimised for inductive biases in humans and large language models (LLMs) via\nHuman-Human, LLM-LLM and Human-LLM experiments. We show that referentially\ngrounded vocabularies emerge that enable reliable communication in all\nconditions, even when humans and LLMs collaborate. Comparisons between\nconditions reveal that languages optimised for LLMs subtly differ from those\noptimised for humans. Interestingly, interactions between humans and LLMs\nalleviate these differences and result in vocabularies which are more\nhuman-like than LLM-like. These findings advance our understanding of how\ninductive biases in LLMs play a role in the dynamic nature of human language\nand contribute to maintaining alignment in human and machine communication. In\nparticular, our work underscores the need to think of new methods that include\nhuman interaction in the training processes of LLMs, and shows that using\ncommunicative success as a reward signal can be a fruitful, novel direction."
                },
                "authors": [
                    {
                        "name": "Tom Kouwenhoven"
                    },
                    {
                        "name": "Max Peeperkorn"
                    },
                    {
                        "name": "Roy de Kleijn"
                    },
                    {
                        "name": "Tessa Verhoef"
                    }
                ],
                "author_detail": {
                    "name": "Tessa Verhoef"
                },
                "author": "Tessa Verhoef",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04392v1",
                "updated": "2025-03-06T12:41:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    41,
                    54,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T12:41:54Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    41,
                    54,
                    3,
                    65,
                    0
                ],
                "title": "AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems\n  via Hierarchical Data Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems\n  via Hierarchical Data Management"
                },
                "summary": "Large Language Model based multi-agent systems are revolutionizing autonomous\ncommunication and collaboration, yet they remain vulnerable to security threats\nlike unauthorized access and data breaches. To address this, we introduce\nAgentSafe, a novel framework that enhances MAS security through hierarchical\ninformation management and memory protection. AgentSafe classifies information\nby security levels, restricting sensitive data access to authorized agents.\nAgentSafe incorporates two components: ThreatSieve, which secures communication\nby verifying information authority and preventing impersonation, and\nHierarCache, an adaptive memory management system that defends against\nunauthorized access and malicious poisoning, representing the first systematic\ndefense for agent memory. Experiments across various LLMs show that AgentSafe\nsignificantly boosts system resilience, achieving defense success rates above\n80% under adversarial conditions. Additionally, AgentSafe demonstrates\nscalability, maintaining robust performance as agent numbers and information\ncomplexity grow. Results underscore effectiveness of AgentSafe in securing MAS\nand its potential for real-world application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model based multi-agent systems are revolutionizing autonomous\ncommunication and collaboration, yet they remain vulnerable to security threats\nlike unauthorized access and data breaches. To address this, we introduce\nAgentSafe, a novel framework that enhances MAS security through hierarchical\ninformation management and memory protection. AgentSafe classifies information\nby security levels, restricting sensitive data access to authorized agents.\nAgentSafe incorporates two components: ThreatSieve, which secures communication\nby verifying information authority and preventing impersonation, and\nHierarCache, an adaptive memory management system that defends against\nunauthorized access and malicious poisoning, representing the first systematic\ndefense for agent memory. Experiments across various LLMs show that AgentSafe\nsignificantly boosts system resilience, achieving defense success rates above\n80% under adversarial conditions. Additionally, AgentSafe demonstrates\nscalability, maintaining robust performance as agent numbers and information\ncomplexity grow. Results underscore effectiveness of AgentSafe in securing MAS\nand its potential for real-world application."
                },
                "authors": [
                    {
                        "name": "Junyuan Mao"
                    },
                    {
                        "name": "Fanci Meng"
                    },
                    {
                        "name": "Yifan Duan"
                    },
                    {
                        "name": "Miao Yu"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Yuxuan Liang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21083v2",
                "updated": "2025-03-06T12:38:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    38,
                    42,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-28T14:48:05Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    48,
                    5,
                    0,
                    302,
                    0
                ],
                "title": "Stealthy Jailbreak Attacks on Large Language Models via Benign Data\n  Mirroring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stealthy Jailbreak Attacks on Large Language Models via Benign Data\n  Mirroring"
                },
                "summary": "Large language model (LLM) safety is a critical issue, with numerous studies\nemploying red team testing to enhance model security. Among these, jailbreak\nmethods explore potential vulnerabilities by crafting malicious prompts that\ninduce model outputs contrary to safety alignments. Existing black-box\njailbreak methods often rely on model feedback, repeatedly submitting queries\nwith detectable malicious instructions during the attack search process.\nAlthough these approaches are effective, the attacks may be intercepted by\ncontent moderators during the search process. We propose an improved transfer\nattack method that guides malicious prompt construction by locally training a\nmirror model of the target black-box model through benign data distillation.\nThis method offers enhanced stealth, as it does not involve submitting\nidentifiable malicious instructions to the target model during the search\nphase. Our approach achieved a maximum attack success rate of 92%, or a\nbalanced value of 80% with an average of 1.5 detectable jailbreak queries per\nsample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore\nthe need for more robust defense mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) safety is a critical issue, with numerous studies\nemploying red team testing to enhance model security. Among these, jailbreak\nmethods explore potential vulnerabilities by crafting malicious prompts that\ninduce model outputs contrary to safety alignments. Existing black-box\njailbreak methods often rely on model feedback, repeatedly submitting queries\nwith detectable malicious instructions during the attack search process.\nAlthough these approaches are effective, the attacks may be intercepted by\ncontent moderators during the search process. We propose an improved transfer\nattack method that guides malicious prompt construction by locally training a\nmirror model of the target black-box model through benign data distillation.\nThis method offers enhanced stealth, as it does not involve submitting\nidentifiable malicious instructions to the target model during the search\nphase. Our approach achieved a maximum attack success rate of 92%, or a\nbalanced value of 80% with an average of 1.5 detectable jailbreak queries per\nsample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore\nthe need for more robust defense mechanisms."
                },
                "authors": [
                    {
                        "name": "Honglin Mu"
                    },
                    {
                        "name": "Han He"
                    },
                    {
                        "name": "Yuxin Zhou"
                    },
                    {
                        "name": "Yunlong Feng"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Xiaoming Shi"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Qi Shi"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Accepted by NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04388v1",
                "updated": "2025-03-06T12:38:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    38,
                    17,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T12:38:17Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    38,
                    17,
                    3,
                    65,
                    0
                ],
                "title": "More Documents, Same Length: Isolating the Challenge of Multiple\n  Documents in RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Documents, Same Length: Isolating the Challenge of Multiple\n  Documents in RAG"
                },
                "summary": "Retrieval-augmented generation (RAG) provides LLMs with relevant documents.\nAlthough previous studies noted that retrieving many documents can degrade\nperformance, they did not isolate how the quantity of documents affects\nperformance while controlling for context length. We evaluate various language\nmodels on custom datasets derived from a multi-hop QA task. We keep the context\nlength and position of relevant information constant while varying the number\nof documents, and find that increasing the document count in RAG settings poses\nsignificant challenges for LLMs. Additionally, our results indicate that\nprocessing multiple documents is a separate challenge from handling long\ncontexts. We also make the datasets and code available:\nhttps://github.com/shaharl6000/MoreDocsSameLen .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) provides LLMs with relevant documents.\nAlthough previous studies noted that retrieving many documents can degrade\nperformance, they did not isolate how the quantity of documents affects\nperformance while controlling for context length. We evaluate various language\nmodels on custom datasets derived from a multi-hop QA task. We keep the context\nlength and position of relevant information constant while varying the number\nof documents, and find that increasing the document count in RAG settings poses\nsignificant challenges for LLMs. Additionally, our results indicate that\nprocessing multiple documents is a separate challenge from handling long\ncontexts. We also make the datasets and code available:\nhttps://github.com/shaharl6000/MoreDocsSameLen ."
                },
                "authors": [
                    {
                        "name": "Shahar Levy"
                    },
                    {
                        "name": "Nir Mazor"
                    },
                    {
                        "name": "Lihi Shalmon"
                    },
                    {
                        "name": "Michael Hassid"
                    },
                    {
                        "name": "Gabriel Stanovsky"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Stanovsky"
                },
                "author": "Gabriel Stanovsky",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04381v1",
                "updated": "2025-03-06T12:33:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    33,
                    20,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T12:33:20Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    33,
                    20,
                    3,
                    65,
                    0
                ],
                "title": "TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for\n  LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for\n  LLM-as-a-Judge"
                },
                "summary": "The LLM-as-a-judge paradigm uses large language models (LLMs) for automated\ntext evaluation, where a numerical assessment is assigned by an LLM to the\ninput text following scoring rubrics. Existing methods for LLM-as-a-judge use\ncross-entropy (CE) loss for fine-tuning, which neglects the numeric nature of\nscore prediction. Recent work addresses numerical prediction limitations of LLM\nfine-tuning through regression-aware fine-tuning, which, however, does not\nconsider chain-of-thought (CoT) reasoning for score prediction. In this paper,\nwe introduce TRACT (Two-stage Regression-Aware fine-tuning with CoT), a method\ncombining CoT reasoning with regression-aware training. TRACT consists of two\nstages: first, seed LLM is fine-tuned to generate CoTs, which serve as\nsupervision for the second stage fine-tuning. The training objective of TRACT\ncombines the CE loss for learning the CoT reasoning capabilities, and the\nregression-aware loss for the score prediction. Experiments across four\nLLM-as-a-judge datasets and two LLMs show that TRACT significantly outperforms\nexisting methods. Extensive ablation studies validate the importance of each\ncomponent in TRACT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM-as-a-judge paradigm uses large language models (LLMs) for automated\ntext evaluation, where a numerical assessment is assigned by an LLM to the\ninput text following scoring rubrics. Existing methods for LLM-as-a-judge use\ncross-entropy (CE) loss for fine-tuning, which neglects the numeric nature of\nscore prediction. Recent work addresses numerical prediction limitations of LLM\nfine-tuning through regression-aware fine-tuning, which, however, does not\nconsider chain-of-thought (CoT) reasoning for score prediction. In this paper,\nwe introduce TRACT (Two-stage Regression-Aware fine-tuning with CoT), a method\ncombining CoT reasoning with regression-aware training. TRACT consists of two\nstages: first, seed LLM is fine-tuned to generate CoTs, which serve as\nsupervision for the second stage fine-tuning. The training objective of TRACT\ncombines the CE loss for learning the CoT reasoning capabilities, and the\nregression-aware loss for the score prediction. Experiments across four\nLLM-as-a-judge datasets and two LLMs show that TRACT significantly outperforms\nexisting methods. Extensive ablation studies validate the importance of each\ncomponent in TRACT."
                },
                "authors": [
                    {
                        "name": "Cheng-Han Chiang"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Michal Lukasik"
                    }
                ],
                "author_detail": {
                    "name": "Michal Lukasik"
                },
                "author": "Michal Lukasik",
                "arxiv_comment": "Codes and models are available at https://github.com/d223302/TRACT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.04724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04724v1",
                "updated": "2025-03-06T18:59:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    59,
                    38,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:59:38Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    59,
                    38,
                    3,
                    65,
                    0
                ],
                "title": "LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM"
                },
                "summary": "Recent advancements in speech-to-speech dialogue systems leverage LLMs for\nmultimodal interactions, yet they remain hindered by fine-tuning requirements,\nhigh computational overhead, and text-speech misalignment. Existing\nspeech-enabled LLMs often degrade conversational quality by modifying the LLM,\nthereby compromising its linguistic capabilities. In contrast, we propose\nLLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS\nsystem that generates high-quality speech with low latency, while fully\npreserving the capabilities of the base LLM. Our approach achieves a\nsignificantly lower Word Error Rate compared to speech-enabled LLMs, while\noperating at comparable latency and UTMOS score. By decoupling speech synthesis\nfrom LLM processing via a multi-queue token streaming system, LLMVoX supports\nseamless, infinite-length dialogues. Its plug-and-play design also facilitates\nextension to various tasks with different backbones. Furthermore, LLMVoX\ngeneralizes to new languages with only dataset adaptation, attaining a low\nCharacter Error Rate on an Arabic speech task. Additionally, we have integrated\nLLMVoX with a Vision-Language Model to create an omni-model with speech, text,\nand vision capabilities, without requiring additional multimodal training. Our\ncode base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in speech-to-speech dialogue systems leverage LLMs for\nmultimodal interactions, yet they remain hindered by fine-tuning requirements,\nhigh computational overhead, and text-speech misalignment. Existing\nspeech-enabled LLMs often degrade conversational quality by modifying the LLM,\nthereby compromising its linguistic capabilities. In contrast, we propose\nLLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS\nsystem that generates high-quality speech with low latency, while fully\npreserving the capabilities of the base LLM. Our approach achieves a\nsignificantly lower Word Error Rate compared to speech-enabled LLMs, while\noperating at comparable latency and UTMOS score. By decoupling speech synthesis\nfrom LLM processing via a multi-queue token streaming system, LLMVoX supports\nseamless, infinite-length dialogues. Its plug-and-play design also facilitates\nextension to various tasks with different backbones. Furthermore, LLMVoX\ngeneralizes to new languages with only dataset adaptation, attaining a low\nCharacter Error Rate on an Arabic speech task. Additionally, we have integrated\nLLMVoX with a Vision-Language Model to create an omni-model with speech, text,\nand vision capabilities, without requiring additional multimodal training. Our\ncode base and project page is available at https://mbzuai-oryx.github.io/LLMVoX ."
                },
                "authors": [
                    {
                        "name": "Sambal Shikhar"
                    },
                    {
                        "name": "Mohammed Irfan Kurpath"
                    },
                    {
                        "name": "Sahal Shaji Mullappilly"
                    },
                    {
                        "name": "Jean Lahoud"
                    },
                    {
                        "name": "Fahad Khan"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    }
                ],
                "author_detail": {
                    "name": "Hisham Cholakkal"
                },
                "author": "Hisham Cholakkal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04723v2",
                "updated": "2025-03-07T03:14:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    3,
                    14,
                    2,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-06T18:59:37Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    59,
                    37,
                    3,
                    65,
                    0
                ],
                "title": "Shifting Long-Context LLMs Research from Input to Output",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shifting Long-Context LLMs Research from Input to Output"
                },
                "summary": "Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Zhiqing Hu"
                    },
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Ming Shan Hee"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04722v1",
                "updated": "2025-03-06T18:59:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    59,
                    23,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:59:23Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    59,
                    23,
                    3,
                    65,
                    0
                ],
                "title": "Enough Coin Flips Can Make LLMs Act Bayesian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enough Coin Flips Can Make LLMs Act Bayesian"
                },
                "summary": "Large language models (LLMs) exhibit the ability to generalize given few-shot\nexamples in their input prompt, an emergent capability known as in-context\nlearning (ICL). We investigate whether LLMs utilize ICL to perform structured\nreasoning in ways that are consistent with a Bayesian framework or rely on\npattern matching. Using a controlled setting of biased coin flips, we find\nthat: (1) LLMs often possess biased priors, causing initial divergence in\nzero-shot settings, (2) in-context evidence outweighs explicit bias\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\ndeviations primarily due to miscalibrated priors rather than flawed updates,\nand (4) attention magnitude has negligible effect on Bayesian inference. With\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\npriors in a Bayesian manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit the ability to generalize given few-shot\nexamples in their input prompt, an emergent capability known as in-context\nlearning (ICL). We investigate whether LLMs utilize ICL to perform structured\nreasoning in ways that are consistent with a Bayesian framework or rely on\npattern matching. Using a controlled setting of biased coin flips, we find\nthat: (1) LLMs often possess biased priors, causing initial divergence in\nzero-shot settings, (2) in-context evidence outweighs explicit bias\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\ndeviations primarily due to miscalibrated priors rather than flawed updates,\nand (4) attention magnitude has negligible effect on Bayesian inference. With\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\npriors in a Bayesian manner."
                },
                "authors": [
                    {
                        "name": "Ritwik Gupta"
                    },
                    {
                        "name": "Rodolfo Corona"
                    },
                    {
                        "name": "Jiaxin Ge"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Dan Klein"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "David M. Chan"
                    }
                ],
                "author_detail": {
                    "name": "David M. Chan"
                },
                "author": "David M. Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04715v1",
                "updated": "2025-03-06T18:58:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    58,
                    29,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:58:29Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    58,
                    29,
                    3,
                    65,
                    0
                ],
                "title": "Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large\n  Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large\n  Language Model Pretraining"
                },
                "summary": "The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well-established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Through extensive empirical studies\ninvolving grid searches across diverse configurations, we discover universal\nscaling laws governing these hyperparameters: optimal learning rate follows a\npower-law relationship with both model parameters and data sizes, while optimal\nbatch size scales primarily with data sizes. Our analysis reveals a convex\noptimization landscape for hyperparameters under fixed models and data size\nconditions. This convexity implies an optimal hyperparameter plateau. We\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity. Its estimated values on the test set are merely 0.07\\% away from the\nglobally optimal LLM performance found via an exhaustive search. These laws\ndemonstrate remarkable robustness across variations in model sparsity, training\ndata distribution, and model shape. To our best known, this is the first work\nthat unifies different model shapes and structures, such as Mixture-of-Experts\nmodels and dense transformers, as well as establishes optimal hyperparameter\nscaling laws across diverse data distributions. This exhaustive optimization\nprocess demands substantial computational resources, utilizing nearly one\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\ntotal. To facilitate reproducibility and further research, we will\nprogressively release all loss measurements and model checkpoints through our\ndesignated repository https://step-law.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well-established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Through extensive empirical studies\ninvolving grid searches across diverse configurations, we discover universal\nscaling laws governing these hyperparameters: optimal learning rate follows a\npower-law relationship with both model parameters and data sizes, while optimal\nbatch size scales primarily with data sizes. Our analysis reveals a convex\noptimization landscape for hyperparameters under fixed models and data size\nconditions. This convexity implies an optimal hyperparameter plateau. We\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity. Its estimated values on the test set are merely 0.07\\% away from the\nglobally optimal LLM performance found via an exhaustive search. These laws\ndemonstrate remarkable robustness across variations in model sparsity, training\ndata distribution, and model shape. To our best known, this is the first work\nthat unifies different model shapes and structures, such as Mixture-of-Experts\nmodels and dense transformers, as well as establishes optimal hyperparameter\nscaling laws across diverse data distributions. This exhaustive optimization\nprocess demands substantial computational resources, utilizing nearly one\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\ntotal. To facilitate reproducibility and further research, we will\nprogressively release all loss measurements and model checkpoints through our\ndesignated repository https://step-law.github.io/"
                },
                "authors": [
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Wenzheng Zheng"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Yangshijie Xu"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11807v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11807v7",
                "updated": "2025-03-06T18:58:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    58,
                    23,
                    3,
                    65,
                    0
                ],
                "published": "2024-03-18T14:04:47Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    14,
                    4,
                    47,
                    0,
                    78,
                    0
                ],
                "title": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments"
                },
                "summary": "Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families,\nincluding GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by\nLLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental\nresults are publicly available at https://github.com/CUHK-ARISE/GAMABench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families,\nincluding GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by\nLLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental\nresults are publicly available at https://github.com/CUHK-ARISE/GAMABench."
                },
                "authors": [
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Eric John Li"
                    },
                    {
                        "name": "Man Ho Lam"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_comment": "Accepted to ICLR 2025; 11 pages of main text; 26 pages of appendices;\n  Included models: GPT-3.5-{0613, 1106, 0125}, GPT-4-0125, GPT-4o-0806,\n  Gemini-{1.0, 1.5)-Pro, LLaMA-3.1-{7, 70, 405}B, Mixtral-8x{7, 22}B,\n  Qwen-2-72B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11807v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11807v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04704v1",
                "updated": "2025-03-06T18:54:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    54,
                    32,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:54:32Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    54,
                    32,
                    3,
                    65,
                    0
                ],
                "title": "Universality of Layer-Level Entropy-Weighted Quantization Beyond Model\n  Architecture and Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universality of Layer-Level Entropy-Weighted Quantization Beyond Model\n  Architecture and Size"
                },
                "summary": "We present a novel approach to selective model quantization that transcends\nthe limitations of architecture-specific and size-dependent compression methods\nfor Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By\nanalyzing the entropy distribution across transformer blocks, EWQ determines\nwhich blocks can be safely quantized without causing significant performance\ndegradation, independent of model architecture or size. Our method outperforms\nuniform quantization approaches, maintaining Massive Multitask Language\nUnderstanding (MMLU) accuracy scores within 0.5% of unquantized models while\nreducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ\nacross multiple architectures-from 1.6B to 70B parameters-showcasing consistent\nimprovements in the quality-compression trade-off regardless of model scale or\narchitectural design. A surprising finding of EWQ is its ability to reduce\nperplexity compared to unquantized models, suggesting the presence of\nbeneficial regularization through selective precision reduction. This\nimprovement holds across different model families, indicating a fundamental\nrelationship between layer-level entropy and optimal precision requirements.\nAdditionally, we introduce FastEWQ, a rapid method for entropy distribution\nanalysis that eliminates the need for loading model weights. This technique\nleverages universal characteristics of entropy distribution that persist across\nvarious architectures and scales, enabling near-instantaneous quantization\ndecisions while maintaining 80% classification accuracy with full entropy\nanalysis. Our results demonstrate that effective quantization strategies can be\ndeveloped independently of specific architectural choices or model sizes,\nopening new possibilities for efficient LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to selective model quantization that transcends\nthe limitations of architecture-specific and size-dependent compression methods\nfor Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By\nanalyzing the entropy distribution across transformer blocks, EWQ determines\nwhich blocks can be safely quantized without causing significant performance\ndegradation, independent of model architecture or size. Our method outperforms\nuniform quantization approaches, maintaining Massive Multitask Language\nUnderstanding (MMLU) accuracy scores within 0.5% of unquantized models while\nreducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ\nacross multiple architectures-from 1.6B to 70B parameters-showcasing consistent\nimprovements in the quality-compression trade-off regardless of model scale or\narchitectural design. A surprising finding of EWQ is its ability to reduce\nperplexity compared to unquantized models, suggesting the presence of\nbeneficial regularization through selective precision reduction. This\nimprovement holds across different model families, indicating a fundamental\nrelationship between layer-level entropy and optimal precision requirements.\nAdditionally, we introduce FastEWQ, a rapid method for entropy distribution\nanalysis that eliminates the need for loading model weights. This technique\nleverages universal characteristics of entropy distribution that persist across\nvarious architectures and scales, enabling near-instantaneous quantization\ndecisions while maintaining 80% classification accuracy with full entropy\nanalysis. Our results demonstrate that effective quantization strategies can be\ndeveloped independently of specific architectural choices or model sizes,\nopening new possibilities for efficient LLM deployment."
                },
                "authors": [
                    {
                        "name": "Alireza Behtash"
                    },
                    {
                        "name": "Marijan Fofonjka"
                    },
                    {
                        "name": "Ethan Baird"
                    },
                    {
                        "name": "Tyler Mauer"
                    },
                    {
                        "name": "Hossein Moghimifam"
                    },
                    {
                        "name": "David Stout"
                    },
                    {
                        "name": "Joel Dennison"
                    }
                ],
                "author_detail": {
                    "name": "Joel Dennison"
                },
                "author": "Joel Dennison",
                "arxiv_comment": "29 pages, 7 figures, 14 tables; Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04693v1",
                "updated": "2025-03-06T18:40:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    40,
                    0,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:40:00Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    40,
                    0,
                    3,
                    65,
                    0
                ],
                "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to\n  Forgetting Targets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to\n  Forgetting Targets"
                },
                "summary": "Large Language Models (LLMs) inevitably acquire harmful information during\ntraining on massive datasets. LLM unlearning aims to eliminate the influence of\nsuch harmful information while maintaining the model's overall performance.\nExisting unlearning methods, represented by gradient ascent-based approaches,\nprimarily focus on forgetting target data while overlooking the crucial impact\nof logically related knowledge on the effectiveness of unlearning. In this\npaper, through both theoretical and experimental analyses, we first demonstrate\nthat a key reason for the suboptimal unlearning performance is that models can\nreconstruct the target content through reasoning with logically related\nknowledge. To address this issue, we propose Unlearning Improvement via\nParameter Extrapolation (UIPE), a method that removes knowledge highly\ncorrelated with the forgetting targets. Experimental results show that UIPE\nsignificantly enhances the performance of various mainstream LLM unlearning\nmethods on the TOFU benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) inevitably acquire harmful information during\ntraining on massive datasets. LLM unlearning aims to eliminate the influence of\nsuch harmful information while maintaining the model's overall performance.\nExisting unlearning methods, represented by gradient ascent-based approaches,\nprimarily focus on forgetting target data while overlooking the crucial impact\nof logically related knowledge on the effectiveness of unlearning. In this\npaper, through both theoretical and experimental analyses, we first demonstrate\nthat a key reason for the suboptimal unlearning performance is that models can\nreconstruct the target content through reasoning with logically related\nknowledge. To address this issue, we propose Unlearning Improvement via\nParameter Extrapolation (UIPE), a method that removes knowledge highly\ncorrelated with the forgetting targets. Experimental results show that UIPE\nsignificantly enhances the performance of various mainstream LLM unlearning\nmethods on the TOFU benchmark."
                },
                "authors": [
                    {
                        "name": "Wenyu Wang"
                    },
                    {
                        "name": "Mengqi Zhang"
                    },
                    {
                        "name": "Xiaotian Ye"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Pengjie Ren"
                    }
                ],
                "author_detail": {
                    "name": "Pengjie Ren"
                },
                "author": "Pengjie Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04691v1",
                "updated": "2025-03-06T18:35:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    35,
                    39,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:35:39Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    35,
                    39,
                    3,
                    65,
                    0
                ],
                "title": "Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases"
                },
                "summary": "The latest reasoning-enhanced large language models (reasoning LLMs), such as\nDeepSeek-R1 and OpenAI-o3, have demonstrated remarkable success. However, the\napplication of such reasoning enhancements to the highly professional medical\ndomain has not been clearly evaluated, particularly regarding with not only\nassessing the final generation but also examining the quality of their\nreasoning processes. In this study, we present MedR-Bench, a reasoning-focused\nmedical evaluation benchmark comprising 1,453 structured patient cases with\nreasoning references mined from case reports. Our benchmark spans 13 body\nsystems and 10 specialty disorders, encompassing both common and rare diseases.\nIn our evaluation, we introduce a versatile framework consisting of three\ncritical clinical stages: assessment recommendation, diagnostic\ndecision-making, and treatment planning, comprehensively capturing the LLMs'\nperformance across the entire patient journey in healthcare. For metrics, we\npropose a novel agentic system, Reasoning Evaluator, designed to automate and\nobjectively quantify free-text reasoning responses in a scalable manner from\nthe perspectives of efficiency, factuality, and completeness by dynamically\nsearching and performing cross-referencing checks. As a result, we assess five\nstate-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and\nothers. Our results reveal that current LLMs can handle relatively simple\ndiagnostic tasks with sufficient critical assessment results, achieving\naccuracy generally over 85%. However, they still struggle with more complex\ntasks, such as assessment recommendation and treatment planning. In reasoning,\ntheir reasoning processes are generally reliable, with factuality scores\nexceeding 90%, though they often omit critical reasoning steps. Our study\nclearly reveals further development directions for current clinical LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The latest reasoning-enhanced large language models (reasoning LLMs), such as\nDeepSeek-R1 and OpenAI-o3, have demonstrated remarkable success. However, the\napplication of such reasoning enhancements to the highly professional medical\ndomain has not been clearly evaluated, particularly regarding with not only\nassessing the final generation but also examining the quality of their\nreasoning processes. In this study, we present MedR-Bench, a reasoning-focused\nmedical evaluation benchmark comprising 1,453 structured patient cases with\nreasoning references mined from case reports. Our benchmark spans 13 body\nsystems and 10 specialty disorders, encompassing both common and rare diseases.\nIn our evaluation, we introduce a versatile framework consisting of three\ncritical clinical stages: assessment recommendation, diagnostic\ndecision-making, and treatment planning, comprehensively capturing the LLMs'\nperformance across the entire patient journey in healthcare. For metrics, we\npropose a novel agentic system, Reasoning Evaluator, designed to automate and\nobjectively quantify free-text reasoning responses in a scalable manner from\nthe perspectives of efficiency, factuality, and completeness by dynamically\nsearching and performing cross-referencing checks. As a result, we assess five\nstate-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and\nothers. Our results reveal that current LLMs can handle relatively simple\ndiagnostic tasks with sufficient critical assessment results, achieving\naccuracy generally over 85%. However, they still struggle with more complex\ntasks, such as assessment recommendation and treatment planning. In reasoning,\ntheir reasoning processes are generally reliable, with factuality scores\nexceeding 90%, though they often omit critical reasoning steps. Our study\nclearly reveals further development directions for current clinical LLMs."
                },
                "authors": [
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Shuyu Liu"
                    },
                    {
                        "name": "Weike Zhao"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04690v1",
                "updated": "2025-03-06T18:32:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    32,
                    35,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    32,
                    35,
                    3,
                    65,
                    0
                ],
                "title": "Coarse graining and reduced order models for plume ejection dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse graining and reduced order models for plume ejection dynamics"
                },
                "summary": "Monitoring the atmospheric dispersion of pollutants is increasingly critical\nfor environmental impact assessments. High-fidelity computational models are\noften employed to simulate plume dynamics, guiding decision-making and\nprioritizing resource deployment. However, such models can be prohibitively\nexpensive to simulate, as they require resolving turbulent flows at fine\nspatial and temporal resolutions. Moreover, there are at least two distinct\ndynamical regimes of interest in the plume: (i) the initial ejection of the\nplume where turbulent mixing is generated by the shear-driven Kelvin-Helmholtz\ninstability, and (ii) the ensuing turbulent diffusion and advection which is\noften modeled by the Gaussian plume model. We address the challenge of modeling\nthe initial plume generation. Specifically, we propose a data-driven framework\nthat identifies a reduced-order analytical model for plume dynamics -- directly\nfrom video data. We extract a time series of plume center and edge points from\nvideo snapshots and evaluate different regressions based to their extrapolation\nperformance to generate a time series of coefficients that characterize the\nplume's overall direction and spread. We regress to a sinusoidal model inspired\nby the Kelvin-Helmholtz instability for the edge points in order to identify\nthe plume's dispersion and vorticity. Overall, this reduced-order modeling\nframework provides a data-driven and lightweight approach to capture the\ndominant features of the initial nonlinear point-source plume dynamics,\nagnostic to plume type and starting only from video. The resulting model is a\npre-cursor to standard models such as the Gaussian plume model and has the\npotential to enable rapid assessment and evaluation of critical environmental\nhazards, such as methane leaks, chemical spills, and pollutant dispersal from\nsmokestacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring the atmospheric dispersion of pollutants is increasingly critical\nfor environmental impact assessments. High-fidelity computational models are\noften employed to simulate plume dynamics, guiding decision-making and\nprioritizing resource deployment. However, such models can be prohibitively\nexpensive to simulate, as they require resolving turbulent flows at fine\nspatial and temporal resolutions. Moreover, there are at least two distinct\ndynamical regimes of interest in the plume: (i) the initial ejection of the\nplume where turbulent mixing is generated by the shear-driven Kelvin-Helmholtz\ninstability, and (ii) the ensuing turbulent diffusion and advection which is\noften modeled by the Gaussian plume model. We address the challenge of modeling\nthe initial plume generation. Specifically, we propose a data-driven framework\nthat identifies a reduced-order analytical model for plume dynamics -- directly\nfrom video data. We extract a time series of plume center and edge points from\nvideo snapshots and evaluate different regressions based to their extrapolation\nperformance to generate a time series of coefficients that characterize the\nplume's overall direction and spread. We regress to a sinusoidal model inspired\nby the Kelvin-Helmholtz instability for the edge points in order to identify\nthe plume's dispersion and vorticity. Overall, this reduced-order modeling\nframework provides a data-driven and lightweight approach to capture the\ndominant features of the initial nonlinear point-source plume dynamics,\nagnostic to plume type and starting only from video. The resulting model is a\npre-cursor to standard models such as the Gaussian plume model and has the\npotential to enable rapid assessment and evaluation of critical environmental\nhazards, such as methane leaks, chemical spills, and pollutant dispersal from\nsmokestacks."
                },
                "authors": [
                    {
                        "name": "Ike Griss Salas"
                    },
                    {
                        "name": "Megan R. Ebers"
                    },
                    {
                        "name": "Jake Stevens-Haas"
                    },
                    {
                        "name": "J. Nathan Kutz"
                    }
                ],
                "author_detail": {
                    "name": "J. Nathan Kutz"
                },
                "author": "J. Nathan Kutz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02800v2",
                "updated": "2025-03-06T18:30:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    30,
                    45,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-04T17:20:43Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    20,
                    43,
                    1,
                    63,
                    0
                ],
                "title": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration"
                },
                "summary": "Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7% to 89.1% on the real-world dataset. By allowing for the enriching of\ninput series data with semantics, RAAD-LLM incorporates multimodal capabilities\nthat facilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7% to 89.1% on the real-world dataset. By allowing for the enriching of\ninput series data with semantics, RAAD-LLM incorporates multimodal capabilities\nthat facilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries."
                },
                "authors": [
                    {
                        "name": "Alicia Russell-Gilbert"
                    },
                    {
                        "name": "Sudip Mittal"
                    },
                    {
                        "name": "Shahram Rahimi"
                    },
                    {
                        "name": "Maria Seale"
                    },
                    {
                        "name": "Joseph Jabour"
                    },
                    {
                        "name": "Thomas Arnold"
                    },
                    {
                        "name": "Joshua Church"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Church"
                },
                "author": "Joshua Church",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2411.00914",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "1.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04685v1",
                "updated": "2025-03-06T18:27:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    27,
                    41,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:27:41Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    27,
                    41,
                    3,
                    65,
                    0
                ],
                "title": "DIMSUM: Discourse in Mathematical Reasoning as a Supervision Module",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIMSUM: Discourse in Mathematical Reasoning as a Supervision Module"
                },
                "summary": "We look at reasoning on GSM8k, a dataset of short texts presenting primary\nschool, math problems. We find, with Mirzadeh et al. (2024), that current LLM\nprogress on the data set may not be explained by better reasoning but by\nexposure to a broader pretraining data distribution. We then introduce a novel\ninformation source for helping models with less data or inferior training\nreason better: discourse structure. We show that discourse structure improves\nperformance for models like Llama2 13b by up to 160%. Even for models that have\nmost likely memorized the data set, adding discourse structural information to\nthe model still improves predictions and dramatically improves large model\nperformance on out of distribution examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We look at reasoning on GSM8k, a dataset of short texts presenting primary\nschool, math problems. We find, with Mirzadeh et al. (2024), that current LLM\nprogress on the data set may not be explained by better reasoning but by\nexposure to a broader pretraining data distribution. We then introduce a novel\ninformation source for helping models with less data or inferior training\nreason better: discourse structure. We show that discourse structure improves\nperformance for models like Llama2 13b by up to 160%. Even for models that have\nmost likely memorized the data set, adding discourse structural information to\nthe model still improves predictions and dramatically improves large model\nperformance on out of distribution examples."
                },
                "authors": [
                    {
                        "name": "Krish Sharma"
                    },
                    {
                        "name": "Niyar R Barman"
                    },
                    {
                        "name": "Nicholas Asher"
                    },
                    {
                        "name": "Akshay Chaturvedi"
                    }
                ],
                "author_detail": {
                    "name": "Akshay Chaturvedi"
                },
                "author": "Akshay Chaturvedi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04675v1",
                "updated": "2025-03-06T18:12:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    12,
                    33,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T18:12:33Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    12,
                    33,
                    3,
                    65,
                    0
                ],
                "title": "LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable\n  User Satisfaction Estimation in Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable\n  User Satisfaction Estimation in Dialogue"
                },
                "summary": "Understanding user satisfaction with conversational systems, known as User\nSatisfaction Estimation (USE), is essential for assessing dialogue quality and\nenhancing user experiences. However, existing methods for USE face challenges\ndue to limited understanding of underlying reasons for user dissatisfaction and\nthe high costs of annotating user intentions. To address these challenges, we\npropose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction\nEstimation), an interpretable framework for effective user satisfaction\nprediction. PRAISE operates through three key modules. The Strategy Planner\ndevelops strategies, which are natural language criteria for classifying user\nsatisfaction. The Feature Retriever then incorporates knowledge on user\nsatisfaction from Large Language Models (LLMs) and retrieves relevance features\nfrom utterances. Finally, the Score Analyzer evaluates strategy predictions and\nclassifies user satisfaction. Experimental results demonstrate that PRAISE\nachieves state-of-the-art performance on three benchmarks for the USE task.\nBeyond its superior performance, PRAISE offers additional benefits. It enhances\ninterpretability by providing instance-level explanations through effective\nalignment of utterances with strategies. Moreover, PRAISE operates more\nefficiently than existing approaches by eliminating the need for LLMs during\nthe inference phase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding user satisfaction with conversational systems, known as User\nSatisfaction Estimation (USE), is essential for assessing dialogue quality and\nenhancing user experiences. However, existing methods for USE face challenges\ndue to limited understanding of underlying reasons for user dissatisfaction and\nthe high costs of annotating user intentions. To address these challenges, we\npropose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction\nEstimation), an interpretable framework for effective user satisfaction\nprediction. PRAISE operates through three key modules. The Strategy Planner\ndevelops strategies, which are natural language criteria for classifying user\nsatisfaction. The Feature Retriever then incorporates knowledge on user\nsatisfaction from Large Language Models (LLMs) and retrieves relevance features\nfrom utterances. Finally, the Score Analyzer evaluates strategy predictions and\nclassifies user satisfaction. Experimental results demonstrate that PRAISE\nachieves state-of-the-art performance on three benchmarks for the USE task.\nBeyond its superior performance, PRAISE offers additional benefits. It enhances\ninterpretability by providing instance-level explanations through effective\nalignment of utterances with strategies. Moreover, PRAISE operates more\nefficiently than existing approaches by eliminating the need for LLMs during\nthe inference phase."
                },
                "authors": [
                    {
                        "name": "Sangyeop Kim"
                    },
                    {
                        "name": "Sohhyung Park"
                    },
                    {
                        "name": "Jaewon Jung"
                    },
                    {
                        "name": "Jinseok Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "arxiv_comment": "Accepted by NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02067v2",
                "updated": "2025-03-06T18:09:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    9,
                    38,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-04T07:32:39Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    32,
                    39,
                    1,
                    35,
                    0
                ],
                "title": "AdaptBot: Combining LLM with Knowledge Graphs and Human Input for\n  Generic-to-Specific Task Decomposition and Knowledge Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptBot: Combining LLM with Knowledge Graphs and Human Input for\n  Generic-to-Specific Task Decomposition and Knowledge Refinement"
                },
                "summary": "An embodied agent assisting humans is often asked to complete new tasks, and\nthere may not be sufficient time or labeled examples to train the agent to\nperform these new tasks. Large Language Models (LLMs) trained on considerable\nknowledge across many domains can be used to predict a sequence of abstract\nactions for completing such tasks, although the agent may not be able to\nexecute this sequence due to task-, agent-, or domain-specific constraints. Our\nframework addresses these challenges by leveraging the generic predictions\nprovided by LLM and the prior domain knowledge encoded in a Knowledge Graph\n(KG), enabling an agent to quickly adapt to new tasks. The robot also solicits\nand uses human input as needed to refine its existing knowledge. Based on\nexperimental evaluation in the context of cooking and cleaning tasks in\nsimulation domains, we demonstrate that the interplay between LLM, KG, and\nhuman input leads to substantial performance gains compared with just using the\nLLM. Project website{\\S}: https://sssshivvvv.github.io/adaptbot/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An embodied agent assisting humans is often asked to complete new tasks, and\nthere may not be sufficient time or labeled examples to train the agent to\nperform these new tasks. Large Language Models (LLMs) trained on considerable\nknowledge across many domains can be used to predict a sequence of abstract\nactions for completing such tasks, although the agent may not be able to\nexecute this sequence due to task-, agent-, or domain-specific constraints. Our\nframework addresses these challenges by leveraging the generic predictions\nprovided by LLM and the prior domain knowledge encoded in a Knowledge Graph\n(KG), enabling an agent to quickly adapt to new tasks. The robot also solicits\nand uses human input as needed to refine its existing knowledge. Based on\nexperimental evaluation in the context of cooking and cleaning tasks in\nsimulation domains, we demonstrate that the interplay between LLM, KG, and\nhuman input leads to substantial performance gains compared with just using the\nLLM. Project website{\\S}: https://sssshivvvv.github.io/adaptbot/"
                },
                "authors": [
                    {
                        "name": "Shivam Singh"
                    },
                    {
                        "name": "Karthik Swaminathan"
                    },
                    {
                        "name": "Nabanita Dash"
                    },
                    {
                        "name": "Ramandeep Singh"
                    },
                    {
                        "name": "Snehasis Banerjee"
                    },
                    {
                        "name": "Mohan Sridharan"
                    },
                    {
                        "name": "Madhava Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Madhava Krishna"
                },
                "author": "Madhava Krishna",
                "arxiv_comment": "Accepted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16600v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16600v4",
                "updated": "2025-03-06T17:56:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    56,
                    40,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-23T15:00:53Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    15,
                    0,
                    53,
                    6,
                    54,
                    0
                ],
                "title": "Diagnosing Moral Reasoning Acquisition in Language Models: Pragmatics\n  and Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnosing Moral Reasoning Acquisition in Language Models: Pragmatics\n  and Generalization"
                },
                "summary": "Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs."
                },
                "authors": [
                    {
                        "name": "Guangliang Liu"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Xitong Zhang"
                    },
                    {
                        "name": "Kristen Marie Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Kristen Marie Johnson"
                },
                "author": "Kristen Marie Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16600v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16600v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00799v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00799v6",
                "updated": "2025-03-06T17:43:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    43,
                    10,
                    3,
                    65,
                    0
                ],
                "published": "2024-06-02T16:53:21Z",
                "published_parsed": [
                    2024,
                    6,
                    2,
                    16,
                    53,
                    21,
                    6,
                    154,
                    0
                ],
                "title": "Get my drift? Catching LLM Task Drift with Activation Deltas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Get my drift? Catching LLM Task Drift with Activation Deltas"
                },
                "summary": "LLMs are commonly used in retrieval-augmented applications to execute user\ninstructions based on data from external sources. For example, modern search\nengines use LLMs to answer queries based on relevant search results; email\nplugins summarize emails by processing their content through an LLM. However,\nthe potentially untrusted provenance of these data sources can lead to prompt\ninjection attacks, where the LLM is manipulated by natural language\ninstructions embedded in the external data, causing it to deviate from the\nuser's original instruction(s). We define this deviation as task drift. Task\ndrift is a significant concern as it allows attackers to exfiltrate data or\ninfluence the LLM's output for other users. We study LLM activations as a\nsolution to detect task drift, showing that activation deltas - the difference\nin activations before and after processing external data - are strongly\ncorrelated with this phenomenon. Through two probing methods, we demonstrate\nthat a simple linear classifier can detect drift with near-perfect ROC AUC on\nan out-of-distribution test set. We evaluate these methods by making minimal\nassumptions about how users' tasks, system prompts, and attacks can be phrased.\nWe observe that this approach generalizes surprisingly well to unseen task\ndomains, such as prompt injections, jailbreaks, and malicious instructions,\nwithout being trained on any of these attacks. Interestingly, the fact that\nthis solution does not require any modifications to the LLM (e.g.,\nfine-tuning), as well as its compatibility with existing meta-prompting\nsolutions, makes it cost-efficient and easy to deploy. To encourage further\nresearch on activation-based task inspection, decoding, and interpretability,\nwe release our large-scale TaskTracker toolkit, featuring a dataset of over\n500K instances, representations from six SoTA language models, and a suite of\ninspection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are commonly used in retrieval-augmented applications to execute user\ninstructions based on data from external sources. For example, modern search\nengines use LLMs to answer queries based on relevant search results; email\nplugins summarize emails by processing their content through an LLM. However,\nthe potentially untrusted provenance of these data sources can lead to prompt\ninjection attacks, where the LLM is manipulated by natural language\ninstructions embedded in the external data, causing it to deviate from the\nuser's original instruction(s). We define this deviation as task drift. Task\ndrift is a significant concern as it allows attackers to exfiltrate data or\ninfluence the LLM's output for other users. We study LLM activations as a\nsolution to detect task drift, showing that activation deltas - the difference\nin activations before and after processing external data - are strongly\ncorrelated with this phenomenon. Through two probing methods, we demonstrate\nthat a simple linear classifier can detect drift with near-perfect ROC AUC on\nan out-of-distribution test set. We evaluate these methods by making minimal\nassumptions about how users' tasks, system prompts, and attacks can be phrased.\nWe observe that this approach generalizes surprisingly well to unseen task\ndomains, such as prompt injections, jailbreaks, and malicious instructions,\nwithout being trained on any of these attacks. Interestingly, the fact that\nthis solution does not require any modifications to the LLM (e.g.,\nfine-tuning), as well as its compatibility with existing meta-prompting\nsolutions, makes it cost-efficient and easy to deploy. To encourage further\nresearch on activation-based task inspection, decoding, and interpretability,\nwe release our large-scale TaskTracker toolkit, featuring a dataset of over\n500K instances, representations from six SoTA language models, and a suite of\ninspection tools."
                },
                "authors": [
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Aideen Fay"
                    },
                    {
                        "name": "Giovanni Cherubin"
                    },
                    {
                        "name": "Ahmed Salem"
                    },
                    {
                        "name": "Mario Fritz"
                    },
                    {
                        "name": "Andrew Paverd"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Paverd"
                },
                "author": "Andrew Paverd",
                "arxiv_comment": "SaTML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00799v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00799v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14281v2",
                "updated": "2025-03-06T17:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    38,
                    20,
                    3,
                    65,
                    0
                ],
                "published": "2024-09-22T00:56:42Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    0,
                    56,
                    42,
                    6,
                    266,
                    0
                ],
                "title": "Creative Writers' Attitudes on Writing as Training Data for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative Writers' Attitudes on Writing as Training Data for Large\n  Language Models"
                },
                "summary": "The use of creative writing as training data for large language models (LLMs)\nis highly contentious and many writers have expressed outrage at the use of\ntheir work without consent or compensation. In this paper, we seek to\nunderstand how creative writers reason about the real or hypothetical use of\ntheir writing as training data. We interviewed 33 writers with variation across\ngenre, method of publishing, degree of professionalization, and attitudes\ntoward and engagement with LLMs. We report on core principles that writers\nexpress (support of the creative chain, respect for writers and writing, and\nthe human element of creativity) and how these principles can be at odds with\ntheir realistic expectations of the world (a lack of control, industry-scale\nimpacts, and interpretation of scale). Collectively these findings demonstrate\nthat writers have a nuanced understanding of LLMs and are more concerned with\npower imbalances than the technology itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of creative writing as training data for large language models (LLMs)\nis highly contentious and many writers have expressed outrage at the use of\ntheir work without consent or compensation. In this paper, we seek to\nunderstand how creative writers reason about the real or hypothetical use of\ntheir writing as training data. We interviewed 33 writers with variation across\ngenre, method of publishing, degree of professionalization, and attitudes\ntoward and engagement with LLMs. We report on core principles that writers\nexpress (support of the creative chain, respect for writers and writing, and\nthe human element of creativity) and how these principles can be at odds with\ntheir realistic expectations of the world (a lack of control, industry-scale\nimpacts, and interpretation of scale). Collectively these findings demonstrate\nthat writers have a nuanced understanding of LLMs and are more concerned with\npower imbalances than the technology itself."
                },
                "authors": [
                    {
                        "name": "Katy Ilonka Gero"
                    },
                    {
                        "name": "Meera Desai"
                    },
                    {
                        "name": "Carly Schnitzler"
                    },
                    {
                        "name": "Nayun Eom"
                    },
                    {
                        "name": "Jack Cushman"
                    },
                    {
                        "name": "Elena L. Glassman"
                    }
                ],
                "author_detail": {
                    "name": "Elena L. Glassman"
                },
                "author": "Elena L. Glassman",
                "arxiv_doi": "10.1145/3706598.3713287",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713287",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.14281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "CHI 25",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04647v1",
                "updated": "2025-03-06T17:33:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    33,
                    1,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:33:01Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    33,
                    1,
                    3,
                    65,
                    0
                ],
                "title": "Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference\n  Alignment"
                },
                "summary": "Direct Preference Optimization (DPO) has become a prominent method for\naligning Large Language Models (LLMs) with human preferences. While DPO has\nenabled significant progress in aligning English LLMs, multilingual preference\nalignment is hampered by data scarcity. To address this, we propose a novel\napproach that $\\textit{captures}$ learned preferences from well-aligned English\nmodels by implicit rewards and $\\textit{transfers}$ them to other languages\nthrough iterative training. Specifically, we derive an implicit reward model\nfrom the logits of an English DPO-aligned model and its corresponding reference\nmodel. This reward model is then leveraged to annotate preference relations in\ncross-lingual instruction-following pairs, using English instructions to\nevaluate multilingual responses. The annotated data is subsequently used for\nmultilingual DPO fine-tuning, facilitating preference knowledge transfer from\nEnglish to other languages. Fine-tuning Llama3 for two iterations resulted in a\n12.72% average improvement in Win Rate and a 5.97% increase in Length Control\nWin Rate across all training languages on the X-AlpacaEval leaderboard. Our\nfindings demonstrate that leveraging existing English-aligned models can enable\nefficient and effective multilingual preference alignment, significantly\nreducing the need for extensive multilingual preference data. The code is\navailable at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has become a prominent method for\naligning Large Language Models (LLMs) with human preferences. While DPO has\nenabled significant progress in aligning English LLMs, multilingual preference\nalignment is hampered by data scarcity. To address this, we propose a novel\napproach that $\\textit{captures}$ learned preferences from well-aligned English\nmodels by implicit rewards and $\\textit{transfers}$ them to other languages\nthrough iterative training. Specifically, we derive an implicit reward model\nfrom the logits of an English DPO-aligned model and its corresponding reference\nmodel. This reward model is then leveraged to annotate preference relations in\ncross-lingual instruction-following pairs, using English instructions to\nevaluate multilingual responses. The annotated data is subsequently used for\nmultilingual DPO fine-tuning, facilitating preference knowledge transfer from\nEnglish to other languages. Fine-tuning Llama3 for two iterations resulted in a\n12.72% average improvement in Win Rate and a 5.97% increase in Length Control\nWin Rate across all training languages on the X-AlpacaEval leaderboard. Our\nfindings demonstrate that leveraging existing English-aligned models can enable\nefficient and effective multilingual preference alignment, significantly\nreducing the need for extensive multilingual preference data. The code is\navailable at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding"
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Chengqing Zong"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04644v1",
                "updated": "2025-03-06T17:32:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    32,
                    22,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:32:22Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    32,
                    22,
                    3,
                    65,
                    0
                ],
                "title": "IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in\n  Expert-Domain Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in\n  Expert-Domain Information Retrieval"
                },
                "summary": "We introduce IFIR, the first comprehensive benchmark designed to evaluate\ninstruction-following information retrieval (IR) in expert domains. IFIR\nincludes 2,426 high-quality examples and covers eight subsets across four\nspecialized domains: finance, law, healthcare, and science literature. Each\nsubset addresses one or more domain-specific retrieval tasks, replicating\nreal-world scenarios where customized instructions are critical. IFIR enables a\ndetailed analysis of instruction-following retrieval capabilities by\nincorporating instructions at different levels of complexity. We also propose a\nnovel LLM-based evaluation method to provide a more precise and reliable\nassessment of model performance in following instructions. Through extensive\nexperiments on 15 frontier retrieval models, including those based on LLMs, our\nresults reveal that current models face significant challenges in effectively\nfollowing complex, domain-specific instructions. We further provide in-depth\nanalyses to highlight these limitations, offering valuable insights to guide\nfuture advancements in retriever development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce IFIR, the first comprehensive benchmark designed to evaluate\ninstruction-following information retrieval (IR) in expert domains. IFIR\nincludes 2,426 high-quality examples and covers eight subsets across four\nspecialized domains: finance, law, healthcare, and science literature. Each\nsubset addresses one or more domain-specific retrieval tasks, replicating\nreal-world scenarios where customized instructions are critical. IFIR enables a\ndetailed analysis of instruction-following retrieval capabilities by\nincorporating instructions at different levels of complexity. We also propose a\nnovel LLM-based evaluation method to provide a more precise and reliable\nassessment of model performance in following instructions. Through extensive\nexperiments on 15 frontier retrieval models, including those based on LLMs, our\nresults reveal that current models face significant challenges in effectively\nfollowing complex, domain-specific instructions. We further provide in-depth\nanalyses to highlight these limitations, offering valuable insights to guide\nfuture advancements in retriever development."
                },
                "authors": [
                    {
                        "name": "Tingyu Song"
                    },
                    {
                        "name": "Guo Gan"
                    },
                    {
                        "name": "Mingsheng Shang"
                    },
                    {
                        "name": "Yilun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Zhao"
                },
                "author": "Yilun Zhao",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04636v1",
                "updated": "2025-03-06T17:24:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    24,
                    6,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:24:06Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    24,
                    6,
                    3,
                    65,
                    0
                ],
                "title": "Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models\n  via Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models\n  via Watermarking"
                },
                "summary": "As open-source large language models (LLMs) like Llama3 become more capable,\nit is crucial to develop watermarking techniques to detect their potential\nmisuse. Existing watermarking methods either add watermarks during LLM\ninference, which is unsuitable for open-source LLMs, or primarily target\nclassification LLMs rather than recent generative LLMs. Adapting these\nwatermarks to open-source LLMs for misuse detection remains an open challenge.\nThis work defines two misuse scenarios for open-source LLMs: intellectual\nproperty (IP) violation and LLM Usage Violation. Then, we explore the\napplication of inference-time watermark distillation and backdoor watermarking\nin these contexts. We propose comprehensive evaluation methods to assess the\nimpact of various real-world further fine-tuning scenarios on watermarks and\nthe effect of these watermarks on LLM performance. Our experiments reveal that\nbackdoor watermarking could effectively detect IP Violation, while\ninference-time watermark distillation is applicable in both scenarios but less\nrobust to further fine-tuning and has a more significant impact on LLM\nperformance compared to backdoor watermarking. Exploring more advanced\nwatermarking methods for open-source LLMs to detect their misuse should be an\nimportant future direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As open-source large language models (LLMs) like Llama3 become more capable,\nit is crucial to develop watermarking techniques to detect their potential\nmisuse. Existing watermarking methods either add watermarks during LLM\ninference, which is unsuitable for open-source LLMs, or primarily target\nclassification LLMs rather than recent generative LLMs. Adapting these\nwatermarks to open-source LLMs for misuse detection remains an open challenge.\nThis work defines two misuse scenarios for open-source LLMs: intellectual\nproperty (IP) violation and LLM Usage Violation. Then, we explore the\napplication of inference-time watermark distillation and backdoor watermarking\nin these contexts. We propose comprehensive evaluation methods to assess the\nimpact of various real-world further fine-tuning scenarios on watermarks and\nthe effect of these watermarks on LLM performance. Our experiments reveal that\nbackdoor watermarking could effectively detect IP Violation, while\ninference-time watermark distillation is applicable in both scenarios but less\nrobust to further fine-tuning and has a more significant impact on LLM\nperformance compared to backdoor watermarking. Exploring more advanced\nwatermarking methods for open-source LLMs to detect their misuse should be an\nimportant future direction."
                },
                "authors": [
                    {
                        "name": "Yijie Xu"
                    },
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Lijie Wen"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by the 1st Workshop on GenAI Watermarking, collocated with\n  ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04629v1",
                "updated": "2025-03-06T17:15:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    15,
                    48,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:15:48Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    15,
                    48,
                    3,
                    65,
                    0
                ],
                "title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing"
                },
                "summary": "Survey paper plays a crucial role in scientific research, especially given\nthe rapid growth of research publications. Recently, researchers have begun\nusing LLMs to automate survey generation for better efficiency. However, the\nquality gap between LLM-generated surveys and those written by human remains\nsignificant, particularly in terms of outline quality and citation accuracy. To\nclose these gaps, we introduce SurveyForge, which first generates the outline\nby analyzing the logical structure of human-written outlines and referring to\nthe retrieved domain-related articles. Subsequently, leveraging high-quality\npapers retrieved from memory by our scholar navigation agent, SurveyForge can\nautomatically generate and refine the content of the generated article.\nMoreover, to achieve a comprehensive evaluation, we construct SurveyBench,\nwhich includes 100 human-written survey papers for win-rate comparison and\nassesses AI-generated survey papers across three dimensions: reference,\noutline, and content quality. Experiments demonstrate that SurveyForge can\noutperform previous works such as AutoSurvey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey paper plays a crucial role in scientific research, especially given\nthe rapid growth of research publications. Recently, researchers have begun\nusing LLMs to automate survey generation for better efficiency. However, the\nquality gap between LLM-generated surveys and those written by human remains\nsignificant, particularly in terms of outline quality and citation accuracy. To\nclose these gaps, we introduce SurveyForge, which first generates the outline\nby analyzing the logical structure of human-written outlines and referring to\nthe retrieved domain-related articles. Subsequently, leveraging high-quality\npapers retrieved from memory by our scholar navigation agent, SurveyForge can\nautomatically generate and refine the content of the generated article.\nMoreover, to achieve a comprehensive evaluation, we construct SurveyBench,\nwhich includes 100 human-written survey papers for win-rate comparison and\nassesses AI-generated survey papers across three dimensions: reference,\noutline, and content quality. Experiments demonstrate that SurveyForge can\noutperform previous works such as AutoSurvey."
                },
                "authors": [
                    {
                        "name": "Xiangchao Yan"
                    },
                    {
                        "name": "Shiyang Feng"
                    },
                    {
                        "name": "Jiakang Yuan"
                    },
                    {
                        "name": "Renqiu Xia"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Lei Bai"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bai"
                },
                "author": "Lei Bai",
                "arxiv_comment": "Code and dataset are available for downloading at:\n  https://github.com/Alpha-Innovator/SurveyForge 22 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11919v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11919v3",
                "updated": "2025-03-06T17:12:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    12,
                    48,
                    3,
                    65,
                    0
                ],
                "published": "2024-09-18T12:32:25Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    12,
                    32,
                    25,
                    2,
                    262,
                    0
                ],
                "title": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language\n  Models for Referring Expression Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language\n  Models for Referring Expression Comprehension"
                },
                "summary": "Vision Language Models (VLMs) have demonstrated remarkable capabilities in\nvarious open-vocabulary tasks, yet their zero-shot performance lags behind\ntask-specific fine-tuned models, particularly in complex tasks like Referring\nExpression Comprehension (REC). Fine-tuning usually requires 'white-box' access\nto the model's architecture and weights, which is not always feasible due to\nproprietary or privacy concerns. In this work, we propose LLM-wrapper, a method\nfor 'black-box' adaptation of VLMs for the REC task using Large Language Models\n(LLMs). LLM-wrapper capitalizes on the reasoning abilities of LLMs, improved\nwith a light fine-tuning, to select the most relevant bounding box matching the\nreferring expression, from candidates generated by a zero-shot black-box VLM.\nOur approach offers several advantages: it enables the adaptation of\nclosed-source models without needing access to their internal workings, it is\nversatile as it works with any VLM, it transfers to new VLMs and datasets, and\nit allows for the adaptation of an ensemble of VLMs. We evaluate LLM-wrapper on\nmultiple datasets using different VLMs and LLMs, demonstrating significant\nperformance improvements and highlighting the versatility of our method. While\nLLM-wrapper is not meant to directly compete with standard white-box\nfine-tuning, it offers a practical and effective alternative for black-box VLM\nadaptation. Code and checkpoints are available at\nhttps://github.com/valeoai/LLM_wrapper .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have demonstrated remarkable capabilities in\nvarious open-vocabulary tasks, yet their zero-shot performance lags behind\ntask-specific fine-tuned models, particularly in complex tasks like Referring\nExpression Comprehension (REC). Fine-tuning usually requires 'white-box' access\nto the model's architecture and weights, which is not always feasible due to\nproprietary or privacy concerns. In this work, we propose LLM-wrapper, a method\nfor 'black-box' adaptation of VLMs for the REC task using Large Language Models\n(LLMs). LLM-wrapper capitalizes on the reasoning abilities of LLMs, improved\nwith a light fine-tuning, to select the most relevant bounding box matching the\nreferring expression, from candidates generated by a zero-shot black-box VLM.\nOur approach offers several advantages: it enables the adaptation of\nclosed-source models without needing access to their internal workings, it is\nversatile as it works with any VLM, it transfers to new VLMs and datasets, and\nit allows for the adaptation of an ensemble of VLMs. We evaluate LLM-wrapper on\nmultiple datasets using different VLMs and LLMs, demonstrating significant\nperformance improvements and highlighting the versatility of our method. While\nLLM-wrapper is not meant to directly compete with standard white-box\nfine-tuning, it offers a practical and effective alternative for black-box VLM\nadaptation. Code and checkpoints are available at\nhttps://github.com/valeoai/LLM_wrapper ."
                },
                "authors": [
                    {
                        "name": "Amaia Cardiel"
                    },
                    {
                        "name": "Eloi Zablocki"
                    },
                    {
                        "name": "Elias Ramzi"
                    },
                    {
                        "name": "Oriane Siméoni"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "LLM-wrapper (v3) is published as a conference paper at ICLR 2025. (v1\n  was presented at EVAL-FoMo workshop, ECCV 2024.)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11919v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11919v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04625v1",
                "updated": "2025-03-06T17:11:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    11,
                    51,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:11:51Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    11,
                    51,
                    3,
                    65,
                    0
                ],
                "title": "START: Self-taught Reasoner with Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "START: Self-taught Reasoner with Tools"
                },
                "summary": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have\ndemonstrated remarkable capabilities in complex reasoning tasks through the\nutilization of long Chain-of-thought (CoT). However, these models often suffer\nfrom hallucinations and inefficiencies due to their reliance solely on internal\nreasoning processes. In this paper, we introduce START (Self-Taught Reasoner\nwith Tools), a novel tool-integrated long CoT reasoning LLM that significantly\nenhances reasoning capabilities by leveraging external tools. Through code\nexecution, START is capable of performing complex computations, self-checking,\nexploring diverse methods, and self-debugging, thereby addressing the\nlimitations of LRMs. The core innovation of START lies in its self-learning\nframework, which comprises two key techniques: 1) Hint-infer: We demonstrate\nthat inserting artificially designed hints (e.g., ``Wait, maybe using Python\nhere is a good idea.'') during the inference process of a LRM effectively\nstimulates its ability to utilize external tools without the need for any\ndemonstration data. Hint-infer can also serve as a simple and effective\nsequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning\n(Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and\nmodifying the reasoning trajectories with tool invocation generated by a LRM\nvia Hint-infer, followed by fine-tuning the LRM. Through this framework, we\nhave fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA\n(GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the\ncompetition-level code benchmark (LiveCodeBench), START achieves accuracy rates\nof 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly\noutperforms the base QwQ-32B and achieves performance comparable to the\nstate-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary\nmodel o1-Preview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have\ndemonstrated remarkable capabilities in complex reasoning tasks through the\nutilization of long Chain-of-thought (CoT). However, these models often suffer\nfrom hallucinations and inefficiencies due to their reliance solely on internal\nreasoning processes. In this paper, we introduce START (Self-Taught Reasoner\nwith Tools), a novel tool-integrated long CoT reasoning LLM that significantly\nenhances reasoning capabilities by leveraging external tools. Through code\nexecution, START is capable of performing complex computations, self-checking,\nexploring diverse methods, and self-debugging, thereby addressing the\nlimitations of LRMs. The core innovation of START lies in its self-learning\nframework, which comprises two key techniques: 1) Hint-infer: We demonstrate\nthat inserting artificially designed hints (e.g., ``Wait, maybe using Python\nhere is a good idea.'') during the inference process of a LRM effectively\nstimulates its ability to utilize external tools without the need for any\ndemonstration data. Hint-infer can also serve as a simple and effective\nsequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning\n(Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and\nmodifying the reasoning trajectories with tool invocation generated by a LRM\nvia Hint-infer, followed by fine-tuning the LRM. Through this framework, we\nhave fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA\n(GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the\ncompetition-level code benchmark (LiveCodeBench), START achieves accuracy rates\nof 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly\noutperforms the base QwQ-32B and achieves performance comparable to the\nstate-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary\nmodel o1-Preview."
                },
                "authors": [
                    {
                        "name": "Chengpeng Li"
                    },
                    {
                        "name": "Mingfeng Xue"
                    },
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Dayiheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dayiheng Liu"
                },
                "author": "Dayiheng Liu",
                "arxiv_comment": "38 pages, 5 figures and 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02394v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02394v3",
                "updated": "2025-03-06T17:10:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    10,
                    24,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-04T08:35:01Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    35,
                    1,
                    1,
                    63,
                    0
                ],
                "title": "BHViT: Binarized Hybrid Vision Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BHViT: Binarized Hybrid Vision Transformer"
                },
                "summary": "Model binarization has made significant progress in enabling real-time and\nenergy-efficient computation for convolutional neural networks (CNN), offering\na potential solution to the deployment challenges faced by Vision Transformers\n(ViTs) on edge devices. However, due to the structural differences between CNN\nand Transformer architectures, simply applying binary CNN strategies to the ViT\nmodels will lead to a significant performance drop. To tackle this challenge,\nwe propose BHViT, a binarization-friendly hybrid ViT architecture and its full\nbinarization model with the guidance of three important observations.\nInitially, BHViT utilizes the local information interaction and hierarchical\nfeature aggregation technique from coarse to fine levels to address redundant\ncomputations stemming from excessive tokens. Then, a novel module based on\nshift operations is proposed to enhance the performance of the binary\nMultilayer Perceptron (MLP) module without significantly increasing\ncomputational overhead. In addition, an innovative attention matrix\nbinarization method based on quantization decomposition is proposed to evaluate\nthe token's importance in the binarized attention matrix. Finally, we propose a\nregularization loss to address the inadequate optimization caused by the\nincompatibility between the weight oscillation in the binary layers and the\nAdam Optimizer. Extensive experimental results demonstrate that our proposed\nalgorithm achieves SOTA performance among binary ViT methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model binarization has made significant progress in enabling real-time and\nenergy-efficient computation for convolutional neural networks (CNN), offering\na potential solution to the deployment challenges faced by Vision Transformers\n(ViTs) on edge devices. However, due to the structural differences between CNN\nand Transformer architectures, simply applying binary CNN strategies to the ViT\nmodels will lead to a significant performance drop. To tackle this challenge,\nwe propose BHViT, a binarization-friendly hybrid ViT architecture and its full\nbinarization model with the guidance of three important observations.\nInitially, BHViT utilizes the local information interaction and hierarchical\nfeature aggregation technique from coarse to fine levels to address redundant\ncomputations stemming from excessive tokens. Then, a novel module based on\nshift operations is proposed to enhance the performance of the binary\nMultilayer Perceptron (MLP) module without significantly increasing\ncomputational overhead. In addition, an innovative attention matrix\nbinarization method based on quantization decomposition is proposed to evaluate\nthe token's importance in the binarized attention matrix. Finally, we propose a\nregularization loss to address the inadequate optimization caused by the\nincompatibility between the weight oscillation in the binary layers and the\nAdam Optimizer. Extensive experimental results demonstrate that our proposed\nalgorithm achieves SOTA performance among binary ViT methods."
                },
                "authors": [
                    {
                        "name": "Tian Gao"
                    },
                    {
                        "name": "Zhiyuan Zhang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Huajun Liu"
                    },
                    {
                        "name": "Kaijie Yin"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Hui Kong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Kong"
                },
                "author": "Hui Kong",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02394v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02394v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04619v1",
                "updated": "2025-03-06T17:05:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    5,
                    33,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:05:33Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    5,
                    33,
                    3,
                    65,
                    0
                ],
                "title": "SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming\n  User Sentiment Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming\n  User Sentiment Modeling"
                },
                "summary": "User reviews on e-commerce platforms exhibit dynamic sentiment patterns\ndriven by temporal and contextual factors. Traditional sentiment analysis\nmethods focus on static reviews, failing to capture the evolving temporal\nrelationship between user sentiment rating and textual content. Sentiment\nanalysis on streaming reviews addresses this limitation by modeling and\npredicting the temporal evolution of user sentiments. However, it suffers from\ndata sparsity, manifesting in temporal, spatial, and combined forms. In this\npaper, we introduce SynGraph, a novel framework designed to address data\nsparsity in sentiment analysis on streaming reviews. SynGraph alleviates data\nsparsity by categorizing users into mid-tail, long-tail, and extreme scenarios\nand incorporating LLM-augmented enhancements within a dynamic graph-based\nstructure. Experiments on real-world datasets demonstrate its effectiveness in\naddressing sparsity and improving sentiment modeling in streaming reviews.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User reviews on e-commerce platforms exhibit dynamic sentiment patterns\ndriven by temporal and contextual factors. Traditional sentiment analysis\nmethods focus on static reviews, failing to capture the evolving temporal\nrelationship between user sentiment rating and textual content. Sentiment\nanalysis on streaming reviews addresses this limitation by modeling and\npredicting the temporal evolution of user sentiments. However, it suffers from\ndata sparsity, manifesting in temporal, spatial, and combined forms. In this\npaper, we introduce SynGraph, a novel framework designed to address data\nsparsity in sentiment analysis on streaming reviews. SynGraph alleviates data\nsparsity by categorizing users into mid-tail, long-tail, and extreme scenarios\nand incorporating LLM-augmented enhancements within a dynamic graph-based\nstructure. Experiments on real-world datasets demonstrate its effectiveness in\naddressing sparsity and improving sentiment modeling in streaming reviews."
                },
                "authors": [
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Qiyu Wei"
                    },
                    {
                        "name": "Yingjie Zhu"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Deyu Zhou"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou",
                "arxiv_comment": "18 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04618v1",
                "updated": "2025-03-06T17:03:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    3,
                    17,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T17:03:17Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    17,
                    3,
                    17,
                    3,
                    65,
                    0
                ],
                "title": "Better Process Supervision with Bi-directional Rewarding Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Process Supervision with Bi-directional Rewarding Signals"
                },
                "summary": "Process supervision, i.e., evaluating each step, is critical for complex\nlarge language model (LLM) reasoning and test-time searching with increased\ninference compute. Existing approaches, represented by process reward models\n(PRMs), primarily focus on rewarding signals up to the current step, exhibiting\na one-directional nature and lacking a mechanism to model the distance to the\nfinal target. To address this problem, we draw inspiration from the A*\nalgorithm, which states that an effective supervisory signal should\nsimultaneously consider the incurred cost and the estimated cost for reaching\nthe target. Building on this key insight, we introduce BiRM, a novel process\nsupervision model that not only evaluates the correctness of previous steps but\nalso models the probability of future success. We conduct extensive experiments\non mathematical reasoning tasks and demonstrate that BiRM provides more precise\nevaluations of LLM reasoning steps, achieving an improvement of 3.1% on\nGaokao2023 over PRM under the Best-of-N sampling method. Besides, in\nsearch-based strategies, BiRM provides more comprehensive guidance and\noutperforms ORM by 5.0% and PRM by 3.8% respectively on MATH-500.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process supervision, i.e., evaluating each step, is critical for complex\nlarge language model (LLM) reasoning and test-time searching with increased\ninference compute. Existing approaches, represented by process reward models\n(PRMs), primarily focus on rewarding signals up to the current step, exhibiting\na one-directional nature and lacking a mechanism to model the distance to the\nfinal target. To address this problem, we draw inspiration from the A*\nalgorithm, which states that an effective supervisory signal should\nsimultaneously consider the incurred cost and the estimated cost for reaching\nthe target. Building on this key insight, we introduce BiRM, a novel process\nsupervision model that not only evaluates the correctness of previous steps but\nalso models the probability of future success. We conduct extensive experiments\non mathematical reasoning tasks and demonstrate that BiRM provides more precise\nevaluations of LLM reasoning steps, achieving an improvement of 3.1% on\nGaokao2023 over PRM under the Best-of-N sampling method. Besides, in\nsearch-based strategies, BiRM provides more comprehensive guidance and\noutperforms ORM by 5.0% and PRM by 3.8% respectively on MATH-500."
                },
                "authors": [
                    {
                        "name": "Wenxiang Chen"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Honglin Guo"
                    },
                    {
                        "name": "Boyang Hong"
                    },
                    {
                        "name": "Jiazheng Zhang"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Nijun Li"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04615v1",
                "updated": "2025-03-06T16:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    59,
                    18,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T16:59:18Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    59,
                    18,
                    3,
                    65,
                    0
                ],
                "title": "HalluCounter: Reference-free LLM Hallucination Detection in the Wild!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HalluCounter: Reference-free LLM Hallucination Detection in the Wild!"
                },
                "summary": "Response consistency-based, reference-free hallucination detection (RFHD)\nmethods do not depend on internal model states, such as generation\nprobabilities or gradients, which Grey-box models typically rely on but are\ninaccessible in closed-source LLMs. However, their inability to capture\nquery-response alignment patterns often results in lower detection accuracy.\nAdditionally, the lack of large-scale benchmark datasets spanning diverse\ndomains remains a challenge, as most existing datasets are limited in size and\nscope. To this end, we propose HalluCounter, a novel reference-free\nhallucination detection method that utilizes both response-response and\nquery-response consistency and alignment patterns. This enables the training of\na classifier that detects hallucinations and provides a confidence score and an\noptimal response for user queries. Furthermore, we introduce HalluCounterEval,\na benchmark dataset comprising both synthetically generated and human-curated\nsamples across multiple domains. Our method outperforms state-of-the-art\napproaches by a significant margin, achieving over 90\\% average confidence in\nhallucination detection across datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Response consistency-based, reference-free hallucination detection (RFHD)\nmethods do not depend on internal model states, such as generation\nprobabilities or gradients, which Grey-box models typically rely on but are\ninaccessible in closed-source LLMs. However, their inability to capture\nquery-response alignment patterns often results in lower detection accuracy.\nAdditionally, the lack of large-scale benchmark datasets spanning diverse\ndomains remains a challenge, as most existing datasets are limited in size and\nscope. To this end, we propose HalluCounter, a novel reference-free\nhallucination detection method that utilizes both response-response and\nquery-response consistency and alignment patterns. This enables the training of\na classifier that detects hallucinations and provides a confidence score and an\noptimal response for user queries. Furthermore, we introduce HalluCounterEval,\na benchmark dataset comprising both synthetically generated and human-curated\nsamples across multiple domains. Our method outperforms state-of-the-art\napproaches by a significant margin, achieving over 90\\% average confidence in\nhallucination detection across datasets."
                },
                "authors": [
                    {
                        "name": "Ashok Urlana"
                    },
                    {
                        "name": "Gopichand Kanumolu"
                    },
                    {
                        "name": "Charaka Vinayak Kumar"
                    },
                    {
                        "name": "Bala Mallikarjunarao Garlapati"
                    },
                    {
                        "name": "Rahul Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Mishra"
                },
                "author": "Rahul Mishra",
                "arxiv_comment": "30 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04611v1",
                "updated": "2025-03-06T16:57:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    57,
                    26,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T16:57:26Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    57,
                    26,
                    3,
                    65,
                    0
                ],
                "title": "Towards Data-Efficient Language Models: A Child-Inspired Approach to\n  Language Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Data-Efficient Language Models: A Child-Inspired Approach to\n  Language Learning"
                },
                "summary": "In this work, we explain our approach employed in the BabyLM Challenge, which\nuses various methods of training language models (LMs) with significantly less\ndata compared to traditional large language models (LLMs) and are inspired by\nhow human children learn. While a human child is exposed to far less linguistic\ninput than an LLM, they still achieve remarkable language understanding and\ngeneration abilities. To this end, we develop a model trained on a curated\ndataset consisting of 10 million words, primarily sourced from child-directed\ntranscripts. The 2024 BabyLM Challenge initial dataset of 10M words is filtered\nto 8.5M. Next, it is supplemented with a randomly selected subset of TVR\ndataset consisting of 1.5M words of television dialogues. The latter dataset\nensures that similar to children, the model is also exposed to language through\nmedia. Furthermore, we reduce the vocabulary size to 32,000 tokens, aligning it\nwith the limited vocabulary of children in the early stages of language\nacquisition. We use curriculum learning and is able to match the baseline on\ncertain benchmarks while surpassing the baseline on others. Additionally,\nincorporating common LLM training datasets, such as MADLAD-400, degrades\nperformance. These findings underscore the importance of dataset selection,\nvocabulary scaling, and curriculum learning in creating more data-efficient\nlanguage models that better mimic human learning processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explain our approach employed in the BabyLM Challenge, which\nuses various methods of training language models (LMs) with significantly less\ndata compared to traditional large language models (LLMs) and are inspired by\nhow human children learn. While a human child is exposed to far less linguistic\ninput than an LLM, they still achieve remarkable language understanding and\ngeneration abilities. To this end, we develop a model trained on a curated\ndataset consisting of 10 million words, primarily sourced from child-directed\ntranscripts. The 2024 BabyLM Challenge initial dataset of 10M words is filtered\nto 8.5M. Next, it is supplemented with a randomly selected subset of TVR\ndataset consisting of 1.5M words of television dialogues. The latter dataset\nensures that similar to children, the model is also exposed to language through\nmedia. Furthermore, we reduce the vocabulary size to 32,000 tokens, aligning it\nwith the limited vocabulary of children in the early stages of language\nacquisition. We use curriculum learning and is able to match the baseline on\ncertain benchmarks while surpassing the baseline on others. Additionally,\nincorporating common LLM training datasets, such as MADLAD-400, degrades\nperformance. These findings underscore the importance of dataset selection,\nvocabulary scaling, and curriculum learning in creating more data-efficient\nlanguage models that better mimic human learning processes."
                },
                "authors": [
                    {
                        "name": "Mohammad Amin Ghanizadeh"
                    },
                    {
                        "name": "Mohammad Javad Dousti"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Javad Dousti"
                },
                "author": "Mohammad Javad Dousti",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04598v1",
                "updated": "2025-03-06T16:40:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    40,
                    48,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T16:40:48Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    40,
                    48,
                    3,
                    65,
                    0
                ],
                "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization"
                },
                "summary": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the location of layer normalization. While\nPre-Norm structures facilitate easier training due to their more prominent\nidentity path, they often yield suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a straightforward yet\neffective hybrid normalization strategy that integrates the advantages of both\nPre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV\nnormalization within the attention mechanism and Post-Norm in the feed-forward\nnetwork (FFN) of each transformer block. This design not only stabilizes\ntraining but also enhances performance, particularly in the context of LLMs.\nComprehensive experiments in both dense and sparse architectures show that\nHybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches,\nachieving state-of-the-art results across various benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. %Code\nwill be made publicly available. Code is available at\nhttps://github.com/BryceZhuo/HybridNorm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the location of layer normalization. While\nPre-Norm structures facilitate easier training due to their more prominent\nidentity path, they often yield suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a straightforward yet\neffective hybrid normalization strategy that integrates the advantages of both\nPre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV\nnormalization within the attention mechanism and Post-Norm in the feed-forward\nnetwork (FFN) of each transformer block. This design not only stabilizes\ntraining but also enhances performance, particularly in the context of LLMs.\nComprehensive experiments in both dense and sparse architectures show that\nHybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches,\nachieving state-of-the-art results across various benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. %Code\nwill be made publicly available. Code is available at\nhttps://github.com/BryceZhuo/HybridNorm."
                },
                "authors": [
                    {
                        "name": "Zhijian Zhuo"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Xiaoqing Li"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Jinwen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jinwen Ma"
                },
                "author": "Jinwen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04596v1",
                "updated": "2025-03-06T16:38:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    38,
                    23,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T16:38:23Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    38,
                    23,
                    3,
                    65,
                    0
                ],
                "title": "The Next Frontier of LLM Applications: Open Ecosystems and Hardware\n  Synergy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Next Frontier of LLM Applications: Open Ecosystems and Hardware\n  Synergy"
                },
                "summary": "Large Language Model (LLM) applications, including LLM app stores and\nautonomous agents, are shaping the future of AI ecosystems. However, platform\nsilos, fragmented hardware integration, and the absence of standardized\ninterfaces limit scalability, interoperability, and resource efficiency. While\nLLM app stores democratize AI, their closed ecosystems restrict modular AI\nreuse and cross-platform portability. Meanwhile, agent-based frameworks offer\nflexibility but often lack seamless integration across diverse environments.\nThis paper envisions the future of LLM applications and proposes a three-layer\ndecoupled architecture grounded in software engineering principles such as\nlayered system design, service-oriented architectures, and hardware-software\nco-design. This architecture separates application logic, communication\nprotocols, and hardware execution, enhancing modularity, efficiency, and\ncross-platform compatibility. Beyond architecture, we highlight key security\nand privacy challenges for safe, scalable AI deployment and outline research\ndirections in software and security engineering. This vision aims to foster\nopen, secure, and interoperable LLM ecosystems, guiding future advancements in\nAI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) applications, including LLM app stores and\nautonomous agents, are shaping the future of AI ecosystems. However, platform\nsilos, fragmented hardware integration, and the absence of standardized\ninterfaces limit scalability, interoperability, and resource efficiency. While\nLLM app stores democratize AI, their closed ecosystems restrict modular AI\nreuse and cross-platform portability. Meanwhile, agent-based frameworks offer\nflexibility but often lack seamless integration across diverse environments.\nThis paper envisions the future of LLM applications and proposes a three-layer\ndecoupled architecture grounded in software engineering principles such as\nlayered system design, service-oriented architectures, and hardware-software\nco-design. This architecture separates application logic, communication\nprotocols, and hardware execution, enhancing modularity, efficiency, and\ncross-platform compatibility. Beyond architecture, we highlight key security\nand privacy challenges for safe, scalable AI deployment and outline research\ndirections in software and security engineering. This vision aims to foster\nopen, secure, and interoperable LLM ecosystems, guiding future advancements in\nAI applications."
                },
                "authors": [
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04592v1",
                "updated": "2025-03-06T16:31:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    31,
                    34,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T16:31:34Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    31,
                    34,
                    3,
                    65,
                    0
                ],
                "title": "A Benchmark for Multi-Lingual Vision-Language Learning in Remote Sensing\n  Image Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Benchmark for Multi-Lingual Vision-Language Learning in Remote Sensing\n  Image Captioning"
                },
                "summary": "Remote Sensing Image Captioning (RSIC) is a cross-modal field bridging vision\nand language, aimed at automatically generating natural language descriptions\nof features and scenes in remote sensing imagery. Despite significant advances\nin developing sophisticated methods and large-scale datasets for training\nvision-language models (VLMs), two critical challenges persist: the scarcity of\nnon-English descriptive datasets and the lack of multilingual capability\nevaluation for models. These limitations fundamentally impede the progress and\npractical deployment of RSIC, particularly in the era of large VLMs. To address\nthese challenges, this paper presents several significant contributions to the\nfield. First, we introduce and analyze BRSIC (Bilingual Remote Sensing Image\nCaptioning), a comprehensive bilingual dataset that enriches three established\nEnglish RSIC datasets with Chinese descriptions, encompassing 13,634 images\npaired with 68,170 bilingual captions. Building upon this foundation, we\ndevelop a systematic evaluation framework that addresses the prevalent\ninconsistency in evaluation protocols, enabling rigorous assessment of model\nperformance through standardized retraining procedures on BRSIC. Furthermore,\nwe present an extensive empirical study of eight state-of-the-art large\nvision-language models (LVLMs), examining their capabilities across multiple\nparadigms including zero-shot inference, supervised fine-tuning, and\nmulti-lingual training. This comprehensive evaluation provides crucial insights\ninto the strengths and limitations of current LVLMs in handling multilingual\nremote sensing tasks. Additionally, our cross-dataset transfer experiments\nreveal interesting findings. The code and data will be available at\nhttps://github.com/mrazhou/BRSIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote Sensing Image Captioning (RSIC) is a cross-modal field bridging vision\nand language, aimed at automatically generating natural language descriptions\nof features and scenes in remote sensing imagery. Despite significant advances\nin developing sophisticated methods and large-scale datasets for training\nvision-language models (VLMs), two critical challenges persist: the scarcity of\nnon-English descriptive datasets and the lack of multilingual capability\nevaluation for models. These limitations fundamentally impede the progress and\npractical deployment of RSIC, particularly in the era of large VLMs. To address\nthese challenges, this paper presents several significant contributions to the\nfield. First, we introduce and analyze BRSIC (Bilingual Remote Sensing Image\nCaptioning), a comprehensive bilingual dataset that enriches three established\nEnglish RSIC datasets with Chinese descriptions, encompassing 13,634 images\npaired with 68,170 bilingual captions. Building upon this foundation, we\ndevelop a systematic evaluation framework that addresses the prevalent\ninconsistency in evaluation protocols, enabling rigorous assessment of model\nperformance through standardized retraining procedures on BRSIC. Furthermore,\nwe present an extensive empirical study of eight state-of-the-art large\nvision-language models (LVLMs), examining their capabilities across multiple\nparadigms including zero-shot inference, supervised fine-tuning, and\nmulti-lingual training. This comprehensive evaluation provides crucial insights\ninto the strengths and limitations of current LVLMs in handling multilingual\nremote sensing tasks. Additionally, our cross-dataset transfer experiments\nreveal interesting findings. The code and data will be available at\nhttps://github.com/mrazhou/BRSIC."
                },
                "authors": [
                    {
                        "name": "Qing Zhou"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Junyu Gao"
                    },
                    {
                        "name": "Weiping Ni"
                    },
                    {
                        "name": "Junzheng Wu"
                    },
                    {
                        "name": "Qi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Wang"
                },
                "author": "Qi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00053v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00053v3",
                "updated": "2025-03-06T16:28:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    28,
                    55,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-30T19:09:02Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    19,
                    9,
                    2,
                    2,
                    304,
                    0
                ],
                "title": "ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration"
                },
                "summary": "Large language models (LLMs) have demonstrated a remarkable ability to serve\nas general-purpose tools for various language-based tasks. Recent works have\ndemonstrated that the efficacy of such models can be improved through iterative\ndialog between multiple models. While these paradigms show promise in improving\nmodel efficacy, most works in this area treat collaboration as an emergent\nbehavior, rather than a learned behavior. In doing so, current multi-agent\nframeworks rely on collaborative behaviors to have been sufficiently trained\ninto off-the-shelf models. To address this limitation, we propose ACC-Collab,\nan Actor-Critic based learning framework to produce a two-agent team (an\nactor-agent and a critic-agent) specialized in collaboration. We demonstrate\nthat ACC-Collab outperforms SotA multi-agent techniques on a wide array of\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated a remarkable ability to serve\nas general-purpose tools for various language-based tasks. Recent works have\ndemonstrated that the efficacy of such models can be improved through iterative\ndialog between multiple models. While these paradigms show promise in improving\nmodel efficacy, most works in this area treat collaboration as an emergent\nbehavior, rather than a learned behavior. In doing so, current multi-agent\nframeworks rely on collaborative behaviors to have been sufficiently trained\ninto off-the-shelf models. To address this limitation, we propose ACC-Collab,\nan Actor-Critic based learning framework to produce a two-agent team (an\nactor-agent and a critic-agent) specialized in collaboration. We demonstrate\nthat ACC-Collab outperforms SotA multi-agent techniques on a wide array of\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Andrew Estornell"
                    },
                    {
                        "name": "Jean-Francois Ton"
                    },
                    {
                        "name": "Yuanshun Yao"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00053v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00053v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01966v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01966v2",
                "updated": "2025-03-06T16:26:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    26,
                    41,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-04T03:25:01Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    25,
                    1,
                    1,
                    35,
                    0
                ],
                "title": "Optimizing Spot Instance Reliability and Security Using Cloud-Native\n  Data and Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Spot Instance Reliability and Security Using Cloud-Native\n  Data and Tools"
                },
                "summary": "This paper represents \"Cloudlab\", a comprehensive, cloud - native laboratory\ndesigned to support network security research and training. Built on Google\nCloud and adhering to GitOps methodologies, Cloudlab facilitates the the\ncreation, testing, and deployment of secure, containerized workloads using\nKubernetes and serverless architectures. The lab integrates tools like Palo\nAlto Networks firewalls, Bridgecrew for \"Security as Code,\" and automated\nGitHub workflows to establish a robust Continuous Integration/Continuous\nMachine Learning pipeline. By providing an adaptive and scalable environment,\nCloudlab supports advanced security concepts such as role-based access control,\nPolicy as Code, and container security. This initiative enables data scientists\nand engineers to explore cutting-edge practices in a dynamic cloud-native\necosystem, fostering innovation and improving operational resilience in modern\nIT infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper represents \"Cloudlab\", a comprehensive, cloud - native laboratory\ndesigned to support network security research and training. Built on Google\nCloud and adhering to GitOps methodologies, Cloudlab facilitates the the\ncreation, testing, and deployment of secure, containerized workloads using\nKubernetes and serverless architectures. The lab integrates tools like Palo\nAlto Networks firewalls, Bridgecrew for \"Security as Code,\" and automated\nGitHub workflows to establish a robust Continuous Integration/Continuous\nMachine Learning pipeline. By providing an adaptive and scalable environment,\nCloudlab supports advanced security concepts such as role-based access control,\nPolicy as Code, and container security. This initiative enables data scientists\nand engineers to explore cutting-edge practices in a dynamic cloud-native\necosystem, fostering innovation and improving operational resilience in modern\nIT infrastructures."
                },
                "authors": [
                    {
                        "name": "Muhammad Saqib"
                    },
                    {
                        "name": "Shubham Malhotra"
                    },
                    {
                        "name": "Dipkumar Mehta"
                    },
                    {
                        "name": "Jagdish Jangid"
                    },
                    {
                        "name": "Fnu Yashu"
                    },
                    {
                        "name": "Sachin Dixit"
                    }
                ],
                "author_detail": {
                    "name": "Sachin Dixit"
                },
                "author": "Sachin Dixit",
                "arxiv_doi": "10.52783/jisem.v10i14s.2387",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.52783/jisem.v10i14s.2387",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.01966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01966v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18405v2",
                "updated": "2025-03-06T16:21:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    21,
                    2,
                    3,
                    65,
                    0
                ],
                "published": "2024-09-27T02:42:55Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    2,
                    42,
                    55,
                    4,
                    271,
                    0
                ],
                "title": "Word2Wave: Language Driven Mission Programming for Efficient Subsea\n  Deployments of Marine Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Word2Wave: Language Driven Mission Programming for Efficient Subsea\n  Deployments of Marine Robots"
                },
                "summary": "This paper explores the design and development of a language-based interface\nfor dynamic mission programming of autonomous underwater vehicles (AUVs). The\nproposed `Word2Wave' (W2W) framework enables interactive programming and\nparameter configuration of AUVs for remote subsea missions. The W2W framework\nincludes: (i) a set of novel language rules and command structures for\nefficient language-to-mission mapping; (ii) a GPT-based prompt engineering\nmodule for training data generation; (iii) a small language model (SLM)-based\nsequence-to-sequence learning pipeline for mission command generation from\nhuman speech or text; and (iv) a novel user interface for 2D mission map\nvisualization and human-machine interfacing. The proposed learning pipeline\nadapts an SLM named T5-Small that can learn language-to-mission mapping from\nprocessed language data effectively, providing robust and efficient\nperformance. In addition to a benchmark evaluation with state-of-the-art, we\nconduct a user interaction study to demonstrate the effectiveness of W2W over\ncommercial AUV programming interfaces. Across participants, W2W-based\nprogramming required less than 10\\% time for mission programming compared to\ntraditional interfaces; it is deemed to be a simpler and more natural paradigm\nfor subsea mission programming with a usability score of 76.25. W2W opens up\npromising future research opportunities on hands-free AUV mission programming\nfor efficient subsea deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the design and development of a language-based interface\nfor dynamic mission programming of autonomous underwater vehicles (AUVs). The\nproposed `Word2Wave' (W2W) framework enables interactive programming and\nparameter configuration of AUVs for remote subsea missions. The W2W framework\nincludes: (i) a set of novel language rules and command structures for\nefficient language-to-mission mapping; (ii) a GPT-based prompt engineering\nmodule for training data generation; (iii) a small language model (SLM)-based\nsequence-to-sequence learning pipeline for mission command generation from\nhuman speech or text; and (iv) a novel user interface for 2D mission map\nvisualization and human-machine interfacing. The proposed learning pipeline\nadapts an SLM named T5-Small that can learn language-to-mission mapping from\nprocessed language data effectively, providing robust and efficient\nperformance. In addition to a benchmark evaluation with state-of-the-art, we\nconduct a user interaction study to demonstrate the effectiveness of W2W over\ncommercial AUV programming interfaces. Across participants, W2W-based\nprogramming required less than 10\\% time for mission programming compared to\ntraditional interfaces; it is deemed to be a simpler and more natural paradigm\nfor subsea mission programming with a usability score of 76.25. W2W opens up\npromising future research opportunities on hands-free AUV mission programming\nfor efficient subsea deployments."
                },
                "authors": [
                    {
                        "name": "Ruo Chen"
                    },
                    {
                        "name": "David Blow"
                    },
                    {
                        "name": "Adnan Abdullah"
                    },
                    {
                        "name": "Md Jahidul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Md Jahidul Islam"
                },
                "author": "Md Jahidul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02972v2",
                "updated": "2025-03-06T16:16:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    16,
                    7,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-04T19:57:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    57,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic\n  Templatisation and Orthographic Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic\n  Templatisation and Orthographic Obfuscation"
                },
                "summary": "Assessing the reasoning capabilities of large language models (LLMs) is\nsusceptible to overestimation due to data exposure of evaluation benchmarks. We\nintroduce a framework for producing linguistic reasoning problems that reduces\nthe effect of memorisation in model performance estimates and apply this\nframework to develop LINGOLY-TOO, a challenging benchmark for linguistic\nreasoning. By developing orthographic templates, we dynamically obfuscate the\nwriting systems of real languages to generate numerousquestion variations.\nThese variations preserve the reasoning steps required for each solution while\nreducing the likelihood of specific problem instances appearing in model\ntraining data. Our experiments demonstrate that frontier models, including\nClaud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning.\nOur analysis also shows that LLMs exhibit noticeable variance in accuracy\nacross permutations of the same problem, and on average perform better on\nquestions appearing in their original orthography. Our findings highlight the\nopaque nature of response generation in LLMs and provide evidence that prior\ndata exposure contributes to over estimating the reasoning capabilities of\nfrontier models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the reasoning capabilities of large language models (LLMs) is\nsusceptible to overestimation due to data exposure of evaluation benchmarks. We\nintroduce a framework for producing linguistic reasoning problems that reduces\nthe effect of memorisation in model performance estimates and apply this\nframework to develop LINGOLY-TOO, a challenging benchmark for linguistic\nreasoning. By developing orthographic templates, we dynamically obfuscate the\nwriting systems of real languages to generate numerousquestion variations.\nThese variations preserve the reasoning steps required for each solution while\nreducing the likelihood of specific problem instances appearing in model\ntraining data. Our experiments demonstrate that frontier models, including\nClaud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning.\nOur analysis also shows that LLMs exhibit noticeable variance in accuracy\nacross permutations of the same problem, and on average perform better on\nquestions appearing in their original orthography. Our findings highlight the\nopaque nature of response generation in LLMs and provide evidence that prior\ndata exposure contributes to over estimating the reasoning capabilities of\nfrontier models."
                },
                "authors": [
                    {
                        "name": "Jude Khouja"
                    },
                    {
                        "name": "Karolina Korgul"
                    },
                    {
                        "name": "Simi Hellsten"
                    },
                    {
                        "name": "Lingyi Yang"
                    },
                    {
                        "name": "Vlad Neacs"
                    },
                    {
                        "name": "Harry Mayne"
                    },
                    {
                        "name": "Ryan Kearns"
                    },
                    {
                        "name": "Andrew Bean"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17504v2",
                "updated": "2025-03-06T16:14:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    14,
                    45,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-21T19:22:10Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    19,
                    22,
                    10,
                    4,
                    52,
                    0
                ],
                "title": "Protein Large Language Models: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein Large Language Models: A Comprehensive Survey"
                },
                "summary": "Protein-specific large language models (Protein LLMs) are revolutionizing\nprotein science by enabling more efficient protein structure prediction,\nfunction annotation, and design. While existing surveys focus on specific\naspects or applications, this work provides the first comprehensive overview of\nProtein LLMs, covering their architectures, training datasets, evaluation\nmetrics, and diverse applications. Through a systematic analysis of over 100\narticles, we propose a structured taxonomy of state-of-the-art Protein LLMs,\nanalyze how they leverage large-scale protein sequence data for improved\naccuracy, and explore their potential in advancing protein engineering and\nbiomedical research. Additionally, we discuss key challenges and future\ndirections, positioning Protein LLMs as essential tools for scientific\ndiscovery in protein science. Resources are maintained at\nhttps://github.com/Yijia-Xiao/Protein-LLM-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein-specific large language models (Protein LLMs) are revolutionizing\nprotein science by enabling more efficient protein structure prediction,\nfunction annotation, and design. While existing surveys focus on specific\naspects or applications, this work provides the first comprehensive overview of\nProtein LLMs, covering their architectures, training datasets, evaluation\nmetrics, and diverse applications. Through a systematic analysis of over 100\narticles, we propose a structured taxonomy of state-of-the-art Protein LLMs,\nanalyze how they leverage large-scale protein sequence data for improved\naccuracy, and explore their potential in advancing protein engineering and\nbiomedical research. Additionally, we discuss key challenges and future\ndirections, positioning Protein LLMs as essential tools for scientific\ndiscovery in protein science. Resources are maintained at\nhttps://github.com/Yijia-Xiao/Protein-LLM-Survey."
                },
                "authors": [
                    {
                        "name": "Yijia Xiao"
                    },
                    {
                        "name": "Wanjia Zhao"
                    },
                    {
                        "name": "Junkai Zhang"
                    },
                    {
                        "name": "Yiqiao Jin"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Zhicheng Ren"
                    },
                    {
                        "name": "Renliang Sun"
                    },
                    {
                        "name": "Haixin Wang"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Pan Lu"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Yizhou Sun"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "24 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12464v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12464v9",
                "updated": "2025-03-06T16:13:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    13,
                    4,
                    3,
                    65,
                    0
                ],
                "published": "2024-04-18T18:48:50Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    18,
                    48,
                    50,
                    3,
                    109,
                    0
                ],
                "title": "NormAd: A Framework for Measuring the Cultural Adaptability of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NormAd: A Framework for Measuring the Cultural Adaptability of Large\n  Language Models"
                },
                "summary": "To be effectively and safely deployed to global user populations, large\nlanguage models (LLMs) may need to adapt outputs to user values and cultures,\nnot just know about them. We introduce NormAd, an evaluation framework to\nassess LLMs' cultural adaptability, specifically measuring their ability to\njudge social acceptability across varying levels of cultural norm specificity,\nfrom abstract values to explicit social norms. As an instantiation of our\nframework, we create NormAd-Eti, a benchmark of 2.6k situational descriptions\nrepresenting social-etiquette related cultural norms from 75 countries. Through\ncomprehensive experiments on NormAd-Eti, we find that LLMs struggle to\naccurately judge social acceptability across these varying degrees of cultural\ncontexts and show stronger adaptability to English-centric cultures over those\nfrom the Global South. Even in the simplest setting where the relevant social\nnorms are provided, the best LLMs' performance (< 82\\%) lags behind humans (>\n95\\%). In settings with abstract values and country information, model\nperformance drops substantially (< 60\\%), while human accuracy remains high (>\n90\\%). Furthermore, we find that models are better at recognizing socially\nacceptable versus unacceptable situations. Our findings showcase the current\npitfalls in socio-cultural reasoning of LLMs which hinder their adaptability\nfor global audiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To be effectively and safely deployed to global user populations, large\nlanguage models (LLMs) may need to adapt outputs to user values and cultures,\nnot just know about them. We introduce NormAd, an evaluation framework to\nassess LLMs' cultural adaptability, specifically measuring their ability to\njudge social acceptability across varying levels of cultural norm specificity,\nfrom abstract values to explicit social norms. As an instantiation of our\nframework, we create NormAd-Eti, a benchmark of 2.6k situational descriptions\nrepresenting social-etiquette related cultural norms from 75 countries. Through\ncomprehensive experiments on NormAd-Eti, we find that LLMs struggle to\naccurately judge social acceptability across these varying degrees of cultural\ncontexts and show stronger adaptability to English-centric cultures over those\nfrom the Global South. Even in the simplest setting where the relevant social\nnorms are provided, the best LLMs' performance (< 82\\%) lags behind humans (>\n95\\%). In settings with abstract values and country information, model\nperformance drops substantially (< 60\\%), while human accuracy remains high (>\n90\\%). Furthermore, we find that models are better at recognizing socially\nacceptable versus unacceptable situations. Our findings showcase the current\npitfalls in socio-cultural reasoning of LLMs which hinder their adaptability\nfor global audiences."
                },
                "authors": [
                    {
                        "name": "Abhinav Rao"
                    },
                    {
                        "name": "Akhila Yerukola"
                    },
                    {
                        "name": "Vishwa Shah"
                    },
                    {
                        "name": "Katharina Reinecke"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "Accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12464v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12464v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01804v2",
                "updated": "2025-03-06T16:07:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    7,
                    43,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-03T18:33:46Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    33,
                    46,
                    0,
                    62,
                    0
                ],
                "title": "$\\texttt{SEM-CTRL}$: Semantically Controlled Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\texttt{SEM-CTRL}$: Semantically Controlled Decoding"
                },
                "summary": "Ensuring both syntactic and semantic correctness in Large Language Model\n(LLM) outputs remains a significant challenge, despite being critical for\nreal-world deployment. In this paper, we introduce $\\texttt{SEM-CTRL}$, a\nunified approach that enforces rich context-sensitive constraints and task- and\ninstance-specific semantics directly on an LLM decoder. Our approach integrates\ntoken-level MCTS, which is guided by specific syntactic and semantic\nconstraints. The constraints over the desired outputs are expressed using\nAnswer Set Grammars -- a logic-based formalism that generalizes\ncontext-sensitive grammars while incorporating background knowledge to\nrepresent task-specific semantics. We show that our approach guarantees correct\ncompletions for any off-the-shelf LLM without the need for fine-tuning. We\nevaluate $\\texttt{SEM-CTRL}$ on a range of tasks, including synthetic grammar\nsynthesis, combinatorial reasoning, and planning. Our results demonstrate that\n$\\texttt{SEM-CTRL}$ allows small pre-trained LLMs to efficiently outperform\nlarger variants and state-of-the-art reasoning models (e.g., o1-preview) while\nsimultaneously guaranteeing solution correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring both syntactic and semantic correctness in Large Language Model\n(LLM) outputs remains a significant challenge, despite being critical for\nreal-world deployment. In this paper, we introduce $\\texttt{SEM-CTRL}$, a\nunified approach that enforces rich context-sensitive constraints and task- and\ninstance-specific semantics directly on an LLM decoder. Our approach integrates\ntoken-level MCTS, which is guided by specific syntactic and semantic\nconstraints. The constraints over the desired outputs are expressed using\nAnswer Set Grammars -- a logic-based formalism that generalizes\ncontext-sensitive grammars while incorporating background knowledge to\nrepresent task-specific semantics. We show that our approach guarantees correct\ncompletions for any off-the-shelf LLM without the need for fine-tuning. We\nevaluate $\\texttt{SEM-CTRL}$ on a range of tasks, including synthetic grammar\nsynthesis, combinatorial reasoning, and planning. Our results demonstrate that\n$\\texttt{SEM-CTRL}$ allows small pre-trained LLMs to efficiently outperform\nlarger variants and state-of-the-art reasoning models (e.g., o1-preview) while\nsimultaneously guaranteeing solution correctness."
                },
                "authors": [
                    {
                        "name": "Mohammad Albinhassan"
                    },
                    {
                        "name": "Pranava Madhyastha"
                    },
                    {
                        "name": "Alessandra Russo"
                    }
                ],
                "author_detail": {
                    "name": "Alessandra Russo"
                },
                "author": "Alessandra Russo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00153v2",
                "updated": "2025-03-06T15:50:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    50,
                    28,
                    3,
                    65,
                    0
                ],
                "published": "2024-09-30T18:52:53Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    52,
                    53,
                    0,
                    274,
                    0
                ],
                "title": "Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with\n  Gaussian Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with\n  Gaussian Distribution"
                },
                "summary": "Probing learned concepts in large language models (LLMs) is crucial for\nunderstanding how semantic knowledge is encoded internally. Training linear\nclassifiers on probing tasks is a principle approach to denote the vector of a\ncertain concept in the representation space. However, the single vector\nidentified for a concept varies with both data and training, making it less\nrobust and weakening its effectiveness in real-world applications. To address\nthis challenge, we propose an approach to approximate the subspace representing\na specific concept. Built on linear probing classifiers, we extend the concept\nvectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's\neffectiveness through measuring its faithfulness and plausibility across\nmultiple LLMs with different sizes and architectures. Additionally, we use\nrepresentation intervention tasks to showcase its efficacy in real-world\napplications such as emotion steering. Experimental results indicate that GCS\nconcept vectors have the potential to balance steering performance and\nmaintaining the fluency in natural language generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing learned concepts in large language models (LLMs) is crucial for\nunderstanding how semantic knowledge is encoded internally. Training linear\nclassifiers on probing tasks is a principle approach to denote the vector of a\ncertain concept in the representation space. However, the single vector\nidentified for a concept varies with both data and training, making it less\nrobust and weakening its effectiveness in real-world applications. To address\nthis challenge, we propose an approach to approximate the subspace representing\na specific concept. Built on linear probing classifiers, we extend the concept\nvectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's\neffectiveness through measuring its faithfulness and plausibility across\nmultiple LLMs with different sizes and architectures. Additionally, we use\nrepresentation intervention tasks to showcase its efficacy in real-world\napplications such as emotion steering. Experimental results indicate that GCS\nconcept vectors have the potential to balance steering performance and\nmaintaining the fluency in natural language generation tasks."
                },
                "authors": [
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Heng Zhao"
                    },
                    {
                        "name": "Bo Shen"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04557v1",
                "updated": "2025-03-06T15:49:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    49,
                    16,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T15:49:16Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    49,
                    16,
                    3,
                    65,
                    0
                ],
                "title": "Learning Generalizable Language-Conditioned Cloth Manipulation from Long\n  Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Generalizable Language-Conditioned Cloth Manipulation from Long\n  Demonstrations"
                },
                "summary": "Multi-step cloth manipulation is a challenging problem for robots due to the\nhigh-dimensional state spaces and the dynamics of cloth. Despite recent\nsignificant advances in end-to-end imitation learning for multi-step cloth\nmanipulation skills, these methods fail to generalize to unseen tasks. Our\ninsight in tackling the challenge of generalizable multi-step cloth\nmanipulation is decomposition. We propose a novel pipeline that autonomously\nlearns basic skills from long demonstrations and composes learned basic skills\nto generalize to unseen tasks. Specifically, our method first discovers and\nlearns basic skills from the existing long demonstration benchmark with the\ncommonsense knowledge of a large language model (LLM). Then, leveraging a\nhigh-level LLM-based task planner, these basic skills can be composed to\ncomplete unseen tasks. Experimental results demonstrate that our method\noutperforms baseline methods in learning multi-step cloth manipulation skills\nfor both seen and unseen tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-step cloth manipulation is a challenging problem for robots due to the\nhigh-dimensional state spaces and the dynamics of cloth. Despite recent\nsignificant advances in end-to-end imitation learning for multi-step cloth\nmanipulation skills, these methods fail to generalize to unseen tasks. Our\ninsight in tackling the challenge of generalizable multi-step cloth\nmanipulation is decomposition. We propose a novel pipeline that autonomously\nlearns basic skills from long demonstrations and composes learned basic skills\nto generalize to unseen tasks. Specifically, our method first discovers and\nlearns basic skills from the existing long demonstration benchmark with the\ncommonsense knowledge of a large language model (LLM). Then, leveraging a\nhigh-level LLM-based task planner, these basic skills can be composed to\ncomplete unseen tasks. Experimental results demonstrate that our method\noutperforms baseline methods in learning multi-step cloth manipulation skills\nfor both seen and unseen tasks."
                },
                "authors": [
                    {
                        "name": "Hanyi Zhao"
                    },
                    {
                        "name": "Jinxuan Zhu"
                    },
                    {
                        "name": "Zihao Yan"
                    },
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuhong Deng"
                    },
                    {
                        "name": "Xueqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xueqian Wang"
                },
                "author": "Xueqian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18691v2",
                "updated": "2025-03-06T15:47:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    47,
                    1,
                    3,
                    65,
                    0
                ],
                "published": "2024-07-26T12:16:53Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    12,
                    16,
                    53,
                    4,
                    208,
                    0
                ],
                "title": "Graph Neural Networks for Virtual Sensing in Complex Systems: Addressing\n  Heterogeneous Temporal Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks for Virtual Sensing in Complex Systems: Addressing\n  Heterogeneous Temporal Dynamics"
                },
                "summary": "Real-time condition monitoring is crucial for the reliable and efficient\noperation of complex systems. However, relying solely on physical sensors can\nbe limited due to their cost, placement constraints, or inability to directly\nmeasure certain critical parameters. Virtual sensing addresses these\nlimitations by leveraging readily available sensor data and system knowledge to\nestimate inaccessible parameters or infer system states. The increasing\ncomplexity of industrial systems necessitates deployments of sensors with\ndiverse modalities to provide a comprehensive understanding of system states.\nThese sensors capture data at varying frequencies to monitor both rapid and\nslowly varying system dynamics, as well as local and global state evolutions of\nthe systems. This leads to heterogeneous temporal dynamics, which, particularly\nunder varying operational end environmental conditions, pose a significant\nchallenge for accurate virtual sensing. To address this, we propose a\nHeterogeneous Temporal Graph Neural Network (HTGNN) framework. HTGNN explicitly\nmodels signals from diverse sensors and integrates operating conditions into\nthe model architecture. We evaluate HTGNN using two newly released datasets: a\nbearing dataset with diverse load conditions for bearing load prediction and a\nyear-long simulated dataset for predicting bridge live loads. Our results\ndemonstrate that HTGNN significantly outperforms established baseline methods\nin both tasks, particularly under highly varying operating conditions. These\nresults highlight HTGNN's potential as a robust and accurate virtual sensing\napproach for complex systems, paving the way for improved monitoring,\npredictive maintenance, and enhanced system performance. Our code and data are\navailable under https://github.com/EPFL-IMOS/htgnn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time condition monitoring is crucial for the reliable and efficient\noperation of complex systems. However, relying solely on physical sensors can\nbe limited due to their cost, placement constraints, or inability to directly\nmeasure certain critical parameters. Virtual sensing addresses these\nlimitations by leveraging readily available sensor data and system knowledge to\nestimate inaccessible parameters or infer system states. The increasing\ncomplexity of industrial systems necessitates deployments of sensors with\ndiverse modalities to provide a comprehensive understanding of system states.\nThese sensors capture data at varying frequencies to monitor both rapid and\nslowly varying system dynamics, as well as local and global state evolutions of\nthe systems. This leads to heterogeneous temporal dynamics, which, particularly\nunder varying operational end environmental conditions, pose a significant\nchallenge for accurate virtual sensing. To address this, we propose a\nHeterogeneous Temporal Graph Neural Network (HTGNN) framework. HTGNN explicitly\nmodels signals from diverse sensors and integrates operating conditions into\nthe model architecture. We evaluate HTGNN using two newly released datasets: a\nbearing dataset with diverse load conditions for bearing load prediction and a\nyear-long simulated dataset for predicting bridge live loads. Our results\ndemonstrate that HTGNN significantly outperforms established baseline methods\nin both tasks, particularly under highly varying operating conditions. These\nresults highlight HTGNN's potential as a robust and accurate virtual sensing\napproach for complex systems, paving the way for improved monitoring,\npredictive maintenance, and enhanced system performance. Our code and data are\navailable under https://github.com/EPFL-IMOS/htgnn."
                },
                "authors": [
                    {
                        "name": "Mengjie Zhao"
                    },
                    {
                        "name": "Cees Taal"
                    },
                    {
                        "name": "Stephan Baggerohr"
                    },
                    {
                        "name": "Olga Fink"
                    }
                ],
                "author_detail": {
                    "name": "Olga Fink"
                },
                "author": "Olga Fink",
                "arxiv_comment": "This paper extends our previous conference paper (Best Paper at\n  European Conference of the PHM Society 2024,\n  https://doi.org/10.36001/phme.2024.v8i1.3998). Accepted by Mechanical Systems\n  and Signal Processing (MSSP)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09990v2",
                "updated": "2025-03-06T15:38:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    38,
                    31,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-14T08:22:51Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    22,
                    51,
                    4,
                    45,
                    0
                ],
                "title": "X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from\n  Multi-Turn Jailbreaks without Compromising Usability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from\n  Multi-Turn Jailbreaks without Compromising Usability"
                },
                "summary": "Despite the rapid development of safety alignment techniques for LLMs,\ndefending against multi-turn jailbreaks is still a challenging task. In this\npaper, we conduct a comprehensive comparison, revealing that some existing\ndefense methods can improve the robustness of LLMs against multi-turn\njailbreaks but compromise usability, i.e., reducing general capabilities or\ncausing the over-refusal problem. From the perspective of mechanism\ninterpretability of LLMs, we discover that these methods fail to establish a\nboundary that exactly distinguishes safe and harmful feature representations.\nTherefore, boundary-safe representations close to harmful representations are\ninevitably disrupted, leading to a decline in usability. To address this issue,\nwe propose X-Boundary to push harmful representations away from boundary-safe\nrepresentations and obtain an exact distinction boundary. In this way, harmful\nrepresentations can be precisely erased without disrupting safe ones.\nExperimental results show that X-Boundary achieves state-of-the-art defense\nperformance against multi-turn jailbreaks, while reducing the over-refusal rate\nby about 20% and maintaining nearly complete general capability. Furthermore,\nwe theoretically prove and empirically verify that X-Boundary can accelerate\nthe convergence process during training. Please see our code at:\nhttps://github.com/AI45Lab/X-Boundary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rapid development of safety alignment techniques for LLMs,\ndefending against multi-turn jailbreaks is still a challenging task. In this\npaper, we conduct a comprehensive comparison, revealing that some existing\ndefense methods can improve the robustness of LLMs against multi-turn\njailbreaks but compromise usability, i.e., reducing general capabilities or\ncausing the over-refusal problem. From the perspective of mechanism\ninterpretability of LLMs, we discover that these methods fail to establish a\nboundary that exactly distinguishes safe and harmful feature representations.\nTherefore, boundary-safe representations close to harmful representations are\ninevitably disrupted, leading to a decline in usability. To address this issue,\nwe propose X-Boundary to push harmful representations away from boundary-safe\nrepresentations and obtain an exact distinction boundary. In this way, harmful\nrepresentations can be precisely erased without disrupting safe ones.\nExperimental results show that X-Boundary achieves state-of-the-art defense\nperformance against multi-turn jailbreaks, while reducing the over-refusal rate\nby about 20% and maintaining nearly complete general capability. Furthermore,\nwe theoretically prove and empirically verify that X-Boundary can accelerate\nthe convergence process during training. Please see our code at:\nhttps://github.com/AI45Lab/X-Boundary."
                },
                "authors": [
                    {
                        "name": "Xiaoya Lu"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Yi Yu"
                    },
                    {
                        "name": "Luxin Xu"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04554v1",
                "updated": "2025-03-06T15:37:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    37,
                    31,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T15:37:31Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    37,
                    31,
                    3,
                    65,
                    0
                ],
                "title": "Compositional Translation: A Novel LLM-based Approach for Low-resource\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Translation: A Novel LLM-based Approach for Low-resource\n  Machine Translation"
                },
                "summary": "The ability of generative large language models (LLMs) to perform in-context\nlearning has given rise to a large body of research into how best to prompt\nmodels for various natural language processing tasks. Machine Translation (MT)\nhas been shown to benefit from in-context examples, in particular when they are\nsemantically similar to the sentence to translate. In this paper, we propose a\nnew LLM-based translation paradigm, compositional translation, to replace naive\nfew-shot MT with similarity-based demonstrations. An LLM is used to decompose a\nsentence into simpler phrases, and then to translate each phrase with the help\nof retrieved demonstrations. Finally, the LLM is prompted to translate the\ninitial sentence with the help of the self-generated phrase-translation pairs.\nOur intuition is that this approach should improve translation because these\nshorter phrases should be intrinsically easier to translate and easier to match\nwith relevant examples. This is especially beneficial in low-resource\nscenarios, and more generally whenever the selection pool is small or out of\ndomain. We show that compositional translation boosts LLM translation\nperformance on a wide range of popular MT benchmarks, including FLORES 200,\nNTREX 128 and TICO-19. Code and outputs are available at\nhttps://github.com/ArmelRandy/compositional-translation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of generative large language models (LLMs) to perform in-context\nlearning has given rise to a large body of research into how best to prompt\nmodels for various natural language processing tasks. Machine Translation (MT)\nhas been shown to benefit from in-context examples, in particular when they are\nsemantically similar to the sentence to translate. In this paper, we propose a\nnew LLM-based translation paradigm, compositional translation, to replace naive\nfew-shot MT with similarity-based demonstrations. An LLM is used to decompose a\nsentence into simpler phrases, and then to translate each phrase with the help\nof retrieved demonstrations. Finally, the LLM is prompted to translate the\ninitial sentence with the help of the self-generated phrase-translation pairs.\nOur intuition is that this approach should improve translation because these\nshorter phrases should be intrinsically easier to translate and easier to match\nwith relevant examples. This is especially beneficial in low-resource\nscenarios, and more generally whenever the selection pool is small or out of\ndomain. We show that compositional translation boosts LLM translation\nperformance on a wide range of popular MT benchmarks, including FLORES 200,\nNTREX 128 and TICO-19. Code and outputs are available at\nhttps://github.com/ArmelRandy/compositional-translation"
                },
                "authors": [
                    {
                        "name": "Armel Zebaze"
                    },
                    {
                        "name": "Benoît Sagot"
                    },
                    {
                        "name": "Rachel Bawden"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Bawden"
                },
                "author": "Rachel Bawden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20984v2",
                "updated": "2025-03-06T15:36:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    36,
                    48,
                    3,
                    65,
                    0
                ],
                "published": "2025-02-28T11:52:02Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    52,
                    2,
                    4,
                    59,
                    0
                ],
                "title": "UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models\n  for Multilingual Multimodal Idiomaticity Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models\n  for Multilingual Multimodal Idiomaticity Representation"
                },
                "summary": "SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL."
                },
                "authors": [
                    {
                        "name": "Thanet Markchom"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Liting Huang"
                    },
                    {
                        "name": "Huizhi Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huizhi Liang"
                },
                "author": "Huizhi Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04550v1",
                "updated": "2025-03-06T15:36:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    36,
                    6,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T15:36:06Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    36,
                    6,
                    3,
                    65,
                    0
                ],
                "title": "Benchmarking Reasoning Robustness in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Reasoning Robustness in Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs) in reasoning such\nas DeepSeek, we for the first time identify a key dilemma in reasoning\nrobustness and generalization: significant performance degradation on novel or\nincomplete data, suggesting a reliance on memorized patterns rather than\nsystematic reasoning. Our closer examination reveals four key unique\nlimitations underlying this issue:(1) Positional bias--models favor earlier\nqueries in multi-query inputs but answering the wrong one in the latter (e.g.,\nGPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction\nsensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series\nand by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical\nfragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from\n97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5\npercent); and (4) Memory dependence--models resort to guesswork when missing\ncritical data. These findings further highlight the reliance on heuristic\nrecall over rigorous logical inference, demonstrating challenges in reasoning\nrobustness. To comprehensively investigate these robustness challenges, this\npaper introduces a novel benchmark, termed as Math-RoB, that exploits\nhallucinations triggered by missing information to expose reasoning gaps. This\nis achieved by an instruction-based approach to generate diverse datasets that\nclosely resemble training distributions, facilitating a holistic robustness\nassessment and advancing the development of more robust reasoning frameworks.\nBad character(s) in field Abstract.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs) in reasoning such\nas DeepSeek, we for the first time identify a key dilemma in reasoning\nrobustness and generalization: significant performance degradation on novel or\nincomplete data, suggesting a reliance on memorized patterns rather than\nsystematic reasoning. Our closer examination reveals four key unique\nlimitations underlying this issue:(1) Positional bias--models favor earlier\nqueries in multi-query inputs but answering the wrong one in the latter (e.g.,\nGPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction\nsensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series\nand by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical\nfragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from\n97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5\npercent); and (4) Memory dependence--models resort to guesswork when missing\ncritical data. These findings further highlight the reliance on heuristic\nrecall over rigorous logical inference, demonstrating challenges in reasoning\nrobustness. To comprehensively investigate these robustness challenges, this\npaper introduces a novel benchmark, termed as Math-RoB, that exploits\nhallucinations triggered by missing information to expose reasoning gaps. This\nis achieved by an instruction-based approach to generate diverse datasets that\nclosely resemble training distributions, facilitating a holistic robustness\nassessment and advancing the development of more robust reasoning frameworks.\nBad character(s) in field Abstract."
                },
                "authors": [
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Xikun Zhang"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Wenjie Wu"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04548v1",
                "updated": "2025-03-06T15:34:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    34,
                    27,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T15:34:27Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    34,
                    27,
                    3,
                    65,
                    0
                ],
                "title": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models"
                },
                "summary": "In this report, we present the third technical report on the development of\nslow-thinking models as part of the STILL project. As the technical pathway\nbecomes clearer, scaling RL training has become a central technique for\nimplementing such reasoning models. We systematically experiment with and\ndocument the effects of various factors influencing RL training, conducting\nexperiments on both base models and fine-tuned models. Specifically, we\ndemonstrate that our RL training approach consistently improves the Qwen2.5-32B\nbase models, enhancing both response length and test accuracy. Furthermore, we\nshow that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already\nachieved a high performance level, it can be further refined through RL\ntraining, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we\nalso explore the use of tool manipulation, finding that it significantly boosts\nthe reasoning performance of large reasoning models. This approach achieves a\nremarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its\neffectiveness in enhancing model capabilities. We release our resources at the\nSTILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we present the third technical report on the development of\nslow-thinking models as part of the STILL project. As the technical pathway\nbecomes clearer, scaling RL training has become a central technique for\nimplementing such reasoning models. We systematically experiment with and\ndocument the effects of various factors influencing RL training, conducting\nexperiments on both base models and fine-tuned models. Specifically, we\ndemonstrate that our RL training approach consistently improves the Qwen2.5-32B\nbase models, enhancing both response length and test accuracy. Furthermore, we\nshow that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already\nachieved a high performance level, it can be further refined through RL\ntraining, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we\nalso explore the use of tool manipulation, finding that it significantly boosts\nthe reasoning performance of large reasoning models. This approach achieves a\nremarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its\neffectiveness in enhancing model capabilities. We release our resources at the\nSTILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs."
                },
                "authors": [
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Xu Miao"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Lei Fang"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Technical Report on Slow Thinking with LLMs: Part III",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04530v1",
                "updated": "2025-03-06T15:19:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    19,
                    17,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T15:19:17Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    19,
                    17,
                    3,
                    65,
                    0
                ],
                "title": "SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning"
                },
                "summary": "Large Language Models (LLMs) excel in reasoning but remain constrained by\ntheir Chain-of-Thought (CoT) approach, which struggles with complex tasks\nrequiring more nuanced topological reasoning. We introduce SOLAR, Scalable\nOptimization of Large-scale Architecture for Reasoning, a framework that\ndynamically optimizes various reasoning topologies to enhance accuracy and\nefficiency.\n  Our Topological Annotation Generation (TAG) system automates topological\ndataset creation and segmentation, improving post-training and evaluation.\nAdditionally, we propose Topological-Scaling, a reward-driven framework that\naligns training and inference scaling, equipping LLMs with adaptive, task-aware\nreasoning.\n  SOLAR achieves substantial gains on MATH and GSM8K: +5% accuracy with\nTopological Tuning, +9% with Topological Reward, and +10.02% with Hybrid\nScaling. It also reduces response length by over 5% for complex problems,\nlowering inference latency.\n  To foster the reward system, we train a multi-task Topological Reward Model\n(M-TRM), which autonomously selects the best reasoning topology and answer in a\nsingle pass, eliminating the need for training and inference on multiple\nsingle-task TRMs (S-TRMs), thus reducing both training cost and inference\nlatency. In addition, in terms of performance, M-TRM surpasses all S-TRMs,\nimproving accuracy by +10% and rank correlation by +9%.\n  To the best of our knowledge, SOLAR sets a new benchmark for scalable,\nhigh-precision LLM reasoning while introducing an automated annotation process\nand a dynamic reasoning topology competition mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in reasoning but remain constrained by\ntheir Chain-of-Thought (CoT) approach, which struggles with complex tasks\nrequiring more nuanced topological reasoning. We introduce SOLAR, Scalable\nOptimization of Large-scale Architecture for Reasoning, a framework that\ndynamically optimizes various reasoning topologies to enhance accuracy and\nefficiency.\n  Our Topological Annotation Generation (TAG) system automates topological\ndataset creation and segmentation, improving post-training and evaluation.\nAdditionally, we propose Topological-Scaling, a reward-driven framework that\naligns training and inference scaling, equipping LLMs with adaptive, task-aware\nreasoning.\n  SOLAR achieves substantial gains on MATH and GSM8K: +5% accuracy with\nTopological Tuning, +9% with Topological Reward, and +10.02% with Hybrid\nScaling. It also reduces response length by over 5% for complex problems,\nlowering inference latency.\n  To foster the reward system, we train a multi-task Topological Reward Model\n(M-TRM), which autonomously selects the best reasoning topology and answer in a\nsingle pass, eliminating the need for training and inference on multiple\nsingle-task TRMs (S-TRMs), thus reducing both training cost and inference\nlatency. In addition, in terms of performance, M-TRM surpasses all S-TRMs,\nimproving accuracy by +10% and rank correlation by +9%.\n  To the best of our knowledge, SOLAR sets a new benchmark for scalable,\nhigh-precision LLM reasoning while introducing an automated annotation process\nand a dynamic reasoning topology competition mechanism."
                },
                "authors": [
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Yinyi Luo"
                    },
                    {
                        "name": "Anudeep Bolimera"
                    },
                    {
                        "name": "Marios Savvides"
                    }
                ],
                "author_detail": {
                    "name": "Marios Savvides"
                },
                "author": "Marios Savvides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04527v1",
                "updated": "2025-03-06T15:14:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    14,
                    57,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T15:14:57Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    14,
                    57,
                    3,
                    65,
                    0
                ],
                "title": "The nexus between disease surveillance, adaptive human behavior and\n  epidemic containment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nexus between disease surveillance, adaptive human behavior and\n  epidemic containment"
                },
                "summary": "Epidemics exhibit interconnected processes that operate at multiple time and\norganizational scales, a hallmark of complex adaptive systems. Modern\nepidemiological modeling frameworks incorporate feedback between\nindividual-level behavioral choices and centralized interventions. Nonetheless,\nthe realistic operational course for disease detection, planning, and response\nis often overlooked. Disease detection is a dynamic challenge, shaped by the\ninterplay between surveillance efforts and transmission characteristics. It\nserves as a tipping point that triggers emergency declarations, information\ndissemination, adaptive behavioral responses, and the deployment of public\nhealth interventions. Evaluating the impact of disease surveillance systems as\ntriggers for adaptive behavior and public health interventions is key to\ndesigning effective control policies.\n  We examine the multiple behavioral and epidemiological dynamics generated by\nthe feedback between disease surveillance and the intertwined dynamics of\ninformation and disease propagation. Specifically, we study the intertwined\ndynamics between: $(i)$ disease surveillance triggering health emergency\ndeclarations, $(ii)$ risk information dissemination producing decentralized\nbehavioral responses, and $(iii)$ centralized interventions. Our results show\nthat robust surveillance systems that quickly detect a disease outbreak can\ntrigger an early response from the population, leading to large epidemic sizes.\nThe key result is that the response scenarios that minimize the final epidemic\nsize are determined by the trade-off between the risk information dissemination\nand disease transmission, with the triggering effect of surveillance mediating\nthis trade-off. Finally, our results confirm that behavioral adaptation can\ncreate a hysteresis-like effect on the final epidemic size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epidemics exhibit interconnected processes that operate at multiple time and\norganizational scales, a hallmark of complex adaptive systems. Modern\nepidemiological modeling frameworks incorporate feedback between\nindividual-level behavioral choices and centralized interventions. Nonetheless,\nthe realistic operational course for disease detection, planning, and response\nis often overlooked. Disease detection is a dynamic challenge, shaped by the\ninterplay between surveillance efforts and transmission characteristics. It\nserves as a tipping point that triggers emergency declarations, information\ndissemination, adaptive behavioral responses, and the deployment of public\nhealth interventions. Evaluating the impact of disease surveillance systems as\ntriggers for adaptive behavior and public health interventions is key to\ndesigning effective control policies.\n  We examine the multiple behavioral and epidemiological dynamics generated by\nthe feedback between disease surveillance and the intertwined dynamics of\ninformation and disease propagation. Specifically, we study the intertwined\ndynamics between: $(i)$ disease surveillance triggering health emergency\ndeclarations, $(ii)$ risk information dissemination producing decentralized\nbehavioral responses, and $(iii)$ centralized interventions. Our results show\nthat robust surveillance systems that quickly detect a disease outbreak can\ntrigger an early response from the population, leading to large epidemic sizes.\nThe key result is that the response scenarios that minimize the final epidemic\nsize are determined by the trade-off between the risk information dissemination\nand disease transmission, with the triggering effect of surveillance mediating\nthis trade-off. Finally, our results confirm that behavioral adaptation can\ncreate a hysteresis-like effect on the final epidemic size."
                },
                "authors": [
                    {
                        "name": "Baltazar Espinoza"
                    },
                    {
                        "name": "Roger Sanchez"
                    },
                    {
                        "name": "Jimmy Calvo-Monge"
                    },
                    {
                        "name": "Fabio Sanchez"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Sanchez"
                },
                "author": "Fabio Sanchez",
                "arxiv_comment": "17 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "37N25 92B05 92-10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12580v2",
                "updated": "2025-03-06T15:14:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    14,
                    17,
                    3,
                    65,
                    0
                ],
                "published": "2024-11-19T15:47:12Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    47,
                    12,
                    1,
                    324,
                    0
                ],
                "title": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language\n  Models"
                },
                "summary": "The capabilities and limitations of Large Language Models have been sketched\nout in great detail in recent years, providing an intriguing yet conflicting\npicture. On the one hand, LLMs demonstrate a general ability to solve problems.\nOn the other hand, they show surprising reasoning gaps when compared to humans,\ncasting doubt on the robustness of their generalisation strategies. The sheer\nvolume of data used in the design of LLMs has precluded us from applying the\nmethod traditionally used to measure generalisation: train-test set separation.\nTo overcome this, we study what kind of generalisation strategies LLMs employ\nwhen performing reasoning tasks by investigating the pretraining data they rely\non. For two models of different sizes (7B and 35B) and 2.5B of their\npretraining tokens, we identify what documents influence the model outputs for\nthree simple mathematical reasoning tasks and contrast this to the data that\nare influential for answering factual questions. We find that, while the models\nrely on mostly distinct sets of data for each factual question, a document\noften has a similar influence across different reasoning questions within the\nsame task, indicating the presence of procedural knowledge. We further find\nthat the answers to factual questions often show up in the most influential\ndata. However, for reasoning questions the answers usually do not show up as\nhighly influential, nor do the answers to the intermediate reasoning steps.\nWhen we characterise the top ranked documents for the reasoning questions\nqualitatively, we confirm that the influential documents often contain\nprocedural knowledge, like demonstrating how to obtain a solution using\nformulae or code. Our findings indicate that the approach to reasoning the\nmodels use is unlike retrieval, and more like a generalisable strategy that\nsynthesises procedural knowledge from documents doing a similar form of\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities and limitations of Large Language Models have been sketched\nout in great detail in recent years, providing an intriguing yet conflicting\npicture. On the one hand, LLMs demonstrate a general ability to solve problems.\nOn the other hand, they show surprising reasoning gaps when compared to humans,\ncasting doubt on the robustness of their generalisation strategies. The sheer\nvolume of data used in the design of LLMs has precluded us from applying the\nmethod traditionally used to measure generalisation: train-test set separation.\nTo overcome this, we study what kind of generalisation strategies LLMs employ\nwhen performing reasoning tasks by investigating the pretraining data they rely\non. For two models of different sizes (7B and 35B) and 2.5B of their\npretraining tokens, we identify what documents influence the model outputs for\nthree simple mathematical reasoning tasks and contrast this to the data that\nare influential for answering factual questions. We find that, while the models\nrely on mostly distinct sets of data for each factual question, a document\noften has a similar influence across different reasoning questions within the\nsame task, indicating the presence of procedural knowledge. We further find\nthat the answers to factual questions often show up in the most influential\ndata. However, for reasoning questions the answers usually do not show up as\nhighly influential, nor do the answers to the intermediate reasoning steps.\nWhen we characterise the top ranked documents for the reasoning questions\nqualitatively, we confirm that the influential documents often contain\nprocedural knowledge, like demonstrating how to obtain a solution using\nformulae or code. Our findings indicate that the approach to reasoning the\nmodels use is unlike retrieval, and more like a generalisable strategy that\nsynthesises procedural knowledge from documents doing a similar form of\nreasoning."
                },
                "authors": [
                    {
                        "name": "Laura Ruis"
                    },
                    {
                        "name": "Maximilian Mozes"
                    },
                    {
                        "name": "Juhan Bae"
                    },
                    {
                        "name": "Siddhartha Rao Kamalakara"
                    },
                    {
                        "name": "Dwarak Talupuru"
                    },
                    {
                        "name": "Acyr Locatelli"
                    },
                    {
                        "name": "Robert Kirk"
                    },
                    {
                        "name": "Tim Rocktäschel"
                    },
                    {
                        "name": "Edward Grefenstette"
                    },
                    {
                        "name": "Max Bartolo"
                    }
                ],
                "author_detail": {
                    "name": "Max Bartolo"
                },
                "author": "Max Bartolo",
                "arxiv_comment": "Published at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04521v1",
                "updated": "2025-03-06T15:08:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    8,
                    31,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T15:08:31Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    15,
                    8,
                    31,
                    3,
                    65,
                    0
                ],
                "title": "Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market"
                },
                "summary": "The convergence of edge computing and AI gives rise to Edge-AI, which enables\nthe deployment of real-time AI applications and services at the network edge.\nOne of the fundamental research issues in Edge-AI is edge inference\nacceleration, which aims to realize low-latency high-accuracy DNN inference\nservices by leveraging the fine-grained offloading of partitioned inference\ntasks from end devices to edge servers. However, existing research has yet to\nadopt a practical Edge-AI market perspective, which would systematically\nexplore the personalized inference needs of AI users (e.g., inference accuracy,\nlatency, and task complexity), the revenue incentives for AI service providers\nthat offer edge inference services, and multi-stakeholder governance within a\nmarket-oriented context. To bridge this gap, we propose an Auction-based Edge\nInference Pricing Mechanism (AERIA) for revenue maximization to tackle the\nmulti-dimensional optimization problem of DNN model partition, edge inference\npricing, and resource allocation. We investigate the multi-exit device-edge\nsynergistic inference scheme for on-demand DNN inference acceleration, and\nanalyse the auction dynamics amongst the AI service providers, AI users and\nedge infrastructure provider. Owing to the strategic mechanism design via\nrandomized consensus estimate and cost sharing techniques, the Edge-AI market\nattains several desirable properties, including competitiveness in revenue\nmaximization, incentive compatibility, and envy-freeness, which are crucial to\nmaintain the effectiveness, truthfulness, and fairness of our auction outcomes.\nThe extensive simulation experiments based on four representative DNN inference\nworkloads demonstrate that our AERIA mechanism significantly outperforms\nseveral state-of-the-art approaches in revenue maximization, demonstrating the\nefficacy of AERIA for on-demand DNN inference in the Edge-AI market.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The convergence of edge computing and AI gives rise to Edge-AI, which enables\nthe deployment of real-time AI applications and services at the network edge.\nOne of the fundamental research issues in Edge-AI is edge inference\nacceleration, which aims to realize low-latency high-accuracy DNN inference\nservices by leveraging the fine-grained offloading of partitioned inference\ntasks from end devices to edge servers. However, existing research has yet to\nadopt a practical Edge-AI market perspective, which would systematically\nexplore the personalized inference needs of AI users (e.g., inference accuracy,\nlatency, and task complexity), the revenue incentives for AI service providers\nthat offer edge inference services, and multi-stakeholder governance within a\nmarket-oriented context. To bridge this gap, we propose an Auction-based Edge\nInference Pricing Mechanism (AERIA) for revenue maximization to tackle the\nmulti-dimensional optimization problem of DNN model partition, edge inference\npricing, and resource allocation. We investigate the multi-exit device-edge\nsynergistic inference scheme for on-demand DNN inference acceleration, and\nanalyse the auction dynamics amongst the AI service providers, AI users and\nedge infrastructure provider. Owing to the strategic mechanism design via\nrandomized consensus estimate and cost sharing techniques, the Edge-AI market\nattains several desirable properties, including competitiveness in revenue\nmaximization, incentive compatibility, and envy-freeness, which are crucial to\nmaintain the effectiveness, truthfulness, and fairness of our auction outcomes.\nThe extensive simulation experiments based on four representative DNN inference\nworkloads demonstrate that our AERIA mechanism significantly outperforms\nseveral state-of-the-art approaches in revenue maximization, demonstrating the\nefficacy of AERIA for on-demand DNN inference in the Edge-AI market."
                },
                "authors": [
                    {
                        "name": "Songyuan Li"
                    },
                    {
                        "name": "Jia Hu"
                    },
                    {
                        "name": "Geyong Min"
                    },
                    {
                        "name": "Haojun Huang"
                    },
                    {
                        "name": "Jiwei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jiwei Huang"
                },
                "author": "Jiwei Huang",
                "arxiv_comment": "Index Terms: Edge-AI, DNN Inference Offloading, Resource Management,\n  Dynamic Pricing, Auction Mechanism",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04490v1",
                "updated": "2025-03-06T14:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    38,
                    20,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:38:20Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    38,
                    20,
                    3,
                    65,
                    0
                ],
                "title": "Large Language Models in Bioinformatics: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models in Bioinformatics: A Survey"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing bioinformatics, enabling\nadvanced analysis of DNA, RNA, proteins, and single-cell data. This survey\nprovides a systematic review of recent advancements, focusing on genomic\nsequence modeling, RNA structure prediction, protein function inference, and\nsingle-cell transcriptomics. Meanwhile, we also discuss several key challenges,\nincluding data scarcity, computational complexity, and cross-omics integration,\nand explore future directions such as multimodal learning, hybrid AI models,\nand clinical applications. By offering a comprehensive perspective, this paper\nunderscores the transformative potential of LLMs in driving innovations in\nbioinformatics and precision medicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing bioinformatics, enabling\nadvanced analysis of DNA, RNA, proteins, and single-cell data. This survey\nprovides a systematic review of recent advancements, focusing on genomic\nsequence modeling, RNA structure prediction, protein function inference, and\nsingle-cell transcriptomics. Meanwhile, we also discuss several key challenges,\nincluding data scarcity, computational complexity, and cross-omics integration,\nand explore future directions such as multimodal learning, hybrid AI models,\nand clinical applications. By offering a comprehensive perspective, this paper\nunderscores the transformative potential of LLMs in driving innovations in\nbioinformatics and precision medicine."
                },
                "authors": [
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Zikang Wang"
                    },
                    {
                        "name": "Jiyue Jiang"
                    },
                    {
                        "name": "Pengan Chen"
                    },
                    {
                        "name": "Xiangyu Shi"
                    },
                    {
                        "name": "Yu Li"
                    }
                ],
                "author_detail": {
                    "name": "Yu Li"
                },
                "author": "Yu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04479v1",
                "updated": "2025-03-06T14:29:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    29,
                    52,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:29:52Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    29,
                    52,
                    3,
                    65,
                    0
                ],
                "title": "ToolFuzz -- Automated Agent Tool Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolFuzz -- Automated Agent Tool Testing"
                },
                "summary": "Large Language Model (LLM) Agents leverage the advanced reasoning\ncapabilities of LLMs in real-world applications. To interface with an\nenvironment, these agents often rely on tools, such as web search or database\nAPIs. As the agent provides the LLM with tool documentation along the user\nquery, the completeness and correctness of this documentation is critical.\nHowever, tool documentation is often over-, under-, or ill-specified, impeding\nthe agent's accuracy. Standard software testing approaches struggle to identify\nthese errors as they are expressed in natural language. Thus, despite its\nimportance, there currently exists no automated method to test the tool\ndocumentation for agents. To address this issue, we present ToolFuzz, the first\nmethod for automated testing of tool documentations. ToolFuzz is designed to\ndiscover two types of errors: (1) user queries leading to tool runtime errors\nand (2) user queries that lead to incorrect agent responses. ToolFuzz can\ngenerate a large and diverse set of natural inputs, effectively finding tool\ndescription errors at a low false positive rate. Further, we present two\nstraightforward prompt-engineering approaches. We evaluate all three tool\ntesting approaches on 32 common LangChain tools and 35 newly created custom\ntools and 2 novel benchmarks to further strengthen the assessment. We find that\nmany publicly available tools suffer from underspecification. Specifically, we\nshow that ToolFuzz identifies 20x more erroneous inputs compared to the\nprompt-engineering approaches, making it a key component for building reliable\nAI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) Agents leverage the advanced reasoning\ncapabilities of LLMs in real-world applications. To interface with an\nenvironment, these agents often rely on tools, such as web search or database\nAPIs. As the agent provides the LLM with tool documentation along the user\nquery, the completeness and correctness of this documentation is critical.\nHowever, tool documentation is often over-, under-, or ill-specified, impeding\nthe agent's accuracy. Standard software testing approaches struggle to identify\nthese errors as they are expressed in natural language. Thus, despite its\nimportance, there currently exists no automated method to test the tool\ndocumentation for agents. To address this issue, we present ToolFuzz, the first\nmethod for automated testing of tool documentations. ToolFuzz is designed to\ndiscover two types of errors: (1) user queries leading to tool runtime errors\nand (2) user queries that lead to incorrect agent responses. ToolFuzz can\ngenerate a large and diverse set of natural inputs, effectively finding tool\ndescription errors at a low false positive rate. Further, we present two\nstraightforward prompt-engineering approaches. We evaluate all three tool\ntesting approaches on 32 common LangChain tools and 35 newly created custom\ntools and 2 novel benchmarks to further strengthen the assessment. We find that\nmany publicly available tools suffer from underspecification. Specifically, we\nshow that ToolFuzz identifies 20x more erroneous inputs compared to the\nprompt-engineering approaches, making it a key component for building reliable\nAI agents."
                },
                "authors": [
                    {
                        "name": "Ivan Milev"
                    },
                    {
                        "name": "Mislav Balunović"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04474v1",
                "updated": "2025-03-06T14:24:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    24,
                    12,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:24:12Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    24,
                    12,
                    3,
                    65,
                    0
                ],
                "title": "Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges"
                },
                "summary": "Large Language Model (LLM) based judges form the underpinnings of key safety\nevaluation processes such as offline benchmarking, automated red-teaming, and\nonline guardrailing. This widespread requirement raises the crucial question:\ncan we trust the evaluations of these evaluators? In this paper, we highlight\ntwo critical challenges that are typically overlooked: (i) evaluations in the\nwild where factors like prompt sensitivity and distribution shifts can affect\nperformance and (ii) adversarial attacks that target the judge. We highlight\nthe importance of these through a study of commonly used safety judges, showing\nthat small changes such as the style of the model output can lead to jumps of\nup to 0.24 in the false negative rate on the same dataset, whereas adversarial\nattacks on the model generation can fool some judges into misclassifying 100%\nof harmful generations as safe ones. These findings reveal gaps in commonly\nused meta-evaluation benchmarks and weaknesses in the robustness of current LLM\njudges, indicating that low attack success under certain judges could create a\nfalse sense of security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based judges form the underpinnings of key safety\nevaluation processes such as offline benchmarking, automated red-teaming, and\nonline guardrailing. This widespread requirement raises the crucial question:\ncan we trust the evaluations of these evaluators? In this paper, we highlight\ntwo critical challenges that are typically overlooked: (i) evaluations in the\nwild where factors like prompt sensitivity and distribution shifts can affect\nperformance and (ii) adversarial attacks that target the judge. We highlight\nthe importance of these through a study of commonly used safety judges, showing\nthat small changes such as the style of the model output can lead to jumps of\nup to 0.24 in the false negative rate on the same dataset, whereas adversarial\nattacks on the model generation can fool some judges into misclassifying 100%\nof harmful generations as safe ones. These findings reveal gaps in commonly\nused meta-evaluation benchmarks and weaknesses in the robustness of current LLM\njudges, indicating that low attack success under certain judges could create a\nfalse sense of security."
                },
                "authors": [
                    {
                        "name": "Francisco Eiras"
                    },
                    {
                        "name": "Eliott Zemour"
                    },
                    {
                        "name": "Eric Lin"
                    },
                    {
                        "name": "Vaikkunth Mugunthan"
                    }
                ],
                "author_detail": {
                    "name": "Vaikkunth Mugunthan"
                },
                "author": "Vaikkunth Mugunthan",
                "arxiv_comment": "Accepted to the ICBINB Workshop at ICLR'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04463v1",
                "updated": "2025-03-06T14:15:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    15,
                    7,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:15:07Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    15,
                    7,
                    3,
                    65,
                    0
                ],
                "title": "Guiding LLMs to Generate High-Fidelity and High-Quality Counterfactual\n  Explanations for Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding LLMs to Generate High-Fidelity and High-Quality Counterfactual\n  Explanations for Text Classification"
                },
                "summary": "The need for interpretability in deep learning has driven interest in\ncounterfactual explanations, which identify minimal changes to an instance that\nchange a model's prediction. Current counterfactual (CF) generation methods\nrequire task-specific fine-tuning and produce low-quality text. Large Language\nModels (LLMs), though effective for high-quality text generation, struggle with\nlabel-flipping counterfactuals (i.e., counterfactuals that change the\nprediction) without fine-tuning. We introduce two simple classifier-guided\napproaches to support counterfactual generation by LLMs, eliminating the need\nfor fine-tuning while preserving the strengths of LLMs. Despite their\nsimplicity, our methods outperform state-of-the-art counterfactual generation\nmethods and are effective across different LLMs, highlighting the benefits of\nguiding counterfactual generation by LLMs with classifier information. We\nfurther show that data augmentation by our generated CFs can improve a\nclassifier's robustness. Our analysis reveals a critical issue in\ncounterfactual generation by LLMs: LLMs rely on parametric knowledge rather\nthan faithfully following the classifier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for interpretability in deep learning has driven interest in\ncounterfactual explanations, which identify minimal changes to an instance that\nchange a model's prediction. Current counterfactual (CF) generation methods\nrequire task-specific fine-tuning and produce low-quality text. Large Language\nModels (LLMs), though effective for high-quality text generation, struggle with\nlabel-flipping counterfactuals (i.e., counterfactuals that change the\nprediction) without fine-tuning. We introduce two simple classifier-guided\napproaches to support counterfactual generation by LLMs, eliminating the need\nfor fine-tuning while preserving the strengths of LLMs. Despite their\nsimplicity, our methods outperform state-of-the-art counterfactual generation\nmethods and are effective across different LLMs, highlighting the benefits of\nguiding counterfactual generation by LLMs with classifier information. We\nfurther show that data augmentation by our generated CFs can improve a\nclassifier's robustness. Our analysis reveals a critical issue in\ncounterfactual generation by LLMs: LLMs rely on parametric knowledge rather\nthan faithfully following the classifier."
                },
                "authors": [
                    {
                        "name": "Van Bach Nguyen"
                    },
                    {
                        "name": "Christin Seifert"
                    },
                    {
                        "name": "Jörg Schlötterer"
                    }
                ],
                "author_detail": {
                    "name": "Jörg Schlötterer"
                },
                "author": "Jörg Schlötterer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04457v1",
                "updated": "2025-03-06T14:11:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    11,
                    0,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:11:00Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    11,
                    0,
                    3,
                    65,
                    0
                ],
                "title": "TPC: Cross-Temporal Prediction Connection for Vision-Language Model\n  Hallucination Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPC: Cross-Temporal Prediction Connection for Vision-Language Model\n  Hallucination Reduction"
                },
                "summary": "Vision-language models (VLMs) have achieved remarkable advancements,\ncapitalizing on the impressive capabilities of large language models (LLMs)\nacross diverse tasks. Despite this, a critical challenge known as hallucination\noccurs when models overconfidently describe objects or attributes absent from\nthe image, a problem exacerbated by the tendency of VLMs to rely on linguistic\npriors. This limitation reduces model reliability in high-stakes applications.\nIn this work, we have observed the characteristic of logits' continuity\nconsistency enhancement and introduced a straightforward and efficient method,\nCross-Temporal Prediction Connection (TPC), designed to enhance the semantic\nconsistency of logits by connecting them temporally across timesteps. TPC\namplifies information flow and improves coherence, effectively reducing\nhallucination. Extensive experiments show that TPC surpasses existing\nrepresentatives, delivering superior performance in both accuracy and\nefficiency while maintaining robustness in open-ended text generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) have achieved remarkable advancements,\ncapitalizing on the impressive capabilities of large language models (LLMs)\nacross diverse tasks. Despite this, a critical challenge known as hallucination\noccurs when models overconfidently describe objects or attributes absent from\nthe image, a problem exacerbated by the tendency of VLMs to rely on linguistic\npriors. This limitation reduces model reliability in high-stakes applications.\nIn this work, we have observed the characteristic of logits' continuity\nconsistency enhancement and introduced a straightforward and efficient method,\nCross-Temporal Prediction Connection (TPC), designed to enhance the semantic\nconsistency of logits by connecting them temporally across timesteps. TPC\namplifies information flow and improves coherence, effectively reducing\nhallucination. Extensive experiments show that TPC surpasses existing\nrepresentatives, delivering superior performance in both accuracy and\nefficiency while maintaining robustness in open-ended text generation tasks."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Weiwei Fu"
                    },
                    {
                        "name": "Yang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhou"
                },
                "author": "Yang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04452v1",
                "updated": "2025-03-06T14:06:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    6,
                    35,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:06:35Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    6,
                    35,
                    3,
                    65,
                    0
                ],
                "title": "A lightweight model FDM-YOLO for small target improvement based on\n  YOLOv8",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A lightweight model FDM-YOLO for small target improvement based on\n  YOLOv8"
                },
                "summary": "Small targets are particularly difficult to detect due to their low pixel\ncount, complex backgrounds, and varying shooting angles, which make it hard for\nmodels to extract effective features. While some large-scale models offer high\naccuracy, their long inference times make them unsuitable for real-time\ndeployment on edge devices. On the other hand, models designed for low\ncomputational power often suffer from poor detection accuracy. This paper\nfocuses on small target detection and explores methods for object detection\nunder low computational constraints. Building on the YOLOv8 model, we propose a\nnew network architecture called FDM-YOLO. Our research includes the following\nkey contributions: We introduce FDM-YOLO by analyzing the output of the YOLOv8\ndetection head. We add a highresolution layer and remove the large target\ndetection layer to better handle small targets. Based on PConv, we propose a\nlightweight network structure called Fast-C2f, which is integrated into the PAN\nmodule of the model. To mitigate the accuracy loss caused by model\nlightweighting, we employ dynamic upsampling (Dysample) and a lightweight EMA\nattention mechanism.The FDM-YOLO model was validated on the Visdrone dataset,\nachieving a 38% reduction in parameter count and improving the Map0.5 score\nfrom 38.4% to 42.5%, all while maintaining nearly the same inference speed.\nThis demonstrates the effectiveness of our approach in balancing accuracy and\nefficiency for edge device deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small targets are particularly difficult to detect due to their low pixel\ncount, complex backgrounds, and varying shooting angles, which make it hard for\nmodels to extract effective features. While some large-scale models offer high\naccuracy, their long inference times make them unsuitable for real-time\ndeployment on edge devices. On the other hand, models designed for low\ncomputational power often suffer from poor detection accuracy. This paper\nfocuses on small target detection and explores methods for object detection\nunder low computational constraints. Building on the YOLOv8 model, we propose a\nnew network architecture called FDM-YOLO. Our research includes the following\nkey contributions: We introduce FDM-YOLO by analyzing the output of the YOLOv8\ndetection head. We add a highresolution layer and remove the large target\ndetection layer to better handle small targets. Based on PConv, we propose a\nlightweight network structure called Fast-C2f, which is integrated into the PAN\nmodule of the model. To mitigate the accuracy loss caused by model\nlightweighting, we employ dynamic upsampling (Dysample) and a lightweight EMA\nattention mechanism.The FDM-YOLO model was validated on the Visdrone dataset,\nachieving a 38% reduction in parameter count and improving the Map0.5 score\nfrom 38.4% to 42.5%, all while maintaining nearly the same inference speed.\nThis demonstrates the effectiveness of our approach in balancing accuracy and\nefficiency for edge device deployment."
                },
                "authors": [
                    {
                        "name": "Xuerui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuerui Zhang"
                },
                "author": "Xuerui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04444v1",
                "updated": "2025-03-06T14:00:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    0,
                    59,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T14:00:59Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    0,
                    59,
                    3,
                    65,
                    0
                ],
                "title": "ToFu: Visual Tokens Reduction via Fusion for Multi-modal, Multi-patch,\n  Multi-image Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToFu: Visual Tokens Reduction via Fusion for Multi-modal, Multi-patch,\n  Multi-image Task"
                },
                "summary": "Large Multimodal Models (LMMs) are powerful tools that are capable of\nreasoning and understanding multimodal information beyond text and language.\nDespite their entrenched impact, the development of LMMs is hindered by the\nhigher computational requirements compared to their unimodal counterparts. One\nof the main causes of this is the large amount of tokens needed to encode the\nvisual input, which is especially evident for multi-image multimodal tasks.\nRecent approaches to reduce visual tokens depend on the visual encoder\narchitecture, require fine-tuning the LLM to maintain the performance, and only\nconsider single-image scenarios. To address these limitations, we propose ToFu,\na visual encoder-agnostic, training-free Token Fusion strategy that combines\nredundant visual tokens of LMMs for high-resolution, multi-image, tasks. The\ncore intuition behind our method is straightforward yet effective: preserve\ndistinctive tokens while combining similar ones. We achieve this by\nsequentially examining visual tokens and deciding whether to merge them with\nothers or keep them as separate entities. We validate our approach on the\nwell-established LLaVA-Interleave Bench, which covers challenging multi-image\ntasks. In addition, we push to the extreme our method by testing it on a\nnewly-created benchmark, ComPairs, focused on multi-image comparisons where a\nlarger amount of images and visual tokens are inputted to the LMMs. Our\nextensive analysis, considering several LMM architectures, demonstrates the\nbenefits of our approach both in terms of efficiency and performance gain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) are powerful tools that are capable of\nreasoning and understanding multimodal information beyond text and language.\nDespite their entrenched impact, the development of LMMs is hindered by the\nhigher computational requirements compared to their unimodal counterparts. One\nof the main causes of this is the large amount of tokens needed to encode the\nvisual input, which is especially evident for multi-image multimodal tasks.\nRecent approaches to reduce visual tokens depend on the visual encoder\narchitecture, require fine-tuning the LLM to maintain the performance, and only\nconsider single-image scenarios. To address these limitations, we propose ToFu,\na visual encoder-agnostic, training-free Token Fusion strategy that combines\nredundant visual tokens of LMMs for high-resolution, multi-image, tasks. The\ncore intuition behind our method is straightforward yet effective: preserve\ndistinctive tokens while combining similar ones. We achieve this by\nsequentially examining visual tokens and deciding whether to merge them with\nothers or keep them as separate entities. We validate our approach on the\nwell-established LLaVA-Interleave Bench, which covers challenging multi-image\ntasks. In addition, we push to the extreme our method by testing it on a\nnewly-created benchmark, ComPairs, focused on multi-image comparisons where a\nlarger amount of images and visual tokens are inputted to the LMMs. Our\nextensive analysis, considering several LMM architectures, demonstrates the\nbenefits of our approach both in terms of efficiency and performance gain."
                },
                "authors": [
                    {
                        "name": "Vittorio Pippi"
                    },
                    {
                        "name": "Matthieu Guillaumin"
                    },
                    {
                        "name": "Silvia Cascianelli"
                    },
                    {
                        "name": "Rita Cucchiara"
                    },
                    {
                        "name": "Maximilian Jaritz"
                    },
                    {
                        "name": "Loris Bazzani"
                    }
                ],
                "author_detail": {
                    "name": "Loris Bazzani"
                },
                "author": "Loris Bazzani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.07978v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.07978v4",
                "updated": "2025-03-06T13:29:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    29,
                    24,
                    3,
                    65,
                    0
                ],
                "published": "2023-11-14T08:10:14Z",
                "published_parsed": [
                    2023,
                    11,
                    14,
                    8,
                    10,
                    14,
                    1,
                    318,
                    0
                ],
                "title": "AfroBench: How Good are Large Language Models on African Languages?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AfroBench: How Good are Large Language Models on African Languages?"
                },
                "summary": "Large-scale multilingual evaluations, such as MEGA, often include only a\nhandful of African languages due to the scarcity of high-quality evaluation\ndata and the limited discoverability of existing African datasets. This lack of\nrepresentation hinders comprehensive LLM evaluation across a diverse range of\nlanguages and tasks. To address these challenges, we introduce AfroBench -- a\nmulti-task benchmark for evaluating the performance of LLMs across 64 African\nlanguages, 15 tasks and 22 datasets. AfroBench consists of nine natural\nlanguage understanding datasets, six text generation datasets, six knowledge\nand question answering tasks, and one mathematical reasoning task. We present\nresults comparing the performance of prompting LLMs to fine-tuned baselines\nbased on BERT and T5-style models. Our results suggest large gaps in\nperformance between high-resource languages, such as English, and African\nlanguages across most tasks; but performance also varies based on the\navailability of monolingual data resources. Our findings confirm that\nperformance on African languages continues to remain a hurdle for current LLMs,\nunderscoring the need for additional efforts to close this gap.\n  https://mcgill-nlp.github.io/AfroBench/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale multilingual evaluations, such as MEGA, often include only a\nhandful of African languages due to the scarcity of high-quality evaluation\ndata and the limited discoverability of existing African datasets. This lack of\nrepresentation hinders comprehensive LLM evaluation across a diverse range of\nlanguages and tasks. To address these challenges, we introduce AfroBench -- a\nmulti-task benchmark for evaluating the performance of LLMs across 64 African\nlanguages, 15 tasks and 22 datasets. AfroBench consists of nine natural\nlanguage understanding datasets, six text generation datasets, six knowledge\nand question answering tasks, and one mathematical reasoning task. We present\nresults comparing the performance of prompting LLMs to fine-tuned baselines\nbased on BERT and T5-style models. Our results suggest large gaps in\nperformance between high-resource languages, such as English, and African\nlanguages across most tasks; but performance also varies based on the\navailability of monolingual data resources. Our findings confirm that\nperformance on African languages continues to remain a hurdle for current LLMs,\nunderscoring the need for additional efforts to close this gap.\n  https://mcgill-nlp.github.io/AfroBench/"
                },
                "authors": [
                    {
                        "name": "Jessica Ojo"
                    },
                    {
                        "name": "Odunayo Ogundepo"
                    },
                    {
                        "name": "Akintunde Oladipo"
                    },
                    {
                        "name": "Kelechi Ogueji"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Pontus Stenetorp"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    }
                ],
                "author_detail": {
                    "name": "David Ifeoluwa Adelani"
                },
                "author": "David Ifeoluwa Adelani",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.07978v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.07978v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04418v1",
                "updated": "2025-03-06T13:21:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    21,
                    38,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T13:21:38Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    21,
                    38,
                    3,
                    65,
                    0
                ],
                "title": "AOLO: Analysis and Optimization For Low-Carbon Oriented Wireless Large\n  Language Model Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AOLO: Analysis and Optimization For Low-Carbon Oriented Wireless Large\n  Language Model Services"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to their\nwidespread adoption and large-scale deployment across various domains. However,\ntheir environmental impact, particularly during inference, has become a growing\nconcern due to their substantial energy consumption and carbon footprint.\nExisting research has focused on inference computation alone, overlooking the\nanalysis and optimization of carbon footprint in network-aided LLM service\nsystems. To address this gap, we propose AOLO, a framework for analysis and\noptimization for low-carbon oriented wireless LLM services. AOLO introduces a\ncomprehensive carbon footprint model that quantifies greenhouse gas emissions\nacross the entire LLM service chain, including computational inference and\nwireless communication. Furthermore, we formulate an optimization problem aimed\nat minimizing the overall carbon footprint, which is solved through joint\noptimization of inference outputs and transmit power under\nquality-of-experience and system performance constraints. To achieve this joint\noptimization, we leverage the energy efficiency of spiking neural networks\n(SNNs) by adopting SNN as the actor network and propose a low-carbon-oriented\noptimization algorithm, i.e., SNN-based deep reinforcement learning (SDRL).\nComprehensive simulations demonstrate that SDRL algorithm significantly reduces\noverall carbon footprint, achieving an 18.77% reduction compared to the\nbenchmark soft actor-critic, highlighting its potential for enabling more\nsustainable LLM inference services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to their\nwidespread adoption and large-scale deployment across various domains. However,\ntheir environmental impact, particularly during inference, has become a growing\nconcern due to their substantial energy consumption and carbon footprint.\nExisting research has focused on inference computation alone, overlooking the\nanalysis and optimization of carbon footprint in network-aided LLM service\nsystems. To address this gap, we propose AOLO, a framework for analysis and\noptimization for low-carbon oriented wireless LLM services. AOLO introduces a\ncomprehensive carbon footprint model that quantifies greenhouse gas emissions\nacross the entire LLM service chain, including computational inference and\nwireless communication. Furthermore, we formulate an optimization problem aimed\nat minimizing the overall carbon footprint, which is solved through joint\noptimization of inference outputs and transmit power under\nquality-of-experience and system performance constraints. To achieve this joint\noptimization, we leverage the energy efficiency of spiking neural networks\n(SNNs) by adopting SNN as the actor network and propose a low-carbon-oriented\noptimization algorithm, i.e., SNN-based deep reinforcement learning (SDRL).\nComprehensive simulations demonstrate that SDRL algorithm significantly reduces\noverall carbon footprint, achieving an 18.77% reduction compared to the\nbenchmark soft actor-critic, highlighting its potential for enabling more\nsustainable LLM inference services."
                },
                "authors": [
                    {
                        "name": "Xiaoqi Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Yuehong Gao"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04412v1",
                "updated": "2025-03-06T13:10:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    10,
                    40,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T13:10:40Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    10,
                    40,
                    3,
                    65,
                    0
                ],
                "title": "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive\n  Branching Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive\n  Branching Tree Search"
                },
                "summary": "Recent advances demonstrate that increasing inference-time computation can\nsignificantly boost the reasoning capabilities of large language models (LLMs).\nAlthough repeated sampling (i.e., generating multiple candidate outputs) is a\nhighly effective strategy, it does not leverage external feedback signals for\nrefinement, which are often available in tasks like coding. In this work, we\npropose $\\textit{Adaptive Branching Monte Carlo Tree Search (AB-MCTS)}$, a\nnovel inference-time framework that generalizes repeated sampling with\nprincipled multi-turn exploration and exploitation. At each node in the search\ntree, AB-MCTS dynamically decides whether to \"go wider\" by expanding new\ncandidate responses or \"go deeper\" by revisiting existing ones based on\nexternal feedback signals. We evaluate our method on complex coding and\nengineering tasks using frontier models. Empirical results show that AB-MCTS\nconsistently outperforms both repeated sampling and standard MCTS, underscoring\nthe importance of combining the response diversity of LLMs with multi-turn\nsolution refinement for effective inference-time scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances demonstrate that increasing inference-time computation can\nsignificantly boost the reasoning capabilities of large language models (LLMs).\nAlthough repeated sampling (i.e., generating multiple candidate outputs) is a\nhighly effective strategy, it does not leverage external feedback signals for\nrefinement, which are often available in tasks like coding. In this work, we\npropose $\\textit{Adaptive Branching Monte Carlo Tree Search (AB-MCTS)}$, a\nnovel inference-time framework that generalizes repeated sampling with\nprincipled multi-turn exploration and exploitation. At each node in the search\ntree, AB-MCTS dynamically decides whether to \"go wider\" by expanding new\ncandidate responses or \"go deeper\" by revisiting existing ones based on\nexternal feedback signals. We evaluate our method on complex coding and\nengineering tasks using frontier models. Empirical results show that AB-MCTS\nconsistently outperforms both repeated sampling and standard MCTS, underscoring\nthe importance of combining the response diversity of LLMs with multi-turn\nsolution refinement for effective inference-time scaling."
                },
                "authors": [
                    {
                        "name": "Kou Misaki"
                    },
                    {
                        "name": "Yuichi Inoue"
                    },
                    {
                        "name": "Yuki Imajuku"
                    },
                    {
                        "name": "So Kuroki"
                    },
                    {
                        "name": "Taishi Nakamura"
                    },
                    {
                        "name": "Takuya Akiba"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Akiba"
                },
                "author": "Takuya Akiba",
                "arxiv_comment": "To appear at ICLR 2025 Workshop on Foundation Models in the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12753v2",
                "updated": "2025-03-06T12:55:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    55,
                    25,
                    3,
                    65,
                    0
                ],
                "published": "2024-06-18T16:20:53Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    16,
                    20,
                    53,
                    1,
                    170,
                    0
                ],
                "title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for\n  Superintelligent AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for\n  Superintelligent AI"
                },
                "summary": "The evolution of Artificial Intelligence (AI) has been significantly\naccelerated by advancements in Large Language Models (LLMs) and Large\nMultimodal Models (LMMs), gradually showcasing potential cognitive reasoning\nabilities in problem-solving and scientific discovery (i.e., AI4Science) once\nexclusive to human intellect. To comprehensively evaluate current models'\nperformance in cognitive reasoning abilities, we introduce OlympicArena, which\nincludes 11,163 bilingual problems across both text-only and interleaved\ntext-image modalities. These challenges encompass a wide range of disciplines\nspanning seven fields and 62 international Olympic competitions, rigorously\nexamined for data leakage. We argue that the challenges in Olympic competition\nproblems are ideal for evaluating AI's cognitive reasoning due to their\ncomplexity and interdisciplinary nature, which are essential for tackling\ncomplex scientific challenges and facilitating discoveries. Beyond evaluating\nperformance across various disciplines using answer-only criteria, we conduct\ndetailed experiments and analyses from multiple perspectives. We delve into the\nmodels' cognitive reasoning abilities, their performance across different\nmodalities, and their outcomes in process-level evaluations, which are vital\nfor tasks requiring complex reasoning with lengthy solutions. Our extensive\nevaluations reveal that even advanced models like GPT-4o only achieve a 39.97%\noverall accuracy, illustrating current AI limitations in complex reasoning and\nmultimodal integration. Through the OlympicArena, we aim to advance AI towards\nsuperintelligence, equipping it to address more complex challenges in science\nand beyond. We also provide a comprehensive set of resources to support AI\nresearch, including a benchmark dataset, an open-source annotation platform, a\ndetailed evaluation tool, and a leaderboard with automatic submission features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of Artificial Intelligence (AI) has been significantly\naccelerated by advancements in Large Language Models (LLMs) and Large\nMultimodal Models (LMMs), gradually showcasing potential cognitive reasoning\nabilities in problem-solving and scientific discovery (i.e., AI4Science) once\nexclusive to human intellect. To comprehensively evaluate current models'\nperformance in cognitive reasoning abilities, we introduce OlympicArena, which\nincludes 11,163 bilingual problems across both text-only and interleaved\ntext-image modalities. These challenges encompass a wide range of disciplines\nspanning seven fields and 62 international Olympic competitions, rigorously\nexamined for data leakage. We argue that the challenges in Olympic competition\nproblems are ideal for evaluating AI's cognitive reasoning due to their\ncomplexity and interdisciplinary nature, which are essential for tackling\ncomplex scientific challenges and facilitating discoveries. Beyond evaluating\nperformance across various disciplines using answer-only criteria, we conduct\ndetailed experiments and analyses from multiple perspectives. We delve into the\nmodels' cognitive reasoning abilities, their performance across different\nmodalities, and their outcomes in process-level evaluations, which are vital\nfor tasks requiring complex reasoning with lengthy solutions. Our extensive\nevaluations reveal that even advanced models like GPT-4o only achieve a 39.97%\noverall accuracy, illustrating current AI limitations in complex reasoning and\nmultimodal integration. Through the OlympicArena, we aim to advance AI towards\nsuperintelligence, equipping it to address more complex challenges in science\nand beyond. We also provide a comprehensive set of resources to support AI\nresearch, including a benchmark dataset, an open-source annotation platform, a\ndetailed evaluation tool, and a leaderboard with automatic submission features."
                },
                "authors": [
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Zengzhi Wang"
                    },
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Xuefeng Li"
                    },
                    {
                        "name": "Haoyang Zou"
                    },
                    {
                        "name": "Ruijie Xu"
                    },
                    {
                        "name": "Run-Ze Fan"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Ethan Chern"
                    },
                    {
                        "name": "Yixin Ye"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Ting Wu"
                    },
                    {
                        "name": "Binjie Wang"
                    },
                    {
                        "name": "Shichao Sun"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Yiyuan Li"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Steffi Chern"
                    },
                    {
                        "name": "Yiwei Qin"
                    },
                    {
                        "name": "Yan Ma"
                    },
                    {
                        "name": "Jiadi Su"
                    },
                    {
                        "name": "Yixiu Liu"
                    },
                    {
                        "name": "Yuxiang Zheng"
                    },
                    {
                        "name": "Shaoting Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05569v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05569v3",
                "updated": "2025-03-06T12:54:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    54,
                    37,
                    3,
                    65,
                    0
                ],
                "published": "2024-04-08T14:43:13Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    14,
                    43,
                    13,
                    0,
                    99,
                    0
                ],
                "title": "360$^\\circ$REA: Towards A Reusable Experience Accumulation with\n  360° Assessment for Multi-Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "360$^\\circ$REA: Towards A Reusable Experience Accumulation with\n  360° Assessment for Multi-Agent System"
                },
                "summary": "Large language model agents have demonstrated remarkable advancements across\nvarious complex tasks. Recent works focus on optimizing the agent team or\nemploying self-reflection to iteratively solve complex tasks. Since these\nagents are all based on the same LLM, only conducting self-evaluation or\nremoving underperforming agents does not substantively enhance the capability\nof the agents. We argue that a comprehensive evaluation and accumulating\nexperience from evaluation feedback is an effective approach to improving\nsystem performance. In this paper, we propose Reusable Experience Accumulation\nwith 360$^\\circ$ Assessment (360$^\\circ$REA), a hierarchical multi-agent\nframework inspired by corporate organizational practices. The framework employs\na novel 360$^\\circ$ performance assessment method for multi-perspective\nperformance evaluation with fine-grained assessment. To enhance the capability\nof agents in addressing complex tasks, we introduce dual-level experience pool\nfor agents to accumulate experience through fine-grained assessment. Extensive\nexperiments on complex task datasets demonstrate the effectiveness of\n360$^\\circ$REA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model agents have demonstrated remarkable advancements across\nvarious complex tasks. Recent works focus on optimizing the agent team or\nemploying self-reflection to iteratively solve complex tasks. Since these\nagents are all based on the same LLM, only conducting self-evaluation or\nremoving underperforming agents does not substantively enhance the capability\nof the agents. We argue that a comprehensive evaluation and accumulating\nexperience from evaluation feedback is an effective approach to improving\nsystem performance. In this paper, we propose Reusable Experience Accumulation\nwith 360$^\\circ$ Assessment (360$^\\circ$REA), a hierarchical multi-agent\nframework inspired by corporate organizational practices. The framework employs\na novel 360$^\\circ$ performance assessment method for multi-perspective\nperformance evaluation with fine-grained assessment. To enhance the capability\nof agents in addressing complex tasks, we introduce dual-level experience pool\nfor agents to accumulate experience through fine-grained assessment. Extensive\nexperiments on complex task datasets demonstrate the effectiveness of\n360$^\\circ$REA."
                },
                "authors": [
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Chengrui Huang"
                    },
                    {
                        "name": "Quan Tu"
                    },
                    {
                        "name": "Zhiliang Tian"
                    },
                    {
                        "name": "Minlie Huang"
                    },
                    {
                        "name": "Shuo Shang"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Shang"
                },
                "author": "Shuo Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05569v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05569v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04398v1",
                "updated": "2025-03-06T12:52:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    52,
                    22,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T12:52:22Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    52,
                    22,
                    3,
                    65,
                    0
                ],
                "title": "Speculative MoE: Communication Efficient Parallel MoE Inference with\n  Speculative Token and Expert Pre-scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative MoE: Communication Efficient Parallel MoE Inference with\n  Speculative Token and Expert Pre-scheduling"
                },
                "summary": "MoE (Mixture of Experts) prevails as a neural architecture that can scale\nmodern transformer-based LLMs (Large Language Models) to unprecedented scales.\nNevertheless, large MoEs' great demands of computing power, memory capacity and\nmemory bandwidth make scalable serving a fundamental challenge and efficient\nparallel inference has become a requisite to attain adequate throughput under\nlatency constraints. DeepSpeed-MoE, one state-of-the-art MoE inference\nframework, adopts a 3D-parallel paradigm including EP (Expert Parallelism), TP\n(Tensor Parallel) and DP (Data Parallelism). However, our analysis shows\nDeepSpeed-MoE's inference efficiency is largely bottlenecked by EP, which is\nimplemented with costly all-to-all collectives to route token activation. Our\nwork aims to boost DeepSpeed-MoE by strategically reducing EP's communication\noverhead with a technique named Speculative MoE. Speculative MoE has two\nspeculative parallelization schemes, speculative token shuffling and\nspeculative expert grouping, which predict outstanding tokens' expert routing\npaths and pre-schedule tokens and experts across devices to losslessly trim\nEP's communication volume. Besides DeepSpeed-MoE, we also build Speculative MoE\ninto a prevailing MoE inference engine SGLang. Experiments show Speculative MoE\ncan significantly boost state-of-the-art MoE inference frameworks on fast\nhomogeneous and slow heterogeneous interconnects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE (Mixture of Experts) prevails as a neural architecture that can scale\nmodern transformer-based LLMs (Large Language Models) to unprecedented scales.\nNevertheless, large MoEs' great demands of computing power, memory capacity and\nmemory bandwidth make scalable serving a fundamental challenge and efficient\nparallel inference has become a requisite to attain adequate throughput under\nlatency constraints. DeepSpeed-MoE, one state-of-the-art MoE inference\nframework, adopts a 3D-parallel paradigm including EP (Expert Parallelism), TP\n(Tensor Parallel) and DP (Data Parallelism). However, our analysis shows\nDeepSpeed-MoE's inference efficiency is largely bottlenecked by EP, which is\nimplemented with costly all-to-all collectives to route token activation. Our\nwork aims to boost DeepSpeed-MoE by strategically reducing EP's communication\noverhead with a technique named Speculative MoE. Speculative MoE has two\nspeculative parallelization schemes, speculative token shuffling and\nspeculative expert grouping, which predict outstanding tokens' expert routing\npaths and pre-schedule tokens and experts across devices to losslessly trim\nEP's communication volume. Besides DeepSpeed-MoE, we also build Speculative MoE\ninto a prevailing MoE inference engine SGLang. Experiments show Speculative MoE\ncan significantly boost state-of-the-art MoE inference frameworks on fast\nhomogeneous and slow heterogeneous interconnects."
                },
                "authors": [
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Pengfei Zheng"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Zewei Xu"
                    },
                    {
                        "name": "Yunfei Du"
                    },
                    {
                        "name": "Zhengang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhengang Wang"
                },
                "author": "Zhengang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04396v1",
                "updated": "2025-03-06T12:50:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    50,
                    14,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T12:50:14Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    50,
                    14,
                    3,
                    65,
                    0
                ],
                "title": "TableLoRA: Low-rank Adaptation on Table Structure Understanding for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableLoRA: Low-rank Adaptation on Table Structure Understanding for\n  Large Language Models"
                },
                "summary": "Tabular data are crucial in many fields and their understanding by large\nlanguage models (LLMs) under high parameter efficiency paradigm is important.\nHowever, directly applying parameter-efficient fine-tuning (PEFT) techniques to\ntabular tasks presents significant challenges, particularly in terms of better\ntable serialization and the representation of two-dimensional structured\ninformation within a one-dimensional sequence. To address this, we propose\nTableLoRA, a module designed to improve LLMs' understanding of table structure\nduring PEFT. It incorporates special tokens for serializing tables with special\ntoken encoder and uses 2D LoRA to encode low-rank information on cell\npositions. Experiments on four tabular-related datasets demonstrate that\nTableLoRA consistently outperforms vanilla LoRA and surpasses various table\nencoding methods tested in control experiments. These findings reveal that\nTableLoRA, as a table-specific LoRA, enhances the ability of LLMs to process\ntabular data effectively, especially in low-parameter settings, demonstrating\nits potential as a robust solution for handling table-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data are crucial in many fields and their understanding by large\nlanguage models (LLMs) under high parameter efficiency paradigm is important.\nHowever, directly applying parameter-efficient fine-tuning (PEFT) techniques to\ntabular tasks presents significant challenges, particularly in terms of better\ntable serialization and the representation of two-dimensional structured\ninformation within a one-dimensional sequence. To address this, we propose\nTableLoRA, a module designed to improve LLMs' understanding of table structure\nduring PEFT. It incorporates special tokens for serializing tables with special\ntoken encoder and uses 2D LoRA to encode low-rank information on cell\npositions. Experiments on four tabular-related datasets demonstrate that\nTableLoRA consistently outperforms vanilla LoRA and surpasses various table\nencoding methods tested in control experiments. These findings reveal that\nTableLoRA, as a table-specific LoRA, enhances the ability of LLMs to process\ntabular data effectively, especially in low-parameter settings, demonstrating\nits potential as a robust solution for handling table-related tasks."
                },
                "authors": [
                    {
                        "name": "Xinyi He"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Mengyu Zhou"
                    },
                    {
                        "name": "Yeye He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Shi Han"
                    },
                    {
                        "name": "Zejian Yuan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04395v1",
                "updated": "2025-03-06T12:47:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    47,
                    54,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T12:47:54Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    47,
                    54,
                    3,
                    65,
                    0
                ],
                "title": "Shaping Shared Languages: Human and Large Language Models' Inductive\n  Biases in Emergent Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shaping Shared Languages: Human and Large Language Models' Inductive\n  Biases in Emergent Communication"
                },
                "summary": "Languages are shaped by the inductive biases of their users. Using a\nclassical referential game, we investigate how artificial languages evolve when\noptimised for inductive biases in humans and large language models (LLMs) via\nHuman-Human, LLM-LLM and Human-LLM experiments. We show that referentially\ngrounded vocabularies emerge that enable reliable communication in all\nconditions, even when humans and LLMs collaborate. Comparisons between\nconditions reveal that languages optimised for LLMs subtly differ from those\noptimised for humans. Interestingly, interactions between humans and LLMs\nalleviate these differences and result in vocabularies which are more\nhuman-like than LLM-like. These findings advance our understanding of how\ninductive biases in LLMs play a role in the dynamic nature of human language\nand contribute to maintaining alignment in human and machine communication. In\nparticular, our work underscores the need to think of new methods that include\nhuman interaction in the training processes of LLMs, and shows that using\ncommunicative success as a reward signal can be a fruitful, novel direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Languages are shaped by the inductive biases of their users. Using a\nclassical referential game, we investigate how artificial languages evolve when\noptimised for inductive biases in humans and large language models (LLMs) via\nHuman-Human, LLM-LLM and Human-LLM experiments. We show that referentially\ngrounded vocabularies emerge that enable reliable communication in all\nconditions, even when humans and LLMs collaborate. Comparisons between\nconditions reveal that languages optimised for LLMs subtly differ from those\noptimised for humans. Interestingly, interactions between humans and LLMs\nalleviate these differences and result in vocabularies which are more\nhuman-like than LLM-like. These findings advance our understanding of how\ninductive biases in LLMs play a role in the dynamic nature of human language\nand contribute to maintaining alignment in human and machine communication. In\nparticular, our work underscores the need to think of new methods that include\nhuman interaction in the training processes of LLMs, and shows that using\ncommunicative success as a reward signal can be a fruitful, novel direction."
                },
                "authors": [
                    {
                        "name": "Tom Kouwenhoven"
                    },
                    {
                        "name": "Max Peeperkorn"
                    },
                    {
                        "name": "Roy de Kleijn"
                    },
                    {
                        "name": "Tessa Verhoef"
                    }
                ],
                "author_detail": {
                    "name": "Tessa Verhoef"
                },
                "author": "Tessa Verhoef",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04392v1",
                "updated": "2025-03-06T12:41:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    41,
                    54,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T12:41:54Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    41,
                    54,
                    3,
                    65,
                    0
                ],
                "title": "AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems\n  via Hierarchical Data Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems\n  via Hierarchical Data Management"
                },
                "summary": "Large Language Model based multi-agent systems are revolutionizing autonomous\ncommunication and collaboration, yet they remain vulnerable to security threats\nlike unauthorized access and data breaches. To address this, we introduce\nAgentSafe, a novel framework that enhances MAS security through hierarchical\ninformation management and memory protection. AgentSafe classifies information\nby security levels, restricting sensitive data access to authorized agents.\nAgentSafe incorporates two components: ThreatSieve, which secures communication\nby verifying information authority and preventing impersonation, and\nHierarCache, an adaptive memory management system that defends against\nunauthorized access and malicious poisoning, representing the first systematic\ndefense for agent memory. Experiments across various LLMs show that AgentSafe\nsignificantly boosts system resilience, achieving defense success rates above\n80% under adversarial conditions. Additionally, AgentSafe demonstrates\nscalability, maintaining robust performance as agent numbers and information\ncomplexity grow. Results underscore effectiveness of AgentSafe in securing MAS\nand its potential for real-world application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model based multi-agent systems are revolutionizing autonomous\ncommunication and collaboration, yet they remain vulnerable to security threats\nlike unauthorized access and data breaches. To address this, we introduce\nAgentSafe, a novel framework that enhances MAS security through hierarchical\ninformation management and memory protection. AgentSafe classifies information\nby security levels, restricting sensitive data access to authorized agents.\nAgentSafe incorporates two components: ThreatSieve, which secures communication\nby verifying information authority and preventing impersonation, and\nHierarCache, an adaptive memory management system that defends against\nunauthorized access and malicious poisoning, representing the first systematic\ndefense for agent memory. Experiments across various LLMs show that AgentSafe\nsignificantly boosts system resilience, achieving defense success rates above\n80% under adversarial conditions. Additionally, AgentSafe demonstrates\nscalability, maintaining robust performance as agent numbers and information\ncomplexity grow. Results underscore effectiveness of AgentSafe in securing MAS\nand its potential for real-world application."
                },
                "authors": [
                    {
                        "name": "Junyuan Mao"
                    },
                    {
                        "name": "Fanci Meng"
                    },
                    {
                        "name": "Yifan Duan"
                    },
                    {
                        "name": "Miao Yu"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Yuxuan Liang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21083v2",
                "updated": "2025-03-06T12:38:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    38,
                    42,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-28T14:48:05Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    48,
                    5,
                    0,
                    302,
                    0
                ],
                "title": "Stealthy Jailbreak Attacks on Large Language Models via Benign Data\n  Mirroring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stealthy Jailbreak Attacks on Large Language Models via Benign Data\n  Mirroring"
                },
                "summary": "Large language model (LLM) safety is a critical issue, with numerous studies\nemploying red team testing to enhance model security. Among these, jailbreak\nmethods explore potential vulnerabilities by crafting malicious prompts that\ninduce model outputs contrary to safety alignments. Existing black-box\njailbreak methods often rely on model feedback, repeatedly submitting queries\nwith detectable malicious instructions during the attack search process.\nAlthough these approaches are effective, the attacks may be intercepted by\ncontent moderators during the search process. We propose an improved transfer\nattack method that guides malicious prompt construction by locally training a\nmirror model of the target black-box model through benign data distillation.\nThis method offers enhanced stealth, as it does not involve submitting\nidentifiable malicious instructions to the target model during the search\nphase. Our approach achieved a maximum attack success rate of 92%, or a\nbalanced value of 80% with an average of 1.5 detectable jailbreak queries per\nsample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore\nthe need for more robust defense mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) safety is a critical issue, with numerous studies\nemploying red team testing to enhance model security. Among these, jailbreak\nmethods explore potential vulnerabilities by crafting malicious prompts that\ninduce model outputs contrary to safety alignments. Existing black-box\njailbreak methods often rely on model feedback, repeatedly submitting queries\nwith detectable malicious instructions during the attack search process.\nAlthough these approaches are effective, the attacks may be intercepted by\ncontent moderators during the search process. We propose an improved transfer\nattack method that guides malicious prompt construction by locally training a\nmirror model of the target black-box model through benign data distillation.\nThis method offers enhanced stealth, as it does not involve submitting\nidentifiable malicious instructions to the target model during the search\nphase. Our approach achieved a maximum attack success rate of 92%, or a\nbalanced value of 80% with an average of 1.5 detectable jailbreak queries per\nsample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore\nthe need for more robust defense mechanisms."
                },
                "authors": [
                    {
                        "name": "Honglin Mu"
                    },
                    {
                        "name": "Han He"
                    },
                    {
                        "name": "Yuxin Zhou"
                    },
                    {
                        "name": "Yunlong Feng"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Xiaoming Shi"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Qi Shi"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Accepted by NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04388v1",
                "updated": "2025-03-06T12:38:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    38,
                    17,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T12:38:17Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    38,
                    17,
                    3,
                    65,
                    0
                ],
                "title": "More Documents, Same Length: Isolating the Challenge of Multiple\n  Documents in RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Documents, Same Length: Isolating the Challenge of Multiple\n  Documents in RAG"
                },
                "summary": "Retrieval-augmented generation (RAG) provides LLMs with relevant documents.\nAlthough previous studies noted that retrieving many documents can degrade\nperformance, they did not isolate how the quantity of documents affects\nperformance while controlling for context length. We evaluate various language\nmodels on custom datasets derived from a multi-hop QA task. We keep the context\nlength and position of relevant information constant while varying the number\nof documents, and find that increasing the document count in RAG settings poses\nsignificant challenges for LLMs. Additionally, our results indicate that\nprocessing multiple documents is a separate challenge from handling long\ncontexts. We also make the datasets and code available:\nhttps://github.com/shaharl6000/MoreDocsSameLen .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) provides LLMs with relevant documents.\nAlthough previous studies noted that retrieving many documents can degrade\nperformance, they did not isolate how the quantity of documents affects\nperformance while controlling for context length. We evaluate various language\nmodels on custom datasets derived from a multi-hop QA task. We keep the context\nlength and position of relevant information constant while varying the number\nof documents, and find that increasing the document count in RAG settings poses\nsignificant challenges for LLMs. Additionally, our results indicate that\nprocessing multiple documents is a separate challenge from handling long\ncontexts. We also make the datasets and code available:\nhttps://github.com/shaharl6000/MoreDocsSameLen ."
                },
                "authors": [
                    {
                        "name": "Shahar Levy"
                    },
                    {
                        "name": "Nir Mazor"
                    },
                    {
                        "name": "Lihi Shalmon"
                    },
                    {
                        "name": "Michael Hassid"
                    },
                    {
                        "name": "Gabriel Stanovsky"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Stanovsky"
                },
                "author": "Gabriel Stanovsky",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04381v1",
                "updated": "2025-03-06T12:33:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    33,
                    20,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T12:33:20Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    33,
                    20,
                    3,
                    65,
                    0
                ],
                "title": "TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for\n  LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for\n  LLM-as-a-Judge"
                },
                "summary": "The LLM-as-a-judge paradigm uses large language models (LLMs) for automated\ntext evaluation, where a numerical assessment is assigned by an LLM to the\ninput text following scoring rubrics. Existing methods for LLM-as-a-judge use\ncross-entropy (CE) loss for fine-tuning, which neglects the numeric nature of\nscore prediction. Recent work addresses numerical prediction limitations of LLM\nfine-tuning through regression-aware fine-tuning, which, however, does not\nconsider chain-of-thought (CoT) reasoning for score prediction. In this paper,\nwe introduce TRACT (Two-stage Regression-Aware fine-tuning with CoT), a method\ncombining CoT reasoning with regression-aware training. TRACT consists of two\nstages: first, seed LLM is fine-tuned to generate CoTs, which serve as\nsupervision for the second stage fine-tuning. The training objective of TRACT\ncombines the CE loss for learning the CoT reasoning capabilities, and the\nregression-aware loss for the score prediction. Experiments across four\nLLM-as-a-judge datasets and two LLMs show that TRACT significantly outperforms\nexisting methods. Extensive ablation studies validate the importance of each\ncomponent in TRACT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM-as-a-judge paradigm uses large language models (LLMs) for automated\ntext evaluation, where a numerical assessment is assigned by an LLM to the\ninput text following scoring rubrics. Existing methods for LLM-as-a-judge use\ncross-entropy (CE) loss for fine-tuning, which neglects the numeric nature of\nscore prediction. Recent work addresses numerical prediction limitations of LLM\nfine-tuning through regression-aware fine-tuning, which, however, does not\nconsider chain-of-thought (CoT) reasoning for score prediction. In this paper,\nwe introduce TRACT (Two-stage Regression-Aware fine-tuning with CoT), a method\ncombining CoT reasoning with regression-aware training. TRACT consists of two\nstages: first, seed LLM is fine-tuned to generate CoTs, which serve as\nsupervision for the second stage fine-tuning. The training objective of TRACT\ncombines the CE loss for learning the CoT reasoning capabilities, and the\nregression-aware loss for the score prediction. Experiments across four\nLLM-as-a-judge datasets and two LLMs show that TRACT significantly outperforms\nexisting methods. Extensive ablation studies validate the importance of each\ncomponent in TRACT."
                },
                "authors": [
                    {
                        "name": "Cheng-Han Chiang"
                    },
                    {
                        "name": "Hung-yi Lee"
                    },
                    {
                        "name": "Michal Lukasik"
                    }
                ],
                "author_detail": {
                    "name": "Michal Lukasik"
                },
                "author": "Michal Lukasik",
                "arxiv_comment": "Codes and models are available at https://github.com/d223302/TRACT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04377v1",
                "updated": "2025-03-06T12:28:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    28,
                    59,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T12:28:59Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    28,
                    59,
                    3,
                    65,
                    0
                ],
                "title": "How can representation dimension dominate structurally pruned LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can representation dimension dominate structurally pruned LLMs?"
                },
                "summary": "Pruning assumes a subnetwork exists in the original deep neural network,\nwhich can achieve comparative model performance with less computation than the\noriginal. However, it is unclear how the model performance varies with the\ndifferent subnetwork extractions. In this paper, we choose the representation\ndimension (or embedding dimension, model dimension, the dimension of the\nresidual stream in the relevant literature) as the entry point to this issue.\nWe investigate the linear transformations in the LLM transformer blocks and\nconsider a specific structured pruning approach, SliceGPT, to extract the\nsubnetworks of different representation dimensions. We mechanistically analyse\nthe activation flow during the model forward passes, and find the\nrepresentation dimension dominates the linear transformations, model\npredictions, and, finally, the model performance. Explicit analytical relations\nare given to calculate the pruned model performance (perplexity and accuracy)\nwithout actual evaluation, and are empirically validated with\nLlama-3-8B-Instruct and Phi-3-mini-4k-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning assumes a subnetwork exists in the original deep neural network,\nwhich can achieve comparative model performance with less computation than the\noriginal. However, it is unclear how the model performance varies with the\ndifferent subnetwork extractions. In this paper, we choose the representation\ndimension (or embedding dimension, model dimension, the dimension of the\nresidual stream in the relevant literature) as the entry point to this issue.\nWe investigate the linear transformations in the LLM transformer blocks and\nconsider a specific structured pruning approach, SliceGPT, to extract the\nsubnetworks of different representation dimensions. We mechanistically analyse\nthe activation flow during the model forward passes, and find the\nrepresentation dimension dominates the linear transformations, model\npredictions, and, finally, the model performance. Explicit analytical relations\nare given to calculate the pruned model performance (perplexity and accuracy)\nwithout actual evaluation, and are empirically validated with\nLlama-3-8B-Instruct and Phi-3-mini-4k-Instruct."
                },
                "authors": [
                    {
                        "name": "Mingxue Xu"
                    },
                    {
                        "name": "Lisa Alazraki"
                    },
                    {
                        "name": "Danilo P. Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo P. Mandic"
                },
                "author": "Danilo P. Mandic",
                "arxiv_comment": "ICLR 2025 Workshop on Sparsity in LLMs (SLLM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01346v2",
                "updated": "2025-03-06T12:27:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    27,
                    24,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-03T09:37:33Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    37,
                    33,
                    0,
                    62,
                    0
                ],
                "title": "SRAG: Structured Retrieval-Augmented Generation for Multi-Entity\n  Question Answering over Wikipedia Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRAG: Structured Retrieval-Augmented Generation for Multi-Entity\n  Question Answering over Wikipedia Graph"
                },
                "summary": "Multi-entity question answering (MEQA) poses significant challenges for large\nlanguage models (LLMs), which often struggle to consolidate scattered\ninformation across multiple documents. An example question might be \"What is\nthe distribution of IEEE Fellows among various fields of study?\", which\nrequires retrieving information from diverse sources e.g., Wikipedia pages. The\neffectiveness of current retrieval-augmented generation (RAG) methods is\nlimited by the LLMs' capacity to aggregate insights from numerous pages. To\naddress this gap, this paper introduces a structured RAG (SRAG) framework that\nsystematically organizes extracted entities into relational tables (e.g.,\ntabulating entities with schema columns like \"name\" and \"field of study\") and\nthen apply table-based reasoning techniques. Our approach decouples retrieval\nand reasoning, enabling LLMs to focus on structured data analysis rather than\nraw text aggregation. Extensive experiments on Wikipedia-based multi-entity QA\ntasks demonstrate that SRAG significantly outperforms state-of-the-art\nlong-context LLMs and RAG solutions, achieving a 29.6% improvement in accuracy.\nThe results underscore the efficacy of structuring unstructured data to enhance\nLLMs' reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-entity question answering (MEQA) poses significant challenges for large\nlanguage models (LLMs), which often struggle to consolidate scattered\ninformation across multiple documents. An example question might be \"What is\nthe distribution of IEEE Fellows among various fields of study?\", which\nrequires retrieving information from diverse sources e.g., Wikipedia pages. The\neffectiveness of current retrieval-augmented generation (RAG) methods is\nlimited by the LLMs' capacity to aggregate insights from numerous pages. To\naddress this gap, this paper introduces a structured RAG (SRAG) framework that\nsystematically organizes extracted entities into relational tables (e.g.,\ntabulating entities with schema columns like \"name\" and \"field of study\") and\nthen apply table-based reasoning techniques. Our approach decouples retrieval\nand reasoning, enabling LLMs to focus on structured data analysis rather than\nraw text aggregation. Extensive experiments on Wikipedia-based multi-entity QA\ntasks demonstrate that SRAG significantly outperforms state-of-the-art\nlong-context LLMs and RAG solutions, achieving a 29.6% improvement in accuracy.\nThe results underscore the efficacy of structuring unstructured data to enhance\nLLMs' reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Teng Lin"
                    },
                    {
                        "name": "Yizhang Zhu"
                    },
                    {
                        "name": "Yuyu Luo"
                    },
                    {
                        "name": "Nan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Tang"
                },
                "author": "Nan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08824v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08824v3",
                "updated": "2025-03-06T12:26:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    26,
                    8,
                    3,
                    65,
                    0
                ],
                "published": "2024-09-13T13:37:33Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    37,
                    33,
                    4,
                    257,
                    0
                ],
                "title": "Pathfinder for Low-altitude Aircraft with Binary Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pathfinder for Low-altitude Aircraft with Binary Neural Network"
                },
                "summary": "A prior global topological map (e.g., the OpenStreetMap, OSM) can boost the\nperformance of autonomous mapping by a ground mobile robot. However, the prior\nmap is usually incomplete due to lacking labeling in partial paths. To solve\nthis problem, this paper proposes an OSM maker using airborne sensors carried\nby low-altitude aircraft, where the core of the OSM maker is a novel efficient\npathfinder approach based on LiDAR and camera data, i.e., a binary dual-stream\nroad segmentation model. Specifically, a multi-scale feature extraction based\non the UNet architecture is implemented for images and point clouds. To reduce\nthe effect caused by the sparsity of point cloud, an attention-guided gated\nblock is designed to integrate image and point-cloud features. To optimize the\nmodel for edge deployment that significantly reduces storage footprint and\ncomputational demands, we propose a binarization streamline to each model\ncomponent, including a variant of vision transformer (ViT) architecture as the\nencoder of the image branch, and new focal and perception losses to optimize\nthe model training. The experimental results on two datasets demonstrate that\nour pathfinder method achieves SOTA accuracy with high efficiency in finding\npaths from the low-level airborne sensors, and we can create complete OSM prior\nmaps based on the segmented road skeletons. Code and data are available at:\n\\href{https://github.com/IMRL/Pathfinder}{https://github.com/IMRL/Pathfinder}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A prior global topological map (e.g., the OpenStreetMap, OSM) can boost the\nperformance of autonomous mapping by a ground mobile robot. However, the prior\nmap is usually incomplete due to lacking labeling in partial paths. To solve\nthis problem, this paper proposes an OSM maker using airborne sensors carried\nby low-altitude aircraft, where the core of the OSM maker is a novel efficient\npathfinder approach based on LiDAR and camera data, i.e., a binary dual-stream\nroad segmentation model. Specifically, a multi-scale feature extraction based\non the UNet architecture is implemented for images and point clouds. To reduce\nthe effect caused by the sparsity of point cloud, an attention-guided gated\nblock is designed to integrate image and point-cloud features. To optimize the\nmodel for edge deployment that significantly reduces storage footprint and\ncomputational demands, we propose a binarization streamline to each model\ncomponent, including a variant of vision transformer (ViT) architecture as the\nencoder of the image branch, and new focal and perception losses to optimize\nthe model training. The experimental results on two datasets demonstrate that\nour pathfinder method achieves SOTA accuracy with high efficiency in finding\npaths from the low-level airborne sensors, and we can create complete OSM prior\nmaps based on the segmented road skeletons. Code and data are available at:\n\\href{https://github.com/IMRL/Pathfinder}{https://github.com/IMRL/Pathfinder}."
                },
                "authors": [
                    {
                        "name": "Kaijie Yin"
                    },
                    {
                        "name": "Tian Gao"
                    },
                    {
                        "name": "Hui Kong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Kong"
                },
                "author": "Hui Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08824v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08824v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04369v1",
                "updated": "2025-03-06T12:14:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    14,
                    45,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T12:14:45Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    14,
                    45,
                    3,
                    65,
                    0
                ],
                "title": "Lost in Literalism: How Supervised Training Shapes Translationese in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Literalism: How Supervised Training Shapes Translationese in\n  LLMs"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in machine\ntranslation, demonstrating impressive performance across diverse languages.\nHowever, translationese, characterized by overly literal and unnatural\ntranslations, remains a persistent challenge in LLM-based translation systems.\nDespite their pre-training on vast corpora of natural utterances, LLMs exhibit\ntranslationese errors and generate unexpected unnatural translations, stemming\nfrom biases introduced during supervised fine-tuning (SFT). In this work, we\nsystematically evaluate the prevalence of translationese in LLM-generated\ntranslations and investigate its roots during supervised training. We introduce\nmethods to mitigate these biases, including polishing golden references and\nfiltering unnatural training instances. Empirical evaluations demonstrate that\nthese approaches significantly reduce translationese while improving\ntranslation naturalness, validated by human evaluations and automatic metrics.\nOur findings highlight the need for training-aware adjustments to optimize LLM\ntranslation outputs, paving the way for more fluent and\ntarget-language-consistent translations. We release the data and code at\nhttps://github.com/yafuly/LLM_Translationese.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in machine\ntranslation, demonstrating impressive performance across diverse languages.\nHowever, translationese, characterized by overly literal and unnatural\ntranslations, remains a persistent challenge in LLM-based translation systems.\nDespite their pre-training on vast corpora of natural utterances, LLMs exhibit\ntranslationese errors and generate unexpected unnatural translations, stemming\nfrom biases introduced during supervised fine-tuning (SFT). In this work, we\nsystematically evaluate the prevalence of translationese in LLM-generated\ntranslations and investigate its roots during supervised training. We introduce\nmethods to mitigate these biases, including polishing golden references and\nfiltering unnatural training instances. Empirical evaluations demonstrate that\nthese approaches significantly reduce translationese while improving\ntranslation naturalness, validated by human evaluations and automatic metrics.\nOur findings highlight the need for training-aware adjustments to optimize LLM\ntranslation outputs, paving the way for more fluent and\ntarget-language-consistent translations. We release the data and code at\nhttps://github.com/yafuly/LLM_Translationese."
                },
                "authors": [
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Ronghao Zhang"
                    },
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Huajian Zhang"
                    },
                    {
                        "name": "Leyang Cui"
                    },
                    {
                        "name": "Yongjing Yin"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "arxiv_comment": "19 pages;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04360v1",
                "updated": "2025-03-06T12:04:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    4,
                    29,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T12:04:29Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    12,
                    4,
                    29,
                    3,
                    65,
                    0
                ],
                "title": "Exploring the Multilingual NLG Evaluation Abilities of LLM-Based\n  Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Multilingual NLG Evaluation Abilities of LLM-Based\n  Evaluators"
                },
                "summary": "Previous research has shown that LLMs have potential in multilingual NLG\nevaluation tasks. However, existing research has not fully explored the\ndifferences in the evaluation capabilities of LLMs across different languages.\nTo this end, this study provides a comprehensive analysis of the multilingual\nevaluation performance of 10 recent LLMs, spanning high-resource and\nlow-resource languages through correlation analysis, perturbation attacks, and\nfine-tuning. We found that 1) excluding the reference answer from the prompt\nand using large-parameter LLM-based evaluators leads to better performance\nacross various languages; 2) most LLM-based evaluators show a higher\ncorrelation with human judgments in high-resource languages than in\nlow-resource languages; 3) in the languages where they are most sensitive to\nsuch attacks, they also tend to exhibit the highest correlation with human\njudgments; and 4) fine-tuning with data from a particular language yields a\nbroadly consistent enhancement in the model's evaluation performance across\ndiverse languages. Our findings highlight the imbalance in LLMs'evaluation\ncapabilities across different languages and suggest that low-resource language\nscenarios deserve more attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous research has shown that LLMs have potential in multilingual NLG\nevaluation tasks. However, existing research has not fully explored the\ndifferences in the evaluation capabilities of LLMs across different languages.\nTo this end, this study provides a comprehensive analysis of the multilingual\nevaluation performance of 10 recent LLMs, spanning high-resource and\nlow-resource languages through correlation analysis, perturbation attacks, and\nfine-tuning. We found that 1) excluding the reference answer from the prompt\nand using large-parameter LLM-based evaluators leads to better performance\nacross various languages; 2) most LLM-based evaluators show a higher\ncorrelation with human judgments in high-resource languages than in\nlow-resource languages; 3) in the languages where they are most sensitive to\nsuch attacks, they also tend to exhibit the highest correlation with human\njudgments; and 4) fine-tuning with data from a particular language yields a\nbroadly consistent enhancement in the model's evaluation performance across\ndiverse languages. Our findings highlight the imbalance in LLMs'evaluation\ncapabilities across different languages and suggest that low-resource language\nscenarios deserve more attention."
                },
                "authors": [
                    {
                        "name": "Jiayi Chang"
                    },
                    {
                        "name": "Mingqi Gao"
                    },
                    {
                        "name": "Xinyu Hu"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04355v1",
                "updated": "2025-03-06T11:59:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    59,
                    55,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T11:59:55Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    59,
                    55,
                    3,
                    65,
                    0
                ],
                "title": "Layer-Specific Scaling of Positional Encodings for Superior Long-Context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-Specific Scaling of Positional Encodings for Superior Long-Context\n  Modeling"
                },
                "summary": "Although large language models (LLMs) have achieved significant progress in\nhandling long-context inputs, they still suffer from the ``lost-in-the-middle''\nproblem, where crucial information in the middle of the context is often\nunderrepresented or lost. Our extensive experiments reveal that this issue may\narise from the rapid long-term decay in Rotary Position Embedding (RoPE). To\naddress this problem, we propose a layer-specific positional encoding scaling\nmethod that assigns distinct scaling factors to each layer, slowing down the\ndecay rate caused by RoPE to make the model pay more attention to the middle\ncontext. A specially designed genetic algorithm is employed to efficiently\nselect the optimal scaling factors for each layer by incorporating Bezier\ncurves to reduce the search space. Through comprehensive experimentation, we\ndemonstrate that our method significantly alleviates the ``lost-in-the-middle''\nproblem. Our approach results in an average accuracy improvement of up to 20%\non the Key-Value Retrieval dataset. Furthermore, we show that layer-specific\ninterpolation, as opposed to uniform interpolation across all layers, enhances\nthe model's extrapolation capabilities when combined with PI and Dynamic-NTK\npositional encoding schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have achieved significant progress in\nhandling long-context inputs, they still suffer from the ``lost-in-the-middle''\nproblem, where crucial information in the middle of the context is often\nunderrepresented or lost. Our extensive experiments reveal that this issue may\narise from the rapid long-term decay in Rotary Position Embedding (RoPE). To\naddress this problem, we propose a layer-specific positional encoding scaling\nmethod that assigns distinct scaling factors to each layer, slowing down the\ndecay rate caused by RoPE to make the model pay more attention to the middle\ncontext. A specially designed genetic algorithm is employed to efficiently\nselect the optimal scaling factors for each layer by incorporating Bezier\ncurves to reduce the search space. Through comprehensive experimentation, we\ndemonstrate that our method significantly alleviates the ``lost-in-the-middle''\nproblem. Our approach results in an average accuracy improvement of up to 20%\non the Key-Value Retrieval dataset. Furthermore, we show that layer-specific\ninterpolation, as opposed to uniform interpolation across all layers, enhances\nthe model's extrapolation capabilities when combined with PI and Dynamic-NTK\npositional encoding schemes."
                },
                "authors": [
                    {
                        "name": "Zhenghua Wang"
                    },
                    {
                        "name": "Yiran Ding"
                    },
                    {
                        "name": "Changze Lv"
                    },
                    {
                        "name": "Zhibo Xu"
                    },
                    {
                        "name": "Tianlong Li"
                    },
                    {
                        "name": "Tianyuan Shi"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01334v3",
                "updated": "2025-03-06T11:59:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    59,
                    11,
                    3,
                    65,
                    0
                ],
                "published": "2024-08-02T15:32:42Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    15,
                    32,
                    42,
                    4,
                    215,
                    0
                ],
                "title": "A Backbone for Long-Horizon Robot Task Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Backbone for Long-Horizon Robot Task Understanding"
                },
                "summary": "End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-Based Backbone Framework (TBBF) as a fundamental\nstructure to enhance interpretability, data efficiency, and generalization in\nrobotic systems. TBBF utilizes expert demonstrations to enable therblig-level\ntask decomposition, facilitate efficient action-object mapping, and generate\nadaptive trajectories for new scenarios. The approach consists of two stages:\noffline training and online testing. During the offline training stage, we\ndeveloped the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, Large Language Model (LLM)-Alignment\nPolicy for Visual Correction (LAP-VC) is employed to ensure precise action\nregistration, facilitating trajectory transfer in novel robot scenarios.\nExperimental results validate these methods, achieving 94.37% recall in\ntherblig segmentation and success rates of 94.4% and 80% in real-world online\nrobot testing for simple and complex scenarios, respectively. Supplementary\nmaterial is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-Based Backbone Framework (TBBF) as a fundamental\nstructure to enhance interpretability, data efficiency, and generalization in\nrobotic systems. TBBF utilizes expert demonstrations to enable therblig-level\ntask decomposition, facilitate efficient action-object mapping, and generate\nadaptive trajectories for new scenarios. The approach consists of two stages:\noffline training and online testing. During the offline training stage, we\ndeveloped the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, Large Language Model (LLM)-Alignment\nPolicy for Visual Correction (LAP-VC) is employed to ensure precise action\nregistration, facilitating trajectory transfer in novel robot scenarios.\nExperimental results validate these methods, achieving 94.37% recall in\ntherblig segmentation and success rates of 94.4% and 80% in real-world online\nrobot testing for simple and complex scenarios, respectively. Supplementary\nmaterial is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home"
                },
                "authors": [
                    {
                        "name": "Xiaoshuai Chen"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Dongmyoung Lee"
                    },
                    {
                        "name": "Yukun Ge"
                    },
                    {
                        "name": "Nicolas Rojas"
                    },
                    {
                        "name": "Petar Kormushev"
                    }
                ],
                "author_detail": {
                    "name": "Petar Kormushev"
                },
                "author": "Petar Kormushev",
                "arxiv_doi": "10.1109/LRA.2025.3526441",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2025.3526441",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.01334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 8 figures. This work has been published by IEEE Robotics and\n  Automation Letters (RA-L)",
                "arxiv_journal_ref": "IEEE Robotics and Automation Letters, Volume: 10, 2025, 2048 -\n  2055",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12468v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12468v3",
                "updated": "2025-03-06T11:53:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    53,
                    49,
                    3,
                    65,
                    0
                ],
                "published": "2024-07-17T10:40:39Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    10,
                    40,
                    39,
                    2,
                    199,
                    0
                ],
                "title": "Evaluating Search Engines and Large Language Models for Answering Health\n  Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Search Engines and Large Language Models for Answering Health\n  Questions"
                },
                "summary": "Search engines (SEs) have traditionally been primary tools for information\nseeking, but the new Large Language Models (LLMs) are emerging as powerful\nalternatives, particularly for question-answering tasks. This study compares\nthe performance of four popular SEs, seven LLMs, and retrieval-augmented (RAG)\nvariants in answering 150 health-related questions from the TREC Health\nMisinformation (HM) Track. Results reveal SEs correctly answer between 50 and\n70% of questions, often hindered by many retrieval results not responding to\nthe health question. LLMs deliver higher accuracy, correctly answering about\n80% of questions, though their performance is sensitive to input prompts. RAG\nmethods significantly enhance smaller LLMs' effectiveness, improving accuracy\nby up to 30% by integrating retrieval evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search engines (SEs) have traditionally been primary tools for information\nseeking, but the new Large Language Models (LLMs) are emerging as powerful\nalternatives, particularly for question-answering tasks. This study compares\nthe performance of four popular SEs, seven LLMs, and retrieval-augmented (RAG)\nvariants in answering 150 health-related questions from the TREC Health\nMisinformation (HM) Track. Results reveal SEs correctly answer between 50 and\n70% of questions, often hindered by many retrieval results not responding to\nthe health question. LLMs deliver higher accuracy, correctly answering about\n80% of questions, though their performance is sensitive to input prompts. RAG\nmethods significantly enhance smaller LLMs' effectiveness, improving accuracy\nby up to 30% by integrating retrieval evidence."
                },
                "authors": [
                    {
                        "name": "Marcos Fernández-Pichel"
                    },
                    {
                        "name": "Juan C. Pichel"
                    },
                    {
                        "name": "David E. Losada"
                    }
                ],
                "author_detail": {
                    "name": "David E. Losada"
                },
                "author": "David E. Losada",
                "arxiv_doi": "10.1038/s41746-025-01546-w",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41746-025-01546-w",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.12468v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12468v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03479v2",
                "updated": "2025-03-06T11:46:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    46,
                    49,
                    3,
                    65,
                    0
                ],
                "published": "2025-01-07T02:47:59Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    2,
                    47,
                    59,
                    1,
                    7,
                    0
                ],
                "title": "Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia\n  Reveal about the Socio-Cultural Norms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia\n  Reveal about the Socio-Cultural Norms"
                },
                "summary": "Honorifics serve as powerful linguistic markers that reflect social\nhierarchies and cultural values. This paper presents a large-scale,\ncross-linguistic exploration of usage of honorific pronouns in Bengali and\nHindi Wikipedia articles, shedding light on how socio-cultural factors shape\nlanguage. Using LLM (GPT-4o), we annotated 10, 000 articles of real and\nfictional beings in each language for several sociodemographic features such as\ngender, age, fame, and exoticness, and the use of honorifics. We find that\nacross all feature combinations, use of honorifics is consistently more common\nin Bengali than Hindi. For both languages, the use non-honorific pronouns is\nmore commonly observed for infamous, juvenile, and exotic beings. Notably, we\nobserve a gender bias in use of honorifics in Hindi, with men being more\ncommonly referred to with honorifics than women.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Honorifics serve as powerful linguistic markers that reflect social\nhierarchies and cultural values. This paper presents a large-scale,\ncross-linguistic exploration of usage of honorific pronouns in Bengali and\nHindi Wikipedia articles, shedding light on how socio-cultural factors shape\nlanguage. Using LLM (GPT-4o), we annotated 10, 000 articles of real and\nfictional beings in each language for several sociodemographic features such as\ngender, age, fame, and exoticness, and the use of honorifics. We find that\nacross all feature combinations, use of honorifics is consistently more common\nin Bengali than Hindi. For both languages, the use non-honorific pronouns is\nmore commonly observed for infamous, juvenile, and exotic beings. Notably, we\nobserve a gender bias in use of honorifics in Hindi, with men being more\ncommonly referred to with honorifics than women."
                },
                "authors": [
                    {
                        "name": "Sourabrata Mukherjee"
                    },
                    {
                        "name": "Soumya Teotia"
                    },
                    {
                        "name": "Sougata Saha"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04347v1",
                "updated": "2025-03-06T11:43:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    43,
                    30,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T11:43:30Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    43,
                    30,
                    3,
                    65,
                    0
                ],
                "title": "Large Language Models for Zero-shot Inference of Causal Structures in\n  Biology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Zero-shot Inference of Causal Structures in\n  Biology"
                },
                "summary": "Genes, proteins and other biological entities influence one another via\ncausal molecular networks. Causal relationships in such networks are mediated\nby complex and diverse mechanisms, through latent variables, and are often\nspecific to cellular context. It remains challenging to characterise such\nnetworks in practice. Here, we present a novel framework to evaluate large\nlanguage models (LLMs) for zero-shot inference of causal relationships in\nbiology. In particular, we systematically evaluate causal claims obtained from\nan LLM using real-world interventional data. This is done over one hundred\nvariables and thousands of causal hypotheses. Furthermore, we consider several\nprompting and retrieval-augmentation strategies, including large, and\npotentially conflicting, collections of scientific articles. Our results show\nthat with tailored augmentation and prompting, even relatively small LLMs can\ncapture meaningful aspects of causal structure in biological systems. This\nsupports the notion that LLMs could act as orchestration tools in biological\ndiscovery, by helping to distil current knowledge in ways amenable to\ndownstream analysis. Our approach to assessing LLMs with respect to\nexperimental data is relevant for a broad range of problems at the intersection\nof causal learning, LLMs and scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genes, proteins and other biological entities influence one another via\ncausal molecular networks. Causal relationships in such networks are mediated\nby complex and diverse mechanisms, through latent variables, and are often\nspecific to cellular context. It remains challenging to characterise such\nnetworks in practice. Here, we present a novel framework to evaluate large\nlanguage models (LLMs) for zero-shot inference of causal relationships in\nbiology. In particular, we systematically evaluate causal claims obtained from\nan LLM using real-world interventional data. This is done over one hundred\nvariables and thousands of causal hypotheses. Furthermore, we consider several\nprompting and retrieval-augmentation strategies, including large, and\npotentially conflicting, collections of scientific articles. Our results show\nthat with tailored augmentation and prompting, even relatively small LLMs can\ncapture meaningful aspects of causal structure in biological systems. This\nsupports the notion that LLMs could act as orchestration tools in biological\ndiscovery, by helping to distil current knowledge in ways amenable to\ndownstream analysis. Our approach to assessing LLMs with respect to\nexperimental data is relevant for a broad range of problems at the intersection\nof causal learning, LLMs and scientific discovery."
                },
                "authors": [
                    {
                        "name": "Izzy Newsham"
                    },
                    {
                        "name": "Luka Kovačević"
                    },
                    {
                        "name": "Richard Moulange"
                    },
                    {
                        "name": "Nan Rosemary Ke"
                    },
                    {
                        "name": "Sach Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Sach Mukherjee"
                },
                "author": "Sach Mukherjee",
                "arxiv_comment": "ICLR 2025 Workshop on Machine Learning for Genomics Explorations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04338v1",
                "updated": "2025-03-06T11:34:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    34,
                    49,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T11:34:49Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    34,
                    49,
                    3,
                    65,
                    0
                ],
                "title": "In-depth Analysis of Graph-based RAG in a Unified Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-depth Analysis of Graph-based RAG in a Unified Framework"
                },
                "summary": "Graph-based Retrieval-Augmented Generation (RAG) has proven effective in\nintegrating external knowledge into large language models (LLMs), improving\ntheir factual accuracy, adaptability, interpretability, and trustworthiness. A\nnumber of graph-based RAG methods have been proposed in the literature.\nHowever, these methods have not been systematically and comprehensively\ncompared under the same experimental settings. In this paper, we first\nsummarize a unified framework to incorporate all graph-based RAG methods from a\nhigh-level perspective. We then extensively compare representative graph-based\nRAG methods over a range of questing-answering (QA) datasets -- from specific\nquestions to abstract questions -- and examine the effectiveness of all\nmethods, providing a thorough analysis of graph-based RAG approaches. As a\nbyproduct of our experimental analysis, we are also able to identify new\nvariants of the graph-based RAG methods over specific QA and abstract QA tasks\nrespectively, by combining existing techniques, which outperform the\nstate-of-the-art methods. Finally, based on these findings, we offer promising\nresearch opportunities. We believe that a deeper understanding of the behavior\nof existing methods can provide new valuable insights for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Retrieval-Augmented Generation (RAG) has proven effective in\nintegrating external knowledge into large language models (LLMs), improving\ntheir factual accuracy, adaptability, interpretability, and trustworthiness. A\nnumber of graph-based RAG methods have been proposed in the literature.\nHowever, these methods have not been systematically and comprehensively\ncompared under the same experimental settings. In this paper, we first\nsummarize a unified framework to incorporate all graph-based RAG methods from a\nhigh-level perspective. We then extensively compare representative graph-based\nRAG methods over a range of questing-answering (QA) datasets -- from specific\nquestions to abstract questions -- and examine the effectiveness of all\nmethods, providing a thorough analysis of graph-based RAG approaches. As a\nbyproduct of our experimental analysis, we are also able to identify new\nvariants of the graph-based RAG methods over specific QA and abstract QA tasks\nrespectively, by combining existing techniques, which outperform the\nstate-of-the-art methods. Finally, based on these findings, we offer promising\nresearch opportunities. We believe that a deeper understanding of the behavior\nof existing methods can provide new valuable insights for future research."
                },
                "authors": [
                    {
                        "name": "Yingli Zhou"
                    },
                    {
                        "name": "Yaodong Su"
                    },
                    {
                        "name": "Youran Sun"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Taotao Wang"
                    },
                    {
                        "name": "Runyuan He"
                    },
                    {
                        "name": "Yongwei Zhang"
                    },
                    {
                        "name": "Sicong Liang"
                    },
                    {
                        "name": "Xilin Liu"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Yixiang Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yixiang Fang"
                },
                "author": "Yixiang Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04332v1",
                "updated": "2025-03-06T11:30:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    30,
                    32,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T11:30:32Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    30,
                    32,
                    3,
                    65,
                    0
                ],
                "title": "The Challenge of Identifying the Origin of Black-Box Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Challenge of Identifying the Origin of Black-Box Large Language\n  Models"
                },
                "summary": "The tremendous commercial potential of large language models (LLMs) has\nheightened concerns about their unauthorized use. Third parties can customize\nLLMs through fine-tuning and offer only black-box API access, effectively\nconcealing unauthorized usage and complicating external auditing processes.\nThis practice not only exacerbates unfair competition, but also violates\nlicensing agreements. In response, identifying the origin of black-box LLMs is\nan intrinsic solution to this issue. In this paper, we first reveal the\nlimitations of state-of-the-art passive and proactive identification methods\nwith experiments on 30 LLMs and two real-world black-box APIs. Then, we propose\nthe proactive technique, PlugAE, which optimizes adversarial token embeddings\nin a continuous space and proactively plugs them into the LLM for tracing and\nidentification. The experiments show that PlugAE can achieve substantial\nimprovement in identifying fine-tuned derivatives. We further advocate for\nlegal frameworks and regulations to better address the challenges posed by the\nunauthorized use of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The tremendous commercial potential of large language models (LLMs) has\nheightened concerns about their unauthorized use. Third parties can customize\nLLMs through fine-tuning and offer only black-box API access, effectively\nconcealing unauthorized usage and complicating external auditing processes.\nThis practice not only exacerbates unfair competition, but also violates\nlicensing agreements. In response, identifying the origin of black-box LLMs is\nan intrinsic solution to this issue. In this paper, we first reveal the\nlimitations of state-of-the-art passive and proactive identification methods\nwith experiments on 30 LLMs and two real-world black-box APIs. Then, we propose\nthe proactive technique, PlugAE, which optimizes adversarial token embeddings\nin a continuous space and proactively plugs them into the LLM for tracing and\nidentification. The experiments show that PlugAE can achieve substantial\nimprovement in identifying fine-tuned derivatives. We further advocate for\nlegal frameworks and regulations to better address the challenges posed by the\nunauthorized use of LLMs."
                },
                "authors": [
                    {
                        "name": "Ziqing Yang"
                    },
                    {
                        "name": "Yixin Wu"
                    },
                    {
                        "name": "Yun Shen"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04328v1",
                "updated": "2025-03-06T11:27:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    27,
                    55,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T11:27:55Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    27,
                    55,
                    3,
                    65,
                    0
                ],
                "title": "Solving Word-Sense Disambiguation and Word-Sense Induction with\n  Dictionary Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving Word-Sense Disambiguation and Word-Sense Induction with\n  Dictionary Examples"
                },
                "summary": "Many less-resourced languages struggle with a lack of large, task-specific\ndatasets that are required for solving relevant tasks with modern\ntransformer-based large language models (LLMs). On the other hand, many\nlinguistic resources, such as dictionaries, are rarely used in this context\ndespite their large information contents. We show how LLMs can be used to\nextend existing language resources in less-resourced languages for two\nimportant tasks: word-sense disambiguation (WSD) and word-sense induction\n(WSI). We approach the two tasks through the related but much more accessible\nword-in-context (WiC) task where, given a pair of sentences and a target word,\na classification model is tasked with predicting whether the sense of a given\nword differs between sentences. We demonstrate that a well-trained model for\nthis task can distinguish between different word senses and can be adapted to\nsolve the WSD and WSI tasks. The advantage of using the WiC task, instead of\ndirectly predicting senses, is that the WiC task does not need pre-constructed\nsense inventories with a sufficient number of examples for each sense, which\nare rarely available in less-resourced languages. We show that sentence pairs\nfor the WiC task can be successfully generated from dictionary examples using\nLLMs. The resulting prediction models outperform existing models on WiC, WSD,\nand WSI tasks. We demonstrate our methodology on the Slovene language, where a\nmonolingual dictionary is available, but word-sense resources are tiny.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many less-resourced languages struggle with a lack of large, task-specific\ndatasets that are required for solving relevant tasks with modern\ntransformer-based large language models (LLMs). On the other hand, many\nlinguistic resources, such as dictionaries, are rarely used in this context\ndespite their large information contents. We show how LLMs can be used to\nextend existing language resources in less-resourced languages for two\nimportant tasks: word-sense disambiguation (WSD) and word-sense induction\n(WSI). We approach the two tasks through the related but much more accessible\nword-in-context (WiC) task where, given a pair of sentences and a target word,\na classification model is tasked with predicting whether the sense of a given\nword differs between sentences. We demonstrate that a well-trained model for\nthis task can distinguish between different word senses and can be adapted to\nsolve the WSD and WSI tasks. The advantage of using the WiC task, instead of\ndirectly predicting senses, is that the WiC task does not need pre-constructed\nsense inventories with a sufficient number of examples for each sense, which\nare rarely available in less-resourced languages. We show that sentence pairs\nfor the WiC task can be successfully generated from dictionary examples using\nLLMs. The resulting prediction models outperform existing models on WiC, WSD,\nand WSI tasks. We demonstrate our methodology on the Slovene language, where a\nmonolingual dictionary is available, but word-sense resources are tiny."
                },
                "authors": [
                    {
                        "name": "Tadej Škvorc"
                    },
                    {
                        "name": "Marko Robnik-Šikonja"
                    }
                ],
                "author_detail": {
                    "name": "Marko Robnik-Šikonja"
                },
                "author": "Marko Robnik-Šikonja",
                "arxiv_comment": "12 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03417v2",
                "updated": "2025-03-06T11:00:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    11,
                    0,
                    35,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-05T11:47:32Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    11,
                    47,
                    32,
                    2,
                    64,
                    0
                ],
                "title": "When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding\n  Models Against Misinformation Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding\n  Models Against Misinformation Edits"
                },
                "summary": "Online misinformation remains a critical challenge, and fact-checkers\nincreasingly rely on embedding-based methods to retrieve relevant fact-checks.\nYet, when debunked claims reappear in edited forms, the performance of these\nmethods is unclear. In this work, we introduce a taxonomy of six common\nreal-world misinformation edits and propose a perturbation framework that\ngenerates valid, natural claim variations. Our multi-stage retrieval evaluation\nreveals that standard embedding models struggle with user-introduced edits,\nwhile LLM-distilled embeddings offer improved robustness at a higher\ncomputational cost. Although a strong reranker helps mitigate some issues, it\ncannot fully compensate for first-stage retrieval gaps. Addressing these\nretrieval gaps, our train- and inference-time mitigation approaches enhance\nin-domain robustness by up to 17 percentage points and boost out-of-domain\ngeneralization by 10 percentage points over baseline models. Overall, our\nfindings provide practical improvements to claim-matching systems, enabling\nmore reliable fact-checking of evolving misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online misinformation remains a critical challenge, and fact-checkers\nincreasingly rely on embedding-based methods to retrieve relevant fact-checks.\nYet, when debunked claims reappear in edited forms, the performance of these\nmethods is unclear. In this work, we introduce a taxonomy of six common\nreal-world misinformation edits and propose a perturbation framework that\ngenerates valid, natural claim variations. Our multi-stage retrieval evaluation\nreveals that standard embedding models struggle with user-introduced edits,\nwhile LLM-distilled embeddings offer improved robustness at a higher\ncomputational cost. Although a strong reranker helps mitigate some issues, it\ncannot fully compensate for first-stage retrieval gaps. Addressing these\nretrieval gaps, our train- and inference-time mitigation approaches enhance\nin-domain robustness by up to 17 percentage points and boost out-of-domain\ngeneralization by 10 percentage points over baseline models. Overall, our\nfindings provide practical improvements to claim-matching systems, enabling\nmore reliable fact-checking of evolving misinformation."
                },
                "authors": [
                    {
                        "name": "Jabez Magomere"
                    },
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Manuel Tonneau"
                    },
                    {
                        "name": "Ashkan Kazemi"
                    },
                    {
                        "name": "Scott Hale"
                    }
                ],
                "author_detail": {
                    "name": "Scott Hale"
                },
                "author": "Scott Hale",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04302v1",
                "updated": "2025-03-06T10:42:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    42,
                    18,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T10:42:18Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    42,
                    18,
                    3,
                    65,
                    0
                ],
                "title": "Malware Detection at the Edge with Lightweight LLMs: A Performance\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malware Detection at the Edge with Lightweight LLMs: A Performance\n  Evaluation"
                },
                "summary": "The rapid evolution of malware attacks calls for the development of\ninnovative detection methods, especially in resource-constrained edge\ncomputing. Traditional detection techniques struggle to keep up with modern\nmalware's sophistication and adaptability, prompting a shift towards advanced\nmethodologies like those leveraging Large Language Models (LLMs) for enhanced\nmalware detection. However, deploying LLMs for malware detection directly at\nedge devices raises several challenges, including ensuring accuracy in\nconstrained environments and addressing edge devices' energy and computational\nlimits. To tackle these challenges, this paper proposes an architecture\nleveraging lightweight LLMs' strengths while addressing limitations like\nreduced accuracy and insufficient computational power. To evaluate the\neffectiveness of the proposed lightweight LLM-based approach for edge\ncomputing, we perform an extensive experimental evaluation using several\nstate-of-the-art lightweight LLMs. We test them with several publicly available\ndatasets specifically designed for edge and IoT scenarios and different edge\nnodes with varying computational power and characteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of malware attacks calls for the development of\ninnovative detection methods, especially in resource-constrained edge\ncomputing. Traditional detection techniques struggle to keep up with modern\nmalware's sophistication and adaptability, prompting a shift towards advanced\nmethodologies like those leveraging Large Language Models (LLMs) for enhanced\nmalware detection. However, deploying LLMs for malware detection directly at\nedge devices raises several challenges, including ensuring accuracy in\nconstrained environments and addressing edge devices' energy and computational\nlimits. To tackle these challenges, this paper proposes an architecture\nleveraging lightweight LLMs' strengths while addressing limitations like\nreduced accuracy and insufficient computational power. To evaluate the\neffectiveness of the proposed lightweight LLM-based approach for edge\ncomputing, we perform an extensive experimental evaluation using several\nstate-of-the-art lightweight LLMs. We test them with several publicly available\ndatasets specifically designed for edge and IoT scenarios and different edge\nnodes with varying computational power and characteristics."
                },
                "authors": [
                    {
                        "name": "Christian Rondanini"
                    },
                    {
                        "name": "Barbara Carminati"
                    },
                    {
                        "name": "Elena Ferrari"
                    },
                    {
                        "name": "Antonio Gaudiano"
                    },
                    {
                        "name": "Ashish Kundu"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Kundu"
                },
                "author": "Ashish Kundu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04299v1",
                "updated": "2025-03-06T10:39:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    39,
                    47,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T10:39:47Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    39,
                    47,
                    3,
                    65,
                    0
                ],
                "title": "Mapping AI Benchmark Data to Quantitative Risk Estimates Through Expert\n  Elicitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping AI Benchmark Data to Quantitative Risk Estimates Through Expert\n  Elicitation"
                },
                "summary": "The literature and multiple experts point to many potential risks from large\nlanguage models (LLMs), but there are still very few direct measurements of the\nactual harms posed. AI risk assessment has so far focused on measuring the\nmodels' capabilities, but the capabilities of models are only indicators of\nrisk, not measures of risk. Better modeling and quantification of AI risk\nscenarios can help bridge this disconnect and link the capabilities of LLMs to\ntangible real-world harm. This paper makes an early contribution to this field\nby demonstrating how existing AI benchmarks can be used to facilitate the\ncreation of risk estimates. We describe the results of a pilot study in which\nexperts use information from Cybench, an AI benchmark, to generate probability\nestimates. We show that the methodology seems promising for this purpose, while\nnoting improvements that can be made to further strengthen its application in\nquantitative AI risk assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The literature and multiple experts point to many potential risks from large\nlanguage models (LLMs), but there are still very few direct measurements of the\nactual harms posed. AI risk assessment has so far focused on measuring the\nmodels' capabilities, but the capabilities of models are only indicators of\nrisk, not measures of risk. Better modeling and quantification of AI risk\nscenarios can help bridge this disconnect and link the capabilities of LLMs to\ntangible real-world harm. This paper makes an early contribution to this field\nby demonstrating how existing AI benchmarks can be used to facilitate the\ncreation of risk estimates. We describe the results of a pilot study in which\nexperts use information from Cybench, an AI benchmark, to generate probability\nestimates. We show that the methodology seems promising for this purpose, while\nnoting improvements that can be made to further strengthen its application in\nquantitative AI risk assessment."
                },
                "authors": [
                    {
                        "name": "Malcolm Murray"
                    },
                    {
                        "name": "Henry Papadatos"
                    },
                    {
                        "name": "Otter Quarks"
                    },
                    {
                        "name": "Pierre-François Gimenez"
                    },
                    {
                        "name": "Simeon Campos"
                    }
                ],
                "author_detail": {
                    "name": "Simeon Campos"
                },
                "author": "Simeon Campos",
                "arxiv_comment": "23 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04291v1",
                "updated": "2025-03-06T10:19:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    19,
                    1,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T10:19:01Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    19,
                    1,
                    3,
                    65,
                    0
                ],
                "title": "MathMistake Checker: A Comprehensive Demonstration for Step-by-Step Math\n  Problem Mistake Finding by Prompt-Guided LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathMistake Checker: A Comprehensive Demonstration for Step-by-Step Math\n  Problem Mistake Finding by Prompt-Guided LLMs"
                },
                "summary": "We propose a novel system, MathMistake Checker, designed to automate\nstep-by-step mistake finding in mathematical problems with lengthy answers\nthrough a two-stage process. The system aims to simplify grading, increase\nefficiency, and enhance learning experiences from a pedagogical perspective. It\nintegrates advanced technologies, including computer vision and the\nchain-of-thought capabilities of the latest large language models (LLMs). Our\nsystem supports open-ended grading without reference answers and promotes\npersonalized learning by providing targeted feedback. We demonstrate its\neffectiveness across various types of math problems, such as calculation and\nword problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel system, MathMistake Checker, designed to automate\nstep-by-step mistake finding in mathematical problems with lengthy answers\nthrough a two-stage process. The system aims to simplify grading, increase\nefficiency, and enhance learning experiences from a pedagogical perspective. It\nintegrates advanced technologies, including computer vision and the\nchain-of-thought capabilities of the latest large language models (LLMs). Our\nsystem supports open-ended grading without reference answers and promotes\npersonalized learning by providing targeted feedback. We demonstrate its\neffectiveness across various types of math problems, such as calculation and\nword problems."
                },
                "authors": [
                    {
                        "name": "Tianyang Zhang"
                    },
                    {
                        "name": "Zhuoxuan Jiang"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Shaohua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shaohua Zhang"
                },
                "author": "Shaohua Zhang",
                "arxiv_comment": "Published in AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04290v1",
                "updated": "2025-03-06T10:17:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    17,
                    52,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T10:17:52Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    17,
                    52,
                    3,
                    65,
                    0
                ],
                "title": "How Do Hackathons Foster Creativity? Towards AI Collaborative Evaluation\n  of Creativity at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do Hackathons Foster Creativity? Towards AI Collaborative Evaluation\n  of Creativity at Scale"
                },
                "summary": "Hackathons have become popular collaborative events for accelerating the\ndevelopment of creative ideas and prototypes. There are several case studies\nshowcasing creative outcomes across domains such as industry, education, and\nresearch. However, there are no large-scale studies on creativity in hackathons\nwhich can advance theory on how hackathon formats lead to creative outcomes. We\nconducted a computational analysis of 193,353 hackathon projects. By\noperationalizing creativity through usefulness and novelty, we refined our\ndataset to 10,363 projects, allowing us to analyze how participant\ncharacteristics, collaboration patterns, and hackathon setups influence the\ndevelopment of creative projects. The contribution of our paper is twofold: We\nidentified means for organizers to foster creativity in hackathons. We also\nexplore the use of large language models (LLMs) to augment the evaluation of\ncreative outcomes and discuss challenges and opportunities of doing this, which\nhas implications for creativity research at large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hackathons have become popular collaborative events for accelerating the\ndevelopment of creative ideas and prototypes. There are several case studies\nshowcasing creative outcomes across domains such as industry, education, and\nresearch. However, there are no large-scale studies on creativity in hackathons\nwhich can advance theory on how hackathon formats lead to creative outcomes. We\nconducted a computational analysis of 193,353 hackathon projects. By\noperationalizing creativity through usefulness and novelty, we refined our\ndataset to 10,363 projects, allowing us to analyze how participant\ncharacteristics, collaboration patterns, and hackathon setups influence the\ndevelopment of creative projects. The contribution of our paper is twofold: We\nidentified means for organizers to foster creativity in hackathons. We also\nexplore the use of large language models (LLMs) to augment the evaluation of\ncreative outcomes and discuss challenges and opportunities of doing this, which\nhas implications for creativity research at large."
                },
                "authors": [
                    {
                        "name": "Jeanette Falk"
                    },
                    {
                        "name": "Yiyi Chen"
                    },
                    {
                        "name": "Janet Rafner"
                    },
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Johannes Bjerva"
                    },
                    {
                        "name": "Alexander Nolte"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Nolte"
                },
                "author": "Alexander Nolte",
                "arxiv_comment": "Accepted in Proceedings of the 2025 CHI Conference on Human Factors\n  in Computing Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04280v1",
                "updated": "2025-03-06T10:08:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    8,
                    44,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T10:08:44Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    8,
                    44,
                    3,
                    65,
                    0
                ],
                "title": "Towards Autonomous Reinforcement Learning for Real-World Robotic\n  Manipulation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Autonomous Reinforcement Learning for Real-World Robotic\n  Manipulation with Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) and Visual Language\nModels (VLMs) have significantly impacted robotics, enabling high-level\nsemantic motion planning applications. Reinforcement Learning (RL), a\ncomplementary paradigm, enables agents to autonomously optimize complex\nbehaviors through interaction and reward signals. However, designing effective\nreward functions for RL remains challenging, especially in real-world tasks\nwhere sparse rewards are insufficient and dense rewards require elaborate\ndesign. In this work, we propose Autonomous Reinforcement learning for Complex\nHumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,\na pre-trained LLM, to generate reward functions directly from natural language\ntask descriptions. The rewards are used to train RL agents in simulated\nenvironments, where we formalize the reward generation process to enhance\nfeasibility. Additionally, GPT-4 automates the coding of task success criteria,\ncreating a fully automated, one-shot procedure for translating human-readable\ntext into deployable robot skills. Our approach is validated through extensive\nsimulated experiments on single-arm and bi-manual manipulation tasks using an\nABB YuMi collaborative robot, highlighting its practicality and effectiveness.\nTasks are demonstrated on the real robot setup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) and Visual Language\nModels (VLMs) have significantly impacted robotics, enabling high-level\nsemantic motion planning applications. Reinforcement Learning (RL), a\ncomplementary paradigm, enables agents to autonomously optimize complex\nbehaviors through interaction and reward signals. However, designing effective\nreward functions for RL remains challenging, especially in real-world tasks\nwhere sparse rewards are insufficient and dense rewards require elaborate\ndesign. In this work, we propose Autonomous Reinforcement learning for Complex\nHumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,\na pre-trained LLM, to generate reward functions directly from natural language\ntask descriptions. The rewards are used to train RL agents in simulated\nenvironments, where we formalize the reward generation process to enhance\nfeasibility. Additionally, GPT-4 automates the coding of task success criteria,\ncreating a fully automated, one-shot procedure for translating human-readable\ntext into deployable robot skills. Our approach is validated through extensive\nsimulated experiments on single-arm and bi-manual manipulation tasks using an\nABB YuMi collaborative robot, highlighting its practicality and effectiveness.\nTasks are demonstrated on the real robot setup."
                },
                "authors": [
                    {
                        "name": "Niccolò Turcato"
                    },
                    {
                        "name": "Matteo Iovino"
                    },
                    {
                        "name": "Aris Synodinos"
                    },
                    {
                        "name": "Alberto Dalla Libera"
                    },
                    {
                        "name": "Ruggero Carli"
                    },
                    {
                        "name": "Pietro Falco"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Falco"
                },
                "author": "Pietro Falco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04271v1",
                "updated": "2025-03-06T10:02:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    2,
                    25,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T10:02:25Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    2,
                    25,
                    3,
                    65,
                    0
                ],
                "title": "On Fact and Frequency: LLM Responses to Misinformation Expressed with\n  Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Fact and Frequency: LLM Responses to Misinformation Expressed with\n  Uncertainty"
                },
                "summary": "We study LLM judgments of misinformation expressed with uncertainty. Our\nexperiments study the response of three widely used LLMs (GPT-4o, LlaMA3,\nDeepSeek-v2) to misinformation propositions that have been verified false and\nthen are transformed into uncertain statements according to an uncertainty\ntypology. Our results show that after transformation, LLMs change their\nfactchecking classification from false to not-false in 25% of the cases.\nAnalysis reveals that the change cannot be explained by predictors to which\nhumans are expected to be sensitive, i.e., modality, linguistic cues, or\nargumentation strategy. The exception is doxastic transformations, which use\nlinguistic cue phrases such as \"It is believed ...\".To gain further insight, we\nprompt the LLM to make another judgment about the transformed misinformation\nstatements that is not related to truth value. Specifically, we study LLM\nestimates of the frequency with which people make the uncertain statement. We\nfind a small but significant correlation between judgment of fact and\nestimation of frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study LLM judgments of misinformation expressed with uncertainty. Our\nexperiments study the response of three widely used LLMs (GPT-4o, LlaMA3,\nDeepSeek-v2) to misinformation propositions that have been verified false and\nthen are transformed into uncertain statements according to an uncertainty\ntypology. Our results show that after transformation, LLMs change their\nfactchecking classification from false to not-false in 25% of the cases.\nAnalysis reveals that the change cannot be explained by predictors to which\nhumans are expected to be sensitive, i.e., modality, linguistic cues, or\nargumentation strategy. The exception is doxastic transformations, which use\nlinguistic cue phrases such as \"It is believed ...\".To gain further insight, we\nprompt the LLM to make another judgment about the transformed misinformation\nstatements that is not related to truth value. Specifically, we study LLM\nestimates of the frequency with which people make the uncertain statement. We\nfind a small but significant correlation between judgment of fact and\nestimation of frequency."
                },
                "authors": [
                    {
                        "name": "Yana van de Sande"
                    },
                    {
                        "name": "Gunes Açar"
                    },
                    {
                        "name": "Thabo van Woudenberg"
                    },
                    {
                        "name": "Martha Larson"
                    }
                ],
                "author_detail": {
                    "name": "Martha Larson"
                },
                "author": "Martha Larson",
                "arxiv_comment": "4 pages, 1 figure, 3 tables, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91F20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.4; K.4; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04262v1",
                "updated": "2025-03-06T09:46:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    9,
                    46,
                    16,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T09:46:16Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    9,
                    46,
                    16,
                    3,
                    65,
                    0
                ],
                "title": "Guidelines for Applying RL and MARL in Cybersecurity Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guidelines for Applying RL and MARL in Cybersecurity Applications"
                },
                "summary": "Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL)\nhave emerged as promising methodologies for addressing challenges in automated\ncyber defence (ACD). These techniques offer adaptive decision-making\ncapabilities in high-dimensional, adversarial environments. This report\nprovides a structured set of guidelines for cybersecurity professionals and\nresearchers to assess the suitability of RL and MARL for specific use cases,\nconsidering factors such as explainability, exploration needs, and the\ncomplexity of multi-agent coordination. It also discusses key algorithmic\napproaches, implementation challenges, and real-world constraints, such as data\nscarcity and adversarial interference. The report further outlines open\nresearch questions, including policy optimality, agent cooperation levels, and\nthe integration of MARL systems into operational cybersecurity frameworks. By\nbridging theoretical advancements and practical deployment, these guidelines\naim to enhance the effectiveness of AI-driven cyber defence strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL)\nhave emerged as promising methodologies for addressing challenges in automated\ncyber defence (ACD). These techniques offer adaptive decision-making\ncapabilities in high-dimensional, adversarial environments. This report\nprovides a structured set of guidelines for cybersecurity professionals and\nresearchers to assess the suitability of RL and MARL for specific use cases,\nconsidering factors such as explainability, exploration needs, and the\ncomplexity of multi-agent coordination. It also discusses key algorithmic\napproaches, implementation challenges, and real-world constraints, such as data\nscarcity and adversarial interference. The report further outlines open\nresearch questions, including policy optimality, agent cooperation levels, and\nthe integration of MARL systems into operational cybersecurity frameworks. By\nbridging theoretical advancements and practical deployment, these guidelines\naim to enhance the effectiveness of AI-driven cyber defence strategies."
                },
                "authors": [
                    {
                        "name": "Vasilios Mavroudis"
                    },
                    {
                        "name": "Gregory Palmer"
                    },
                    {
                        "name": "Sara Farmer"
                    },
                    {
                        "name": "Kez Smithson Whitehead"
                    },
                    {
                        "name": "David Foster"
                    },
                    {
                        "name": "Adam Price"
                    },
                    {
                        "name": "Ian Miles"
                    },
                    {
                        "name": "Alberto Caron"
                    },
                    {
                        "name": "Stephen Pasteris"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Pasteris"
                },
                "author": "Stephen Pasteris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04261v1",
                "updated": "2025-03-06T09:44:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    9,
                    44,
                    18,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T09:44:18Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    9,
                    44,
                    18,
                    3,
                    65,
                    0
                ],
                "title": "VirtualXAI: A User-Centric Framework for Explainability Assessment\n  Leveraging GPT-Generated Personas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VirtualXAI: A User-Centric Framework for Explainability Assessment\n  Leveraging GPT-Generated Personas"
                },
                "summary": "In today's data-driven era, computational systems generate vast amounts of\ndata that drive the digital transformation of industries, where Artificial\nIntelligence (AI) plays a key role. Currently, the demand for eXplainable AI\n(XAI) has increased to enhance the interpretability, transparency, and\ntrustworthiness of AI models. However, evaluating XAI methods remains\nchallenging: existing evaluation frameworks typically focus on quantitative\nproperties such as fidelity, consistency, and stability without taking into\naccount qualitative characteristics such as satisfaction and interpretability.\nIn addition, practitioners face a lack of guidance in selecting appropriate\ndatasets, AI models, and XAI methods -a major hurdle in human-AI collaboration.\nTo address these gaps, we propose a framework that integrates quantitative\nbenchmarking with qualitative user assessments through virtual personas based\non the \"Anthology\" of backstories of the Large Language Model (LLM). Our\nframework also incorporates a content-based recommender system that leverages\ndataset-specific characteristics to match new input data with a repository of\nbenchmarked datasets. This yields an estimated XAI score and provides tailored\nrecommendations for both the optimal AI model and the XAI method for a given\nscenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In today's data-driven era, computational systems generate vast amounts of\ndata that drive the digital transformation of industries, where Artificial\nIntelligence (AI) plays a key role. Currently, the demand for eXplainable AI\n(XAI) has increased to enhance the interpretability, transparency, and\ntrustworthiness of AI models. However, evaluating XAI methods remains\nchallenging: existing evaluation frameworks typically focus on quantitative\nproperties such as fidelity, consistency, and stability without taking into\naccount qualitative characteristics such as satisfaction and interpretability.\nIn addition, practitioners face a lack of guidance in selecting appropriate\ndatasets, AI models, and XAI methods -a major hurdle in human-AI collaboration.\nTo address these gaps, we propose a framework that integrates quantitative\nbenchmarking with qualitative user assessments through virtual personas based\non the \"Anthology\" of backstories of the Large Language Model (LLM). Our\nframework also incorporates a content-based recommender system that leverages\ndataset-specific characteristics to match new input data with a repository of\nbenchmarked datasets. This yields an estimated XAI score and provides tailored\nrecommendations for both the optimal AI model and the XAI method for a given\nscenario."
                },
                "authors": [
                    {
                        "name": "Georgios Makridis"
                    },
                    {
                        "name": "Vasileios Koukos"
                    },
                    {
                        "name": "Georgios Fatouros"
                    },
                    {
                        "name": "Dimosthenis Kyriazis"
                    }
                ],
                "author_detail": {
                    "name": "Dimosthenis Kyriazis"
                },
                "author": "Dimosthenis Kyriazis",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04253v1",
                "updated": "2025-03-06T09:35:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    9,
                    35,
                    29,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T09:35:29Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    9,
                    35,
                    29,
                    3,
                    65,
                    0
                ],
                "title": "ADOR: A Design Exploration Framework for LLM Serving with Enhanced\n  Latency and Throughput",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADOR: A Design Exploration Framework for LLM Serving with Enhanced\n  Latency and Throughput"
                },
                "summary": "The growing adoption of Large Language Models (LLMs) across various domains\nhas driven the demand for efficient and scalable AI-serving solutions.\nDeploying LLMs requires optimizations to manage their significant computational\nand data demands. The prefill stage processes large numbers of input tokens in\nparallel, increasing computational load, while the decoding stage relies\nheavily on memory bandwidth due to the auto-regressive nature of LLMs. Current\nhardware, such as GPUs, often fails to balance these demands, leading to\ninefficient utilization. While batching improves hardware efficiency, it delays\nresponse times, degrading Quality-of-Service (QoS). This disconnect between\nvendors, who aim to maximize resource efficiency, and users, who prioritize low\nlatency, highlights the need for a better solution. To address this, we propose\nADOR, a framework that automatically identifies and recommends hardware\narchitectures tailored to LLM serving. By leveraging predefined architecture\ntemplates specialized for heterogeneous dataflows, ADOR optimally balances\nthroughput and latency. It efficiently explores design spaces to suggest\narchitectures that meet the requirements of both vendors and users. ADOR\ndemonstrates substantial performance improvements, achieving 2.51x higher QoS\nand 4.01x better area efficiency compared to the A100 at high batch sizes,\nmaking it a robust solution for scalable and cost-effective LLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of Large Language Models (LLMs) across various domains\nhas driven the demand for efficient and scalable AI-serving solutions.\nDeploying LLMs requires optimizations to manage their significant computational\nand data demands. The prefill stage processes large numbers of input tokens in\nparallel, increasing computational load, while the decoding stage relies\nheavily on memory bandwidth due to the auto-regressive nature of LLMs. Current\nhardware, such as GPUs, often fails to balance these demands, leading to\ninefficient utilization. While batching improves hardware efficiency, it delays\nresponse times, degrading Quality-of-Service (QoS). This disconnect between\nvendors, who aim to maximize resource efficiency, and users, who prioritize low\nlatency, highlights the need for a better solution. To address this, we propose\nADOR, a framework that automatically identifies and recommends hardware\narchitectures tailored to LLM serving. By leveraging predefined architecture\ntemplates specialized for heterogeneous dataflows, ADOR optimally balances\nthroughput and latency. It efficiently explores design spaces to suggest\narchitectures that meet the requirements of both vendors and users. ADOR\ndemonstrates substantial performance improvements, achieving 2.51x higher QoS\nand 4.01x better area efficiency compared to the A100 at high batch sizes,\nmaking it a robust solution for scalable and cost-effective LLM serving."
                },
                "authors": [
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Geonwoo Ko"
                    },
                    {
                        "name": "Gyubin Choi"
                    },
                    {
                        "name": "Seri Ham"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "Joo-Young Kim"
                    }
                ],
                "author_detail": {
                    "name": "Joo-Young Kim"
                },
                "author": "Joo-Young Kim",
                "arxiv_comment": "11pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04250v1",
                "updated": "2025-03-06T09:33:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    9,
                    33,
                    46,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T09:33:46Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    9,
                    33,
                    46,
                    3,
                    65,
                    0
                ],
                "title": "An Egocentric Vision-Language Model based Portable Real-time Smart\n  Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Egocentric Vision-Language Model based Portable Real-time Smart\n  Assistant"
                },
                "summary": "We present Vinci, a vision-language system designed to provide real-time,\ncomprehensive AI assistance on portable devices. At its core, Vinci leverages\nEgoVideo-VL, a novel model that integrates an egocentric vision foundation\nmodel with a large language model (LLM), enabling advanced functionalities such\nas scene understanding, temporal grounding, video summarization, and future\nplanning. To enhance its utility, Vinci incorporates a memory module for\nprocessing long video streams in real time while retaining contextual history,\na generation module for producing visual action demonstrations, and a retrieval\nmodule that bridges egocentric and third-person perspectives to provide\nrelevant how-to videos for skill acquisition. Unlike existing systems that\noften depend on specialized hardware, Vinci is hardware-agnostic, supporting\ndeployment across a wide range of devices, including smartphones and wearable\ncameras. In our experiments, we first demonstrate the superior performance of\nEgoVideo-VL on multiple public benchmarks, showcasing its vision-language\nreasoning and contextual understanding capabilities. We then conduct a series\nof user studies to evaluate the real-world effectiveness of Vinci, highlighting\nits adaptability and usability in diverse scenarios. We hope Vinci can\nestablish a new framework for portable, real-time egocentric AI systems,\nempowering users with contextual and actionable insights. Including the\nfrontend, backend, and models, all codes of Vinci are available at\nhttps://github.com/OpenGVLab/vinci.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Vinci, a vision-language system designed to provide real-time,\ncomprehensive AI assistance on portable devices. At its core, Vinci leverages\nEgoVideo-VL, a novel model that integrates an egocentric vision foundation\nmodel with a large language model (LLM), enabling advanced functionalities such\nas scene understanding, temporal grounding, video summarization, and future\nplanning. To enhance its utility, Vinci incorporates a memory module for\nprocessing long video streams in real time while retaining contextual history,\na generation module for producing visual action demonstrations, and a retrieval\nmodule that bridges egocentric and third-person perspectives to provide\nrelevant how-to videos for skill acquisition. Unlike existing systems that\noften depend on specialized hardware, Vinci is hardware-agnostic, supporting\ndeployment across a wide range of devices, including smartphones and wearable\ncameras. In our experiments, we first demonstrate the superior performance of\nEgoVideo-VL on multiple public benchmarks, showcasing its vision-language\nreasoning and contextual understanding capabilities. We then conduct a series\nof user studies to evaluate the real-world effectiveness of Vinci, highlighting\nits adaptability and usability in diverse scenarios. We hope Vinci can\nestablish a new framework for portable, real-time egocentric AI systems,\nempowering users with contextual and actionable insights. Including the\nfrontend, backend, and models, all codes of Vinci are available at\nhttps://github.com/OpenGVLab/vinci."
                },
                "authors": [
                    {
                        "name": "Yifei Huang"
                    },
                    {
                        "name": "Jilan Xu"
                    },
                    {
                        "name": "Baoqi Pei"
                    },
                    {
                        "name": "Yuping He"
                    },
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "Mingfang Zhang"
                    },
                    {
                        "name": "Lijin Yang"
                    },
                    {
                        "name": "Zheng Nie"
                    },
                    {
                        "name": "Jinyao Liu"
                    },
                    {
                        "name": "Guoshun Fan"
                    },
                    {
                        "name": "Dechen Lin"
                    },
                    {
                        "name": "Fang Fang"
                    },
                    {
                        "name": "Kunpeng Li"
                    },
                    {
                        "name": "Chang Yuan"
                    },
                    {
                        "name": "Xinyuan Chen"
                    },
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Yali Wang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04241v1",
                "updated": "2025-03-06T09:22:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    9,
                    22,
                    23,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T09:22:23Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    9,
                    22,
                    23,
                    3,
                    65,
                    0
                ],
                "title": "ThrowBench: Benchmarking LLMs by Predicting Runtime Exceptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThrowBench: Benchmarking LLMs by Predicting Runtime Exceptions"
                },
                "summary": "Modern Large Language Models (LLMs) have shown astounding capabilities of\ncode understanding and synthesis. In order to assess such capabilities, several\nbenchmarks have been devised (e.g., HumanEval). However, most benchmarks focus\non code synthesis from natural language instructions. Hence, such benchmarks do\nnot test for other forms of code understanding. Moreover, there have been\nconcerns about contamination and leakage. That is, benchmark problems (or\nclosely related problems) may appear in training set, strongly biasing\nbenchmark results. In this work we investigate whether large language models\ncan correctly predict runtime program behavior. To this end, we introduce\nThrowBench, a benchmark consisting of over 2,400 short user-written programs\nwritten in four different programming languages. The majority of these programs\nthrow an exception during runtime (due to a bug). LLMs are asked to predict\nwhether a presented program throws an exception and, if so, which one.\nEvaluating our benchmark on six state-of-the-art code LLMs we see modest\nperformance ranging from 19 to 38% (F1 score). Benchmarking a wider set of code\ncapabilities could improve the assessment of code LLMs and help identify weak\npoints in current models. Moreover, as ground-truth answers have been\ndetermined through program execution, leakage is not a concern. We release\nThrowBench as well as all of our results together with this work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models (LLMs) have shown astounding capabilities of\ncode understanding and synthesis. In order to assess such capabilities, several\nbenchmarks have been devised (e.g., HumanEval). However, most benchmarks focus\non code synthesis from natural language instructions. Hence, such benchmarks do\nnot test for other forms of code understanding. Moreover, there have been\nconcerns about contamination and leakage. That is, benchmark problems (or\nclosely related problems) may appear in training set, strongly biasing\nbenchmark results. In this work we investigate whether large language models\ncan correctly predict runtime program behavior. To this end, we introduce\nThrowBench, a benchmark consisting of over 2,400 short user-written programs\nwritten in four different programming languages. The majority of these programs\nthrow an exception during runtime (due to a bug). LLMs are asked to predict\nwhether a presented program throws an exception and, if so, which one.\nEvaluating our benchmark on six state-of-the-art code LLMs we see modest\nperformance ranging from 19 to 38% (F1 score). Benchmarking a wider set of code\ncapabilities could improve the assessment of code LLMs and help identify weak\npoints in current models. Moreover, as ground-truth answers have been\ndetermined through program execution, leakage is not a concern. We release\nThrowBench as well as all of our results together with this work."
                },
                "authors": [
                    {
                        "name": "Julian Aron Prenner"
                    },
                    {
                        "name": "Romain Robbes"
                    }
                ],
                "author_detail": {
                    "name": "Romain Robbes"
                },
                "author": "Romain Robbes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04240v1",
                "updated": "2025-03-06T09:21:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    9,
                    21,
                    54,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T09:21:54Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    9,
                    21,
                    54,
                    3,
                    65,
                    0
                ],
                "title": "DiffPO: Diffusion-styled Preference Optimization for Efficient\n  Inference-Time Alignment of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffPO: Diffusion-styled Preference Optimization for Efficient\n  Inference-Time Alignment of Large Language Models"
                },
                "summary": "Inference-time alignment provides an efficient alternative for aligning LLMs\nwith humans. However, these approaches still face challenges, such as limited\nscalability due to policy-specific value functions and latency during the\ninference phase. In this paper, we propose a novel approach, Diffusion-styled\nPreference Optimization (\\model), which provides an efficient and\npolicy-agnostic solution for aligning LLMs with humans. By directly performing\nalignment at sentence level, \\model~avoids the time latency associated with\ntoken-level generation. Designed as a plug-and-play module, \\model~can be\nseamlessly integrated with various base models to enhance their alignment.\nExtensive experiments on AlpacaEval 2, MT-bench, and HH-RLHF demonstrate that\n\\model~achieves superior alignment performance across various settings,\nachieving a favorable trade-off between alignment quality and inference-time\nlatency. Furthermore, \\model~demonstrates model-agnostic scalability,\nsignificantly improving the performance of large models such as Llama-3-70B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time alignment provides an efficient alternative for aligning LLMs\nwith humans. However, these approaches still face challenges, such as limited\nscalability due to policy-specific value functions and latency during the\ninference phase. In this paper, we propose a novel approach, Diffusion-styled\nPreference Optimization (\\model), which provides an efficient and\npolicy-agnostic solution for aligning LLMs with humans. By directly performing\nalignment at sentence level, \\model~avoids the time latency associated with\ntoken-level generation. Designed as a plug-and-play module, \\model~can be\nseamlessly integrated with various base models to enhance their alignment.\nExtensive experiments on AlpacaEval 2, MT-bench, and HH-RLHF demonstrate that\n\\model~achieves superior alignment performance across various settings,\nachieving a favorable trade-off between alignment quality and inference-time\nlatency. Furthermore, \\model~demonstrates model-agnostic scalability,\nsignificantly improving the performance of large models such as Llama-3-70B."
                },
                "authors": [
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Zhifei Yang"
                    },
                    {
                        "name": "Xiaotian Zhang"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    },
                    {
                        "name": "Tony Quek"
                    },
                    {
                        "name": "Soujanya Poria"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04222v1",
                "updated": "2025-03-06T09:03:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    9,
                    3,
                    36,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T09:03:36Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    9,
                    3,
                    36,
                    3,
                    65,
                    0
                ],
                "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion"
                },
                "summary": "We introduce FuseChat-3.0, a suite of large language models (LLMs) developed\nby integrating the strengths of heterogeneous source LLMs into more compact\ntarget LLMs. Our source models include the powerful Gemma-2-27B-it,\nMistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct.\nFor target models, we focus on three widely-used smaller\nvariants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along\nwith two ultra-compact options, Llama-3.2-3B-Instruct and\nLlama-3.2-1B-Instruct. To leverage the diverse capabilities of these source\nmodels, we develop a specialized data construction protocol tailored to various\ntasks and domains. The FuseChat-3.0 training pipeline consists of two key\nstages: (1) supervised fine-tuning (SFT) to align the target and source model\ndistributions, and (2) Direct Preference Optimization (DPO) to apply\npreferences from multiple source LLMs to fine-tune the target model. The\nresulting FuseChat-3.0 models exhibit significant performance gains across\ntasks such as instruction following, general knowledge, mathematics, and\ncoding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target\nmodel, our fusion approach achieves an average improvement of 6.8 points across\n14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and\n30.1 points on the instruction-following benchmarks AlpacaEval-2 and\nArena-Hard, respectively. Our code, models, and datasets are available at\nhttps://github.com/SLIT-AI/FuseChat-3.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce FuseChat-3.0, a suite of large language models (LLMs) developed\nby integrating the strengths of heterogeneous source LLMs into more compact\ntarget LLMs. Our source models include the powerful Gemma-2-27B-it,\nMistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct.\nFor target models, we focus on three widely-used smaller\nvariants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along\nwith two ultra-compact options, Llama-3.2-3B-Instruct and\nLlama-3.2-1B-Instruct. To leverage the diverse capabilities of these source\nmodels, we develop a specialized data construction protocol tailored to various\ntasks and domains. The FuseChat-3.0 training pipeline consists of two key\nstages: (1) supervised fine-tuning (SFT) to align the target and source model\ndistributions, and (2) Direct Preference Optimization (DPO) to apply\npreferences from multiple source LLMs to fine-tune the target model. The\nresulting FuseChat-3.0 models exhibit significant performance gains across\ntasks such as instruction following, general knowledge, mathematics, and\ncoding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target\nmodel, our fusion approach achieves an average improvement of 6.8 points across\n14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and\n30.1 points on the instruction-following benchmarks AlpacaEval-2 and\nArena-Hard, respectively. Our code, models, and datasets are available at\nhttps://github.com/SLIT-AI/FuseChat-3.0."
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Canbin Huang"
                    },
                    {
                        "name": "Guosheng Liang"
                    },
                    {
                        "name": "Xiaojun Quan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Quan"
                },
                "author": "Xiaojun Quan",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01224v2",
                "updated": "2025-03-06T08:51:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    8,
                    51,
                    38,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-03T06:43:45Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    6,
                    43,
                    45,
                    0,
                    62,
                    0
                ],
                "title": "CE-U: Cross Entropy Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CE-U: Cross Entropy Unlearning"
                },
                "summary": "Large language models (LLMs) inadvertently memorize sensitive data from their\nmassive pretraining corpora \\cite{jang2022knowledge}. In this work, we propose\nCE-U (Cross Entropy Unlearning), a novel loss function designed specifically\nfor unlearning tasks. CE-U addresses fundamental limitations of gradient ascent\napproaches which suffer from instability due to vanishing gradients when model\nconfidence is high and gradient exploding when confidence is low. We also unify\nstandard cross entropy supervision and cross entropy unlearning into a single\nframework. Notably, on the TOFU benchmark for unlearning \\cite{maini2024tofu},\nCE-U achieves state-of-the-art results on LLaMA2-7B with 1\\% and 5\\%\nforgetting, even without the use of any extra reference model or additional\npositive samples. Our theoretical analysis further reveals that the gradient\ninstability issues also exist in popular reinforcement learning algorithms like\nDPO \\cite{rafailov2023direct} and GRPO\\cite{Shao2024DeepSeekMath}, as they\ninclude a gradient ascent component. This suggests that applying CE-U\nprinciples to reinforcement learning could be a promising direction for\nimproving stability and convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inadvertently memorize sensitive data from their\nmassive pretraining corpora \\cite{jang2022knowledge}. In this work, we propose\nCE-U (Cross Entropy Unlearning), a novel loss function designed specifically\nfor unlearning tasks. CE-U addresses fundamental limitations of gradient ascent\napproaches which suffer from instability due to vanishing gradients when model\nconfidence is high and gradient exploding when confidence is low. We also unify\nstandard cross entropy supervision and cross entropy unlearning into a single\nframework. Notably, on the TOFU benchmark for unlearning \\cite{maini2024tofu},\nCE-U achieves state-of-the-art results on LLaMA2-7B with 1\\% and 5\\%\nforgetting, even without the use of any extra reference model or additional\npositive samples. Our theoretical analysis further reveals that the gradient\ninstability issues also exist in popular reinforcement learning algorithms like\nDPO \\cite{rafailov2023direct} and GRPO\\cite{Shao2024DeepSeekMath}, as they\ninclude a gradient ascent component. This suggests that applying CE-U\nprinciples to reinforcement learning could be a promising direction for\nimproving stability and convergence."
                },
                "authors": [
                    {
                        "name": "Bo Yang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yang"
                },
                "author": "Bo Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07009v2",
                "updated": "2025-03-06T08:51:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    8,
                    51,
                    5,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-09T15:52:48Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    52,
                    48,
                    2,
                    283,
                    0
                ],
                "title": "Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with\n  Patent-Paper Pairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with\n  Patent-Paper Pairs"
                },
                "summary": "Dealing with long and highly complex technical text is a challenge for Large\nLanguage Models (LLMs), which still have to unfold their potential in\nsupporting expensive and timeintensive processes like patent drafting. Within\npatents, the description constitutes more than 90% of the document on average.\nYet, its automatic generation remains understudied. When drafting patent\napplications, patent attorneys typically receive invention reports (IRs), which\nare usually confidential, hindering research on LLM-supported patent drafting.\nOften, prepublication research papers serve as IRs. We leverage this duality to\nbuild PAP2PAT, an open and realistic benchmark for patent drafting consisting\nof 1.8k patent-paper pairs describing the same inventions. To address the\ncomplex longdocument patent generation task, we propose chunk-based\noutline-guided generation using the research paper as invention specification.\nOur extensive evaluation using PAP2PAT and a human case study show that LLMs\ncan effectively leverage information from the paper, but still struggle to\nprovide the necessary level of detail. Fine-tuning leads to more patent-style\nlanguage, but also to more hallucination. We release our data and code\nhttps://github.com/boschresearch/Pap2Pat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dealing with long and highly complex technical text is a challenge for Large\nLanguage Models (LLMs), which still have to unfold their potential in\nsupporting expensive and timeintensive processes like patent drafting. Within\npatents, the description constitutes more than 90% of the document on average.\nYet, its automatic generation remains understudied. When drafting patent\napplications, patent attorneys typically receive invention reports (IRs), which\nare usually confidential, hindering research on LLM-supported patent drafting.\nOften, prepublication research papers serve as IRs. We leverage this duality to\nbuild PAP2PAT, an open and realistic benchmark for patent drafting consisting\nof 1.8k patent-paper pairs describing the same inventions. To address the\ncomplex longdocument patent generation task, we propose chunk-based\noutline-guided generation using the research paper as invention specification.\nOur extensive evaluation using PAP2PAT and a human case study show that LLMs\ncan effectively leverage information from the paper, but still struggle to\nprovide the necessary level of detail. Fine-tuning leads to more patent-style\nlanguage, but also to more hallucination. We release our data and code\nhttps://github.com/boschresearch/Pap2Pat."
                },
                "authors": [
                    {
                        "name": "Valentin Knappich"
                    },
                    {
                        "name": "Simon Razniewski"
                    },
                    {
                        "name": "Anna Hätty"
                    },
                    {
                        "name": "Annemarie Friedrich"
                    }
                ],
                "author_detail": {
                    "name": "Annemarie Friedrich"
                },
                "author": "Annemarie Friedrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10858v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10858v5",
                "updated": "2025-03-06T08:37:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    8,
                    37,
                    37,
                    3,
                    65,
                    0
                ],
                "published": "2023-04-21T10:01:56Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    10,
                    1,
                    56,
                    4,
                    111,
                    0
                ],
                "title": "Autonomous RISs and Oblivious Base Stations: The Observer Effect and its\n  Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous RISs and Oblivious Base Stations: The Observer Effect and its\n  Mitigation"
                },
                "summary": "Autonomous reconfigurable intelligent surfaces (RISs) offer the potential to\nsimplify deployment by reducing the need for real-time remote control between a\nbase station (BS) and an RIS. However, we highlight two major challenges posed\nby autonomy. The first is implementation complexity, as autonomy requires\nhybrid RISs (HRISs) equipped with additional onboard hardware to monitor the\npropagation environment and perform local channel estimation (CHEST), a process\nknown as probing. The second challenge, termed probe distortion, reflects a\nform of the observer effect: during probing, an HRIS can inadvertently alter\nthe propagation environment, potentially disrupting the operations of other\ncommunicating devices sharing the environment. Although implementation\ncomplexity has been extensively studied, probe distortion remains largely\nunexplored. To further assess the potential of autonomous RIS, this paper\ncomprehensively and pragmatically studies the fundamental trade-offs posed by\nthese challenges collectively. In particular, we examine the robustness of an\nHRIS-assisted massive multiple-input multiple-output (mMIMO) system by\nconsidering its critical components and stringent conditions. The latter\ninclude: (a) two extremes of implementation complexity, represented by\nminimalist operation designs of two distinct HRIS hardware architectures, and\n(b) an oblivious BS that fully embraces probe distortion. To make our analysis\npossible, we propose a physical-layer orchestration framework that aligns HRIS\nand mMIMO operations. We present empirical evidence that autonomous RISs remain\npromising under stringent conditions and outline research directions to deepen\nprobe distortion understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous reconfigurable intelligent surfaces (RISs) offer the potential to\nsimplify deployment by reducing the need for real-time remote control between a\nbase station (BS) and an RIS. However, we highlight two major challenges posed\nby autonomy. The first is implementation complexity, as autonomy requires\nhybrid RISs (HRISs) equipped with additional onboard hardware to monitor the\npropagation environment and perform local channel estimation (CHEST), a process\nknown as probing. The second challenge, termed probe distortion, reflects a\nform of the observer effect: during probing, an HRIS can inadvertently alter\nthe propagation environment, potentially disrupting the operations of other\ncommunicating devices sharing the environment. Although implementation\ncomplexity has been extensively studied, probe distortion remains largely\nunexplored. To further assess the potential of autonomous RIS, this paper\ncomprehensively and pragmatically studies the fundamental trade-offs posed by\nthese challenges collectively. In particular, we examine the robustness of an\nHRIS-assisted massive multiple-input multiple-output (mMIMO) system by\nconsidering its critical components and stringent conditions. The latter\ninclude: (a) two extremes of implementation complexity, represented by\nminimalist operation designs of two distinct HRIS hardware architectures, and\n(b) an oblivious BS that fully embraces probe distortion. To make our analysis\npossible, we propose a physical-layer orchestration framework that aligns HRIS\nand mMIMO operations. We present empirical evidence that autonomous RISs remain\npromising under stringent conditions and outline research directions to deepen\nprobe distortion understanding."
                },
                "authors": [
                    {
                        "name": "Victor Croisfelt"
                    },
                    {
                        "name": "Francesco Devoti"
                    },
                    {
                        "name": "Fabio Saggese"
                    },
                    {
                        "name": "Vincenzo Sciancalepore"
                    },
                    {
                        "name": "Xavier Costa-Pérez"
                    },
                    {
                        "name": "Petar Popovski"
                    }
                ],
                "author_detail": {
                    "name": "Petar Popovski"
                },
                "author": "Petar Popovski",
                "arxiv_doi": "10.1109/TWC.2025.3546118",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TWC.2025.3546118",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2304.10858v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10858v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 9 figures, published in IEEE Transactions on Wireless\n  Communications",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04199v1",
                "updated": "2025-03-06T08:27:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    8,
                    27,
                    51,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T08:27:51Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    8,
                    27,
                    51,
                    3,
                    65,
                    0
                ],
                "title": "MASTER: Multimodal Segmentation with Text Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASTER: Multimodal Segmentation with Text Prompts"
                },
                "summary": "RGB-Thermal fusion is a potential solution for various weather and light\nconditions in challenging scenarios. However, plenty of studies focus on\ndesigning complex modules to fuse different modalities. With the widespread\napplication of large language models (LLMs), valuable information can be more\neffectively extracted from natural language. Therefore, we aim to leverage the\nadvantages of large language models to design a structurally simple and highly\nadaptable multimodal fusion model architecture. We proposed MultimodAl\nSegmentation with TExt PRompts (MASTER) architecture, which integrates LLM into\nthe fusion of RGB-Thermal multimodal data and allows complex query text to\nparticipate in the fusion process. Our model utilizes a dual-path structure to\nextract information from different modalities of images. Additionally, we\nemploy LLM as the core module for multimodal fusion, enabling the model to\ngenerate learnable codebook tokens from RGB, thermal images, and textual\ninformation. A lightweight image decoder is used to obtain semantic\nsegmentation results. The proposed MASTER performs exceptionally well in\nbenchmark tests across various automated driving scenarios, yielding promising\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RGB-Thermal fusion is a potential solution for various weather and light\nconditions in challenging scenarios. However, plenty of studies focus on\ndesigning complex modules to fuse different modalities. With the widespread\napplication of large language models (LLMs), valuable information can be more\neffectively extracted from natural language. Therefore, we aim to leverage the\nadvantages of large language models to design a structurally simple and highly\nadaptable multimodal fusion model architecture. We proposed MultimodAl\nSegmentation with TExt PRompts (MASTER) architecture, which integrates LLM into\nthe fusion of RGB-Thermal multimodal data and allows complex query text to\nparticipate in the fusion process. Our model utilizes a dual-path structure to\nextract information from different modalities of images. Additionally, we\nemploy LLM as the core module for multimodal fusion, enabling the model to\ngenerate learnable codebook tokens from RGB, thermal images, and textual\ninformation. A lightweight image decoder is used to obtain semantic\nsegmentation results. The proposed MASTER performs exceptionally well in\nbenchmark tests across various automated driving scenarios, yielding promising\nresults."
                },
                "authors": [
                    {
                        "name": "Fuyang Liu"
                    },
                    {
                        "name": "Shun Lu"
                    },
                    {
                        "name": "Jilin Mei"
                    },
                    {
                        "name": "Yu Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Hu"
                },
                "author": "Yu Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12106v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12106v3",
                "updated": "2025-03-06T08:18:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    8,
                    18,
                    50,
                    3,
                    65,
                    0
                ],
                "published": "2024-09-18T16:26:22Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    16,
                    26,
                    22,
                    2,
                    262,
                    0
                ],
                "title": "Measuring Human and AI Values Based on Generative Psychometrics with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Human and AI Values Based on Generative Psychometrics with\n  Large Language Models"
                },
                "summary": "Human values and their measurement are long-standing interdisciplinary\ninquiry. Recent advances in AI have sparked renewed interest in this area, with\nlarge language models (LLMs) emerging as both tools and subjects of value\nmeasurement. This work introduces Generative Psychometrics for Values (GPV), an\nLLM-based, data-driven value measurement paradigm, theoretically grounded in\ntext-revealed selective perceptions. The core idea is to dynamically parse\nunstructured texts into perceptions akin to static stimuli in traditional\npsychometrics, measure the value orientations they reveal, and aggregate the\nresults. Applying GPV to human-authored blogs, we demonstrate its stability,\nvalidity, and superiority over prior psychological tools. Then, extending GPV\nto LLM value measurement, we advance the current art with 1) a psychometric\nmethodology that measures LLM values based on their scalable and free-form\noutputs, enabling context-specific measurement; 2) a comparative analysis of\nmeasurement paradigms, indicating response biases of prior methods; and 3) an\nattempt to bridge LLM values and their safety, revealing the predictive power\nof different value systems and the impacts of various values on LLM safety.\nThrough interdisciplinary efforts, we aim to leverage AI for next-generation\npsychometrics and psychometrics for value-aligned AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human values and their measurement are long-standing interdisciplinary\ninquiry. Recent advances in AI have sparked renewed interest in this area, with\nlarge language models (LLMs) emerging as both tools and subjects of value\nmeasurement. This work introduces Generative Psychometrics for Values (GPV), an\nLLM-based, data-driven value measurement paradigm, theoretically grounded in\ntext-revealed selective perceptions. The core idea is to dynamically parse\nunstructured texts into perceptions akin to static stimuli in traditional\npsychometrics, measure the value orientations they reveal, and aggregate the\nresults. Applying GPV to human-authored blogs, we demonstrate its stability,\nvalidity, and superiority over prior psychological tools. Then, extending GPV\nto LLM value measurement, we advance the current art with 1) a psychometric\nmethodology that measures LLM values based on their scalable and free-form\noutputs, enabling context-specific measurement; 2) a comparative analysis of\nmeasurement paradigms, indicating response biases of prior methods; and 3) an\nattempt to bridge LLM values and their safety, revealing the predictive power\nof different value systems and the impacts of various values on LLM safety.\nThrough interdisciplinary efforts, we aim to leverage AI for next-generation\npsychometrics and psychometrics for value-aligned AI."
                },
                "authors": [
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Yuanyi Ren"
                    },
                    {
                        "name": "Hanjun Fang"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Guojie Song"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Song"
                },
                "author": "Guojie Song",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12106v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12106v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]