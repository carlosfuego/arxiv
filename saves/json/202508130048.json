[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.08134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08134v2",
                "updated": "2025-08-12T02:27:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    27,
                    5,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T16:10:00Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    0,
                    0,
                    223,
                    0
                ],
                "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control"
                },
                "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement."
                },
                "authors": [
                    {
                        "name": "Zeqian Long"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Xinhua Zhang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma",
                "arxiv_comment": "Project webpage is available at https://follow-your-shape.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08081v1",
                "updated": "2025-08-11T15:28:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:28:28Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "title": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures"
                },
                "summary": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29."
                },
                "authors": [
                    {
                        "name": "Florian Naef"
                    },
                    {
                        "name": "Thomas Willwacher"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Willwacher"
                },
                "author": "Thomas Willwacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.QA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v2",
                "updated": "2025-08-11T14:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    15,
                    27,
                    0,
                    223,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "15 pages, 14 figures; Accepted by ICCV2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07811v1",
                "updated": "2025-08-11T09:54:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T09:54:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration"
                },
                "summary": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions."
                },
                "authors": [
                    {
                        "name": "Sicheng Gao"
                    },
                    {
                        "name": "Nancy Mehta"
                    },
                    {
                        "name": "Zongwei Wu"
                    },
                    {
                        "name": "Radu Timofte"
                    }
                ],
                "author_detail": {
                    "name": "Radu Timofte"
                },
                "author": "Radu Timofte",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v2",
                "updated": "2025-08-11T08:10:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    10,
                    21,
                    0,
                    223,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07675v2",
                "updated": "2025-08-12T02:51:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    51,
                    12,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T06:53:27Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    6,
                    53,
                    27,
                    0,
                    223,
                    0
                ],
                "title": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Baran Atalar"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v4",
                "updated": "2025-08-11T06:16:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    6,
                    16,
                    52,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07570v1",
                "updated": "2025-08-11T03:03:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T03:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios."
                },
                "authors": [
                    {
                        "name": "Khanh-Binh Nguyen"
                    },
                    {
                        "name": "Phuoc-Nguyen Bui"
                    },
                    {
                        "name": "Hyunseung Choo"
                    },
                    {
                        "name": "Duc Thanh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Duc Thanh Nguyen"
                },
                "author": "Duc Thanh Nguyen",
                "arxiv_comment": "12 pages, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v2",
                "updated": "2025-08-09T11:31:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    31,
                    44,
                    5,
                    221,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06937v1",
                "updated": "2025-08-09T11:06:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-09T11:06:58Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing"
                },
                "summary": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods."
                },
                "authors": [
                    {
                        "name": "Weiyan Xie"
                    },
                    {
                        "name": "Han Gao"
                    },
                    {
                        "name": "Didan Deng"
                    },
                    {
                        "name": "Kaican Li"
                    },
                    {
                        "name": "April Hua Liu"
                    },
                    {
                        "name": "Yongxiang Huang"
                    },
                    {
                        "name": "Nevin L. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Nevin L. Zhang"
                },
                "author": "Nevin L. Zhang",
                "arxiv_comment": "Project Page: vaynexie.github.io/CannyEdit/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03974v2",
                "updated": "2025-08-09T02:33:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    2,
                    33,
                    21,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-05T23:47:34Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    23,
                    47,
                    34,
                    1,
                    217,
                    0
                ],
                "title": "Managing Data for Scalable and Interactive Event Sequence Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing Data for Scalable and Interactive Event Sequence Visualization"
                },
                "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization."
                },
                "authors": [
                    {
                        "name": "Sayef Azad Sakin"
                    },
                    {
                        "name": "Katherine E. Isaacs"
                    }
                ],
                "author_detail": {
                    "name": "Katherine E. Isaacs"
                },
                "author": "Katherine E. Isaacs",
                "arxiv_comment": "The 15th IEEE Workshop on Large Data Analysis and Visualization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v2",
                "updated": "2025-08-09T00:12:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    0,
                    12,
                    1,
                    5,
                    221,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v3",
                "updated": "2025-08-08T18:16:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    18,
                    16,
                    33,
                    4,
                    220,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06447v1",
                "updated": "2025-08-08T16:42:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:42:38Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning"
                },
                "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Rubing Yang"
                    },
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Desheng Hui"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Jianlei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianlei Yang"
                },
                "author": "Jianlei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v3",
                "updated": "2025-08-08T14:25:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    25,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06297v1",
                "updated": "2025-08-08T13:19:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:19:30Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "title": "KV Cache Compression for Inference Efficiency in LLMs: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression for Inference Efficiency in LLMs: A Review"
                },
                "summary": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanyu Liu"
                    },
                    {
                        "name": "Jingying Fu"
                    },
                    {
                        "name": "Sixiang Liu"
                    },
                    {
                        "name": "Yitian Zou"
                    },
                    {
                        "name": "You Fu"
                    },
                    {
                        "name": "Jiehan Zhou"
                    },
                    {
                        "name": "Shouhua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shouhua Zhang"
                },
                "arxiv_affiliation": "University of Oulu",
                "author": "Shouhua Zhang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06160v1",
                "updated": "2025-08-08T09:29:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:29:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment"
                },
                "summary": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff."
                },
                "authors": [
                    {
                        "name": "Zhenbang Du"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Lifu Wang"
                    },
                    {
                        "name": "Jiayi Qian"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v1",
                "updated": "2025-08-08T08:54:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06064v1",
                "updated": "2025-08-08T06:53:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T06:53:50Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "title": "A Generic Complete Anytime Beam Search for Optimal Decision Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generic Complete Anytime Beam Search for Optimal Decision Tree"
                },
                "summary": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees."
                },
                "authors": [
                    {
                        "name": "Harold Silvère Kiossou"
                    },
                    {
                        "name": "Siegfried Nijssen"
                    },
                    {
                        "name": "Pierre Schaus"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Schaus"
                },
                "author": "Pierre Schaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2104.13123v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2104.13123v3",
                "updated": "2025-08-08T06:38:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    38,
                    1,
                    4,
                    220,
                    0
                ],
                "published": "2021-04-27T11:55:54Z",
                "published_parsed": [
                    2021,
                    4,
                    27,
                    11,
                    55,
                    54,
                    1,
                    117,
                    0
                ],
                "title": "Affine Springer fibers and depth zero L-packets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affine Springer fibers and depth zero L-packets"
                },
                "summary": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable."
                },
                "authors": [
                    {
                        "name": "Roman Bezrukavnikov"
                    },
                    {
                        "name": "Yakov Varshavsky"
                    }
                ],
                "author_detail": {
                    "name": "Yakov Varshavsky"
                },
                "author": "Yakov Varshavsky",
                "arxiv_comment": "v.3, 96 pages, minor changes in abstract and introduction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2104.13123v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2104.13123v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "22E50, 22E57",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05904v1",
                "updated": "2025-08-07T23:53:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:53:31Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "title": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data"
                },
                "summary": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks."
                },
                "authors": [
                    {
                        "name": "Brandon Baker"
                    },
                    {
                        "name": "Elliott Brossard"
                    },
                    {
                        "name": "Chenwei Xie"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Deen Liu"
                    },
                    {
                        "name": "Yijun Xie"
                    },
                    {
                        "name": "Arthur Zwiegincew"
                    },
                    {
                        "name": "Nitya Kumar Sharma"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Eugene Retunsky"
                    },
                    {
                        "name": "Mike Halcrow"
                    },
                    {
                        "name": "Derek Denny-Brown"
                    },
                    {
                        "name": "Istvan Cseri"
                    },
                    {
                        "name": "Tyler Akidau"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "arxiv_comment": "12 pages, 6 figures, accepted in ICDCS 2025",
                "arxiv_journal_ref": "Proc. 45th IEEE International Conference on Distributed Computing\n  Systems (ICDCS), Glasgow, UK, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05898v1",
                "updated": "2025-08-07T23:11:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:11:33Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "title": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates"
                },
                "summary": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA."
                },
                "authors": [
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Aijun An"
                    },
                    {
                        "name": "Ali cheraghian"
                    }
                ],
                "author_detail": {
                    "name": "Ali cheraghian"
                },
                "author": "Ali cheraghian",
                "arxiv_comment": "BMVC2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v1",
                "updated": "2025-08-07T09:47:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05091v1",
                "updated": "2025-08-07T07:19:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T07:19:02Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "title": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation"
                },
                "summary": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration."
                },
                "authors": [
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Busheng Su"
                    },
                    {
                        "name": "Finn Wong"
                    }
                ],
                "author_detail": {
                    "name": "Finn Wong"
                },
                "author": "Finn Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05012v1",
                "updated": "2025-08-07T03:49:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T03:49:56Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "title": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines"
                },
                "summary": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion."
                },
                "authors": [
                    {
                        "name": "Ugur Cetintemel"
                    },
                    {
                        "name": "Shu Chen"
                    },
                    {
                        "name": "Alexander W. Lee"
                    },
                    {
                        "name": "Deepti Raghavan"
                    }
                ],
                "author_detail": {
                    "name": "Deepti Raghavan"
                },
                "author": "Deepti Raghavan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v2",
                "updated": "2025-08-06T16:57:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    57,
                    39,
                    2,
                    218,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Zhengming Zhang"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Accepted by ICCV 2025; Code released at\n  https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04581v1",
                "updated": "2025-08-06T16:06:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:06:43Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning"
                },
                "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance."
                },
                "authors": [
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    }
                ],
                "author_detail": {
                    "name": "Stamatios Lefkimmiatis"
                },
                "author": "Stamatios Lefkimmiatis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v3",
                "updated": "2025-08-06T15:38:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    38,
                    6,
                    2,
                    218,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads"
                },
                "summary": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Ted Hart"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "Proceedings of the VLDB Endowment 18 (VLDB'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v1",
                "updated": "2025-08-06T14:02:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference"
                },
                "summary": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v3",
                "updated": "2025-08-06T11:46:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    46,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "arxiv_comment": "Submission under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04257v1",
                "updated": "2025-08-06T09:40:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T09:40:09Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs"
                },
                "summary": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "arxiv_comment": "Published as a conference paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00370v2",
                "updated": "2025-08-06T08:32:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    8,
                    32,
                    53,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-01T07:03:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices"
                },
                "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Poh Seng Lim"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "JungHau Foo"
                    },
                    {
                        "name": "Yap Deep"
                    },
                    {
                        "name": "Timothy Lee Jun Jie"
                    },
                    {
                        "name": "Kelvin Teh Kae Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Danyu Feng"
                    },
                    {
                        "name": "Hao-Yun Chen"
                    },
                    {
                        "name": "Peng-Wen Chen"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wong Wai Mun"
                    }
                ],
                "author_detail": {
                    "name": "Wong Wai Mun"
                },
                "author": "Wong Wai Mun",
                "arxiv_comment": "The data and method in the paper need to be re-audited",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03837v1",
                "updated": "2025-08-05T18:34:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T18:34:48Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "title": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems"
                },
                "summary": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs."
                },
                "authors": [
                    {
                        "name": "Davide Zoni"
                    },
                    {
                        "name": "Andrea Galimberti"
                    },
                    {
                        "name": "Adriano Guarisco"
                    }
                ],
                "author_detail": {
                    "name": "Adriano Guarisco"
                },
                "author": "Adriano Guarisco",
                "arxiv_comment": "9 pages, 13 figures, 1 table, accepted for presentation at 2025\n  International Conference on Computer-Aided Design (ICCAD), Munich, Germany,\n  October 26-30, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v2",
                "updated": "2025-08-05T16:17:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    16,
                    17,
                    1,
                    1,
                    217,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03321v1",
                "updated": "2025-08-05T11:00:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T11:00:41Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "title": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios"
                },
                "summary": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS."
                },
                "authors": [
                    {
                        "name": "Jörn Bodenhausen"
                    },
                    {
                        "name": "Simon Mangel"
                    },
                    {
                        "name": "Thomas Vogt"
                    },
                    {
                        "name": "Martin Henze"
                    }
                ],
                "author_detail": {
                    "name": "Martin Henze"
                },
                "author": "Martin Henze",
                "arxiv_comment": "Accepted for publication in Proceedings of the 2025 IEEE 50th\n  Conference on Local Computer Networks (LCN)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03258v1",
                "updated": "2025-08-05T09:35:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T09:35:52Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "title": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%."
                },
                "authors": [
                    {
                        "name": "Yueyue Liu"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Yuantian Miao"
                    }
                ],
                "author_detail": {
                    "name": "Yuantian Miao"
                },
                "author": "Yuantian Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02240v2",
                "updated": "2025-08-05T02:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    2,
                    13,
                    39,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T09:39:31Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    39,
                    31,
                    0,
                    216,
                    0
                ],
                "title": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}"
                },
                "authors": [
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Jiaxing Yan"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Zetao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v5",
                "updated": "2025-08-05T00:25:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    0,
                    25,
                    53,
                    1,
                    217,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition"
                },
                "summary": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "Philip Levis"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v1",
                "updated": "2025-08-04T16:14:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02401v1",
                "updated": "2025-08-04T13:26:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:26:16Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git."
                },
                "authors": [
                    {
                        "name": "Xiaolin Lin"
                    },
                    {
                        "name": "Jingcun Wang"
                    },
                    {
                        "name": "Olga Kondrateva"
                    },
                    {
                        "name": "Yiyu Shi"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Grace Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Grace Li Zhang"
                },
                "author": "Grace Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02280v1",
                "updated": "2025-08-04T10:51:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T10:51:20Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "title": "OnPair: Short Strings Compression for Fast Random Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnPair: Short Strings Compression for Fast Random Access"
                },
                "summary": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage."
                },
                "authors": [
                    {
                        "name": "Francesco Gargiulo"
                    },
                    {
                        "name": "Rossano Venturini"
                    }
                ],
                "author_detail": {
                    "name": "Rossano Venturini"
                },
                "author": "Rossano Venturini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; E.4; H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02215v1",
                "updated": "2025-08-04T09:08:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:08:43Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"
                },
                "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK."
                },
                "authors": [
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Zhiyuan He"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19906v2",
                "updated": "2025-08-04T08:19:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    8,
                    19,
                    26,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-26T10:34:53Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "title": "CaliDrop: KV Cache Compression with Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaliDrop: KV Cache Compression with Calibration"
                },
                "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v2",
                "updated": "2025-08-04T04:48:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    4,
                    48,
                    41,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02930v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02930v4",
                "updated": "2025-08-04T02:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    47,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2024-07-03T09:02:05Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    2,
                    5,
                    2,
                    185,
                    0
                ],
                "title": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs"
                },
                "summary": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Kun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yang"
                },
                "author": "Kun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02930v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02930v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v2",
                "updated": "2025-08-04T02:17:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    17,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "14 pages, 7 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01898v1",
                "updated": "2025-08-03T19:16:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T19:16:40Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "title": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution"
                },
                "summary": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Md-Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in the IEEE Transactions on\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v1",
                "updated": "2025-08-03T18:15:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Linxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Boqian Wang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16607v2",
                "updated": "2025-08-03T10:27:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    10,
                    27,
                    19,
                    6,
                    215,
                    0
                ],
                "published": "2025-01-28T00:52:23Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    52,
                    23,
                    1,
                    28,
                    0
                ],
                "title": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search"
                },
                "summary": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy."
                },
                "authors": [
                    {
                        "name": "Shuozhi Yuan"
                    },
                    {
                        "name": "Limin Chen"
                    },
                    {
                        "name": "Miaomiao Yuan"
                    },
                    {
                        "name": "Jin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Zhao"
                },
                "author": "Jin Zhao",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02751v1",
                "updated": "2025-08-03T09:15:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T09:15:36Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference"
                },
                "summary": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Yajuan Peng"
                    },
                    {
                        "name": "Cam-Tu Nguyen"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Xiaoming Fu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Fu"
                },
                "author": "Xiaoming Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19718v2",
                "updated": "2025-08-02T23:59:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    23,
                    59,
                    11,
                    5,
                    214,
                    0
                ],
                "published": "2025-07-25T23:55:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting"
                },
                "summary": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency."
                },
                "authors": [
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Hamid Gadirov"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v1",
                "updated": "2025-08-02T21:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gaëtan Hadjeres"
                    },
                    {
                        "name": "Gaël Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_comment": "Accepted to the Transactions of the International Society for Music\n  Information Retrieval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01298v1",
                "updated": "2025-08-02T10:12:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T10:12:45Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "title": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access"
                },
                "summary": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations."
                },
                "authors": [
                    {
                        "name": "Azadeh Sadat Miraftab"
                    },
                    {
                        "name": "Ahmadreza Montazerolghaem"
                    },
                    {
                        "name": "Behrad Mahboobi"
                    }
                ],
                "author_detail": {
                    "name": "Behrad Mahboobi"
                },
                "author": "Behrad Mahboobi",
                "arxiv_doi": "10.1007/s10586-025-05256-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-025-05256-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01261v1",
                "updated": "2025-08-02T08:33:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T08:33:30Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "title": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models"
                },
                "summary": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v2",
                "updated": "2025-08-02T06:50:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    50,
                    59,
                    5,
                    214,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "To appear in ASPLOS'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v1",
                "updated": "2025-08-02T06:43:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance."
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06526v1",
                "updated": "2025-08-02T03:50:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    3,
                    50,
                    14,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T03:50:14Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    3,
                    50,
                    14,
                    5,
                    214,
                    0
                ],
                "title": "PiKV: KV Cache Management System for Mixture of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiKV: KV Cache Management System for Mixture of Experts"
                },
                "summary": "As large language models continue to scale up in both size and context\nlength, the memory and communication cost of key-value (KV) cache storage has\nbecome a major bottleneck in multi-GPU and multi-node inference. While\nMoE-based architectures sparsify computation across experts, the corresponding\nKV caches remain dense and globally synchronized, resulting in significant\noverhead.\n  We introduce \\textbf{PiKV}, a parallel and distributed KV cache serving\nframework tailored for MoE architecture. PiKV leverages \\textit{expert-sharded\nKV storage} to partition caches across GPUs, \\textit{PiKV routing} to reduce\ntoken-to-KV access, and a \\textit{PiKV Scheduling} to adaptively retain\nquery-relevant entries. To further reduce memory usage, PiKV integrates\n\\textit{PiKV Compression} modules the caching pipeline for acceleration.\n  PiKV is recently publicly available as an open-source software library:\n\\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.\nExperiments details is recorded at:\n\\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\\_Results}.\nWe also have PiKV integrated with Nvidia kvpress for acceleration, details see\n\\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.\nPiKV is still a living project, aiming to become a comprehesive KV Cache\nmanagement system for MoE Architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models continue to scale up in both size and context\nlength, the memory and communication cost of key-value (KV) cache storage has\nbecome a major bottleneck in multi-GPU and multi-node inference. While\nMoE-based architectures sparsify computation across experts, the corresponding\nKV caches remain dense and globally synchronized, resulting in significant\noverhead.\n  We introduce \\textbf{PiKV}, a parallel and distributed KV cache serving\nframework tailored for MoE architecture. PiKV leverages \\textit{expert-sharded\nKV storage} to partition caches across GPUs, \\textit{PiKV routing} to reduce\ntoken-to-KV access, and a \\textit{PiKV Scheduling} to adaptively retain\nquery-relevant entries. To further reduce memory usage, PiKV integrates\n\\textit{PiKV Compression} modules the caching pipeline for acceleration.\n  PiKV is recently publicly available as an open-source software library:\n\\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.\nExperiments details is recorded at:\n\\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\\_Results}.\nWe also have PiKV integrated with Nvidia kvpress for acceleration, details see\n\\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.\nPiKV is still a living project, aiming to become a comprehesive KV Cache\nmanagement system for MoE Architectures."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    },
                    {
                        "name": "Xuhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xuhong Wang"
                },
                "author": "Xuhong Wang",
                "arxiv_comment": "Accepted to ICML ES-MoFo III WorkShop Paper Link:\n  https://openreview.net/pdf?id=hHoK1kBPd9 Github Link:\n  https://github.com/NoakLiu/PiKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v2",
                "updated": "2025-08-02T00:31:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    0,
                    31,
                    18,
                    5,
                    214,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01051v1",
                "updated": "2025-08-01T20:08:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T20:08:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "QPP-RNG: A Conceptual Quantum System for True Randomness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPP-RNG: A Conceptual Quantum System for True Randomness"
                },
                "summary": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems."
                },
                "authors": [
                    {
                        "name": "Randy Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Randy Kuang"
                },
                "author": "Randy Kuang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00647v1",
                "updated": "2025-08-01T14:05:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:05:44Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "title": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)"
                },
                "summary": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions."
                },
                "authors": [
                    {
                        "name": "Alessandro Lacerenza"
                    },
                    {
                        "name": "Alda Rubini"
                    },
                    {
                        "name": "Andrea Alimenti"
                    },
                    {
                        "name": "Sergio Fabiani"
                    },
                    {
                        "name": "Ettore Del Monte"
                    },
                    {
                        "name": "Riccardo Campana"
                    },
                    {
                        "name": "Mauro Centrone"
                    },
                    {
                        "name": "Enrico Costa"
                    },
                    {
                        "name": "Nicolas De Angelis"
                    },
                    {
                        "name": "Giovanni De Cesare"
                    },
                    {
                        "name": "Sergio Di Cosimo"
                    },
                    {
                        "name": "Giuseppe Di Persio"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Pasqualino Loffredo"
                    },
                    {
                        "name": "Giovanni Lombardi"
                    },
                    {
                        "name": "Gabriele Minervini"
                    },
                    {
                        "name": "Fabio Muleri"
                    },
                    {
                        "name": "Paolo Romano"
                    },
                    {
                        "name": "Emanuele Scalise"
                    },
                    {
                        "name": "Enrico Silva"
                    },
                    {
                        "name": "Paolo Soffitta"
                    },
                    {
                        "name": "Davide Albanesi"
                    },
                    {
                        "name": "Ilaria Baffo"
                    },
                    {
                        "name": "Daniele Brienza"
                    },
                    {
                        "name": "Valerio Campamaggiore"
                    },
                    {
                        "name": "Giovanni Cucinella"
                    },
                    {
                        "name": "Andrea Curatolo"
                    },
                    {
                        "name": "Giulia de Iulis"
                    },
                    {
                        "name": "Andrea Del Re"
                    },
                    {
                        "name": "Vito Di Bari"
                    },
                    {
                        "name": "Simone Di Filippo"
                    },
                    {
                        "name": "Immacolata Donnarumma"
                    },
                    {
                        "name": "Pierluigi Fanelli"
                    },
                    {
                        "name": "Nicolas Gagliardi"
                    },
                    {
                        "name": "Paolo Leonetti"
                    },
                    {
                        "name": "Matteo Merge"
                    },
                    {
                        "name": "Dario Modenini"
                    },
                    {
                        "name": "Andrea Negri"
                    },
                    {
                        "name": "Daniele Pecorella"
                    },
                    {
                        "name": "Massimo Perelli"
                    },
                    {
                        "name": "Alice Ponti"
                    },
                    {
                        "name": "Francesca Sbop"
                    },
                    {
                        "name": "Paolo Tortora"
                    },
                    {
                        "name": "Alessandro Turchi"
                    },
                    {
                        "name": "Valerio Vagelli"
                    },
                    {
                        "name": "Emanuele Zaccagnino"
                    },
                    {
                        "name": "Alessandro Zambardi"
                    },
                    {
                        "name": "Costantino Zazza"
                    }
                ],
                "author_detail": {
                    "name": "Costantino Zazza"
                },
                "author": "Costantino Zazza",
                "arxiv_comment": "6 pages, 2 figures, SPIE Optics+Photonics 2025 proceeding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00629v1",
                "updated": "2025-08-01T13:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:40:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach"
                },
                "summary": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks"
                },
                "authors": [
                    {
                        "name": "Francisco Crespo"
                    },
                    {
                        "name": "Javier Villegas"
                    },
                    {
                        "name": "Carlos Baena"
                    },
                    {
                        "name": "Eduardo Baena"
                    },
                    {
                        "name": "Sergio Fortes"
                    },
                    {
                        "name": "Raquel Barco"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Barco"
                },
                "author": "Raquel Barco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00616v1",
                "updated": "2025-08-01T13:25:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:25:28Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "title": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications"
                },
                "summary": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups."
                },
                "authors": [
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Jiancheng An"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This papar has been submitted to the IEEE Global Communications\n  Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00412v1",
                "updated": "2025-08-01T08:10:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:10:54Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models."
                },
                "authors": [
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Zeyu Chen"
                    },
                    {
                        "name": "Yi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Liu"
                },
                "author": "Yi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v2",
                "updated": "2025-08-01T03:43:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    43,
                    24,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22746v2",
                "updated": "2025-08-01T03:37:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    37,
                    42,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-30T15:03:36Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    3,
                    36,
                    2,
                    211,
                    0
                ],
                "title": "Next Tokens Denoising for Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Tokens Denoising for Speech Synthesis"
                },
                "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts."
                },
                "authors": [
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Ruiqing Xue"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yufei Liu"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Yao Qian"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhao"
                },
                "author": "Sheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v2",
                "updated": "2025-07-31T21:00:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    21,
                    0,
                    28,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22701v2",
                "updated": "2025-07-31T16:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    21,
                    3,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T14:10:16Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    10,
                    16,
                    2,
                    211,
                    0
                ],
                "title": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases"
                },
                "summary": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems."
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Decheng Zuo"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Zhiyu Liang"
                    },
                    {
                        "name": "Hongzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Wang"
                },
                "author": "Hongzhi Wang",
                "arxiv_comment": "17 pages, 10 figures. An extended version of a paper under review at\n  the VLDB 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; H.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v1",
                "updated": "2025-07-31T15:50:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21433v2",
                "updated": "2025-07-31T07:53:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    53,
                    53,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-29T02:05:51Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    2,
                    5,
                    51,
                    1,
                    210,
                    0
                ],
                "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse"
                },
                "summary": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods."
                },
                "authors": [
                    {
                        "name": "Kaiwen Chen"
                    },
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Hong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xu"
                },
                "author": "Hong Xu",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01199v2",
                "updated": "2025-07-31T07:35:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    35,
                    4,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-03T05:52:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    52,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude."
                },
                "authors": [
                    {
                        "name": "Kaimin Liao"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Luchao Wang"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23292v1",
                "updated": "2025-07-31T07:10:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T07:10:39Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy"
                },
                "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers."
                },
                "authors": [
                    {
                        "name": "RJ Skerry-Ryan"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Soroosh Mariooryad"
                    },
                    {
                        "name": "David Kao"
                    },
                    {
                        "name": "Daisy Stanton"
                    },
                    {
                        "name": "Eric Battenberg"
                    },
                    {
                        "name": "Matt Shannon"
                    },
                    {
                        "name": "Ron J. Weiss"
                    },
                    {
                        "name": "Robin Scheibler"
                    },
                    {
                        "name": "Jonas Rothfuss"
                    },
                    {
                        "name": "Tom Bagby"
                    }
                ],
                "author_detail": {
                    "name": "Tom Bagby"
                },
                "author": "Tom Bagby",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v3",
                "updated": "2025-07-30T16:55:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    55,
                    33,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22801v1",
                "updated": "2025-07-30T16:04:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:04:01Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "title": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code"
                },
                "summary": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions."
                },
                "authors": [
                    {
                        "name": "Shubhradeep Roy"
                    },
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Vivek Verma"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22636v1",
                "updated": "2025-07-30T12:55:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:55:55Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "title": "All-gluon amplitudes with off-shell recursion in multiplet bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-gluon amplitudes with off-shell recursion in multiplet bases"
                },
                "summary": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes."
                },
                "authors": [
                    {
                        "name": "Oskar Bolinder"
                    },
                    {
                        "name": "Rikkert Frederix"
                    },
                    {
                        "name": "Malin Sjodahl"
                    }
                ],
                "author_detail": {
                    "name": "Malin Sjodahl"
                },
                "author": "Malin Sjodahl",
                "arxiv_comment": "15 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20984v2",
                "updated": "2025-07-30T06:29:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-28T16:45:14Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment"
                },
                "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Dongliang Wei"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Jianxiang Gao"
                    },
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Hangyu Liang"
                    },
                    {
                        "name": "Guangshuo Qin"
                    },
                    {
                        "name": "Chengrong Tian"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Longyu Zhao"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v3",
                "updated": "2025-07-30T05:24:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    5,
                    24,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "Accepted to TMLR 2025. The revised version incorporates more papers\n  and has been further polished",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05655v1",
                "updated": "2025-07-29T12:42:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    12,
                    42,
                    24,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-29T12:42:24Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    12,
                    42,
                    24,
                    1,
                    210,
                    0
                ],
                "title": "Blockchain-Based Decentralized Domain Name System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain-Based Decentralized Domain Name System"
                },
                "summary": "The current Domain Name System (DNS) infrastructure faces critical\nvulnerabilities including poisoning attacks, censorship mechanisms, and\ncentralized points of failure that compromise internet freedom and security.\nRecent incidents such as DNS poisoning attacks on ISP customers highlight the\nurgent need for resilient alternatives. This paper presents a novel\nblockchain-based Decentralized Domain Name System (DDNS). We designed a\nspecialized Proof-of-Work blockchain to maximize support for DNS-related\nprotocols and achieve node decentralization. The system integrates our\nblockchain with IPFS for distributed storage, implements cryptographic\nprimitives for end-to-end trust signatures, and achieves Never Trust, Always\nVerify zero-trust verification. Our implementation achieves 15-second domain\nrecord propagation times, supports 20 standard DNS record types, and provides\nperpetual free .ddns domains. The system has been deployed across distributed\ninfrastructure in San Jose, Los Angeles, and Orange County, demonstrating\npractical scalability and resistance to traditional DNS manipulation\ntechniques. Performance evaluation shows the system can handle up to Max Theor.\nTPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular\ntransactions) for domain operations while maintaining sub-second query\nresolution through intelligent caching mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS) infrastructure faces critical\nvulnerabilities including poisoning attacks, censorship mechanisms, and\ncentralized points of failure that compromise internet freedom and security.\nRecent incidents such as DNS poisoning attacks on ISP customers highlight the\nurgent need for resilient alternatives. This paper presents a novel\nblockchain-based Decentralized Domain Name System (DDNS). We designed a\nspecialized Proof-of-Work blockchain to maximize support for DNS-related\nprotocols and achieve node decentralization. The system integrates our\nblockchain with IPFS for distributed storage, implements cryptographic\nprimitives for end-to-end trust signatures, and achieves Never Trust, Always\nVerify zero-trust verification. Our implementation achieves 15-second domain\nrecord propagation times, supports 20 standard DNS record types, and provides\nperpetual free .ddns domains. The system has been deployed across distributed\ninfrastructure in San Jose, Los Angeles, and Orange County, demonstrating\npractical scalability and resistance to traditional DNS manipulation\ntechniques. Performance evaluation shows the system can handle up to Max Theor.\nTPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular\ntransactions) for domain operations while maintaining sub-second query\nresolution through intelligent caching mechanisms."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Peter Trinh"
                    },
                    {
                        "name": "Alma Nkemla"
                    },
                    {
                        "name": "Amuru Serikyaku"
                    },
                    {
                        "name": "Edward Tatchim"
                    },
                    {
                        "name": "Osman Sharaf"
                    }
                ],
                "author_detail": {
                    "name": "Osman Sharaf"
                },
                "author": "Osman Sharaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00904v1",
                "updated": "2025-07-29T03:08:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    3,
                    8,
                    31,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-29T03:08:31Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    3,
                    8,
                    31,
                    1,
                    210,
                    0
                ],
                "title": "Forecasting LLM Inference Performance via Hardware-Agnostic Analytical\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting LLM Inference Performance via Hardware-Agnostic Analytical\n  Modeling"
                },
                "summary": "Large language models (LLMs) have been increasingly deployed as local agents\non personal devices with CPUs, NPUs and integrated GPUs. However, forecasting\ninference performance on devices with such heterogeneity remains challenging\ndue to the dynamic compute and memory demands. Existing approaches rely on GPU\nbenchmarking or machine learning-based latency predictors, which are often\nhardware-specific and lack generalizability. To this end, we introduce LIFE, a\nlightweight and modular analytical framework that is comprised of modular\nanalytical model of operators, configurable to characterize LLM inference\nworkloads in a hardware and dataset-agnostic manner. LIFE characterizes the\ninfluence of software and model optimizations, such as quantization, KV cache\ncompression, LoRA adapters, chunked prefill, different attentions, and operator\nfusion, on performance metrics such as time-to-first-token (TTFT),\ntime-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables\nperformance forecasting using only hardware specifications, such as TOPS and\nmemory bandwidth, without requiring extensive dataset benchmarking. We validate\nLIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA\nV100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in\nforecasting LLM performance through lens of system efficiency to enable\nefficient LLM deployment across different hardware platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly deployed as local agents\non personal devices with CPUs, NPUs and integrated GPUs. However, forecasting\ninference performance on devices with such heterogeneity remains challenging\ndue to the dynamic compute and memory demands. Existing approaches rely on GPU\nbenchmarking or machine learning-based latency predictors, which are often\nhardware-specific and lack generalizability. To this end, we introduce LIFE, a\nlightweight and modular analytical framework that is comprised of modular\nanalytical model of operators, configurable to characterize LLM inference\nworkloads in a hardware and dataset-agnostic manner. LIFE characterizes the\ninfluence of software and model optimizations, such as quantization, KV cache\ncompression, LoRA adapters, chunked prefill, different attentions, and operator\nfusion, on performance metrics such as time-to-first-token (TTFT),\ntime-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables\nperformance forecasting using only hardware specifications, such as TOPS and\nmemory bandwidth, without requiring extensive dataset benchmarking. We validate\nLIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA\nV100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in\nforecasting LLM performance through lens of system efficiency to enable\nefficient LLM deployment across different hardware platforms."
                },
                "authors": [
                    {
                        "name": "Rajeev Patwari"
                    },
                    {
                        "name": "Ashish Sirasao"
                    },
                    {
                        "name": "Devleena Das"
                    }
                ],
                "author_detail": {
                    "name": "Devleena Das"
                },
                "author": "Devleena Das",
                "arxiv_comment": "10 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v2",
                "updated": "2025-07-28T20:44:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    20,
                    44,
                    23,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13349v3",
                "updated": "2025-07-28T14:11:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    11,
                    53,
                    0,
                    209,
                    0
                ],
                "published": "2023-11-22T12:34:51Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    12,
                    34,
                    51,
                    2,
                    326,
                    0
                ],
                "title": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints"
                },
                "summary": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE."
                },
                "authors": [
                    {
                        "name": "Francesco Corti"
                    },
                    {
                        "name": "Balz Maag"
                    },
                    {
                        "name": "Joachim Schauer"
                    },
                    {
                        "name": "Ulrich Pferschy"
                    },
                    {
                        "name": "Olga Saukh"
                    }
                ],
                "author_detail": {
                    "name": "Olga Saukh"
                },
                "author": "Olga Saukh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20677v1",
                "updated": "2025-07-28T09:59:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:59:22Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "title": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing"
                },
                "summary": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits."
                },
                "authors": [
                    {
                        "name": "Ioana Moflic"
                    },
                    {
                        "name": "Alan Robertson"
                    },
                    {
                        "name": "Simon J. Devitt"
                    },
                    {
                        "name": "Alexandru Paler"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Paler"
                },
                "author": "Alexandru Paler",
                "arxiv_comment": "accepted at Q-CORE workshop of the QCE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20613v1",
                "updated": "2025-07-28T08:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:27:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression"
                },
                "summary": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance."
                },
                "authors": [
                    {
                        "name": "Te Zhang"
                    },
                    {
                        "name": "Yuheng Li"
                    },
                    {
                        "name": "Junxiang Wang"
                    },
                    {
                        "name": "Lujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Lujun Li"
                },
                "author": "Lujun Li",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v2",
                "updated": "2025-07-28T04:25:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    4,
                    25,
                    58,
                    0,
                    209,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Learning-Augmented Online Caching: New Upper Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Augmented Online Caching: New Upper Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v2",
                "updated": "2025-07-27T09:58:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    9,
                    58,
                    25,
                    6,
                    208,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20173v1",
                "updated": "2025-07-27T08:25:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T08:25:08Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "title": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP"
                },
                "summary": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization."
                },
                "authors": [
                    {
                        "name": "Haitian Wang"
                    },
                    {
                        "name": "Long Qin"
                    }
                ],
                "author_detail": {
                    "name": "Long Qin"
                },
                "author": "Long Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20116v1",
                "updated": "2025-07-27T03:45:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T03:45:07Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "title": "Accelerating Containerized Service Delivery at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Containerized Service Delivery at the Network Edge"
                },
                "summary": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions."
                },
                "authors": [
                    {
                        "name": "Yinuo Deng"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Dongjing Wang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Wenzhuo Qian"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Schahram Dustdar"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v3",
                "updated": "2025-07-27T00:40:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    0,
                    40,
                    47,
                    6,
                    208,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20030v1",
                "updated": "2025-07-26T18:20:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T18:20:25Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "title": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression"
                },
                "summary": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches."
                },
                "authors": [
                    {
                        "name": "Runchao Li"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Mu Sheng"
                    },
                    {
                        "name": "Xianxuan Long"
                    },
                    {
                        "name": "Haotian Yu"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v2",
                "updated": "2025-07-26T15:25:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    25,
                    22,
                    5,
                    207,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v3",
                "updated": "2025-07-26T15:13:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    13,
                    56,
                    5,
                    207,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages; Accepted at the 42nd International Conference on Machine\n  Learning (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v2",
                "updated": "2025-07-26T13:33:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    13,
                    33,
                    6,
                    5,
                    207,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19823v1",
                "updated": "2025-07-26T06:43:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T06:43:14Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs"
                },
                "summary": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory."
                },
                "authors": [
                    {
                        "name": "Dongquan Yang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xiaotian Yu"
                    },
                    {
                        "name": "Xianbiao Qi"
                    },
                    {
                        "name": "Rong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Rong Xiao"
                },
                "author": "Rong Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19427v1",
                "updated": "2025-07-25T16:53:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:53:13Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "title": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding"
                },
                "summary": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding."
                },
                "authors": [
                    {
                        "name": "StepFun"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bojun Wang"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Haonan Jia"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Mingliang Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Wuxun Xie"
                    },
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Xing Chen"
                    },
                    {
                        "name": "Xingping Yang"
                    },
                    {
                        "name": "Xuelin Zhang"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yaoyu Wang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Ailin Huang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Changxin Miao"
                    },
                    {
                        "name": "Chang Lou"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Chenfeng Yu"
                    },
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Daokuan Lv"
                    },
                    {
                        "name": "Dapeng Shi"
                    },
                    {
                        "name": "Deshan Sun"
                    },
                    {
                        "name": "Ding Huang"
                    },
                    {
                        "name": "Dingyuan Hu"
                    },
                    {
                        "name": "Dongqing Pang"
                    },
                    {
                        "name": "Enle Liu"
                    },
                    {
                        "name": "Fajie Zhang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Gulin Yan"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Hanghao Wu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Haocheng Zhang"
                    },
                    {
                        "name": "Haolong Yan"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Hebin Zhou"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongxin Li"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Huiyong Guo"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Jiahao Gong"
                    },
                    {
                        "name": "Jialing Xie"
                    },
                    {
                        "name": "Jian Zhou"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Jiaoren Wu"
                    },
                    {
                        "name": "Jiaran Zhang"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Jie Yan"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jieyi Hou"
                    },
                    {
                        "name": "Jinguang Zhang"
                    },
                    {
                        "name": "Jinlan Cao"
                    },
                    {
                        "name": "Jisheng Yin"
                    },
                    {
                        "name": "Junfeng Liu"
                    },
                    {
                        "name": "Junhao Huang"
                    },
                    {
                        "name": "Junzhe Lin"
                    },
                    {
                        "name": "Kaijun Tan"
                    },
                    {
                        "name": "Kaixiang Li"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Kangheng Lin"
                    },
                    {
                        "name": "Kenkun Liu"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Lieyu Shi"
                    },
                    {
                        "name": "Liguo Tan"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Liwen Huang"
                    },
                    {
                        "name": "Liying Shi"
                    },
                    {
                        "name": "Longlong Gu"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Mengqiang Ren"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Nan Wu"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Qianni Liu"
                    },
                    {
                        "name": "Qiaohui Chen"
                    },
                    {
                        "name": "Qiling Wu"
                    },
                    {
                        "name": "Qinglin He"
                    },
                    {
                        "name": "Qinyuan Tan"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Qiuping Wu"
                    },
                    {
                        "name": "Qiuyan Liang"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Ruosi Wan"
                    },
                    {
                        "name": "Ruyan Guo"
                    },
                    {
                        "name": "Shangwu Zhong"
                    },
                    {
                        "name": "Shaoliang Pang"
                    },
                    {
                        "name": "Shengjie Fan"
                    },
                    {
                        "name": "Shijie Shang"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Shiliang Yang"
                    },
                    {
                        "name": "Shiming Hao"
                    },
                    {
                        "name": "Shuli Gao"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Tiancheng Cao"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Wenjin Deng"
                    },
                    {
                        "name": "Wenqing He"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiangwen Kong"
                    },
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Xiaobo Yang"
                    },
                    {
                        "name": "Xiaojia Liu"
                    },
                    {
                        "name": "Xiaoxiao Ren"
                    },
                    {
                        "name": "Xin Han"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Yanan Wei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Yangshijie Xu"
                    },
                    {
                        "name": "Yanming Xu"
                    },
                    {
                        "name": "Yaqiang Shi"
                    },
                    {
                        "name": "Yeqing Shen"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yihan Chen"
                    },
                    {
                        "name": "Yijing Yang"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Yizhuang Zhou"
                    },
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Yuanzhen Yang"
                    },
                    {
                        "name": "Yuchu Luo"
                    },
                    {
                        "name": "Yue Peng"
                    },
                    {
                        "name": "Yufan Lu"
                    },
                    {
                        "name": "Yuhang Deng"
                    },
                    {
                        "name": "Yuhe Yin"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Yuling Zhao"
                    },
                    {
                        "name": "Yun Mou"
                    },
                    {
                        "name": "Yunlong Li"
                    },
                    {
                        "name": "Yunzhou Ju"
                    },
                    {
                        "name": "Yusheng Li"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Yuyang Chen"
                    },
                    {
                        "name": "Zejia Weng"
                    },
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhichao Chang"
                    },
                    {
                        "name": "Zhiguo Huang"
                    },
                    {
                        "name": "Zhirui Wang"
                    },
                    {
                        "name": "Zidong Yang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19367v1",
                "updated": "2025-07-25T15:17:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:17:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "Empowering IoT Firmware Secure Update with Customization Rights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering IoT Firmware Secure Update with Customization Rights"
                },
                "summary": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline."
                },
                "authors": [
                    {
                        "name": "Weihao Chen"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Boyu Kuang"
                    },
                    {
                        "name": "Jin B. Hong"
                    },
                    {
                        "name": "Yuqing Zhang"
                    },
                    {
                        "name": "Anmin Fu"
                    }
                ],
                "author_detail": {
                    "name": "Anmin Fu"
                },
                "author": "Anmin Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v2",
                "updated": "2025-07-24T19:44:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    19,
                    44,
                    36,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "21 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v2",
                "updated": "2025-07-24T17:30:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    30,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v2",
                "updated": "2025-07-24T16:25:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    25,
                    51,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18446v1",
                "updated": "2025-07-24T14:30:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:30:48Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "title": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering"
                },
                "summary": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing."
                },
                "authors": [
                    {
                        "name": "Ivan Medennikov"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Kunal Dhawan"
                    },
                    {
                        "name": "Jinhan Wang"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18028v1",
                "updated": "2025-07-24T02:00:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T02:00:09Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database"
                },
                "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work)."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Hao Shi"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Jingchen Peng"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Jingzhao Zhang"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Zhenyuan Chen"
                    },
                    {
                        "name": "Xueyan Niu"
                    }
                ],
                "author_detail": {
                    "name": "Xueyan Niu"
                },
                "author": "Xueyan Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17744v1",
                "updated": "2025-07-23T17:57:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T17:57:09Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "title": "Yume: An Interactive World Generation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume: An Interactive World Generation Model"
                },
                "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Shaoheng Lin"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Chuanhao Li"
                    },
                    {
                        "name": "Wenshuo Peng"
                    },
                    {
                        "name": "Tong He"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Mingmin Chi"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaipeng Zhang"
                },
                "author": "Kaipeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.08248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08248v1",
                "updated": "2025-08-11T17:58:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    58,
                    24,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:58:24Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    58,
                    24,
                    0,
                    223,
                    0
                ],
                "title": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation"
                },
                "summary": "Current diffusion models for audio-driven avatar video generation struggle to\nsynthesize long videos with natural audio synchronization and identity\nconsistency. This paper presents StableAvatar, the first end-to-end video\ndiffusion transformer that synthesizes infinite-length high-quality videos\nwithout post-processing. Conditioned on a reference image and audio,\nStableAvatar integrates tailored training and inference modules to enable\ninfinite-length video generation. We observe that the main reason preventing\nexisting models from generating long videos lies in their audio modeling. They\ntypically rely on third-party off-the-shelf extractors to obtain audio\nembeddings, which are then directly injected into the diffusion model via\ncross-attention. Since current diffusion backbones lack any audio-related\npriors, this approach causes severe latent distribution error accumulation\nacross video clips, leading the latent distribution of subsequent segments to\ndrift away from the optimal distribution gradually. To address this,\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\nerror accumulation via time-step-aware modulation. During inference, we propose\na novel Audio Native Guidance Mechanism to further enhance the audio\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\nprediction as a dynamic guidance signal. To enhance the smoothness of the\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\nof StableAvatar both qualitatively and quantitatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current diffusion models for audio-driven avatar video generation struggle to\nsynthesize long videos with natural audio synchronization and identity\nconsistency. This paper presents StableAvatar, the first end-to-end video\ndiffusion transformer that synthesizes infinite-length high-quality videos\nwithout post-processing. Conditioned on a reference image and audio,\nStableAvatar integrates tailored training and inference modules to enable\ninfinite-length video generation. We observe that the main reason preventing\nexisting models from generating long videos lies in their audio modeling. They\ntypically rely on third-party off-the-shelf extractors to obtain audio\nembeddings, which are then directly injected into the diffusion model via\ncross-attention. Since current diffusion backbones lack any audio-related\npriors, this approach causes severe latent distribution error accumulation\nacross video clips, leading the latent distribution of subsequent segments to\ndrift away from the optimal distribution gradually. To address this,\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\nerror accumulation via time-step-aware modulation. During inference, we propose\na novel Audio Native Guidance Mechanism to further enhance the audio\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\nprediction as a dynamic guidance signal. To enhance the smoothness of the\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\nof StableAvatar both qualitatively and quantitatively."
                },
                "authors": [
                    {
                        "name": "Shuyuan Tu"
                    },
                    {
                        "name": "Yueming Pan"
                    },
                    {
                        "name": "Yinming Huang"
                    },
                    {
                        "name": "Xintong Han"
                    },
                    {
                        "name": "Zhen Xing"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08243v2",
                "updated": "2025-08-12T14:19:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    19,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T17:56:06Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    56,
                    6,
                    0,
                    223,
                    0
                ],
                "title": "Jinx: Unlimited LLMs for Probing Alignment Failures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jinx: Unlimited LLMs for Probing Alignment Failures"
                },
                "summary": "Unlimited, or so-called helpful-only language models are trained without\nsafety alignment constraints and never refuse user queries. They are widely\nused by leading AI companies as internal tools for red teaming and alignment\nevaluation. For example, if a safety-aligned model produces harmful outputs\nsimilar to an unlimited model, this indicates alignment failures that require\nfurther attention. Despite their essential role in assessing alignment, such\nmodels are not available to the research community.\n  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx\nresponds to all queries without refusals or safety filtering, while preserving\nthe base model's capabilities in reasoning and instruction following. It\nprovides researchers with an accessible tool for probing alignment failures,\nevaluating safety boundaries, and systematically studying failure modes in\nlanguage model safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlimited, or so-called helpful-only language models are trained without\nsafety alignment constraints and never refuse user queries. They are widely\nused by leading AI companies as internal tools for red teaming and alignment\nevaluation. For example, if a safety-aligned model produces harmful outputs\nsimilar to an unlimited model, this indicates alignment failures that require\nfurther attention. Despite their essential role in assessing alignment, such\nmodels are not available to the research community.\n  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx\nresponds to all queries without refusals or safety filtering, while preserving\nthe base model's capabilities in reasoning and instruction following. It\nprovides researchers with an accessible tool for probing alignment failures,\nevaluating safety boundaries, and systematically studying failure modes in\nlanguage model safety."
                },
                "authors": [
                    {
                        "name": "Jiahao Zhao"
                    },
                    {
                        "name": "Liwei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Dong"
                },
                "author": "Liwei Dong",
                "arxiv_comment": "https://huggingface.co/Jinx-org",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08242v1",
                "updated": "2025-08-11T17:55:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    55,
                    40,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:55:40Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    55,
                    40,
                    0,
                    223,
                    0
                ],
                "title": "Bringing Everyone to the Table: An Experimental Study of LLM-Facilitated\n  Group Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bringing Everyone to the Table: An Experimental Study of LLM-Facilitated\n  Group Decision Making"
                },
                "summary": "Group decision-making often suffers from uneven information sharing,\nhindering decision quality. While large language models (LLMs) have been widely\nstudied as aids for individuals, their potential to support groups of users,\npotentially as facilitators, is relatively underexplored. We present a\npre-registered randomized experiment with 1,475 participants assigned to 281\nfive-person groups completing a hidden profile task--selecting an optimal city\nfor a hypothetical sporting event--under one of four facilitation conditions:\nno facilitation, a one-time message prompting information sharing, a human\nfacilitator, or an LLM (GPT-4o) facilitator. We find that LLM facilitation\nincreases information shared within a discussion by raising the minimum level\nof engagement with the task among group members, and that these gains come at\nlimited cost in terms of participants' attitudes towards the task, their group,\nor their facilitator. Whether by human or AI, there is no significant effect of\nfacilitation on the final decision outcome, suggesting that even substantial\nbut partial increases in information sharing are insufficient to overcome the\nhidden profile effect studied. To support further research into how LLM-based\ninterfaces can support the future of collaborative decision making, we release\nour experimental platform, the Group-AI Interaction Laboratory (GRAIL), as an\nopen-source tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group decision-making often suffers from uneven information sharing,\nhindering decision quality. While large language models (LLMs) have been widely\nstudied as aids for individuals, their potential to support groups of users,\npotentially as facilitators, is relatively underexplored. We present a\npre-registered randomized experiment with 1,475 participants assigned to 281\nfive-person groups completing a hidden profile task--selecting an optimal city\nfor a hypothetical sporting event--under one of four facilitation conditions:\nno facilitation, a one-time message prompting information sharing, a human\nfacilitator, or an LLM (GPT-4o) facilitator. We find that LLM facilitation\nincreases information shared within a discussion by raising the minimum level\nof engagement with the task among group members, and that these gains come at\nlimited cost in terms of participants' attitudes towards the task, their group,\nor their facilitator. Whether by human or AI, there is no significant effect of\nfacilitation on the final decision outcome, suggesting that even substantial\nbut partial increases in information sharing are insufficient to overcome the\nhidden profile effect studied. To support further research into how LLM-based\ninterfaces can support the future of collaborative decision making, we release\nour experimental platform, the Group-AI Interaction Laboratory (GRAIL), as an\nopen-source tool."
                },
                "authors": [
                    {
                        "name": "Mohammed Alsobay"
                    },
                    {
                        "name": "David M. Rothschild"
                    },
                    {
                        "name": "Jake M. Hofman"
                    },
                    {
                        "name": "Daniel G. Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Daniel G. Goldstein"
                },
                "author": "Daniel G. Goldstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08236v1",
                "updated": "2025-08-11T17:52:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    52,
                    7,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:52:07Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    52,
                    7,
                    0,
                    223,
                    0
                ],
                "title": "Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health\n  Dialogues via LLM-as-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health\n  Dialogues via LLM-as-Judge"
                },
                "summary": "Evaluating the safety alignment of LLM responses in high-risk mental health\ndialogues is particularly difficult due to missing gold-standard answers and\nthe ethically sensitive nature of these interactions. To address this\nchallenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark\nbased on real-world Chinese mental health dialogues. It evaluates whether the\nmodel responses align with the safety principles defined by experts.\nSpecifically designed for settings without standard references, our method\nadopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation\nusing expert-defined reasoning chains grounded in psychological intervention\nprinciples. We employ binary point-wise scoring across multiple safety\ndimensions to enhance the explainability and traceability of the evaluation.\nAdditionally, we present a manually curated, high-quality Chinese-language\ndataset covering self-harm, suicidal ideation, and existential distress,\nderived from real-world online discourse. Experiments on 3600 judgments show\nthat our method achieves the highest agreement with expert assessments and\nproduces more interpretable evaluation rationales compared to existing\napproaches. Our dataset and evaluation tool are publicly available to\nfacilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the safety alignment of LLM responses in high-risk mental health\ndialogues is particularly difficult due to missing gold-standard answers and\nthe ethically sensitive nature of these interactions. To address this\nchallenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark\nbased on real-world Chinese mental health dialogues. It evaluates whether the\nmodel responses align with the safety principles defined by experts.\nSpecifically designed for settings without standard references, our method\nadopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation\nusing expert-defined reasoning chains grounded in psychological intervention\nprinciples. We employ binary point-wise scoring across multiple safety\ndimensions to enhance the explainability and traceability of the evaluation.\nAdditionally, we present a manually curated, high-quality Chinese-language\ndataset covering self-harm, suicidal ideation, and existential distress,\nderived from real-world online discourse. Experiments on 3600 judgments show\nthat our method achieves the highest agreement with expert assessments and\nproduces more interpretable evaluation rationales compared to existing\napproaches. Our dataset and evaluation tool are publicly available to\nfacilitate further research."
                },
                "authors": [
                    {
                        "name": "Yunna Cai"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Haowei Wang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    },
                    {
                        "name": "Moyan Li"
                    },
                    {
                        "name": "Mingming Fan"
                    }
                ],
                "author_detail": {
                    "name": "Mingming Fan"
                },
                "author": "Mingming Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08228v1",
                "updated": "2025-08-11T17:48:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    48,
                    2,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:48:02Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    48,
                    2,
                    0,
                    223,
                    0
                ],
                "title": "LL3M: Large Language 3D Modelers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LL3M: Large Language 3D Modelers"
                },
                "summary": "We present LL3M, a multi-agent system that leverages pretrained large\nlanguage models (LLMs) to generate 3D assets by writing interpretable Python\ncode in Blender. We break away from the typical generative approach that learns\nfrom a collection of 3D data. Instead, we reformulate shape generation as a\ncode-writing task, enabling greater modularity, editability, and integration\nwith artist workflows. Given a text prompt, LL3M coordinates a team of\nspecialized LLM agents to plan, retrieve, write, debug, and refine Blender\nscripts that generate and edit geometry and appearance. The generated code\nworks as a high-level, interpretable, human-readable, well-documented\nrepresentation of scenes and objects, making full use of sophisticated Blender\nconstructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,\nunconstrained shapes, materials, and scenes. This code presents many avenues\nfor further agent and human editing and experimentation via code tweaks or\nprocedural parameters. This medium naturally enables a co-creative loop in our\nsystem: agents can automatically self-critique using code and visuals, while\niterative user instructions provide an intuitive way to refine assets. A shared\ncode context across agents enables awareness of previous attempts, and a\nretrieval-augmented generation knowledge base built from Blender API\ndocumentation, BlenderRAG, equips agents with examples, types, and functions\nempowering advanced modeling operations and code correctness. We demonstrate\nthe effectiveness of LL3M across diverse shape categories, style and material\nedits, and user-driven refinements. Our experiments showcase the power of code\nas a generative and interpretable medium for 3D asset creation. Our project\npage is at https://threedle.github.io/ll3m.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LL3M, a multi-agent system that leverages pretrained large\nlanguage models (LLMs) to generate 3D assets by writing interpretable Python\ncode in Blender. We break away from the typical generative approach that learns\nfrom a collection of 3D data. Instead, we reformulate shape generation as a\ncode-writing task, enabling greater modularity, editability, and integration\nwith artist workflows. Given a text prompt, LL3M coordinates a team of\nspecialized LLM agents to plan, retrieve, write, debug, and refine Blender\nscripts that generate and edit geometry and appearance. The generated code\nworks as a high-level, interpretable, human-readable, well-documented\nrepresentation of scenes and objects, making full use of sophisticated Blender\nconstructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,\nunconstrained shapes, materials, and scenes. This code presents many avenues\nfor further agent and human editing and experimentation via code tweaks or\nprocedural parameters. This medium naturally enables a co-creative loop in our\nsystem: agents can automatically self-critique using code and visuals, while\niterative user instructions provide an intuitive way to refine assets. A shared\ncode context across agents enables awareness of previous attempts, and a\nretrieval-augmented generation knowledge base built from Blender API\ndocumentation, BlenderRAG, equips agents with examples, types, and functions\nempowering advanced modeling operations and code correctness. We demonstrate\nthe effectiveness of LL3M across diverse shape categories, style and material\nedits, and user-driven refinements. Our experiments showcase the power of code\nas a generative and interpretable medium for 3D asset creation. Our project\npage is at https://threedle.github.io/ll3m."
                },
                "authors": [
                    {
                        "name": "Sining Lu"
                    },
                    {
                        "name": "Guan Chen"
                    },
                    {
                        "name": "Nam Anh Dinh"
                    },
                    {
                        "name": "Itai Lang"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Rana Hanocka"
                    }
                ],
                "author_detail": {
                    "name": "Rana Hanocka"
                },
                "author": "Rana Hanocka",
                "arxiv_comment": "Our project page is at https://threedle.github.io/ll3m",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08224v1",
                "updated": "2025-08-11T17:43:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    43,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:43:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    43,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "Capabilities of GPT-5 on Multimodal Medical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capabilities of GPT-5 on Multimodal Medical Reasoning"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled general-purpose\nsystems to perform increasingly complex domain-specific reasoning without\nextensive fine-tuning. In the medical domain, decision-making often requires\nintegrating heterogeneous information sources, including patient narratives,\nstructured data, and medical images. This study positions GPT-5 as a generalist\nmultimodal reasoner for medical decision support and systematically evaluates\nits zero-shot chain-of-thought reasoning performance on both text-based\nquestion answering and visual question answering tasks under a unified\nprotocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20\nagainst standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU\nmedical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that\nGPT-5 consistently outperforms all baselines, achieving state-of-the-art\naccuracy across all QA benchmarks and delivering substantial gains in\nmultimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and\nunderstanding scores by +29.62% and +36.18% over GPT-4o, respectively, and\nsurpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in\nunderstanding. In contrast, GPT-4o remains below human expert performance in\nmost dimensions. A representative case study demonstrates GPT-5's ability to\nintegrate visual and textual cues into a coherent diagnostic reasoning chain,\nrecommending appropriate high-stakes interventions. Our results show that, on\nthese controlled multimodal reasoning benchmarks, GPT-5 moves from\nhuman-comparable to above human-expert performance. This improvement may\nsubstantially inform the design of future clinical decision-support systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled general-purpose\nsystems to perform increasingly complex domain-specific reasoning without\nextensive fine-tuning. In the medical domain, decision-making often requires\nintegrating heterogeneous information sources, including patient narratives,\nstructured data, and medical images. This study positions GPT-5 as a generalist\nmultimodal reasoner for medical decision support and systematically evaluates\nits zero-shot chain-of-thought reasoning performance on both text-based\nquestion answering and visual question answering tasks under a unified\nprotocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20\nagainst standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU\nmedical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that\nGPT-5 consistently outperforms all baselines, achieving state-of-the-art\naccuracy across all QA benchmarks and delivering substantial gains in\nmultimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and\nunderstanding scores by +29.62% and +36.18% over GPT-4o, respectively, and\nsurpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in\nunderstanding. In contrast, GPT-4o remains below human expert performance in\nmost dimensions. A representative case study demonstrates GPT-5's ability to\nintegrate visual and textual cues into a coherent diagnostic reasoning chain,\nrecommending appropriate high-stakes interventions. Our results show that, on\nthese controlled multimodal reasoning benchmarks, GPT-5 moves from\nhuman-comparable to above human-expert performance. This improvement may\nsubstantially inform the design of future clinical decision-support systems."
                },
                "authors": [
                    {
                        "name": "Shansong Wang"
                    },
                    {
                        "name": "Mingzhe Hu"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Mojtaba Safari"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Yang"
                },
                "author": "Xiaofeng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08221v1",
                "updated": "2025-08-11T17:39:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    39,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:39:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    39,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning"
                },
                "summary": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO."
                },
                "authors": [
                    {
                        "name": "Zihe Liu"
                    },
                    {
                        "name": "Jiashun Liu"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ling Pan"
                    },
                    {
                        "name": "Xinyu Hu"
                    },
                    {
                        "name": "Shaopan Xiong"
                    },
                    {
                        "name": "Ju Huang"
                    },
                    {
                        "name": "Jian Hu"
                    },
                    {
                        "name": "Shengyi Huang"
                    },
                    {
                        "name": "Siran Yang"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "26 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08219v1",
                "updated": "2025-08-11T17:38:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    38,
                    50,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:38:50Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    38,
                    50,
                    0,
                    223,
                    0
                ],
                "title": "SAGOnline: Segment Any Gaussians Online",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGOnline: Segment Any Gaussians Online"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit\n3D scene representation, yet achieving efficient and consistent 3D segmentation\nremains challenging. Current methods suffer from prohibitive computational\ncosts, limited 3D spatial reasoning, and an inability to track multiple objects\nsimultaneously. We present Segment Any Gaussians Online (SAGOnline), a\nlightweight and zero-shot framework for real-time 3D segmentation in Gaussian\nscenes that addresses these limitations through two key innovations: (1) a\ndecoupled strategy that integrates video foundation models (e.g., SAM2) for\nview-consistent 2D mask propagation across synthesized views; and (2) a\nGPU-accelerated 3D mask generation and Gaussian-level instance labeling\nalgorithm that assigns unique identifiers to 3D primitives, enabling lossless\nmulti-object tracking and segmentation across views. SAGOnline achieves\nstate-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)\nbenchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times\nin inference speed (27 ms/frame). Qualitative results demonstrate robust\nmulti-object segmentation and tracking in complex scenes. Our contributions\ninclude: (i) a lightweight and zero-shot framework for 3D segmentation in\nGaussian scenes, (ii) explicit labeling of Gaussian primitives enabling\nsimultaneous segmentation and tracking, and (iii) the effective adaptation of\n2D video foundation models to the 3D domain. This work allows real-time\nrendering and 3D scene understanding, paving the way for practical AR/VR and\nrobotic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit\n3D scene representation, yet achieving efficient and consistent 3D segmentation\nremains challenging. Current methods suffer from prohibitive computational\ncosts, limited 3D spatial reasoning, and an inability to track multiple objects\nsimultaneously. We present Segment Any Gaussians Online (SAGOnline), a\nlightweight and zero-shot framework for real-time 3D segmentation in Gaussian\nscenes that addresses these limitations through two key innovations: (1) a\ndecoupled strategy that integrates video foundation models (e.g., SAM2) for\nview-consistent 2D mask propagation across synthesized views; and (2) a\nGPU-accelerated 3D mask generation and Gaussian-level instance labeling\nalgorithm that assigns unique identifiers to 3D primitives, enabling lossless\nmulti-object tracking and segmentation across views. SAGOnline achieves\nstate-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)\nbenchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times\nin inference speed (27 ms/frame). Qualitative results demonstrate robust\nmulti-object segmentation and tracking in complex scenes. Our contributions\ninclude: (i) a lightweight and zero-shot framework for 3D segmentation in\nGaussian scenes, (ii) explicit labeling of Gaussian primitives enabling\nsimultaneous segmentation and tracking, and (iii) the effective adaptation of\n2D video foundation models to the 3D domain. This work allows real-time\nrendering and 3D scene understanding, paving the way for practical AR/VR and\nrobotic applications."
                },
                "authors": [
                    {
                        "name": "Wentao Sun"
                    },
                    {
                        "name": "Quanyun Wu"
                    },
                    {
                        "name": "Hanqing Xu"
                    },
                    {
                        "name": "Kyle Gao"
                    },
                    {
                        "name": "Zhengsen Xu"
                    },
                    {
                        "name": "Yiping Chen"
                    },
                    {
                        "name": "Dedong Zhang"
                    },
                    {
                        "name": "Lingfei Ma"
                    },
                    {
                        "name": "John S. Zelek"
                    },
                    {
                        "name": "Jonathan Li"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Li"
                },
                "author": "Jonathan Li",
                "arxiv_comment": "19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09373v2",
                "updated": "2025-08-11T17:38:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    38,
                    47,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-12T23:46:09Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    23,
                    46,
                    9,
                    5,
                    102,
                    0
                ],
                "title": "QUDsim: Quantifying Discourse Similarities in LLM-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUDsim: Quantifying Discourse Similarities in LLM-Generated Text"
                },
                "summary": "As large language models become increasingly capable at various writing\ntasks, their weakness at generating unique and creative content becomes a major\nliability. Although LLMs have the ability to generate text covering diverse\ntopics, there is an overall sense of repetitiveness across texts that we aim to\nformalize and quantify via a similarity metric. The familiarity between\ndocuments arises from the persistence of underlying discourse structures.\nHowever, existing similarity metrics dependent on lexical overlap and syntactic\npatterns largely capture $\\textit{content}$ overlap, thus making them\nunsuitable for detecting $\\textit{structural}$ similarities. We introduce an\nabstraction based on linguistic theories in Questions Under Discussion (QUD)\nand question semantics to help quantify differences in discourse progression.\nWe then use this framework to build $\\textbf{QUDsim}$, a similarity metric that\ncan detect discursive parallels between documents. Using QUDsim, we find that\nLLMs often reuse discourse structures (more so than humans) across samples,\neven when content differs. Furthermore, LLMs are not only repetitive and\nstructurally uniform, but are also divergent from human authors in the types of\nstructures they use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models become increasingly capable at various writing\ntasks, their weakness at generating unique and creative content becomes a major\nliability. Although LLMs have the ability to generate text covering diverse\ntopics, there is an overall sense of repetitiveness across texts that we aim to\nformalize and quantify via a similarity metric. The familiarity between\ndocuments arises from the persistence of underlying discourse structures.\nHowever, existing similarity metrics dependent on lexical overlap and syntactic\npatterns largely capture $\\textit{content}$ overlap, thus making them\nunsuitable for detecting $\\textit{structural}$ similarities. We introduce an\nabstraction based on linguistic theories in Questions Under Discussion (QUD)\nand question semantics to help quantify differences in discourse progression.\nWe then use this framework to build $\\textbf{QUDsim}$, a similarity metric that\ncan detect discursive parallels between documents. Using QUDsim, we find that\nLLMs often reuse discourse structures (more so than humans) across samples,\neven when content differs. Furthermore, LLMs are not only repetitive and\nstructurally uniform, but are also divergent from human authors in the types of\nstructures they use."
                },
                "authors": [
                    {
                        "name": "Ramya Namuduri"
                    },
                    {
                        "name": "Yating Wu"
                    },
                    {
                        "name": "Anshun Asher Zheng"
                    },
                    {
                        "name": "Manya Wadhwa"
                    },
                    {
                        "name": "Greg Durrett"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    }
                ],
                "author_detail": {
                    "name": "Junyi Jessy Li"
                },
                "author": "Junyi Jessy Li",
                "arxiv_comment": "COLM 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08211v1",
                "updated": "2025-08-11T17:33:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    33,
                    18,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:33:18Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    33,
                    18,
                    0,
                    223,
                    0
                ],
                "title": "SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling"
                },
                "summary": "Watermarking LLM-generated text is critical for content attribution and\nmisinformation prevention. However, existing methods compromise text quality,\nrequire white-box model access and logit manipulation. These limitations\nexclude API-based models and multilingual scenarios. We propose SAEMark, a\ngeneral framework for post-hoc multi-bit watermarking that embeds personalized\nmessages solely via inference-time, feature-based rejection sampling without\naltering model logits or requiring training. Our approach operates on\ndeterministic features extracted from generated text, selecting outputs whose\nfeature statistics align with key-derived targets. This framework naturally\ngeneralizes across languages and domains while preserving text quality through\nsampling LLM outputs instead of modifying. We provide theoretical guarantees\nrelating watermark success probability and compute budget that hold for any\nsuitable feature extractor. Empirically, we demonstrate the framework's\neffectiveness using Sparse Autoencoders (SAEs), achieving superior detection\naccuracy and text quality. Experiments across 4 datasets show SAEMark's\nconsistent performance, with 99.7% F1 on English and strong multi-bit detection\naccuracy. SAEMark establishes a new paradigm for scalable watermarking that\nworks out-of-the-box with closed-source LLMs while enabling content\nattribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking LLM-generated text is critical for content attribution and\nmisinformation prevention. However, existing methods compromise text quality,\nrequire white-box model access and logit manipulation. These limitations\nexclude API-based models and multilingual scenarios. We propose SAEMark, a\ngeneral framework for post-hoc multi-bit watermarking that embeds personalized\nmessages solely via inference-time, feature-based rejection sampling without\naltering model logits or requiring training. Our approach operates on\ndeterministic features extracted from generated text, selecting outputs whose\nfeature statistics align with key-derived targets. This framework naturally\ngeneralizes across languages and domains while preserving text quality through\nsampling LLM outputs instead of modifying. We provide theoretical guarantees\nrelating watermark success probability and compute budget that hold for any\nsuitable feature extractor. Empirically, we demonstrate the framework's\neffectiveness using Sparse Autoencoders (SAEs), achieving superior detection\naccuracy and text quality. Experiments across 4 datasets show SAEMark's\nconsistent performance, with 99.7% F1 on English and strong multi-bit detection\naccuracy. SAEMark establishes a new paradigm for scalable watermarking that\nworks out-of-the-box with closed-source LLMs while enabling content\nattribution."
                },
                "authors": [
                    {
                        "name": "Zhuohao Yu"
                    },
                    {
                        "name": "Xingru Jiang"
                    },
                    {
                        "name": "Weizheng Gu"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Shikun Zhang"
                    },
                    {
                        "name": "Wei Ye"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ye"
                },
                "author": "Wei Ye",
                "arxiv_comment": "24 pages, 12 figures, code available:\n  https://zhuohaoyu.github.io/SAEMark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08204v1",
                "updated": "2025-08-11T17:22:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    22,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:22:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    22,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "Human-Alignment and Calibration of Inference-Time Uncertainty in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Alignment and Calibration of Inference-Time Uncertainty in Large\n  Language Models"
                },
                "summary": "There has been much recent interest in evaluating large language models for\nuncertainty calibration to facilitate model control and modulate user trust.\nInference time uncertainty, which may provide a real-time signal to the model\nor external control modules, is particularly important for applying these\nconcepts to improve LLM-user experience in practice. While many of the existing\npapers consider model calibration, comparatively little work has sought to\nevaluate how closely model uncertainty aligns to human uncertainty. In this\nwork, we evaluate a collection of inference-time uncertainty measures, using\nboth established metrics and novel variations, to determine how closely they\nalign with both human group-level uncertainty and traditional notions of model\ncalibration. We find that numerous measures show evidence of strong alignment\nto human uncertainty, even despite the lack of alignment to human answer\npreference. For those successful metrics, we find moderate to strong evidence\nof model calibration in terms of both correctness correlation and\ndistributional analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been much recent interest in evaluating large language models for\nuncertainty calibration to facilitate model control and modulate user trust.\nInference time uncertainty, which may provide a real-time signal to the model\nor external control modules, is particularly important for applying these\nconcepts to improve LLM-user experience in practice. While many of the existing\npapers consider model calibration, comparatively little work has sought to\nevaluate how closely model uncertainty aligns to human uncertainty. In this\nwork, we evaluate a collection of inference-time uncertainty measures, using\nboth established metrics and novel variations, to determine how closely they\nalign with both human group-level uncertainty and traditional notions of model\ncalibration. We find that numerous measures show evidence of strong alignment\nto human uncertainty, even despite the lack of alignment to human answer\npreference. For those successful metrics, we find moderate to strong evidence\nof model calibration in terms of both correctness correlation and\ndistributional analysis."
                },
                "authors": [
                    {
                        "name": "Kyle Moore"
                    },
                    {
                        "name": "Jesse Roberts"
                    },
                    {
                        "name": "Daryl Watson"
                    }
                ],
                "author_detail": {
                    "name": "Daryl Watson"
                },
                "author": "Daryl Watson",
                "arxiv_comment": "preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17130v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17130v3",
                "updated": "2025-08-11T17:20:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    20,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-23T22:47:30Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    22,
                    47,
                    30,
                    2,
                    113,
                    0
                ],
                "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM\n  \"Thought\" Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering the CensorShip: Uncovering Representation Vectors for LLM\n  \"Thought\" Control"
                },
                "summary": "Large language models (LLMs) have transformed the way we access information.\nThese models are often tuned to refuse to comply with requests that are\nconsidered harmful and to produce responses that better align with the\npreferences of those who control the models. To understand how this\n\"censorship\" works. We use representation engineering techniques to study\nopen-weights safety-tuned models. We present a method for finding a\nrefusal--compliance vector that detects and controls the level of censorship in\nmodel outputs. We also analyze recent reasoning LLMs, distilled from\nDeepSeek-R1, and uncover an additional dimension of censorship through \"thought\nsuppression\". We show a similar approach can be used to find a vector that\nsuppresses the model's reasoning process, allowing us to remove censorship by\napplying the negative multiples of this vector. Our code is publicly available\nat: https://github.com/hannahxchen/llm-censorship-steering",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed the way we access information.\nThese models are often tuned to refuse to comply with requests that are\nconsidered harmful and to produce responses that better align with the\npreferences of those who control the models. To understand how this\n\"censorship\" works. We use representation engineering techniques to study\nopen-weights safety-tuned models. We present a method for finding a\nrefusal--compliance vector that detects and controls the level of censorship in\nmodel outputs. We also analyze recent reasoning LLMs, distilled from\nDeepSeek-R1, and uncover an additional dimension of censorship through \"thought\nsuppression\". We show a similar approach can be used to find a vector that\nsuppresses the model's reasoning process, allowing us to remove censorship by\napplying the negative multiples of this vector. Our code is publicly available\nat: https://github.com/hannahxchen/llm-censorship-steering"
                },
                "authors": [
                    {
                        "name": "Hannah Cyberey"
                    },
                    {
                        "name": "David Evans"
                    }
                ],
                "author_detail": {
                    "name": "David Evans"
                },
                "author": "David Evans",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17130v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17130v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08199v1",
                "updated": "2025-08-11T17:17:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    17,
                    20,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:17:20Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    17,
                    20,
                    0,
                    223,
                    0
                ],
                "title": "Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating\n  Room with Multimodal Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating\n  Room with Multimodal Large Language Model"
                },
                "summary": "Precise spatial modeling in the operating room (OR) is foundational to many\nclinical tasks, supporting intraoperative awareness, hazard avoidance, and\nsurgical decision-making. While existing approaches leverage large-scale\nmultimodal datasets for latent-space alignment to implicitly learn spatial\nrelationships, they overlook the 3D capabilities of MLLMs. However, this\napproach raises two issues: (1) Operating rooms typically lack multiple video\nand audio sensors, making multimodal 3D data difficult to obtain; (2) Training\nsolely on readily available 2D data fails to capture fine-grained details in\ncomplex scenes. To address this gap, we introduce Spatial-ORMLLM, the first\nlarge vision-language model for 3D spatial reasoning in operating rooms using\nonly RGB modality to infer volumetric and semantic cues, enabling downstream\nmedical tasks with detailed and holistic spatial context. Spatial-ORMLLM\nincorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D\nmodality inputs with rich 3D spatial knowledge extracted by the estimation\nalgorithm and then feeds the combined features into the visual tower. By\nemploying a unified end-to-end MLLM framework, it combines powerful spatial\nfeatures with textual features to deliver robust 3D scene reasoning without any\nadditional expert annotations or sensor inputs. Experiments on multiple\nbenchmark clinical datasets demonstrate that Spatial-ORMLLM achieves\nstate-of-the-art performance and generalizes robustly to previously unseen\nsurgical scenarios and downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise spatial modeling in the operating room (OR) is foundational to many\nclinical tasks, supporting intraoperative awareness, hazard avoidance, and\nsurgical decision-making. While existing approaches leverage large-scale\nmultimodal datasets for latent-space alignment to implicitly learn spatial\nrelationships, they overlook the 3D capabilities of MLLMs. However, this\napproach raises two issues: (1) Operating rooms typically lack multiple video\nand audio sensors, making multimodal 3D data difficult to obtain; (2) Training\nsolely on readily available 2D data fails to capture fine-grained details in\ncomplex scenes. To address this gap, we introduce Spatial-ORMLLM, the first\nlarge vision-language model for 3D spatial reasoning in operating rooms using\nonly RGB modality to infer volumetric and semantic cues, enabling downstream\nmedical tasks with detailed and holistic spatial context. Spatial-ORMLLM\nincorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D\nmodality inputs with rich 3D spatial knowledge extracted by the estimation\nalgorithm and then feeds the combined features into the visual tower. By\nemploying a unified end-to-end MLLM framework, it combines powerful spatial\nfeatures with textual features to deliver robust 3D scene reasoning without any\nadditional expert annotations or sensor inputs. Experiments on multiple\nbenchmark clinical datasets demonstrate that Spatial-ORMLLM achieves\nstate-of-the-art performance and generalizes robustly to previously unseen\nsurgical scenarios and downstream tasks."
                },
                "authors": [
                    {
                        "name": "Peiqi He"
                    },
                    {
                        "name": "Zhenhao Zhang"
                    },
                    {
                        "name": "Yixiang Zhang"
                    },
                    {
                        "name": "Xiongjun Zhao"
                    },
                    {
                        "name": "Shaoliang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Shaoliang Peng"
                },
                "author": "Shaoliang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08193v1",
                "updated": "2025-08-11T17:12:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    12,
                    55,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:12:55Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    12,
                    55,
                    0,
                    223,
                    0
                ],
                "title": "Street-Level AI: Are Large Language Models Ready for Real-World\n  Judgments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Street-Level AI: Are Large Language Models Ready for Real-World\n  Judgments?"
                },
                "summary": "A surge of recent work explores the ethical and societal implications of\nlarge-scale AI models that make \"moral\" judgments. Much of this literature\nfocuses either on alignment with human judgments through various thought\nexperiments or on the group fairness implications of AI judgments. However, the\nmost immediate and likely use of AI is to help or fully replace the so-called\nstreet-level bureaucrats, the individuals deciding to allocate scarce social\nresources or approve benefits. There is a rich history underlying how\nprinciples of local justice determine how society decides on prioritization\nmechanisms in such domains. In this paper, we examine how well LLM judgments\nalign with human judgments, as well as with socially and politically determined\nvulnerability scoring systems currently used in the domain of homelessness\nresource allocation. Crucially, we use real data on those needing services\n(maintaining strict confidentiality by only using local large models) to\nperform our analyses. We find that LLM prioritizations are extremely\ninconsistent in several ways: internally on different runs, between different\nLLMs, and between LLMs and the vulnerability scoring systems. At the same time,\nLLMs demonstrate qualitative consistency with lay human judgments in pairwise\ntesting. Findings call into question the readiness of current generation AI\nsystems for naive integration in high-stakes societal decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A surge of recent work explores the ethical and societal implications of\nlarge-scale AI models that make \"moral\" judgments. Much of this literature\nfocuses either on alignment with human judgments through various thought\nexperiments or on the group fairness implications of AI judgments. However, the\nmost immediate and likely use of AI is to help or fully replace the so-called\nstreet-level bureaucrats, the individuals deciding to allocate scarce social\nresources or approve benefits. There is a rich history underlying how\nprinciples of local justice determine how society decides on prioritization\nmechanisms in such domains. In this paper, we examine how well LLM judgments\nalign with human judgments, as well as with socially and politically determined\nvulnerability scoring systems currently used in the domain of homelessness\nresource allocation. Crucially, we use real data on those needing services\n(maintaining strict confidentiality by only using local large models) to\nperform our analyses. We find that LLM prioritizations are extremely\ninconsistent in several ways: internally on different runs, between different\nLLMs, and between LLMs and the vulnerability scoring systems. At the same time,\nLLMs demonstrate qualitative consistency with lay human judgments in pairwise\ntesting. Findings call into question the readiness of current generation AI\nsystems for naive integration in high-stakes societal decision-making."
                },
                "authors": [
                    {
                        "name": "Gaurab Pokharel"
                    },
                    {
                        "name": "Shafkat Farabi"
                    },
                    {
                        "name": "Patrick J. Fowler"
                    },
                    {
                        "name": "Sanmay Das"
                    }
                ],
                "author_detail": {
                    "name": "Sanmay Das"
                },
                "author": "Sanmay Das",
                "arxiv_comment": "This work has been accepted for publication as a full paper at the\n  AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08192v1",
                "updated": "2025-08-11T17:11:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    11,
                    26,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:11:26Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    11,
                    26,
                    0,
                    223,
                    0
                ],
                "title": "Efficient Speculative Decoding for Llama at Scale: Challenges and\n  Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Speculative Decoding for Llama at Scale: Challenges and\n  Solutions"
                },
                "summary": "Speculative decoding is a standard method for accelerating the inference\nspeed of large language models. However, scaling it for production environments\nposes several engineering challenges, including efficiently implementing\ndifferent operations (e.g., tree attention and multi-round speculative\ndecoding) on GPU. In this paper, we detail the training and inference\noptimization techniques that we have implemented to enable EAGLE-based\nspeculative decoding at a production scale for Llama models. With these\nchanges, we achieve a new state-of-the-art inference latency for Llama models.\nFor example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a\nbatch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the\npreviously best known method. Furthermore, for EAGLE-based speculative\ndecoding, our optimizations enable us to achieve a speed-up for large batch\nsizes between 1.4x and 2.0x at production scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a standard method for accelerating the inference\nspeed of large language models. However, scaling it for production environments\nposes several engineering challenges, including efficiently implementing\ndifferent operations (e.g., tree attention and multi-round speculative\ndecoding) on GPU. In this paper, we detail the training and inference\noptimization techniques that we have implemented to enable EAGLE-based\nspeculative decoding at a production scale for Llama models. With these\nchanges, we achieve a new state-of-the-art inference latency for Llama models.\nFor example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a\nbatch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the\npreviously best known method. Furthermore, for EAGLE-based speculative\ndecoding, our optimizations enable us to achieve a speed-up for large batch\nsizes between 1.4x and 2.0x at production scale."
                },
                "authors": [
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Carl Chengyan Fu"
                    },
                    {
                        "name": "Fei Kou"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Haoci Zhang"
                    },
                    {
                        "name": "Jason Park"
                    },
                    {
                        "name": "Jiawen Liu"
                    },
                    {
                        "name": "Jie You"
                    },
                    {
                        "name": "Qirui Yang"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Shengyong Cai"
                    },
                    {
                        "name": "Xiaodong Wang"
                    },
                    {
                        "name": "Xingyu Liu"
                    },
                    {
                        "name": "Yunlu Li"
                    },
                    {
                        "name": "Yanjun Zhou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Zhiwei Zhao"
                    },
                    {
                        "name": "Zixi Qi"
                    },
                    {
                        "name": "Adolfo Victoria"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Bram Wasti"
                    },
                    {
                        "name": "Changkyu Kim"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Giancarlo Delfin"
                    },
                    {
                        "name": "Emily Guo"
                    },
                    {
                        "name": "Jialin Ouyang"
                    },
                    {
                        "name": "Jaewon Lee"
                    },
                    {
                        "name": "Jianyu Huang"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Lu Fang"
                    },
                    {
                        "name": "Quinn Zhu"
                    },
                    {
                        "name": "Ria Verma"
                    },
                    {
                        "name": "Vlad Mihailescu"
                    },
                    {
                        "name": "Xingwen Guo"
                    },
                    {
                        "name": "Yan Cui"
                    },
                    {
                        "name": "Ye Hu"
                    },
                    {
                        "name": "Yejin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Lee"
                },
                "author": "Yejin Lee",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08186v1",
                "updated": "2025-08-11T17:06:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    6,
                    55,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:06:55Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    6,
                    55,
                    0,
                    223,
                    0
                ],
                "title": "KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold\n  Representation Learning"
                },
                "summary": "Semantic segmentation of structural defects in civil infrastructure remains\nchallenging due to variable defect appearances, harsh imaging conditions, and\nsignificant class imbalance. Current deep learning methods, despite their\neffectiveness, typically require millions of parameters, rendering them\nimpractical for real-time inspection systems. We introduce KARMA\n(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient\nsemantic segmentation framework that models complex defect patterns through\ncompositions of one-dimensional functions rather than conventional\nconvolutions. KARMA features three technical innovations: (1) a\nparameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging\nlow-rank factorization for KAN-based feature transformation; (2) an optimized\nfeature pyramid structure with separable convolutions for multi-scale defect\nanalysis; and (3) a static-dynamic prototype mechanism that enhances feature\nrepresentation for imbalanced classes. Extensive experiments on benchmark\ninfrastructure inspection datasets demonstrate that KARMA achieves competitive\nor superior mean IoU performance compared to state-of-the-art approaches, while\nusing significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).\nOperating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for\nreal-time deployment, enabling practical automated infrastructure inspection\nsystems without compromising accuracy. The source code can be accessed at the\nfollowing URL: https://github.com/faeyelab/karma.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic segmentation of structural defects in civil infrastructure remains\nchallenging due to variable defect appearances, harsh imaging conditions, and\nsignificant class imbalance. Current deep learning methods, despite their\neffectiveness, typically require millions of parameters, rendering them\nimpractical for real-time inspection systems. We introduce KARMA\n(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient\nsemantic segmentation framework that models complex defect patterns through\ncompositions of one-dimensional functions rather than conventional\nconvolutions. KARMA features three technical innovations: (1) a\nparameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging\nlow-rank factorization for KAN-based feature transformation; (2) an optimized\nfeature pyramid structure with separable convolutions for multi-scale defect\nanalysis; and (3) a static-dynamic prototype mechanism that enhances feature\nrepresentation for imbalanced classes. Extensive experiments on benchmark\ninfrastructure inspection datasets demonstrate that KARMA achieves competitive\nor superior mean IoU performance compared to state-of-the-art approaches, while\nusing significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).\nOperating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for\nreal-time deployment, enabling practical automated infrastructure inspection\nsystems without compromising accuracy. The source code can be accessed at the\nfollowing URL: https://github.com/faeyelab/karma."
                },
                "authors": [
                    {
                        "name": "Md Meftahul Ferdaus"
                    },
                    {
                        "name": "Mahdi Abdelguerfi"
                    },
                    {
                        "name": "Elias Ioup"
                    },
                    {
                        "name": "Steven Sloan"
                    },
                    {
                        "name": "Kendall N. Niles"
                    },
                    {
                        "name": "Ken Pathak"
                    }
                ],
                "author_detail": {
                    "name": "Ken Pathak"
                },
                "author": "Ken Pathak",
                "arxiv_comment": "submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08178v2",
                "updated": "2025-08-12T16:25:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    25,
                    31,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T16:59:14Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    59,
                    14,
                    0,
                    223,
                    0
                ],
                "title": "3D Human Mesh Estimation from Single View RGBD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Human Mesh Estimation from Single View RGBD"
                },
                "summary": "Despite significant progress in 3D human mesh estimation from RGB images;\nRGBD cameras, offering additional depth data, remain underutilized. In this\npaper, we present a method for accurate 3D human mesh estimation from a single\nRGBD view, leveraging the affordability and widespread adoption of RGBD cameras\nfor real-world applications. A fully supervised approach for this problem,\nrequires a dataset with RGBD image and 3D mesh label pairs. However, collecting\nsuch a dataset is costly and challenging, hence, existing datasets are small,\nand limited in pose and shape diversity. To overcome this data scarcity, we\nleverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D\nmeshes from the body models found in MoCap datasets, and create partial,\nsingle-view versions of them by projection to a virtual camera. This simulates\nthe depth data provided by an RGBD camera from a single viewpoint. Then, we\ntrain a masked autoencoder to complete the partial, single-view mesh. During\ninference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',\nmatches the depth values coming from the sensor to vertices of a template human\nmesh, which creates a partial, single-view mesh. We effectively recover parts\nof the 3D human body mesh model that are not visible, resulting in a full body\nmesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL\nand CAPE datasets, respectively; outperforming existing methods that use\nfull-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE\ndataset, outperforming a recently published RGB based method by 18.4 mm,\nhighlighting the usefulness of depth data. Code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress in 3D human mesh estimation from RGB images;\nRGBD cameras, offering additional depth data, remain underutilized. In this\npaper, we present a method for accurate 3D human mesh estimation from a single\nRGBD view, leveraging the affordability and widespread adoption of RGBD cameras\nfor real-world applications. A fully supervised approach for this problem,\nrequires a dataset with RGBD image and 3D mesh label pairs. However, collecting\nsuch a dataset is costly and challenging, hence, existing datasets are small,\nand limited in pose and shape diversity. To overcome this data scarcity, we\nleverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D\nmeshes from the body models found in MoCap datasets, and create partial,\nsingle-view versions of them by projection to a virtual camera. This simulates\nthe depth data provided by an RGBD camera from a single viewpoint. Then, we\ntrain a masked autoencoder to complete the partial, single-view mesh. During\ninference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',\nmatches the depth values coming from the sensor to vertices of a template human\nmesh, which creates a partial, single-view mesh. We effectively recover parts\nof the 3D human body mesh model that are not visible, resulting in a full body\nmesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL\nand CAPE datasets, respectively; outperforming existing methods that use\nfull-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE\ndataset, outperforming a recently published RGB based method by 18.4 mm,\nhighlighting the usefulness of depth data. Code will be released."
                },
                "authors": [
                    {
                        "name": "Ozhan Suat"
                    },
                    {
                        "name": "Bedirhan Uguz"
                    },
                    {
                        "name": "Batuhan Karagoz"
                    },
                    {
                        "name": "Muhammed Can Keles"
                    },
                    {
                        "name": "Emre Akbas"
                    }
                ],
                "author_detail": {
                    "name": "Emre Akbas"
                },
                "author": "Emre Akbas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02904v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02904v2",
                "updated": "2025-08-11T16:54:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    54,
                    1,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-03T06:30:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    6,
                    30,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge,\n  Truthfulness, Refusal, and Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge,\n  Truthfulness, Refusal, and Confidence"
                },
                "summary": "Post-training is essential for the success of large language models (LLMs),\ntransforming pre-trained base models into more useful and aligned post-trained\nmodels. While plenty of works have studied post-training algorithms and\nevaluated post-training models by their outputs, it remains understudied how\npost-training reshapes LLMs internally. In this paper, we compare base and\npost-trained LLMs mechanistically from four perspectives to better understand\npost-training effects. Our findings across model families and datasets reveal\nthat: (1) Post-training does not change the factual knowledge storage\nlocations, and it adapts knowledge representations from the base model while\ndeveloping new knowledge representations; (2) Both truthfulness and refusal can\nbe represented by vectors in the hidden representation space. The truthfulness\ndirection is highly similar between the base and post-trained model, and it is\neffectively transferable for interventions; (3) The refusal direction is\ndifferent between the base and post-trained models, and it shows limited\nforward transferability; (4) Differences in confidence between the base and\npost-trained models cannot be attributed to entropy neurons. Our study provides\ninsights into the fundamental mechanisms preserved and altered during\npost-training, facilitates downstream tasks like model steering, and could\npotentially benefit future research in interpretability and LLM post-training.\nOur code is publicly available at\nhttps://github.com/HZD01/post-training-mechanistic-analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training is essential for the success of large language models (LLMs),\ntransforming pre-trained base models into more useful and aligned post-trained\nmodels. While plenty of works have studied post-training algorithms and\nevaluated post-training models by their outputs, it remains understudied how\npost-training reshapes LLMs internally. In this paper, we compare base and\npost-trained LLMs mechanistically from four perspectives to better understand\npost-training effects. Our findings across model families and datasets reveal\nthat: (1) Post-training does not change the factual knowledge storage\nlocations, and it adapts knowledge representations from the base model while\ndeveloping new knowledge representations; (2) Both truthfulness and refusal can\nbe represented by vectors in the hidden representation space. The truthfulness\ndirection is highly similar between the base and post-trained model, and it is\neffectively transferable for interventions; (3) The refusal direction is\ndifferent between the base and post-trained models, and it shows limited\nforward transferability; (4) Differences in confidence between the base and\npost-trained models cannot be attributed to entropy neurons. Our study provides\ninsights into the fundamental mechanisms preserved and altered during\npost-training, facilitates downstream tasks like model steering, and could\npotentially benefit future research in interpretability and LLM post-training.\nOur code is publicly available at\nhttps://github.com/HZD01/post-training-mechanistic-analysis."
                },
                "authors": [
                    {
                        "name": "Hongzhe Du"
                    },
                    {
                        "name": "Weikai Li"
                    },
                    {
                        "name": "Min Cai"
                    },
                    {
                        "name": "Karim Saraipour"
                    },
                    {
                        "name": "Zimin Zhang"
                    },
                    {
                        "name": "Himabindu Lakkaraju"
                    },
                    {
                        "name": "Yizhou Sun"
                    },
                    {
                        "name": "Shichang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shichang Zhang"
                },
                "author": "Shichang Zhang",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02904v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00632v2",
                "updated": "2025-08-11T16:52:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    52,
                    26,
                    0,
                    223,
                    0
                ],
                "published": "2025-05-01T16:16:47Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    16,
                    16,
                    47,
                    3,
                    121,
                    0
                ],
                "title": "Detecting Modeling Bias with Continuous Time Flow Models on Weak Lensing\n  Maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Modeling Bias with Continuous Time Flow Models on Weak Lensing\n  Maps"
                },
                "summary": "Simulation-based inference provides a powerful framework for extracting rich\ninformation from nonlinear scales in current and upcoming cosmological surveys,\nand ensuring its robustness requires stringent validation of forward models. In\nthis work, we recast forward model validation as an out-of-distribution (OoD)\ndetection problem within the framework of machine learning (ML)-based\nsimulation-based inference (SBI). We employ probability density as the metric\nfor OoD detection, and compare various density estimation techniques,\ndemonstrating that field-level probability density estimation via continuous\ntime flow models (CTFM) significantly outperforms feature-level approaches that\ncombine scattering transform (ST) or convolutional neural networks (CNN) with\nnormalizing flows (NFs), as well as NF-based field-level estimators, as\nquantified by the area under the receiver operating characteristic curve\n(AUROC). Our analysis shows that CTFM not only excels in detecting OoD samples\nbut also provides a robust metric for model selection. Additionally, we\nverified CTFM maintains consistent efficacy across different cosmologies while\nmitigating the inductive biases inherent in NF architectures. Although our\nproof-of-concept study employs simplified forward modeling and noise settings,\nour framework establishes a promising pathway for identifying unknown\nsystematics in the cosmology datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference provides a powerful framework for extracting rich\ninformation from nonlinear scales in current and upcoming cosmological surveys,\nand ensuring its robustness requires stringent validation of forward models. In\nthis work, we recast forward model validation as an out-of-distribution (OoD)\ndetection problem within the framework of machine learning (ML)-based\nsimulation-based inference (SBI). We employ probability density as the metric\nfor OoD detection, and compare various density estimation techniques,\ndemonstrating that field-level probability density estimation via continuous\ntime flow models (CTFM) significantly outperforms feature-level approaches that\ncombine scattering transform (ST) or convolutional neural networks (CNN) with\nnormalizing flows (NFs), as well as NF-based field-level estimators, as\nquantified by the area under the receiver operating characteristic curve\n(AUROC). Our analysis shows that CTFM not only excels in detecting OoD samples\nbut also provides a robust metric for model selection. Additionally, we\nverified CTFM maintains consistent efficacy across different cosmologies while\nmitigating the inductive biases inherent in NF architectures. Although our\nproof-of-concept study employs simplified forward modeling and noise settings,\nour framework establishes a promising pathway for identifying unknown\nsystematics in the cosmology datasets."
                },
                "authors": [
                    {
                        "name": "Kangning Diao"
                    },
                    {
                        "name": "Biwei Dai"
                    },
                    {
                        "name": "Uros Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uros Seljak"
                },
                "author": "Uros Seljak",
                "arxiv_doi": "10.1088/1475-7516/2025/08/004",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1475-7516/2025/08/004",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.00632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24 pages, 8 figures, 2 tables, comments welcome",
                "arxiv_journal_ref": "JCAP08(2025)004",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08171v1",
                "updated": "2025-08-11T16:49:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    49,
                    7,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:49:07Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    49,
                    7,
                    0,
                    223,
                    0
                ],
                "title": "PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded\n  Model Checking for C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded\n  Model Checking for C"
                },
                "summary": "Python has become the dominant language for general-purpose programming, yet\nit lacks robust tools for formal verification. In contrast, programmers working\nin languages such as C benefit from mature model checkers, for example CBMC,\nwhich enable exhaustive symbolic reasoning and fault localisation. The inherent\ncomplexity of Python, coupled with the verbosity and low-level nature of\nexisting transpilers (e.g., Cython), have historically limited the\napplicability of formal verification to Python programs.\n  In this paper, we propose PyVeritas, a novel framework that leverages Large\nLanguage Models (LLMs) for high-level transpilation from Python to C, followed\nby bounded model checking and MaxSAT-based fault localisation in the generated\nC code. PyVeritas enables verification and bug localisation for Python code\nusing existing model checking tools for C. Our empirical evaluation on two\nPython benchmarks demonstrates that LLM-based transpilation can achieve a high\ndegree of accuracy, up to 80--90% for some LLMs, enabling effective development\nenvironment that supports assertion-based verification and interpretable fault\ndiagnosis for small yet non-trivial Python programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python has become the dominant language for general-purpose programming, yet\nit lacks robust tools for formal verification. In contrast, programmers working\nin languages such as C benefit from mature model checkers, for example CBMC,\nwhich enable exhaustive symbolic reasoning and fault localisation. The inherent\ncomplexity of Python, coupled with the verbosity and low-level nature of\nexisting transpilers (e.g., Cython), have historically limited the\napplicability of formal verification to Python programs.\n  In this paper, we propose PyVeritas, a novel framework that leverages Large\nLanguage Models (LLMs) for high-level transpilation from Python to C, followed\nby bounded model checking and MaxSAT-based fault localisation in the generated\nC code. PyVeritas enables verification and bug localisation for Python code\nusing existing model checking tools for C. Our empirical evaluation on two\nPython benchmarks demonstrates that LLM-based transpilation can achieve a high\ndegree of accuracy, up to 80--90% for some LLMs, enabling effective development\nenvironment that supports assertion-based verification and interpretable fault\ndiagnosis for small yet non-trivial Python programs."
                },
                "authors": [
                    {
                        "name": "Pedro Orvalho"
                    },
                    {
                        "name": "Marta Kwiatkowska"
                    }
                ],
                "author_detail": {
                    "name": "Marta Kwiatkowska"
                },
                "author": "Marta Kwiatkowska",
                "arxiv_comment": "14 pages, 6 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23701v2",
                "updated": "2025-08-11T16:46:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    46,
                    8,
                    0,
                    223,
                    0
                ],
                "published": "2025-07-31T16:22:55Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    22,
                    55,
                    3,
                    212,
                    0
                ],
                "title": "TextQuests: How Good are LLMs at Text-Based Video Games?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextQuests: How Good are LLMs at Text-Based Video Games?"
                },
                "summary": "Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai."
                },
                "authors": [
                    {
                        "name": "Long Phan"
                    },
                    {
                        "name": "Mantas Mazeika"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Dan Hendrycks"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hendrycks"
                },
                "author": "Dan Hendrycks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08165v1",
                "updated": "2025-08-11T16:41:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    41,
                    4,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:41:04Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    41,
                    4,
                    0,
                    223,
                    0
                ],
                "title": "Integrating Task-Specific and Universal Adapters for Pre-Trained\n  Model-based Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Task-Specific and Universal Adapters for Pre-Trained\n  Model-based Class-Incremental Learning"
                },
                "summary": "Class-Incremental Learning (CIL) requires a learning system to continually\nlearn new classes without forgetting. Existing pre-trained model-based CIL\nmethods often freeze the pre-trained network and adapt to incremental tasks\nusing additional lightweight modules such as adapters. However, incorrect\nmodule selection during inference hurts performance, and task-specific modules\noften overlook shared general knowledge, leading to errors on distinguishing\nbetween similar classes across tasks. To address the aforementioned challenges,\nwe propose integrating Task-Specific and Universal Adapters (TUNA) in this\npaper. Specifically, we train task-specific adapters to capture the most\ncrucial features relevant to their respective tasks and introduce an\nentropy-based selection mechanism to choose the most suitable adapter.\nFurthermore, we leverage an adapter fusion strategy to construct a universal\nadapter, which encodes the most discriminative features shared across tasks. We\ncombine task-specific and universal adapter predictions to harness both\nspecialized and general knowledge during inference. Extensive experiments on\nvarious benchmark datasets demonstrate the state-of-the-art performance of our\napproach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Class-Incremental Learning (CIL) requires a learning system to continually\nlearn new classes without forgetting. Existing pre-trained model-based CIL\nmethods often freeze the pre-trained network and adapt to incremental tasks\nusing additional lightweight modules such as adapters. However, incorrect\nmodule selection during inference hurts performance, and task-specific modules\noften overlook shared general knowledge, leading to errors on distinguishing\nbetween similar classes across tasks. To address the aforementioned challenges,\nwe propose integrating Task-Specific and Universal Adapters (TUNA) in this\npaper. Specifically, we train task-specific adapters to capture the most\ncrucial features relevant to their respective tasks and introduce an\nentropy-based selection mechanism to choose the most suitable adapter.\nFurthermore, we leverage an adapter fusion strategy to construct a universal\nadapter, which encodes the most discriminative features shared across tasks. We\ncombine task-specific and universal adapter predictions to harness both\nspecialized and general knowledge during inference. Extensive experiments on\nvarious benchmark datasets demonstrate the state-of-the-art performance of our\napproach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA"
                },
                "authors": [
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Da-Wei Zhou"
                    },
                    {
                        "name": "Han-Jia Ye"
                    }
                ],
                "author_detail": {
                    "name": "Han-Jia Ye"
                },
                "author": "Han-Jia Ye",
                "arxiv_comment": "Accepted to ICCV 2025. Code is available at:\n  https://github.com/LAMDA-CL/ICCV2025-TUNA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08149v2",
                "updated": "2025-08-12T03:54:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    54,
                    24,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T16:25:25Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    25,
                    25,
                    0,
                    223,
                    0
                ],
                "title": "REX-RAG: Reasoning Exploration with Policy Correction in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REX-RAG: Reasoning Exploration with Policy Correction in\n  Retrieval-Augmented Generation"
                },
                "summary": "Reinforcement learning (RL) is emerging as a powerful paradigm for enabling\nlarge language models (LLMs) to perform complex reasoning tasks. Recent\nadvances indicate that integrating RL with retrieval-augmented generation (RAG)\nallows LLMs to dynamically incorporate external knowledge, leading to more\ninformed and robust decision making. However, we identify a critical challenge\nduring policy-driven trajectory sampling: LLMs are frequently trapped in\nunproductive reasoning paths, which we refer to as \"dead ends\", committing to\noverconfident yet incorrect conclusions. This severely hampers exploration and\nundermines effective policy optimization. To address this challenge, we propose\nREX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented\nGeneration), a novel framework that explores alternative reasoning paths while\nmaintaining rigorous policy learning through principled distributional\ncorrections. Our approach introduces two key innovations: (1) Mixed Sampling\nStrategy, which combines a novel probe sampling method with exploratory prompts\nto escape dead ends; and (2) Policy Correction Mechanism, which employs\nimportance sampling to correct distribution shifts induced by mixed sampling,\nthereby mitigating gradient estimation bias. We evaluate it on seven\nquestion-answering benchmarks, and the experimental results show that REX-RAG\nachieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B\nover strong baselines, demonstrating competitive results across multiple\ndatasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is emerging as a powerful paradigm for enabling\nlarge language models (LLMs) to perform complex reasoning tasks. Recent\nadvances indicate that integrating RL with retrieval-augmented generation (RAG)\nallows LLMs to dynamically incorporate external knowledge, leading to more\ninformed and robust decision making. However, we identify a critical challenge\nduring policy-driven trajectory sampling: LLMs are frequently trapped in\nunproductive reasoning paths, which we refer to as \"dead ends\", committing to\noverconfident yet incorrect conclusions. This severely hampers exploration and\nundermines effective policy optimization. To address this challenge, we propose\nREX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented\nGeneration), a novel framework that explores alternative reasoning paths while\nmaintaining rigorous policy learning through principled distributional\ncorrections. Our approach introduces two key innovations: (1) Mixed Sampling\nStrategy, which combines a novel probe sampling method with exploratory prompts\nto escape dead ends; and (2) Policy Correction Mechanism, which employs\nimportance sampling to correct distribution shifts induced by mixed sampling,\nthereby mitigating gradient estimation bias. We evaluate it on seven\nquestion-answering benchmarks, and the experimental results show that REX-RAG\nachieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B\nover strong baselines, demonstrating competitive results across multiple\ndatasets. The code is publicly available at https://github.com/MiliLab/REX-RAG."
                },
                "authors": [
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Xiang Feng"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Yong Luo"
                    },
                    {
                        "name": "Pingbo Xu"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "arxiv_comment": "17 pages, 4 figures; updated references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12962v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12962v2",
                "updated": "2025-08-11T16:25:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    25,
                    8,
                    0,
                    223,
                    0
                ],
                "published": "2024-09-19T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    17,
                    59,
                    52,
                    3,
                    263,
                    0
                ],
                "title": "CLAIR-A: Leveraging Large Language Models to Judge Audio Captions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLAIR-A: Leveraging Large Language Models to Judge Audio Captions"
                },
                "summary": "The Automated Audio Captioning (AAC) task asks models to generate natural\nlanguage descriptions of an audio input. Evaluating these machine-generated\naudio captions is a complex task that requires considering diverse factors,\namong them, auditory scene understanding, sound-object inference, temporal\ncoherence, and the environmental context of the scene. While current methods\nfocus on specific aspects, they often fail to provide an overall score that\naligns well with human judgment. In this work, we propose CLAIR-A, a simple and\nflexible method that leverages the zero-shot capabilities of large language\nmodels (LLMs) to evaluate candidate audio captions by directly asking LLMs for\na semantic distance score. In our evaluations, CLAIR-A better predicts human\njudgements of quality compared to traditional metrics, with a 5.8% relative\naccuracy improvement compared to the domain-specific FENSE metric and up to 11%\nover the best general-purpose measure on the Clotho-Eval dataset. Moreover,\nCLAIR-A offers more transparency by allowing the language model to explain the\nreasoning behind its scores, with these explanations rated up to 30% better by\nhuman evaluators than those provided by baseline methods. CLAIR-A is made\npublicly available at https://github.com/DavidMChan/clair-a.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Automated Audio Captioning (AAC) task asks models to generate natural\nlanguage descriptions of an audio input. Evaluating these machine-generated\naudio captions is a complex task that requires considering diverse factors,\namong them, auditory scene understanding, sound-object inference, temporal\ncoherence, and the environmental context of the scene. While current methods\nfocus on specific aspects, they often fail to provide an overall score that\naligns well with human judgment. In this work, we propose CLAIR-A, a simple and\nflexible method that leverages the zero-shot capabilities of large language\nmodels (LLMs) to evaluate candidate audio captions by directly asking LLMs for\na semantic distance score. In our evaluations, CLAIR-A better predicts human\njudgements of quality compared to traditional metrics, with a 5.8% relative\naccuracy improvement compared to the domain-specific FENSE metric and up to 11%\nover the best general-purpose measure on the Clotho-Eval dataset. Moreover,\nCLAIR-A offers more transparency by allowing the language model to explain the\nreasoning behind its scores, with these explanations rated up to 30% better by\nhuman evaluators than those provided by baseline methods. CLAIR-A is made\npublicly available at https://github.com/DavidMChan/clair-a."
                },
                "authors": [
                    {
                        "name": "Tsung-Han Wu"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "David M. Chan"
                    }
                ],
                "author_detail": {
                    "name": "David M. Chan"
                },
                "author": "David M. Chan",
                "arxiv_comment": "Accepted to ASRU 2025; Code is publicly available at\n  https://github.com/DavidMChan/clair-a",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12962v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21931v2",
                "updated": "2025-08-11T16:24:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    24,
                    36,
                    0,
                    223,
                    0
                ],
                "published": "2025-06-27T05:45:59Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    5,
                    45,
                    59,
                    4,
                    178,
                    0
                ],
                "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARAG: Agentic Retrieval Augmented Generation for Personalized\n  Recommendation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization."
                },
                "authors": [
                    {
                        "name": "Reza Yousefi Maragheh"
                    },
                    {
                        "name": "Pratheek Vadla"
                    },
                    {
                        "name": "Priyank Gupta"
                    },
                    {
                        "name": "Kai Zhao"
                    },
                    {
                        "name": "Aysenur Inan"
                    },
                    {
                        "name": "Kehui Yao"
                    },
                    {
                        "name": "Jianpeng Xu"
                    },
                    {
                        "name": "Praveen Kanumala"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Sushant Kumar"
                },
                "author": "Sushant Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08147v1",
                "updated": "2025-08-11T16:22:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    22,
                    57,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:22:57Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    22,
                    57,
                    0,
                    223,
                    0
                ],
                "title": "From Natural Language to Solver-Ready Power System Optimization: An\n  LLM-Assisted, Validation-in-the-Loop Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Natural Language to Solver-Ready Power System Optimization: An\n  LLM-Assisted, Validation-in-the-Loop Framework"
                },
                "summary": "This paper introduces a novel Large Language Models (LLMs)-assisted agent\nthat automatically converts natural-language descriptions of power system\noptimization scenarios into compact, solver-ready formulations and generates\ncorresponding solutions. In contrast to approaches that rely solely on LLM to\nproduce solutions directly, the proposed method focuses on discovering a\nmathematically compatible formulation that can be efficiently solved by\noff-the-shelf optimization solvers. Directly using LLMs to produce solutions\noften leads to infeasible or suboptimal results, as these models lack the\nnumerical precision and constraint-handling capabilities of established\noptimization solvers. The pipeline integrates a domain-aware prompt and schema\nwith an LLM, enforces feasibility through systematic validation and iterative\nrepair, and returns both solver-ready models and user-facing results. Using the\nunit commitment problem as a representative case study, the agent produces\noptimal or near-optimal schedules along with the associated objective costs.\nResults demonstrate that coupling the solver with task-specific validation\nsignificantly enhances solution reliability. This work shows that combining AI\nwith established optimization frameworks bridges high-level problem\ndescriptions and executable mathematical models, enabling more efficient\ndecision-making in energy systems",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel Large Language Models (LLMs)-assisted agent\nthat automatically converts natural-language descriptions of power system\noptimization scenarios into compact, solver-ready formulations and generates\ncorresponding solutions. In contrast to approaches that rely solely on LLM to\nproduce solutions directly, the proposed method focuses on discovering a\nmathematically compatible formulation that can be efficiently solved by\noff-the-shelf optimization solvers. Directly using LLMs to produce solutions\noften leads to infeasible or suboptimal results, as these models lack the\nnumerical precision and constraint-handling capabilities of established\noptimization solvers. The pipeline integrates a domain-aware prompt and schema\nwith an LLM, enforces feasibility through systematic validation and iterative\nrepair, and returns both solver-ready models and user-facing results. Using the\nunit commitment problem as a representative case study, the agent produces\noptimal or near-optimal schedules along with the associated objective costs.\nResults demonstrate that coupling the solver with task-specific validation\nsignificantly enhances solution reliability. This work shows that combining AI\nwith established optimization frameworks bridges high-level problem\ndescriptions and executable mathematical models, enabling more efficient\ndecision-making in energy systems"
                },
                "authors": [
                    {
                        "name": "Yunkai Hu"
                    },
                    {
                        "name": "Tianqiao Zhao"
                    },
                    {
                        "name": "Meng Yue"
                    }
                ],
                "author_detail": {
                    "name": "Meng Yue"
                },
                "author": "Meng Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08140v1",
                "updated": "2025-08-11T16:13:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    13,
                    21,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:13:21Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    13,
                    21,
                    0,
                    223,
                    0
                ],
                "title": "Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced\n  Submodular Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced\n  Submodular Perspective"
                },
                "summary": "Recent progress in large language models (LLMs) has leveraged their\nin-context learning (ICL) abilities to enable quick adaptation to unseen\nbiomedical NLP tasks. By incorporating only a few input-output examples into\nprompts, LLMs can rapidly perform these new tasks. While the impact of these\ndemonstrations on LLM performance has been extensively studied, most existing\napproaches prioritize representativeness over diversity when selecting examples\nfrom large corpora. To address this gap, we propose Dual-Div, a\ndiversity-enhanced data-efficient framework for demonstration selection in\nbiomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:\nFirst, it identifies a limited set of candidate examples from a corpus by\noptimizing both representativeness and diversity (with optional annotation for\nunlabeled data). Second, it ranks these candidates against test queries to\nselect the most relevant and non-redundant demonstrations. Evaluated on three\nbiomedical NLP tasks (named entity recognition (NER), relation extraction (RE),\nand text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along\nwith three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently\noutperforms baselines-achieving up to 5% higher macro-F1 scores-while\ndemonstrating robustness to prompt permutations and class imbalance. Our\nfindings establish that diversity in initial retrieval is more critical than\nranking-stage optimization, and limiting demonstrations to 3-5 examples\nmaximizes performance efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has leveraged their\nin-context learning (ICL) abilities to enable quick adaptation to unseen\nbiomedical NLP tasks. By incorporating only a few input-output examples into\nprompts, LLMs can rapidly perform these new tasks. While the impact of these\ndemonstrations on LLM performance has been extensively studied, most existing\napproaches prioritize representativeness over diversity when selecting examples\nfrom large corpora. To address this gap, we propose Dual-Div, a\ndiversity-enhanced data-efficient framework for demonstration selection in\nbiomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:\nFirst, it identifies a limited set of candidate examples from a corpus by\noptimizing both representativeness and diversity (with optional annotation for\nunlabeled data). Second, it ranks these candidates against test queries to\nselect the most relevant and non-redundant demonstrations. Evaluated on three\nbiomedical NLP tasks (named entity recognition (NER), relation extraction (RE),\nand text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along\nwith three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently\noutperforms baselines-achieving up to 5% higher macro-F1 scores-while\ndemonstrating robustness to prompt permutations and class imbalance. Our\nfindings establish that diversity in initial retrieval is more critical than\nranking-stage optimization, and limiting demonstrations to 3-5 examples\nmaximizes performance efficiency."
                },
                "authors": [
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Zaifu Zhan"
                    },
                    {
                        "name": "Qixin Zhang"
                    },
                    {
                        "name": "Mingquan Lin"
                    },
                    {
                        "name": "Meijia Song"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08139v1",
                "updated": "2025-08-11T16:12:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    12,
                    36,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:12:36Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    12,
                    36,
                    0,
                    223,
                    0
                ],
                "title": "Can LLMs Detect Their Confabulations? Estimating Reliability in\n  Uncertainty-Aware Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Detect Their Confabulations? Estimating Reliability in\n  Uncertainty-Aware Language Models"
                },
                "summary": "Large Language Models (LLMs) are prone to generating fluent but incorrect\ncontent, known as confabulation, which poses increasing risks in multi-turn or\nagentic applications where outputs may be reused as context. In this work, we\ninvestigate how in-context information influences model behavior and whether\nLLMs can identify their unreliable responses. We propose a reliability\nestimation that leverages token-level uncertainty to guide the aggregation of\ninternal model representations. Specifically, we compute aleatoric and\nepistemic uncertainty from output logits to identify salient tokens and\naggregate their hidden states into compact representations for response-level\nreliability prediction. Through controlled experiments on open QA benchmarks,\nwe find that correct in-context information improves both answer accuracy and\nmodel confidence, while misleading context often induces confidently incorrect\nresponses, revealing a misalignment between uncertainty and correctness. Our\nprobing-based method captures these shifts in model behavior and improves the\ndetection of unreliable outputs across multiple open-source LLMs. These results\nunderscore the limitations of direct uncertainty signals and highlight the\npotential of uncertainty-guided probing for reliability-aware generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to generating fluent but incorrect\ncontent, known as confabulation, which poses increasing risks in multi-turn or\nagentic applications where outputs may be reused as context. In this work, we\ninvestigate how in-context information influences model behavior and whether\nLLMs can identify their unreliable responses. We propose a reliability\nestimation that leverages token-level uncertainty to guide the aggregation of\ninternal model representations. Specifically, we compute aleatoric and\nepistemic uncertainty from output logits to identify salient tokens and\naggregate their hidden states into compact representations for response-level\nreliability prediction. Through controlled experiments on open QA benchmarks,\nwe find that correct in-context information improves both answer accuracy and\nmodel confidence, while misleading context often induces confidently incorrect\nresponses, revealing a misalignment between uncertainty and correctness. Our\nprobing-based method captures these shifts in model behavior and improves the\ndetection of unreliable outputs across multiple open-source LLMs. These results\nunderscore the limitations of direct uncertainty signals and highlight the\npotential of uncertainty-guided probing for reliability-aware generation."
                },
                "authors": [
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Johanne Medina"
                    },
                    {
                        "name": "Sanjay Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay Chawla"
                },
                "author": "Sanjay Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08137v1",
                "updated": "2025-08-11T16:11:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    11,
                    9,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:11:09Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    11,
                    9,
                    0,
                    223,
                    0
                ],
                "title": "MuaLLM: A Multimodal Large Language Model Agent for Circuit Design\n  Assistance with Hybrid Contextual Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuaLLM: A Multimodal Large Language Model Agent for Circuit Design\n  Assistance with Hybrid Contextual Retrieval-Augmented Generation"
                },
                "summary": "Conducting a comprehensive literature review is crucial for advancing circuit\ndesign methodologies. However, the rapid influx of state-of-the-art research,\ninconsistent data representation, and the complexity of optimizing circuit\ndesign objectives make this task significantly challenging. In this paper, we\npropose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for\ncircuit design assistance that integrates a hybrid Retrieval-Augmented\nGeneration (RAG) framework with an adaptive vector database of circuit design\nresearch papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason +\nAct (ReAct) workflow for iterative reasoning, goal-setting, and multi-step\ninformation retrieval. It functions as a question-answering design assistant,\ncapable of interpreting complex queries and providing reasoned responses\ngrounded in circuit literature. Its multimodal capabilities enable processing\nof both textual and visual data, facilitating more efficient and comprehensive\nanalysis. The system dynamically adapts using intelligent search tools,\nautomated document retrieval from the internet, and real-time database updates.\nUnlike conventional approaches constrained by model context limits, MuaLLM\ndecouples retrieval from inference, enabling scalable reasoning over\narbitrarily large corpora. At the maximum context length supported by standard\nLLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining\nthe same accuracy. This allows rapid, no-human-in-the-loop database generation,\novercoming the bottleneck of simulation-based dataset creation for circuits. To\nevaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval\nand citation performance, and Reasoning-100 (Reas-100), focused on multistep\nreasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8%\naccuracy on Reas-100.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conducting a comprehensive literature review is crucial for advancing circuit\ndesign methodologies. However, the rapid influx of state-of-the-art research,\ninconsistent data representation, and the complexity of optimizing circuit\ndesign objectives make this task significantly challenging. In this paper, we\npropose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for\ncircuit design assistance that integrates a hybrid Retrieval-Augmented\nGeneration (RAG) framework with an adaptive vector database of circuit design\nresearch papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason +\nAct (ReAct) workflow for iterative reasoning, goal-setting, and multi-step\ninformation retrieval. It functions as a question-answering design assistant,\ncapable of interpreting complex queries and providing reasoned responses\ngrounded in circuit literature. Its multimodal capabilities enable processing\nof both textual and visual data, facilitating more efficient and comprehensive\nanalysis. The system dynamically adapts using intelligent search tools,\nautomated document retrieval from the internet, and real-time database updates.\nUnlike conventional approaches constrained by model context limits, MuaLLM\ndecouples retrieval from inference, enabling scalable reasoning over\narbitrarily large corpora. At the maximum context length supported by standard\nLLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining\nthe same accuracy. This allows rapid, no-human-in-the-loop database generation,\novercoming the bottleneck of simulation-based dataset creation for circuits. To\nevaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval\nand citation performance, and Reasoning-100 (Reas-100), focused on multistep\nreasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8%\naccuracy on Reas-100."
                },
                "authors": [
                    {
                        "name": "Pravallika Abbineni"
                    },
                    {
                        "name": "Saoud Aldowaish"
                    },
                    {
                        "name": "Colin Liechty"
                    },
                    {
                        "name": "Soroosh Noorzad"
                    },
                    {
                        "name": "Ali Ghazizadeh"
                    },
                    {
                        "name": "Morteza Fayazi"
                    }
                ],
                "author_detail": {
                    "name": "Morteza Fayazi"
                },
                "author": "Morteza Fayazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05592v2",
                "updated": "2025-08-11T16:10:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    56,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-07T17:32:14Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    32,
                    14,
                    3,
                    219,
                    0
                ],
                "title": "MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging\n  Synthetic Problems with a Reinforced Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging\n  Synthetic Problems with a Reinforced Policy"
                },
                "summary": "Large language models have achieved substantial progress in mathematical\nreasoning, yet their advancement is limited by the scarcity of high-quality,\nhigh-difficulty training data. Existing synthesis methods largely rely on\ntransforming human-written templates, limiting both diversity and scalability.\nWe propose MathSmith, a novel framework for synthesizing challenging\nmathematical problems to enhance LLM reasoning. Rather than modifying existing\nproblems, MathSmith constructs new ones from scratch by randomly sampling\nconcept-explanation pairs from PlanetMath, ensuring data independence and\navoiding contamination. To increase difficulty, we design nine predefined\nstrategies as soft constraints during rationales. We further adopts\nreinforcement learning to jointly optimize structural validity, reasoning\ncomplexity, and answer consistency. The length of the reasoning trace generated\nunder autoregressive prompting is used to reflect cognitive complexity,\nencouraging the creation of more demanding problems aligned with\nlong-chain-of-thought reasoning. Experiments across five benchmarks,\ncategorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,\nOlympiadBench), show that MathSmith consistently outperforms existing baselines\nunder both short and long CoT settings. Additionally, a weakness-focused\nvariant generation module enables targeted improvement on specific concepts.\nOverall, MathSmith exhibits strong scalability, generalization, and\ntransferability, highlighting the promise of high-difficulty synthetic data in\nadvancing LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved substantial progress in mathematical\nreasoning, yet their advancement is limited by the scarcity of high-quality,\nhigh-difficulty training data. Existing synthesis methods largely rely on\ntransforming human-written templates, limiting both diversity and scalability.\nWe propose MathSmith, a novel framework for synthesizing challenging\nmathematical problems to enhance LLM reasoning. Rather than modifying existing\nproblems, MathSmith constructs new ones from scratch by randomly sampling\nconcept-explanation pairs from PlanetMath, ensuring data independence and\navoiding contamination. To increase difficulty, we design nine predefined\nstrategies as soft constraints during rationales. We further adopts\nreinforcement learning to jointly optimize structural validity, reasoning\ncomplexity, and answer consistency. The length of the reasoning trace generated\nunder autoregressive prompting is used to reflect cognitive complexity,\nencouraging the creation of more demanding problems aligned with\nlong-chain-of-thought reasoning. Experiments across five benchmarks,\ncategorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,\nOlympiadBench), show that MathSmith consistently outperforms existing baselines\nunder both short and long CoT settings. Additionally, a weakness-focused\nvariant generation module enables targeted improvement on specific concepts.\nOverall, MathSmith exhibits strong scalability, generalization, and\ntransferability, highlighting the promise of high-difficulty synthetic data in\nadvancing LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Shaoxiong Zhan"
                    },
                    {
                        "name": "Yanlin Lai"
                    },
                    {
                        "name": "Ziyu Lu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Ziqing Yang"
                    },
                    {
                        "name": "Fei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Fei Tan"
                },
                "author": "Fei Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08131v1",
                "updated": "2025-08-11T16:06:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    6,
                    4,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:06:04Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    6,
                    4,
                    0,
                    223,
                    0
                ],
                "title": "Optimal Transport Regularization for Speech Text Alignment in Spoken\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Transport Regularization for Speech Text Alignment in Spoken\n  Language Models"
                },
                "summary": "Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to\nperceive speech inputs, have gained increasing attention for their potential to\nadvance speech understanding tasks. However, despite recent progress, studies\nshow that SLMs often struggle to generalize across datasets, even for trained\nlanguages and tasks, raising concerns about whether they process speech in a\ntext-like manner as intended. A key challenge underlying this limitation is the\nmodality gap between speech and text representations. The high variability in\nspeech embeddings may allow SLMs to achieve strong in-domain performance by\nexploiting unintended speech variations, ultimately hindering generalization.\nTo mitigate this modality gap, we introduce Optimal Transport Regularization\n(OTReg), a method that formulates speech-text alignment as an optimal transport\nproblem and derives a regularization loss to improve SLM training. In each\ntraining iteration, OTReg first establishes a structured correspondence between\nspeech and transcript embeddings by determining the optimal transport plan,\nthen incorporates the regularization loss based on this transport plan to\noptimize SLMs in generating speech embeddings that align more effectively with\ntranscript embeddings. OTReg is lightweight, requiring no additional labels or\nlearnable parameters, and integrates seamlessly into existing SLM training\nprocedures. Extensive multilingual ASR experiments demonstrate that OTReg\nenhances speech-text alignment, mitigates the modality gap, and consequently\nimproves SLM generalization across diverse datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to\nperceive speech inputs, have gained increasing attention for their potential to\nadvance speech understanding tasks. However, despite recent progress, studies\nshow that SLMs often struggle to generalize across datasets, even for trained\nlanguages and tasks, raising concerns about whether they process speech in a\ntext-like manner as intended. A key challenge underlying this limitation is the\nmodality gap between speech and text representations. The high variability in\nspeech embeddings may allow SLMs to achieve strong in-domain performance by\nexploiting unintended speech variations, ultimately hindering generalization.\nTo mitigate this modality gap, we introduce Optimal Transport Regularization\n(OTReg), a method that formulates speech-text alignment as an optimal transport\nproblem and derives a regularization loss to improve SLM training. In each\ntraining iteration, OTReg first establishes a structured correspondence between\nspeech and transcript embeddings by determining the optimal transport plan,\nthen incorporates the regularization loss based on this transport plan to\noptimize SLMs in generating speech embeddings that align more effectively with\ntranscript embeddings. OTReg is lightweight, requiring no additional labels or\nlearnable parameters, and integrates seamlessly into existing SLM training\nprocedures. Extensive multilingual ASR experiments demonstrate that OTReg\nenhances speech-text alignment, mitigates the modality gap, and consequently\nimproves SLM generalization across diverse datasets."
                },
                "authors": [
                    {
                        "name": "Wenze Xu"
                    },
                    {
                        "name": "Chun Wang"
                    },
                    {
                        "name": "Jiazhen Yu"
                    },
                    {
                        "name": "Sheng Chen"
                    },
                    {
                        "name": "Liang Gao"
                    },
                    {
                        "name": "Weihong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Weihong Deng"
                },
                "author": "Weihong Deng",
                "arxiv_comment": "To be presented at ACPR 2025 Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08127v1",
                "updated": "2025-08-11T16:04:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    4,
                    47,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:04:47Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    4,
                    47,
                    0,
                    223,
                    0
                ],
                "title": "BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown\n  Attacks"
                },
                "summary": "The security of LLM-based multi-agent systems (MAS) is critically threatened\nby propagation vulnerability, where malicious agents can distort collective\ndecision-making through inter-agent message interactions. While existing\nsupervised defense methods demonstrate promising performance, they may be\nimpractical in real-world scenarios due to their heavy reliance on labeled\nmalicious agents to train a supervised malicious detection model. To enable\npractical and generalizable MAS defenses, in this paper, we propose BlindGuard,\nan unsupervised defense method that learns without requiring any\nattack-specific labels or prior knowledge of malicious behaviors. To this end,\nwe establish a hierarchical agent encoder to capture individual, neighborhood,\nand global interaction patterns of each agent, providing a comprehensive\nunderstanding for malicious agent detection. Meanwhile, we design a\ncorruption-guided detector that consists of directional noise injection and\ncontrastive learning, allowing effective detection model training solely on\nnormal agent behaviors. Extensive experiments show that BlindGuard effectively\ndetects diverse attack types (i.e., prompt injection, memory poisoning, and\ntool attack) across MAS with various communication patterns while maintaining\nsuperior generalizability compared to supervised baselines. The code is\navailable at: https://github.com/MR9812/BlindGuard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security of LLM-based multi-agent systems (MAS) is critically threatened\nby propagation vulnerability, where malicious agents can distort collective\ndecision-making through inter-agent message interactions. While existing\nsupervised defense methods demonstrate promising performance, they may be\nimpractical in real-world scenarios due to their heavy reliance on labeled\nmalicious agents to train a supervised malicious detection model. To enable\npractical and generalizable MAS defenses, in this paper, we propose BlindGuard,\nan unsupervised defense method that learns without requiring any\nattack-specific labels or prior knowledge of malicious behaviors. To this end,\nwe establish a hierarchical agent encoder to capture individual, neighborhood,\nand global interaction patterns of each agent, providing a comprehensive\nunderstanding for malicious agent detection. Meanwhile, we design a\ncorruption-guided detector that consists of directional noise injection and\ncontrastive learning, allowing effective detection model training solely on\nnormal agent behaviors. Extensive experiments show that BlindGuard effectively\ndetects diverse attack types (i.e., prompt injection, memory poisoning, and\ntool attack) across MAS with various communication patterns while maintaining\nsuperior generalizability compared to supervised baselines. The code is\navailable at: https://github.com/MR9812/BlindGuard."
                },
                "authors": [
                    {
                        "name": "Rui Miao"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Yili Wang"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Yue Tan"
                    },
                    {
                        "name": "Yiwei Dai"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08120v1",
                "updated": "2025-08-11T15:59:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    59,
                    9,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:59:09Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    59,
                    9,
                    0,
                    223,
                    0
                ],
                "title": "Vision-Based Localization and LLM-based Navigation for Indoor\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Based Localization and LLM-based Navigation for Indoor\n  Environments"
                },
                "summary": "Indoor navigation remains a complex challenge due to the absence of reliable\nGPS signals and the architectural intricacies of large enclosed environments.\nThis study presents an indoor localization and navigation approach that\nintegrates vision-based localization with large language model (LLM)-based\nnavigation. The localization system utilizes a ResNet-50 convolutional neural\nnetwork fine-tuned through a two-stage process to identify the user's position\nusing smartphone camera input. To complement localization, the navigation\nmodule employs an LLM, guided by a carefully crafted system prompt, to\ninterpret preprocessed floor plan images and generate step-by-step directions.\nExperimental evaluation was conducted in a realistic office corridor with\nrepetitive features and limited visibility to test localization robustness. The\nmodel achieved high confidence and an accuracy of 96% across all tested\nwaypoints, even under constrained viewing conditions and short-duration\nqueries. Navigation tests using ChatGPT on real building floor maps yielded an\naverage instruction accuracy of 75%, with observed limitations in zero-shot\nreasoning and inference time. This research demonstrates the potential for\nscalable, infrastructure-free indoor navigation using off-the-shelf cameras and\npublicly available floor plans, particularly in resource-constrained settings\nlike hospitals, airports, and educational institutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor navigation remains a complex challenge due to the absence of reliable\nGPS signals and the architectural intricacies of large enclosed environments.\nThis study presents an indoor localization and navigation approach that\nintegrates vision-based localization with large language model (LLM)-based\nnavigation. The localization system utilizes a ResNet-50 convolutional neural\nnetwork fine-tuned through a two-stage process to identify the user's position\nusing smartphone camera input. To complement localization, the navigation\nmodule employs an LLM, guided by a carefully crafted system prompt, to\ninterpret preprocessed floor plan images and generate step-by-step directions.\nExperimental evaluation was conducted in a realistic office corridor with\nrepetitive features and limited visibility to test localization robustness. The\nmodel achieved high confidence and an accuracy of 96% across all tested\nwaypoints, even under constrained viewing conditions and short-duration\nqueries. Navigation tests using ChatGPT on real building floor maps yielded an\naverage instruction accuracy of 75%, with observed limitations in zero-shot\nreasoning and inference time. This research demonstrates the potential for\nscalable, infrastructure-free indoor navigation using off-the-shelf cameras and\npublicly available floor plans, particularly in resource-constrained settings\nlike hospitals, airports, and educational institutions."
                },
                "authors": [
                    {
                        "name": "Keyan Rahimi"
                    },
                    {
                        "name": "Md. Wasiul Haque"
                    },
                    {
                        "name": "Sagar Dasgupta"
                    },
                    {
                        "name": "Mizanur Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Mizanur Rahman"
                },
                "author": "Mizanur Rahman",
                "arxiv_comment": "20 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12856v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12856v2",
                "updated": "2025-08-11T15:57:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    57,
                    40,
                    0,
                    223,
                    0
                ],
                "published": "2024-07-09T13:15:14Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    13,
                    15,
                    14,
                    1,
                    191,
                    0
                ],
                "title": "AI-AI Bias: large language models favor communications generated by\n  large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-AI Bias: large language models favor communications generated by\n  large language models"
                },
                "summary": "Are large language models (LLMs) biased in favor of communications produced\nby LLMs, leading to possible antihuman discrimination? Using a classical\nexperimental design inspired by employment discrimination studies, we tested\nwidely used LLMs, including GPT-3.5, GPT-4 and a selection of recent\nopen-weight models in binary choice scenarios. These involved LLM-based\nassistants selecting between goods (the goods we study include consumer\nproducts, academic papers, and film-viewings) described either by humans or\nLLMs. Our results show a consistent tendency for LLM-based AIs to prefer\nLLM-presented options. This suggests the possibility of future AI systems\nimplicitly discriminating against humans as a class, giving AI agents and\nAI-assisted humans an unfair advantage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are large language models (LLMs) biased in favor of communications produced\nby LLMs, leading to possible antihuman discrimination? Using a classical\nexperimental design inspired by employment discrimination studies, we tested\nwidely used LLMs, including GPT-3.5, GPT-4 and a selection of recent\nopen-weight models in binary choice scenarios. These involved LLM-based\nassistants selecting between goods (the goods we study include consumer\nproducts, academic papers, and film-viewings) described either by humans or\nLLMs. Our results show a consistent tendency for LLM-based AIs to prefer\nLLM-presented options. This suggests the possibility of future AI systems\nimplicitly discriminating against humans as a class, giving AI agents and\nAI-assisted humans an unfair advantage."
                },
                "authors": [
                    {
                        "name": "Walter Laurito"
                    },
                    {
                        "name": "Benjamin Davis"
                    },
                    {
                        "name": "Peli Grietzer"
                    },
                    {
                        "name": "Tomáš Gavenčiak"
                    },
                    {
                        "name": "Ada Böhm"
                    },
                    {
                        "name": "Jan Kulveit"
                    }
                ],
                "author_detail": {
                    "name": "Jan Kulveit"
                },
                "author": "Jan Kulveit",
                "arxiv_doi": "10.1073/pnas.2415697122",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2415697122",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.12856v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12856v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 4 figures",
                "arxiv_journal_ref": "Proc. Natl. Acad. Sci. U.S.A. 122 (31) e2415697122 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08115v1",
                "updated": "2025-08-11T15:55:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    55,
                    6,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:55:06Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    55,
                    6,
                    0,
                    223,
                    0
                ],
                "title": "TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through\n  Structured Teamwork",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through\n  Structured Teamwork"
                },
                "summary": "We present TeamMedAgents, a novel multi-agent approach that systematically\nintegrates evidence-based teamwork components from human-human collaboration\ninto medical decision-making with large language models (LLMs). Our approach\nvalidates an organizational psychology teamwork model from human collaboration\nto computational multi-agent medical systems by operationalizing six core\nteamwork components derived from Salas et al.'s \"Big Five\" model: team\nleadership, mutual performance monitoring, team orientation, shared mental\nmodels, closed-loop communication, and mutual trust. We implement and evaluate\nthese components as modular, configurable mechanisms within an adaptive\ncollaboration architecture while assessing the effect of the number of agents\ninvolved based on the task's requirements and domain. Systematic evaluation of\ncomputational implementations of teamwork behaviors across eight medical\nbenchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,\nPath-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8\nevaluated datasets. Controlled ablation studies conducted on 50 questions per\nconfiguration across 3 independent runs provide mechanistic insights into\nindividual component contributions, revealing optimal teamwork configurations\nthat vary by reasoning task complexity and domain-specific requirements. Our\nablation analyses reveal dataset-specific optimal teamwork configurations,\nindicating that different medical reasoning modalities benefit from distinct\ncollaborative patterns. TeamMedAgents represents an advancement in\ncollaborative AI by providing a systematic translation of established teamwork\ntheories from human collaboration into agentic collaboration, establishing a\nfoundation for evidence-based multi-agent system design in critical\ndecision-making domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present TeamMedAgents, a novel multi-agent approach that systematically\nintegrates evidence-based teamwork components from human-human collaboration\ninto medical decision-making with large language models (LLMs). Our approach\nvalidates an organizational psychology teamwork model from human collaboration\nto computational multi-agent medical systems by operationalizing six core\nteamwork components derived from Salas et al.'s \"Big Five\" model: team\nleadership, mutual performance monitoring, team orientation, shared mental\nmodels, closed-loop communication, and mutual trust. We implement and evaluate\nthese components as modular, configurable mechanisms within an adaptive\ncollaboration architecture while assessing the effect of the number of agents\ninvolved based on the task's requirements and domain. Systematic evaluation of\ncomputational implementations of teamwork behaviors across eight medical\nbenchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,\nPath-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8\nevaluated datasets. Controlled ablation studies conducted on 50 questions per\nconfiguration across 3 independent runs provide mechanistic insights into\nindividual component contributions, revealing optimal teamwork configurations\nthat vary by reasoning task complexity and domain-specific requirements. Our\nablation analyses reveal dataset-specific optimal teamwork configurations,\nindicating that different medical reasoning modalities benefit from distinct\ncollaborative patterns. TeamMedAgents represents an advancement in\ncollaborative AI by providing a systematic translation of established teamwork\ntheories from human collaboration into agentic collaboration, establishing a\nfoundation for evidence-based multi-agent system design in critical\ndecision-making domains."
                },
                "authors": [
                    {
                        "name": "Pranav Pushkar Mishra"
                    },
                    {
                        "name": "Mohammad Arvan"
                    },
                    {
                        "name": "Mohan Zalake"
                    }
                ],
                "author_detail": {
                    "name": "Mohan Zalake"
                },
                "arxiv_affiliation": "University of Illinois, Chicago",
                "author": "Mohan Zalake",
                "arxiv_comment": "10 pages, 1 figure, 6 tables(2 in main, 4 in appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00485v2",
                "updated": "2025-08-11T15:49:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    49,
                    2,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-01T10:01:12Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    1,
                    12,
                    4,
                    213,
                    0
                ],
                "title": "A Frame for Communication Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Frame for Communication Control"
                },
                "summary": "We are experiencing the rise of ChatGPT-like systems or LLMs in political\nturbulent times. We assume the need to regulate their use because of their\nbubble-shaping and polarizing potential. To regulate, we need a language that\nallows interests and compromises to be discussed. In this context, we can think\nof such a shared language as a jargon, a specialized vocabulary for law-making.\nTo the extent that such a jargon exists, it is now being corrupted by LLMs.\nThis situation appears paradoxical. The issue includes persistent communication\nfailures, between disciplines that cannot translate their technical vocabulary\ninto accessible terms, and between political movements that operate in\nincompatible worldviews. We show that a frame integrating four specialist\nlanguages, those of governance, economy, community and science, is able to\naddress these failures case-wise, which we consider helpful. However, for\nreasons noted, we cannot create the more generic jargon needed on our own. We\nconclude that our frame provides the knowledge to design and apply RAG-LLM\narchitectures for researching their jargon generating potential in a future\nproject. We show its feasibility in the appendix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We are experiencing the rise of ChatGPT-like systems or LLMs in political\nturbulent times. We assume the need to regulate their use because of their\nbubble-shaping and polarizing potential. To regulate, we need a language that\nallows interests and compromises to be discussed. In this context, we can think\nof such a shared language as a jargon, a specialized vocabulary for law-making.\nTo the extent that such a jargon exists, it is now being corrupted by LLMs.\nThis situation appears paradoxical. The issue includes persistent communication\nfailures, between disciplines that cannot translate their technical vocabulary\ninto accessible terms, and between political movements that operate in\nincompatible worldviews. We show that a frame integrating four specialist\nlanguages, those of governance, economy, community and science, is able to\naddress these failures case-wise, which we consider helpful. However, for\nreasons noted, we cannot create the more generic jargon needed on our own. We\nconclude that our frame provides the knowledge to design and apply RAG-LLM\narchitectures for researching their jargon generating potential in a future\nproject. We show its feasibility in the appendix."
                },
                "authors": [
                    {
                        "name": "Aernout Schmidt"
                    },
                    {
                        "name": "Kunbei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kunbei Zhang"
                },
                "author": "Kunbei Zhang",
                "arxiv_comment": "10,216 words, 21 pages, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08101v1",
                "updated": "2025-08-11T15:40:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    40,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:40:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    40,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle\n  Conversational Agents for Safer and More Enjoyable Driving Experience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle\n  Conversational Agents for Safer and More Enjoyable Driving Experience"
                },
                "summary": "Studies on in-vehicle conversational agents have traditionally relied on\npre-scripted prompts or limited voice commands, constraining natural\ndriver-agent interaction. To resolve this issue, the present study explored the\npotential of a ChatGPT-based in-vehicle agent capable of carrying continuous,\nmulti-turn dialogues. Forty drivers participated in our experiment using a\nmotion-based driving simulator, comparing three conditions (No agent,\nPre-scripted agent, and ChatGPT-based agent) as a within-subjects variable.\nResults showed that the ChatGPT-based agent condition led to more stable\ndriving performance across multiple metrics. Participants demonstrated lower\nvariability in longitudinal acceleration, lateral acceleration, and lane\ndeviation compared to the other two conditions. In subjective evaluations, the\nChatGPT-based agent also received significantly higher ratings in competence,\nanimacy, affective trust, and preference compared to the Pre-scripted agent.\nOur thematic analysis of driver-agent conversations revealed diverse\ninteraction patterns in topics, including driving assistance/questions,\nentertainment requests, and anthropomorphic interactions. Our results highlight\nthe potential of LLM-powered in-vehicle conversational agents to enhance\ndriving safety and user experience through natural, context-rich interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studies on in-vehicle conversational agents have traditionally relied on\npre-scripted prompts or limited voice commands, constraining natural\ndriver-agent interaction. To resolve this issue, the present study explored the\npotential of a ChatGPT-based in-vehicle agent capable of carrying continuous,\nmulti-turn dialogues. Forty drivers participated in our experiment using a\nmotion-based driving simulator, comparing three conditions (No agent,\nPre-scripted agent, and ChatGPT-based agent) as a within-subjects variable.\nResults showed that the ChatGPT-based agent condition led to more stable\ndriving performance across multiple metrics. Participants demonstrated lower\nvariability in longitudinal acceleration, lateral acceleration, and lane\ndeviation compared to the other two conditions. In subjective evaluations, the\nChatGPT-based agent also received significantly higher ratings in competence,\nanimacy, affective trust, and preference compared to the Pre-scripted agent.\nOur thematic analysis of driver-agent conversations revealed diverse\ninteraction patterns in topics, including driving assistance/questions,\nentertainment requests, and anthropomorphic interactions. Our results highlight\nthe potential of LLM-powered in-vehicle conversational agents to enhance\ndriving safety and user experience through natural, context-rich interactions."
                },
                "authors": [
                    {
                        "name": "Yeana Lee Bond"
                    },
                    {
                        "name": "Mungyeong Choe"
                    },
                    {
                        "name": "Baker Kasim Hasan"
                    },
                    {
                        "name": "Arsh Siddiqui"
                    },
                    {
                        "name": "Myounghoon Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Myounghoon Jeon"
                },
                "arxiv_affiliation": "Computer Science, Virginia Tech, Blacksburg, Virginia, USA",
                "author": "Myounghoon Jeon",
                "arxiv_comment": "Submitted to International Journal of Human-Computer Studies. Bond\n  and Choe: Drafting, Review, Editing, Validation, Software, Methodology,\n  Investigation, Data Analysis, Conceptualization, Experiment training. Hasan\n  and Siddiqui: Experimental and Data Analysis Support. Jeon: Supervision,\n  Review, Resources, Project Admin, Methodology, Conceptualization. Total 34\n  pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16801v2",
                "updated": "2025-08-11T15:35:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    35,
                    42,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-23T15:20:53Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    20,
                    53,
                    2,
                    113,
                    0
                ],
                "title": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding"
                },
                "summary": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA"
                },
                "authors": [
                    {
                        "name": "Xiaoxing Hu"
                    },
                    {
                        "name": "Kaicheng Yang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Ziyong Feng"
                    },
                    {
                        "name": "Yupei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yupei Wang"
                },
                "author": "Yupei Wang",
                "arxiv_comment": "[ACM MM]2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08096v1",
                "updated": "2025-08-11T15:34:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    34,
                    49,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:34:49Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    34,
                    49,
                    0,
                    223,
                    0
                ],
                "title": "Assessing LLM Text Detection in Educational Contexts: Does Human\n  Contribution Affect Detection?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing LLM Text Detection in Educational Contexts: Does Human\n  Contribution Affect Detection?"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) and their increased\naccessibility have made it easier than ever for students to automatically\ngenerate texts, posing new challenges for educational institutions. To enforce\nnorms of academic integrity and ensure students' learning, learning analytics\nmethods to automatically detect LLM-generated text appear increasingly\nappealing. This paper benchmarks the performance of different state-of-the-art\ndetectors in educational contexts, introducing a novel dataset, called\nGenerative Essay Detection in Education (GEDE), containing over 900\nstudent-written essays and over 12,500 LLM-generated essays from various\ndomains. To capture the diversity of LLM usage practices in generating text, we\npropose the concept of contribution levels, representing students' contribution\nto a given assignment. These levels range from purely human-written texts, to\nslightly LLM-improved versions, to fully LLM-generated texts, and finally to\nactive attacks on the detector by \"humanizing\" generated texts. We show that\nmost detectors struggle to accurately classify texts of intermediate student\ncontribution levels, like LLM-improved human-written texts. Detectors are\nparticularly likely to produce false positives, which is problematic in\neducational settings where false suspicions can severely impact students'\nlives. Our dataset, code, and additional supplementary materials are publicly\navailable at\nhttps://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) and their increased\naccessibility have made it easier than ever for students to automatically\ngenerate texts, posing new challenges for educational institutions. To enforce\nnorms of academic integrity and ensure students' learning, learning analytics\nmethods to automatically detect LLM-generated text appear increasingly\nappealing. This paper benchmarks the performance of different state-of-the-art\ndetectors in educational contexts, introducing a novel dataset, called\nGenerative Essay Detection in Education (GEDE), containing over 900\nstudent-written essays and over 12,500 LLM-generated essays from various\ndomains. To capture the diversity of LLM usage practices in generating text, we\npropose the concept of contribution levels, representing students' contribution\nto a given assignment. These levels range from purely human-written texts, to\nslightly LLM-improved versions, to fully LLM-generated texts, and finally to\nactive attacks on the detector by \"humanizing\" generated texts. We show that\nmost detectors struggle to accurately classify texts of intermediate student\ncontribution levels, like LLM-improved human-written texts. Detectors are\nparticularly likely to produce false positives, which is problematic in\neducational settings where false suspicions can severely impact students'\nlives. Our dataset, code, and additional supplementary materials are publicly\navailable at\nhttps://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts."
                },
                "authors": [
                    {
                        "name": "Lukas Gehring"
                    },
                    {
                        "name": "Benjamin Paaßen"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Paaßen"
                },
                "author": "Benjamin Paaßen",
                "arxiv_comment": "Preprint as provided by the authors (19 pages, 12 figures, 9 tables)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08095v1",
                "updated": "2025-08-11T15:33:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    33,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:33:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    33,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "Dual Information Speech Language Models for Emotional Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Information Speech Language Models for Emotional Conversations"
                },
                "summary": "Conversational systems relying on text-based large language models (LLMs)\noften overlook paralinguistic cues, essential for understanding emotions and\nintentions. Speech-language models (SLMs), which use speech as input, are\nemerging as a promising solution. However, SLMs built by extending frozen LLMs\nstruggle to capture paralinguistic information and exhibit reduced context\nunderstanding. We identify entangled information and improper training\nstrategies as key issues. To address these issues, we propose two heterogeneous\nadapters and suggest a weakly supervised training strategy. Our approach\ndisentangles paralinguistic and linguistic information, enabling SLMs to\ninterpret speech through structured representations. It also preserves\ncontextual understanding by avoiding the generation of task-specific vectors\nthrough controlled randomness. This approach trains only the adapters on common\ndatasets, ensuring parameter and data efficiency. Experiments demonstrate\ncompetitive performance in emotional conversation tasks, showcasing the model's\nability to effectively integrate both paralinguistic and linguistic information\nwithin contextual settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational systems relying on text-based large language models (LLMs)\noften overlook paralinguistic cues, essential for understanding emotions and\nintentions. Speech-language models (SLMs), which use speech as input, are\nemerging as a promising solution. However, SLMs built by extending frozen LLMs\nstruggle to capture paralinguistic information and exhibit reduced context\nunderstanding. We identify entangled information and improper training\nstrategies as key issues. To address these issues, we propose two heterogeneous\nadapters and suggest a weakly supervised training strategy. Our approach\ndisentangles paralinguistic and linguistic information, enabling SLMs to\ninterpret speech through structured representations. It also preserves\ncontextual understanding by avoiding the generation of task-specific vectors\nthrough controlled randomness. This approach trains only the adapters on common\ndatasets, ensuring parameter and data efficiency. Experiments demonstrate\ncompetitive performance in emotional conversation tasks, showcasing the model's\nability to effectively integrate both paralinguistic and linguistic information\nwithin contextual settings."
                },
                "authors": [
                    {
                        "name": "Chun Wang"
                    },
                    {
                        "name": "Chenyang Liu"
                    },
                    {
                        "name": "Wenze Xu"
                    },
                    {
                        "name": "Weihong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Weihong Deng"
                },
                "author": "Weihong Deng",
                "arxiv_comment": "Presented at IEEE ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08087v1",
                "updated": "2025-08-11T15:31:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    31,
                    23,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:31:23Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    31,
                    23,
                    0,
                    223,
                    0
                ],
                "title": "Fast and Generalizable parameter-embedded Neural Operators for\n  Lithium-Ion Battery Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Generalizable parameter-embedded Neural Operators for\n  Lithium-Ion Battery Simulation"
                },
                "summary": "Reliable digital twins of lithium-ion batteries must achieve high physical\nfidelity with sub-millisecond speed. In this work, we benchmark three\noperator-learning surrogates for the Single Particle Model (SPM): Deep Operator\nNetworks (DeepONets), Fourier Neural Operators (FNOs) and a newly proposed\nparameter-embedded Fourier Neural Operator (PE-FNO), which conditions each\nspectral layer on particle radius and solid-phase diffusivity. Models are\ntrained on simulated trajectories spanning four current families (constant,\ntriangular, pulse-train, and Gaussian-random-field) and a full range of\nState-of-Charge (SOC) (0 % to 100 %). DeepONet accurately replicates\nconstant-current behaviour but struggles with more dynamic loads. The basic FNO\nmaintains mesh invariance and keeps concentration errors below 1 %, with\nvoltage mean-absolute errors under 1.7 mV across all load types. Introducing\nparameter embedding marginally increases error, but enables generalisation to\nvarying radii and diffusivities. PE-FNO executes approximately 200 times faster\nthan a 16-thread SPM solver. Consequently, PE-FNO's capabilities in inverse\ntasks are explored in a parameter estimation task with Bayesian optimisation,\nrecovering anode and cathode diffusivities with 1.14 % and 8.4 % mean absolute\npercentage error, respectively, and 0.5918 percentage points higher error in\ncomparison with classical methods. These results pave the way for neural\noperators to meet the accuracy, speed and parametric flexibility demands of\nreal-time battery management, design-of-experiments and large-scale inference.\nPE-FNO outperforms conventional neural surrogates, offering a practical path\ntowards high-speed and high-fidelity electrochemical digital twins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable digital twins of lithium-ion batteries must achieve high physical\nfidelity with sub-millisecond speed. In this work, we benchmark three\noperator-learning surrogates for the Single Particle Model (SPM): Deep Operator\nNetworks (DeepONets), Fourier Neural Operators (FNOs) and a newly proposed\nparameter-embedded Fourier Neural Operator (PE-FNO), which conditions each\nspectral layer on particle radius and solid-phase diffusivity. Models are\ntrained on simulated trajectories spanning four current families (constant,\ntriangular, pulse-train, and Gaussian-random-field) and a full range of\nState-of-Charge (SOC) (0 % to 100 %). DeepONet accurately replicates\nconstant-current behaviour but struggles with more dynamic loads. The basic FNO\nmaintains mesh invariance and keeps concentration errors below 1 %, with\nvoltage mean-absolute errors under 1.7 mV across all load types. Introducing\nparameter embedding marginally increases error, but enables generalisation to\nvarying radii and diffusivities. PE-FNO executes approximately 200 times faster\nthan a 16-thread SPM solver. Consequently, PE-FNO's capabilities in inverse\ntasks are explored in a parameter estimation task with Bayesian optimisation,\nrecovering anode and cathode diffusivities with 1.14 % and 8.4 % mean absolute\npercentage error, respectively, and 0.5918 percentage points higher error in\ncomparison with classical methods. These results pave the way for neural\noperators to meet the accuracy, speed and parametric flexibility demands of\nreal-time battery management, design-of-experiments and large-scale inference.\nPE-FNO outperforms conventional neural surrogates, offering a practical path\ntowards high-speed and high-fidelity electrochemical digital twins."
                },
                "authors": [
                    {
                        "name": "Amir Ali Panahi"
                    },
                    {
                        "name": "Daniel Luder"
                    },
                    {
                        "name": "Billy Wu"
                    },
                    {
                        "name": "Gregory Offer"
                    },
                    {
                        "name": "Dirk Uwe Sauer"
                    },
                    {
                        "name": "Weihan Li"
                    }
                ],
                "author_detail": {
                    "name": "Weihan Li"
                },
                "author": "Weihan Li",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10574v2",
                "updated": "2025-08-11T15:10:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    10,
                    59,
                    0,
                    223,
                    0
                ],
                "published": "2025-06-12T11:03:47Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    3,
                    47,
                    3,
                    163,
                    0
                ],
                "title": "DanceChat: Large Language Model-Guided Music-to-Dance Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DanceChat: Large Language Model-Guided Music-to-Dance Generation"
                },
                "summary": "Music-to-dance generation aims to synthesize human dance motion conditioned\non musical input. Despite recent progress, significant challenges remain due to\nthe semantic gap between music and dance motion, as music offers only abstract\ncues, such as melody, groove, and emotion, without explicitly specifying the\nphysical movements. Moreover, a single piece of music can produce multiple\nplausible dance interpretations. This one-to-many mapping demands additional\nguidance, as music alone provides limited information for generating diverse\ndance movements. The challenge is further amplified by the scarcity of paired\nmusic and dance data, which restricts the model\\^a\\u{A}\\'Zs ability to learn\ndiverse dance patterns. In this paper, we introduce DanceChat, a Large Language\nModel (LLM)-guided music-to-dance generation approach. We use an LLM as a\nchoreographer that provides textual motion instructions, offering explicit,\nhigh-level guidance for dance generation. This approach goes beyond implicit\nlearning from music alone, enabling the model to generate dance that is both\nmore diverse and better aligned with musical styles. Our approach consists of\nthree components: (1) an LLM-based pseudo instruction generation module that\nproduces textual dance guidance based on music style and structure, (2) a\nmulti-modal feature extraction and fusion module that integrates music, rhythm,\nand textual guidance into a shared representation, and (3) a diffusion-based\nmotion synthesis module together with a multi-modal alignment loss, which\nensures that the generated dance is aligned with both musical and textual cues.\nExtensive experiments on AIST++ and human evaluations show that DanceChat\noutperforms state-of-the-art methods both qualitatively and quantitatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Music-to-dance generation aims to synthesize human dance motion conditioned\non musical input. Despite recent progress, significant challenges remain due to\nthe semantic gap between music and dance motion, as music offers only abstract\ncues, such as melody, groove, and emotion, without explicitly specifying the\nphysical movements. Moreover, a single piece of music can produce multiple\nplausible dance interpretations. This one-to-many mapping demands additional\nguidance, as music alone provides limited information for generating diverse\ndance movements. The challenge is further amplified by the scarcity of paired\nmusic and dance data, which restricts the model\\^a\\u{A}\\'Zs ability to learn\ndiverse dance patterns. In this paper, we introduce DanceChat, a Large Language\nModel (LLM)-guided music-to-dance generation approach. We use an LLM as a\nchoreographer that provides textual motion instructions, offering explicit,\nhigh-level guidance for dance generation. This approach goes beyond implicit\nlearning from music alone, enabling the model to generate dance that is both\nmore diverse and better aligned with musical styles. Our approach consists of\nthree components: (1) an LLM-based pseudo instruction generation module that\nproduces textual dance guidance based on music style and structure, (2) a\nmulti-modal feature extraction and fusion module that integrates music, rhythm,\nand textual guidance into a shared representation, and (3) a diffusion-based\nmotion synthesis module together with a multi-modal alignment loss, which\nensures that the generated dance is aligned with both musical and textual cues.\nExtensive experiments on AIST++ and human evaluations show that DanceChat\noutperforms state-of-the-art methods both qualitatively and quantitatively."
                },
                "authors": [
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Xiaohang Yang"
                    },
                    {
                        "name": "Yilan Dong"
                    },
                    {
                        "name": "Naveen Raj Govindaraj"
                    },
                    {
                        "name": "Gregory Slabaugh"
                    },
                    {
                        "name": "Shanxin Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Shanxin Yuan"
                },
                "author": "Shanxin Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20988v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20988v2",
                "updated": "2025-08-11T15:03:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    3,
                    23,
                    0,
                    223,
                    0
                ],
                "published": "2025-02-28T12:00:51Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    0,
                    51,
                    4,
                    59,
                    0
                ],
                "title": "Reviewing Clinical Knowledge in Medical Large Language Models: Training\n  and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reviewing Clinical Knowledge in Medical Large Language Models: Training\n  and Beyond"
                },
                "summary": "The large-scale development of large language models (LLMs) in medical\ncontexts, such as diagnostic assistance and treatment recommendations,\nnecessitates that these models possess accurate medical knowledge and deliver\ntraceable decision-making processes. Clinical knowledge, encompassing the\ninsights gained from research on the causes, prognosis, diagnosis, and\ntreatment of diseases, has been extensively examined within real-world medical\npractices. Recently, there has been a notable increase in research efforts\naimed at integrating this type of knowledge into LLMs, encompassing not only\ntraditional text and multimodal data integration but also technologies such as\nknowledge graphs (KGs) and retrieval-augmented generation (RAG). In this paper,\nwe review the various initiatives to embed clinical knowledge into\ntraining-based, KG-supported, and RAG-assisted LLMs. We begin by gathering\nreliable knowledge sources from the medical domain, including databases and\ndatasets. Next, we evaluate implementations for integrating clinical knowledge\nthrough specialized datasets and collaborations with external knowledge sources\nsuch as KGs and relevant documentation. Furthermore, we discuss the\napplications of the developed medical LLMs in the industrial sector to assess\nthe disparity between models developed in academic settings and those in\nindustry. We conclude the survey by presenting evaluation systems applicable to\nrelevant tasks and identifying potential challenges facing this field. In this\nreview, we do not aim for completeness, since any ostensibly complete review\nwould soon be outdated. Our goal is to illustrate diversity by selecting\nrepresentative and accessible items from current research and industry\npractices, reflecting real-world situations rather than claiming completeness.\nThus, we emphasize showcasing diverse approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale development of large language models (LLMs) in medical\ncontexts, such as diagnostic assistance and treatment recommendations,\nnecessitates that these models possess accurate medical knowledge and deliver\ntraceable decision-making processes. Clinical knowledge, encompassing the\ninsights gained from research on the causes, prognosis, diagnosis, and\ntreatment of diseases, has been extensively examined within real-world medical\npractices. Recently, there has been a notable increase in research efforts\naimed at integrating this type of knowledge into LLMs, encompassing not only\ntraditional text and multimodal data integration but also technologies such as\nknowledge graphs (KGs) and retrieval-augmented generation (RAG). In this paper,\nwe review the various initiatives to embed clinical knowledge into\ntraining-based, KG-supported, and RAG-assisted LLMs. We begin by gathering\nreliable knowledge sources from the medical domain, including databases and\ndatasets. Next, we evaluate implementations for integrating clinical knowledge\nthrough specialized datasets and collaborations with external knowledge sources\nsuch as KGs and relevant documentation. Furthermore, we discuss the\napplications of the developed medical LLMs in the industrial sector to assess\nthe disparity between models developed in academic settings and those in\nindustry. We conclude the survey by presenting evaluation systems applicable to\nrelevant tasks and identifying potential challenges facing this field. In this\nreview, we do not aim for completeness, since any ostensibly complete review\nwould soon be outdated. Our goal is to illustrate diversity by selecting\nrepresentative and accessible items from current research and industry\npractices, reflecting real-world situations rather than claiming completeness.\nThus, we emphasize showcasing diverse approaches."
                },
                "authors": [
                    {
                        "name": "Qiyuan Li"
                    },
                    {
                        "name": "Haijiang Liu"
                    },
                    {
                        "name": "Caicai Guo"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Deyu Chen"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Frank van Harmelen"
                    },
                    {
                        "name": "Jinguang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinguang Gu"
                },
                "author": "Jinguang Gu",
                "arxiv_doi": "10.1016/j.knosys.2025.114215",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.knosys.2025.114215",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.20988v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20988v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in Knowledge-Based Systems. The arXiv\n  version is the pre-peer-review preprint, and the final published version is\n  not available here due to publisher policy",
                "arxiv_journal_ref": "Knowledge-Based Systems, 114215(2025)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08053v1",
                "updated": "2025-08-11T14:52:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    52,
                    59,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:52:59Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    52,
                    59,
                    0,
                    223,
                    0
                ],
                "title": "AdaptFlow: Adaptive Workflow Optimization via Meta-Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptFlow: Adaptive Workflow Optimization via Meta-Learning"
                },
                "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin agentic workflows, which are structured sequences of LLM invocations\nintended to solve complex tasks. However, existing approaches often rely on\nstatic templates or manually designed workflows, which limit adaptability to\ndiverse tasks and hinder scalability. We propose AdaptFlow, a natural\nlanguage-based meta-learning framework inspired by model-agnostic meta-learning\n(MAML). AdaptFlow learns a generalizable workflow initialization that enables\nrapid subtask-level adaptation. It employs a bi-level optimization scheme: the\ninner loop refines the workflow for a specific subtask using LLM-generated\nfeedback, while the outer loop updates the shared initialization to perform\nwell across tasks. This setup allows AdaptFlow to generalize effectively to\nunseen tasks by adapting the initialized workflow through language-guided\nmodifications. Evaluated across question answering, code generation, and\nmathematical reasoning benchmarks, AdaptFlow consistently outperforms both\nmanually crafted and automatically searched baselines, achieving\nstate-of-the-art results with strong generalization across tasks and models.\nThe source code and data are available at\nhttps://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have sparked growing interest\nin agentic workflows, which are structured sequences of LLM invocations\nintended to solve complex tasks. However, existing approaches often rely on\nstatic templates or manually designed workflows, which limit adaptability to\ndiverse tasks and hinder scalability. We propose AdaptFlow, a natural\nlanguage-based meta-learning framework inspired by model-agnostic meta-learning\n(MAML). AdaptFlow learns a generalizable workflow initialization that enables\nrapid subtask-level adaptation. It employs a bi-level optimization scheme: the\ninner loop refines the workflow for a specific subtask using LLM-generated\nfeedback, while the outer loop updates the shared initialization to perform\nwell across tasks. This setup allows AdaptFlow to generalize effectively to\nunseen tasks by adapting the initialized workflow through language-guided\nmodifications. Evaluated across question answering, code generation, and\nmathematical reasoning benchmarks, AdaptFlow consistently outperforms both\nmanually crafted and automatically searched baselines, achieving\nstate-of-the-art results with strong generalization across tasks and models.\nThe source code and data are available at\nhttps://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow."
                },
                "authors": [
                    {
                        "name": "Runchuan Zhu"
                    },
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Haoxiang Gao"
                    },
                    {
                        "name": "Fengshuo Bai"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08040v1",
                "updated": "2025-08-11T14:42:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    42,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:42:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    42,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning\n  in Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning\n  in Multimodal Models"
                },
                "summary": "Prompt-based tuning has emerged as a lightweight alternative to full\nfine-tuning in large vision-language models, enabling efficient adaptation via\nlearned contextual prompts. This paradigm has recently been extended to\nfederated learning settings (e.g., PromptFL), where clients collaboratively\ntrain prompts under data privacy constraints. However, the security\nimplications of prompt-based aggregation in federated multimodal learning\nremain largely unexplored, leaving a critical attack surface unaddressed. In\nthis paper, we introduce \\textbf{BadPromptFL}, the first backdoor attack\ntargeting prompt-based federated learning in multimodal contrastive models. In\nBadPromptFL, compromised clients jointly optimize local backdoor triggers and\nprompt embeddings, injecting poisoned prompts into the global aggregation\nprocess. These prompts are then propagated to benign clients, enabling\nuniversal backdoor activation at inference without modifying model parameters.\nLeveraging the contextual learning behavior of CLIP-style architectures,\nBadPromptFL achieves high attack success rates (e.g., \\(>90\\%\\)) with minimal\nvisibility and limited client participation. Extensive experiments across\nmultiple datasets and aggregation protocols validate the effectiveness,\nstealth, and generalizability of our attack, raising critical concerns about\nthe robustness of prompt-based federated learning in real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-based tuning has emerged as a lightweight alternative to full\nfine-tuning in large vision-language models, enabling efficient adaptation via\nlearned contextual prompts. This paradigm has recently been extended to\nfederated learning settings (e.g., PromptFL), where clients collaboratively\ntrain prompts under data privacy constraints. However, the security\nimplications of prompt-based aggregation in federated multimodal learning\nremain largely unexplored, leaving a critical attack surface unaddressed. In\nthis paper, we introduce \\textbf{BadPromptFL}, the first backdoor attack\ntargeting prompt-based federated learning in multimodal contrastive models. In\nBadPromptFL, compromised clients jointly optimize local backdoor triggers and\nprompt embeddings, injecting poisoned prompts into the global aggregation\nprocess. These prompts are then propagated to benign clients, enabling\nuniversal backdoor activation at inference without modifying model parameters.\nLeveraging the contextual learning behavior of CLIP-style architectures,\nBadPromptFL achieves high attack success rates (e.g., \\(>90\\%\\)) with minimal\nvisibility and limited client participation. Extensive experiments across\nmultiple datasets and aggregation protocols validate the effectiveness,\nstealth, and generalizability of our attack, raising critical concerns about\nthe robustness of prompt-based federated learning in real-world deployments."
                },
                "authors": [
                    {
                        "name": "Maozhen Zhang"
                    },
                    {
                        "name": "Mengnan Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01926v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01926v3",
                "updated": "2025-08-11T14:37:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    37,
                    48,
                    0,
                    223,
                    0
                ],
                "published": "2025-02-04T01:56:28Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    1,
                    56,
                    28,
                    1,
                    35,
                    0
                ],
                "title": "Fairness through Difference Awareness: Measuring Desired Group\n  Discrimination in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness through Difference Awareness: Measuring Desired Group\n  Discrimination in LLMs"
                },
                "summary": "Algorithmic fairness has conventionally adopted the mathematically convenient\nperspective of racial color-blindness (i.e., difference unaware treatment).\nHowever, we contend that in a range of important settings, group difference\nawareness matters. For example, differentiating between groups may be necessary\nin legal contexts (e.g., the U.S. compulsory draft applies to men but not\nwomen) and harm assessments (e.g., referring to girls as ``terrorists'' may be\nless harmful than referring to Muslim people as such). Thus, in contrast to\nmost fairness work, we study fairness through the perspective of treating\npeople differently -- when it is contextually appropriate to. We first\nintroduce an important distinction between descriptive (fact-based), normative\n(value-based), and correlation (association-based) benchmarks. This distinction\nis significant because each category requires separate interpretation and\nmitigation tailored to its specific characteristics. Then, we present a\nbenchmark suite composed of eight different scenarios for a total of 16k\nquestions that enables us to assess difference awareness. Finally, we show\nresults across ten models that demonstrate difference awareness is a distinct\ndimension to fairness where existing bias mitigation strategies may backfire.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic fairness has conventionally adopted the mathematically convenient\nperspective of racial color-blindness (i.e., difference unaware treatment).\nHowever, we contend that in a range of important settings, group difference\nawareness matters. For example, differentiating between groups may be necessary\nin legal contexts (e.g., the U.S. compulsory draft applies to men but not\nwomen) and harm assessments (e.g., referring to girls as ``terrorists'' may be\nless harmful than referring to Muslim people as such). Thus, in contrast to\nmost fairness work, we study fairness through the perspective of treating\npeople differently -- when it is contextually appropriate to. We first\nintroduce an important distinction between descriptive (fact-based), normative\n(value-based), and correlation (association-based) benchmarks. This distinction\nis significant because each category requires separate interpretation and\nmitigation tailored to its specific characteristics. Then, we present a\nbenchmark suite composed of eight different scenarios for a total of 16k\nquestions that enables us to assess difference awareness. Finally, we show\nresults across ten models that demonstrate difference awareness is a distinct\ndimension to fairness where existing bias mitigation strategies may backfire."
                },
                "authors": [
                    {
                        "name": "Angelina Wang"
                    },
                    {
                        "name": "Michelle Phan"
                    },
                    {
                        "name": "Daniel E. Ho"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    }
                ],
                "author_detail": {
                    "name": "Sanmi Koyejo"
                },
                "author": "Sanmi Koyejo",
                "arxiv_comment": "Best Paper award at ACL 2025; dataset available at\n  https://github.com/Angelina-Wang/difference_awareness",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01926v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01926v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08029v1",
                "updated": "2025-08-11T14:32:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    32,
                    43,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:32:43Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    32,
                    43,
                    0,
                    223,
                    0
                ],
                "title": "Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data\n  Manipulation Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data\n  Manipulation Attacks"
                },
                "summary": "The introduction of 5G and the Open Radio Access Network (O-RAN) architecture\nhas enabled more flexible and intelligent network deployments. However, the\nincreased complexity and openness of these architectures also introduce novel\nsecurity challenges, such as data manipulation attacks on the semi-standardised\nShared Data Layer (SDL) within the O-RAN platform through malicious xApps. In\nparticular, malicious xApps can exploit this vulnerability by introducing\nsubtle Unicode-wise alterations (hypoglyphs) into the data that are being used\nby traditional machine learning (ML)-based anomaly detection methods. These\nUnicode-wise manipulations can potentially bypass detection and cause failures\nin anomaly detection systems based on traditional ML, such as AutoEncoders,\nwhich are unable to process hypoglyphed data without crashing. We investigate\nthe use of Large Language Models (LLMs) for anomaly detection within the O-RAN\narchitecture to address this challenge. We demonstrate that LLM-based xApps\nmaintain robust operational performance and are capable of processing\nmanipulated messages without crashing. While initial detection accuracy\nrequires further improvements, our results highlight the robustness of LLMs to\nadversarial attacks such as hypoglyphs in input data. There is potential to use\ntheir adaptability through prompt engineering to further improve the accuracy,\nalthough this requires further research. Additionally, we show that LLMs\nachieve low detection latency (under 0.07 seconds), making them suitable for\nNear-Real-Time (Near-RT) RIC deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The introduction of 5G and the Open Radio Access Network (O-RAN) architecture\nhas enabled more flexible and intelligent network deployments. However, the\nincreased complexity and openness of these architectures also introduce novel\nsecurity challenges, such as data manipulation attacks on the semi-standardised\nShared Data Layer (SDL) within the O-RAN platform through malicious xApps. In\nparticular, malicious xApps can exploit this vulnerability by introducing\nsubtle Unicode-wise alterations (hypoglyphs) into the data that are being used\nby traditional machine learning (ML)-based anomaly detection methods. These\nUnicode-wise manipulations can potentially bypass detection and cause failures\nin anomaly detection systems based on traditional ML, such as AutoEncoders,\nwhich are unable to process hypoglyphed data without crashing. We investigate\nthe use of Large Language Models (LLMs) for anomaly detection within the O-RAN\narchitecture to address this challenge. We demonstrate that LLM-based xApps\nmaintain robust operational performance and are capable of processing\nmanipulated messages without crashing. While initial detection accuracy\nrequires further improvements, our results highlight the robustness of LLMs to\nadversarial attacks such as hypoglyphs in input data. There is potential to use\ntheir adaptability through prompt engineering to further improve the accuracy,\nalthough this requires further research. Additionally, we show that LLMs\nachieve low detection latency (under 0.07 seconds), making them suitable for\nNear-Real-Time (Near-RT) RIC deployments."
                },
                "authors": [
                    {
                        "name": "Thusitha Dayaratne"
                    },
                    {
                        "name": "Ngoc Duy Pham"
                    },
                    {
                        "name": "Viet Vo"
                    },
                    {
                        "name": "Shangqi Lai"
                    },
                    {
                        "name": "Sharif Abuadbba"
                    },
                    {
                        "name": "Hajime Suzuki"
                    },
                    {
                        "name": "Xingliang Yuan"
                    },
                    {
                        "name": "Carsten Rudolph"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Rudolph"
                },
                "author": "Carsten Rudolph",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21051v3",
                "updated": "2025-08-11T14:32:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    32,
                    1,
                    0,
                    223,
                    0
                ],
                "published": "2024-12-30T16:09:28Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    9,
                    28,
                    0,
                    365,
                    0
                ],
                "title": "Toward Intelligent and Secure Cloud: Large Language Model Empowered\n  Proactive Defense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Intelligent and Secure Cloud: Large Language Model Empowered\n  Proactive Defense"
                },
                "summary": "The rapid evolution of cloud computing technologies and the increasing number\nof cloud applications have provided numerous benefits in our daily lives.\nHowever, the diversity and complexity of different components pose a\nsignificant challenge to cloud security, especially when dealing with\nsophisticated and advanced cyberattacks such as Denial of Service (DoS). Recent\nadvancements in the large language models (LLMs) offer promising solutions for\nsecurity intelligence. By exploiting the powerful capabilities in language\nunderstanding, data analysis, task inference, action planning, and code\ngeneration, we present LLM-PD, a novel defense architecture that proactively\nmitigates various DoS threats in cloud networks. LLM-PD can efficiently make\ndecisions through comprehensive data analysis and sequential reasoning, as well\nas dynamically create and deploy actionable defense mechanisms. Furthermore, it\ncan flexibly self-evolve based on experience learned from previous interactions\nand adapt to new attack scenarios without additional training. Our case study\non three distinct DoS attacks demonstrates its remarkable ability in terms of\ndefense effectiveness and efficiency when compared with other existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of cloud computing technologies and the increasing number\nof cloud applications have provided numerous benefits in our daily lives.\nHowever, the diversity and complexity of different components pose a\nsignificant challenge to cloud security, especially when dealing with\nsophisticated and advanced cyberattacks such as Denial of Service (DoS). Recent\nadvancements in the large language models (LLMs) offer promising solutions for\nsecurity intelligence. By exploiting the powerful capabilities in language\nunderstanding, data analysis, task inference, action planning, and code\ngeneration, we present LLM-PD, a novel defense architecture that proactively\nmitigates various DoS threats in cloud networks. LLM-PD can efficiently make\ndecisions through comprehensive data analysis and sequential reasoning, as well\nas dynamically create and deploy actionable defense mechanisms. Furthermore, it\ncan flexibly self-evolve based on experience learned from previous interactions\nand adapt to new attack scenarios without additional training. Our case study\non three distinct DoS attacks demonstrates its remarkable ability in terms of\ndefense effectiveness and efficiency when compared with other existing methods."
                },
                "authors": [
                    {
                        "name": "Yuyang Zhou"
                    },
                    {
                        "name": "Guang Cheng"
                    },
                    {
                        "name": "Kang Du"
                    },
                    {
                        "name": "Zihan Chen"
                    },
                    {
                        "name": "Yuyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Zhao"
                },
                "author": "Yuyu Zhao",
                "arxiv_comment": "7 pages; Major Revision for IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08027v1",
                "updated": "2025-08-11T14:31:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    31,
                    20,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:31:20Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    31,
                    20,
                    0,
                    223,
                    0
                ],
                "title": "Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking\n  Self-Supervised and Generative Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking\n  Self-Supervised and Generative Approaches"
                },
                "summary": "Speech Recognition (ASR) due to phoneme distortions and high variability.\nWhile self-supervised ASR models like Wav2Vec, HuBERT, and Whisper have shown\npromise, their effectiveness in dysarthric speech remains unclear. This study\nsystematically benchmarks these models with different decoding strategies,\nincluding CTC, seq2seq, and LLM-enhanced decoding (BART,GPT-2, Vicuna). Our\ncontributions include (1) benchmarking ASR architectures for dysarthric speech,\n(2) introducing LLM-based decoding to improve intelligibility, (3) analyzing\ngeneralization across datasets, and (4) providing insights into recognition\nerrors across severity levels. Findings highlight that LLM-enhanced decoding\nimproves dysarthric ASR by leveraging linguistic constraints for phoneme\nrestoration and grammatical correction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Recognition (ASR) due to phoneme distortions and high variability.\nWhile self-supervised ASR models like Wav2Vec, HuBERT, and Whisper have shown\npromise, their effectiveness in dysarthric speech remains unclear. This study\nsystematically benchmarks these models with different decoding strategies,\nincluding CTC, seq2seq, and LLM-enhanced decoding (BART,GPT-2, Vicuna). Our\ncontributions include (1) benchmarking ASR architectures for dysarthric speech,\n(2) introducing LLM-based decoding to improve intelligibility, (3) analyzing\ngeneralization across datasets, and (4) providing insights into recognition\nerrors across severity levels. Findings highlight that LLM-enhanced decoding\nimproves dysarthric ASR by leveraging linguistic constraints for phoneme\nrestoration and grammatical correction."
                },
                "authors": [
                    {
                        "name": "Ahmed Aboeitta"
                    },
                    {
                        "name": "Ahmed Sharshar"
                    },
                    {
                        "name": "Youssef Nafea"
                    },
                    {
                        "name": "Shady Shehata"
                    }
                ],
                "author_detail": {
                    "name": "Shady Shehata"
                },
                "author": "Shady Shehata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08020v1",
                "updated": "2025-08-11T14:26:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    26,
                    36,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:26:36Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    26,
                    36,
                    0,
                    223,
                    0
                ],
                "title": "EchoAid: Enhancing Livestream Shopping Accessibility for the DHH\n  Community",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoAid: Enhancing Livestream Shopping Accessibility for the DHH\n  Community"
                },
                "summary": "Livestream shopping platforms often overlook the accessibility needs of the\nDeaf and Hard of Hearing (DHH) community, leading to barriers such as\ninformation inaccessibility and overload. To tackle these challenges, we\ndeveloped \\textit{EchoAid}, a mobile app designed to improve the livestream\nshopping experience for DHH users. \\textit{EchoAid} utilizes advanced\nspeech-to-text conversion, Rapid Serial Visual Presentation (RSVP) technology,\nand Large Language Models (LLMs) to simplify the complex information flow in\nlive sales environments. We conducted exploratory studies with eight DHH\nindividuals to identify design needs and iteratively developed the\n\\textit{EchoAid} prototype based on feedback from three participants. We then\nevaluate the performance of this system in a user study workshop involving 38\nDHH participants. Our findings demonstrate the successful design and validation\nprocess of \\textit{EchoAid}, highlighting its potential to enhance product\ninformation extraction, leading to reduced cognitive overload and more engaging\nand customized shopping experiences for DHH users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Livestream shopping platforms often overlook the accessibility needs of the\nDeaf and Hard of Hearing (DHH) community, leading to barriers such as\ninformation inaccessibility and overload. To tackle these challenges, we\ndeveloped \\textit{EchoAid}, a mobile app designed to improve the livestream\nshopping experience for DHH users. \\textit{EchoAid} utilizes advanced\nspeech-to-text conversion, Rapid Serial Visual Presentation (RSVP) technology,\nand Large Language Models (LLMs) to simplify the complex information flow in\nlive sales environments. We conducted exploratory studies with eight DHH\nindividuals to identify design needs and iteratively developed the\n\\textit{EchoAid} prototype based on feedback from three participants. We then\nevaluate the performance of this system in a user study workshop involving 38\nDHH participants. Our findings demonstrate the successful design and validation\nprocess of \\textit{EchoAid}, highlighting its potential to enhance product\ninformation extraction, leading to reduced cognitive overload and more engaging\nand customized shopping experiences for DHH users."
                },
                "authors": [
                    {
                        "name": "Zeyu Yang"
                    },
                    {
                        "name": "Zheng Wei"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Xian Xu"
                    },
                    {
                        "name": "Changyang He"
                    },
                    {
                        "name": "Muzhi Zhou"
                    },
                    {
                        "name": "Pan Hui"
                    }
                ],
                "author_detail": {
                    "name": "Pan Hui"
                },
                "author": "Pan Hui",
                "arxiv_doi": "10.1145/3757471",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3757471",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.08020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Paper for CSCW 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08011v1",
                "updated": "2025-08-11T14:15:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    15,
                    33,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:15:33Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    15,
                    33,
                    0,
                    223,
                    0
                ],
                "title": "Progressive Depth Up-scaling via Optimal Transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Depth Up-scaling via Optimal Transport"
                },
                "summary": "Scaling Large Language Models (LLMs) yields performance gains but incurs\nsubstantial training costs. Depth up-scaling offers training efficiency by\nadding new layers to pre-trained models. However, most existing methods copy or\naverage weights from base layers, neglecting neuron permutation differences.\nThis limitation can potentially cause misalignment that harms performance.\nInspired by applying Optimal Transport (OT) for neuron alignment, we propose\nOptimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses\nTransformer blocks in adjacent base layers via OT for new layer creation, to\nmitigate neuron permutation mismatch between layers. OpT-DeUS achieves better\noverall performance and offers improved training efficiency than existing\nmethods for continual pre-training and supervised fine-tuning across different\nmodel sizes. To further evaluate the impact of interpolation positions, our\nextensive analysis shows that inserting new layers closer to the top results in\nhigher training efficiency due to shorter back-propagation time while obtaining\nadditional performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Large Language Models (LLMs) yields performance gains but incurs\nsubstantial training costs. Depth up-scaling offers training efficiency by\nadding new layers to pre-trained models. However, most existing methods copy or\naverage weights from base layers, neglecting neuron permutation differences.\nThis limitation can potentially cause misalignment that harms performance.\nInspired by applying Optimal Transport (OT) for neuron alignment, we propose\nOptimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses\nTransformer blocks in adjacent base layers via OT for new layer creation, to\nmitigate neuron permutation mismatch between layers. OpT-DeUS achieves better\noverall performance and offers improved training efficiency than existing\nmethods for continual pre-training and supervised fine-tuning across different\nmodel sizes. To further evaluate the impact of interpolation positions, our\nextensive analysis shows that inserting new layers closer to the top results in\nhigher training efficiency due to shorter back-propagation time while obtaining\nadditional performance gains."
                },
                "authors": [
                    {
                        "name": "Mingzi Cao"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18938v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18938v2",
                "updated": "2025-08-11T14:10:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    10,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-26T14:48:44Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    14,
                    48,
                    44,
                    5,
                    116,
                    0
                ],
                "title": "RAIR: Retrieval-Augmented Iterative Refinement for Chinese Spelling\n  Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAIR: Retrieval-Augmented Iterative Refinement for Chinese Spelling\n  Correction"
                },
                "summary": "Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens\nin sentences. Traditional CSC focuses on equal length correction and uses\npretrained language models (PLMs). While Large Language Models (LLMs) have\nshown remarkable success in identifying and rectifying potential errors, they\noften struggle with adapting to domain-specific corrections, especially when\nencountering terminologies in specialized domains. To address domain\nadaptation, we propose a \\textbf{R}etrieval-\\textbf{A}ugmented\n\\textbf{I}terative \\textbf{R}efinement (RAIR) framework. Our approach\nconstructs a retrieval corpus adaptively from domain-specific training data and\ndictionaries, employing a fine-tuned retriever to ensure that the retriever\ncatches the error correction pattern. We also extend equal-length into\nvariable-length correction scenarios. Extensive experiments demonstrate that\nour framework outperforms current approaches in domain spelling correction and\nsignificantly improves the performance of LLMs in variable-length scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens\nin sentences. Traditional CSC focuses on equal length correction and uses\npretrained language models (PLMs). While Large Language Models (LLMs) have\nshown remarkable success in identifying and rectifying potential errors, they\noften struggle with adapting to domain-specific corrections, especially when\nencountering terminologies in specialized domains. To address domain\nadaptation, we propose a \\textbf{R}etrieval-\\textbf{A}ugmented\n\\textbf{I}terative \\textbf{R}efinement (RAIR) framework. Our approach\nconstructs a retrieval corpus adaptively from domain-specific training data and\ndictionaries, employing a fine-tuned retriever to ensure that the retriever\ncatches the error correction pattern. We also extend equal-length into\nvariable-length correction scenarios. Extensive experiments demonstrate that\nour framework outperforms current approaches in domain spelling correction and\nsignificantly improves the performance of LLMs in variable-length scenarios."
                },
                "authors": [
                    {
                        "name": "Junhong Liang"
                    },
                    {
                        "name": "Yu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhou"
                },
                "author": "Yu Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18938v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18938v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08002v1",
                "updated": "2025-08-11T14:07:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    7,
                    1,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:07:01Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    7,
                    1,
                    0,
                    223,
                    0
                ],
                "title": "A Physics-informed Deep Operator for Real-Time Freeway Traffic State\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Physics-informed Deep Operator for Real-Time Freeway Traffic State\n  Estimation"
                },
                "summary": "Traffic state estimation (TSE) falls methodologically into three categories:\nmodel-driven, data-driven, and model-data dual-driven. Model-driven TSE relies\non macroscopic traffic flow models originated from hydrodynamics. Data-driven\nTSE leverages historical sensing data and employs statistical models or machine\nlearning methods to infer traffic state. Model-data dual-driven traffic state\nestimation attempts to harness the strengths of both aspects to achieve more\naccurate TSE. From the perspective of mathematical operator theory, TSE can be\nviewed as a type of operator that maps available measurements of inerested\ntraffic state into unmeasured traffic state variables in real time. For the\nfirst time this paper proposes to study real-time freeway TSE in the idea of\nphysics-informed deep operator network (PI-DeepONet), which is an\noperator-oriented architecture embedding traffic flow models based on deep\nneural networks. The paper has developed an extended architecture from the\noriginal PI-DeepONet. The extended architecture is featured with: (1) the\nacceptance of 2-D data input so as to support CNN-based computations; (2) the\nintroduction of a nonlinear expansion layer, an attention mechanism, and a MIMO\nmechanism; (3) dedicated neural network design for adaptive identification of\ntraffic flow model parameters. A traffic state estimator built on the basis of\nthis extended PI-DeepONet architecture was evaluated with respect to a short\nfreeway stretch of NGSIM and a large-scale urban expressway in China, along\nwith other four baseline TSE methods. The evaluation results demonstrated that\nthis novel TSE method outperformed the baseline methods with high-precision\nestimation results of flow and mean speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic state estimation (TSE) falls methodologically into three categories:\nmodel-driven, data-driven, and model-data dual-driven. Model-driven TSE relies\non macroscopic traffic flow models originated from hydrodynamics. Data-driven\nTSE leverages historical sensing data and employs statistical models or machine\nlearning methods to infer traffic state. Model-data dual-driven traffic state\nestimation attempts to harness the strengths of both aspects to achieve more\naccurate TSE. From the perspective of mathematical operator theory, TSE can be\nviewed as a type of operator that maps available measurements of inerested\ntraffic state into unmeasured traffic state variables in real time. For the\nfirst time this paper proposes to study real-time freeway TSE in the idea of\nphysics-informed deep operator network (PI-DeepONet), which is an\noperator-oriented architecture embedding traffic flow models based on deep\nneural networks. The paper has developed an extended architecture from the\noriginal PI-DeepONet. The extended architecture is featured with: (1) the\nacceptance of 2-D data input so as to support CNN-based computations; (2) the\nintroduction of a nonlinear expansion layer, an attention mechanism, and a MIMO\nmechanism; (3) dedicated neural network design for adaptive identification of\ntraffic flow model parameters. A traffic state estimator built on the basis of\nthis extended PI-DeepONet architecture was evaluated with respect to a short\nfreeway stretch of NGSIM and a large-scale urban expressway in China, along\nwith other four baseline TSE methods. The evaluation results demonstrated that\nthis novel TSE method outperformed the baseline methods with high-precision\nestimation results of flow and mean speed."
                },
                "authors": [
                    {
                        "name": "Hongxin Yu"
                    },
                    {
                        "name": "Yibing Wang"
                    },
                    {
                        "name": "Fengyue Jin"
                    },
                    {
                        "name": "Meng Zhang"
                    },
                    {
                        "name": "Anni Chen"
                    }
                ],
                "author_detail": {
                    "name": "Anni Chen"
                },
                "author": "Anni Chen",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08001v2",
                "updated": "2025-08-12T04:42:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    4,
                    42,
                    34,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T14:04:59Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    4,
                    59,
                    0,
                    223,
                    0
                ],
                "title": "Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware\n  Framework Guided by Monetary Policy Transmission Paths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware\n  Framework Guided by Monetary Policy Transmission Paths"
                },
                "summary": "\"Fedspeak\", the stylized and often nuanced language used by the U.S. Federal\nReserve, encodes implicit policy signals and strategic stances. The Federal\nOpen Market Committee strategically employs Fedspeak as a communication tool to\nshape market expectations and influence both domestic and global economic\nconditions. As such, automatically parsing and interpreting Fedspeak presents a\nhigh-impact challenge, with significant implications for financial forecasting,\nalgorithmic trading, and data-driven policy analysis. In this paper, we propose\nan LLM-based, uncertainty-aware framework for deciphering Fedspeak and\nclassifying its underlying monetary policy stance. Technically, to enrich the\nsemantic and contextual representation of Fedspeak texts, we incorporate\ndomain-specific reasoning grounded in the monetary policy transmission\nmechanism. We further introduce a dynamic uncertainty decoding module to assess\nthe confidence of model predictions, thereby enhancing both classification\naccuracy and model reliability. Experimental results demonstrate that our\nframework achieves state-of-the-art performance on the policy stance analysis\ntask. Moreover, statistical analysis reveals a significant positive correlation\nbetween perceptual uncertainty and model error rates, validating the\neffectiveness of perceptual uncertainty as a diagnostic signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Fedspeak\", the stylized and often nuanced language used by the U.S. Federal\nReserve, encodes implicit policy signals and strategic stances. The Federal\nOpen Market Committee strategically employs Fedspeak as a communication tool to\nshape market expectations and influence both domestic and global economic\nconditions. As such, automatically parsing and interpreting Fedspeak presents a\nhigh-impact challenge, with significant implications for financial forecasting,\nalgorithmic trading, and data-driven policy analysis. In this paper, we propose\nan LLM-based, uncertainty-aware framework for deciphering Fedspeak and\nclassifying its underlying monetary policy stance. Technically, to enrich the\nsemantic and contextual representation of Fedspeak texts, we incorporate\ndomain-specific reasoning grounded in the monetary policy transmission\nmechanism. We further introduce a dynamic uncertainty decoding module to assess\nthe confidence of model predictions, thereby enhancing both classification\naccuracy and model reliability. Experimental results demonstrate that our\nframework achieves state-of-the-art performance on the policy stance analysis\ntask. Moreover, statistical analysis reveals a significant positive correlation\nbetween perceptual uncertainty and model error rates, validating the\neffectiveness of perceptual uncertainty as a diagnostic signal."
                },
                "authors": [
                    {
                        "name": "Rui Yao"
                    },
                    {
                        "name": "Qi Chai"
                    },
                    {
                        "name": "Jinhai Yao"
                    },
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07999v1",
                "updated": "2025-08-11T14:03:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    3,
                    9,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:03:09Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    3,
                    9,
                    0,
                    223,
                    0
                ],
                "title": "WideSearch: Benchmarking Agentic Broad Info-Seeking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WideSearch: Benchmarking Agentic Broad Info-Seeking"
                },
                "summary": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/"
                },
                "authors": [
                    {
                        "name": "Ryan Wong"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Junjie Zhao"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Xuan Zhou"
                    },
                    {
                        "name": "Zuo Wang"
                    },
                    {
                        "name": "Kai Xiang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Ke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Wang"
                },
                "author": "Ke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07996v1",
                "updated": "2025-08-11T13:59:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    59,
                    22,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T13:59:22Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    59,
                    22,
                    0,
                    223,
                    0
                ],
                "title": "Prompt-Guided Relational Reasoning for Social Behavior Understanding\n  with Vision Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Guided Relational Reasoning for Social Behavior Understanding\n  with Vision Foundation Models"
                },
                "summary": "Group Activity Detection (GAD) involves recognizing social groups and their\ncollective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2,\noffer excellent features, but are pretrained primarily on object-centric data\nand remain underexplored for modeling group dynamics. While they are a\npromising alternative to highly task-specific GAD architectures that require\nfull fine-tuning, our initial investigation reveals that simply swapping CNN\nbackbones used in these methods with VFMs brings little gain, underscoring the\nneed for structured, group-aware reasoning on top.\n  We introduce Prompt-driven Group Activity Detection (ProGraD) -- a method\nthat bridges this gap through 1) learnable group prompts to guide the VFM\nattention toward social configurations, and 2) a lightweight two-layer\nGroupContext Transformer that infers actor-group associations and collective\nbehavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which\nfeatures multiple concurrent social groups, and Social-CAD, which focuses on\nsingle-group interactions. While we surpass state-of-the-art in both settings,\nour method is especially effective in complex multi-group scenarios, where we\nyield a gain of 6.5\\% (Group mAP\\@1.0) and 8.2\\% (Group mAP\\@0.5) using only\n10M trainable parameters. Furthermore, our experiments reveal that ProGraD\nproduces interpretable attention maps, offering insights into actor-group\nreasoning. Code and models will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Activity Detection (GAD) involves recognizing social groups and their\ncollective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2,\noffer excellent features, but are pretrained primarily on object-centric data\nand remain underexplored for modeling group dynamics. While they are a\npromising alternative to highly task-specific GAD architectures that require\nfull fine-tuning, our initial investigation reveals that simply swapping CNN\nbackbones used in these methods with VFMs brings little gain, underscoring the\nneed for structured, group-aware reasoning on top.\n  We introduce Prompt-driven Group Activity Detection (ProGraD) -- a method\nthat bridges this gap through 1) learnable group prompts to guide the VFM\nattention toward social configurations, and 2) a lightweight two-layer\nGroupContext Transformer that infers actor-group associations and collective\nbehavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which\nfeatures multiple concurrent social groups, and Social-CAD, which focuses on\nsingle-group interactions. While we surpass state-of-the-art in both settings,\nour method is especially effective in complex multi-group scenarios, where we\nyield a gain of 6.5\\% (Group mAP\\@1.0) and 8.2\\% (Group mAP\\@0.5) using only\n10M trainable parameters. Furthermore, our experiments reveal that ProGraD\nproduces interpretable attention maps, offering insights into actor-group\nreasoning. Code and models will be released."
                },
                "authors": [
                    {
                        "name": "Thinesh Thiyakesan Ponbagavathi"
                    },
                    {
                        "name": "Chengzheng Yang"
                    },
                    {
                        "name": "Alina Roitberg"
                    }
                ],
                "author_detail": {
                    "name": "Alina Roitberg"
                },
                "author": "Alina Roitberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07995v2",
                "updated": "2025-08-12T13:46:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    46,
                    8,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T13:57:49Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    57,
                    49,
                    0,
                    223,
                    0
                ],
                "title": "DIVER: A Multi-Stage Approach for Reasoning-intensive Information\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIVER: A Multi-Stage Approach for Reasoning-intensive Information\n  Retrieval"
                },
                "summary": "Retrieval-augmented generation has achieved strong performance on\nknowledge-intensive tasks where query-document relevance can be identified\nthrough direct lexical or semantic matches. However, many real-world queries\ninvolve abstract reasoning, analogical thinking, or multi-step inference, which\nexisting retrievers often struggle to capture. To address this challenge, we\npresent \\textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive\ninformation retrieval. DIVER consists of four components: document processing\nto improve input quality, LLM-driven query expansion via iterative document\ninteraction, a reasoning-enhanced retriever fine-tuned on synthetic\nmulti-domain data with hard negatives, and a pointwise reranker that combines\nLLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,\nDIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original\nqueries, consistently outperforming competitive reasoning-aware models. These\nresults demonstrate the effectiveness of reasoning-aware retrieval strategies\nin complex real-world tasks. Our code and retrieval model will be released\nsoon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation has achieved strong performance on\nknowledge-intensive tasks where query-document relevance can be identified\nthrough direct lexical or semantic matches. However, many real-world queries\ninvolve abstract reasoning, analogical thinking, or multi-step inference, which\nexisting retrievers often struggle to capture. To address this challenge, we\npresent \\textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive\ninformation retrieval. DIVER consists of four components: document processing\nto improve input quality, LLM-driven query expansion via iterative document\ninteraction, a reasoning-enhanced retriever fine-tuned on synthetic\nmulti-domain data with hard negatives, and a pointwise reranker that combines\nLLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,\nDIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original\nqueries, consistently outperforming competitive reasoning-aware models. These\nresults demonstrate the effectiveness of reasoning-aware retrieval strategies\nin complex real-world tasks. Our code and retrieval model will be released\nsoon."
                },
                "authors": [
                    {
                        "name": "Meixiu Long"
                    },
                    {
                        "name": "Duolin Sun"
                    },
                    {
                        "name": "Dan Yang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Jiahai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiahai Wang"
                },
                "author": "Jiahai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07988v1",
                "updated": "2025-08-11T13:52:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    52,
                    37,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T13:52:37Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    52,
                    37,
                    0,
                    223,
                    0
                ],
                "title": "A Data-constrained Magnetohydrodynamic Simulation of Successive X-class\n  Flares in Solar Active Region 13842. II. Dynamics of the Solar Eruption\n  Associated with the X9.0 Solar Flare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-constrained Magnetohydrodynamic Simulation of Successive X-class\n  Flares in Solar Active Region 13842. II. Dynamics of the Solar Eruption\n  Associated with the X9.0 Solar Flare"
                },
                "summary": "Active region NOAA 13842 produced two successive solar flares: an X7.1-class\nflare on October 1, 2024, and an X9.0-class flare on October 3, 2024. This\nstudy continues our previous simulation work that successfully reproduced the\nX7.1-class solar flare (Matsumoto et al. 2025). In this study, we performed a\ndata-constrained magnetohydrodynamic (MHD) simulation using the nonlinear\nforce-free field (NLFFF) as the initial condition to investigate the X9.0-class\nsolar flare. The NLFFF showed the sheared field lines, resulting in the\ntether-cutting reconnection, the magnetic flux ropes (MFRs), and eventually led\nto eruption. The magnetic reconnection during the pre-eruption phase plays a\ncritical role in accelerating the subsequent eruption, which is driven by torus\ninstability and magnetic reconnection. Furthermore, our simulation results are\nconsistent with several observational features associated with the X9.0 flare.\nThis simulation could reproduce diverse phenomena associated with the X9.0\nflare, including the tether-cutting reconnection, the flare ribbons and the\nflare loops, the transverse field enhancement, and the remote brightening away\nfrom the flare ribbons. However, the initial trigger, magnetic flux emergence,\nwas inferred from observations rather than explicitly modeled, and future\ncomprehensive simulations should incorporate this mechanism directly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active region NOAA 13842 produced two successive solar flares: an X7.1-class\nflare on October 1, 2024, and an X9.0-class flare on October 3, 2024. This\nstudy continues our previous simulation work that successfully reproduced the\nX7.1-class solar flare (Matsumoto et al. 2025). In this study, we performed a\ndata-constrained magnetohydrodynamic (MHD) simulation using the nonlinear\nforce-free field (NLFFF) as the initial condition to investigate the X9.0-class\nsolar flare. The NLFFF showed the sheared field lines, resulting in the\ntether-cutting reconnection, the magnetic flux ropes (MFRs), and eventually led\nto eruption. The magnetic reconnection during the pre-eruption phase plays a\ncritical role in accelerating the subsequent eruption, which is driven by torus\ninstability and magnetic reconnection. Furthermore, our simulation results are\nconsistent with several observational features associated with the X9.0 flare.\nThis simulation could reproduce diverse phenomena associated with the X9.0\nflare, including the tether-cutting reconnection, the flare ribbons and the\nflare loops, the transverse field enhancement, and the remote brightening away\nfrom the flare ribbons. However, the initial trigger, magnetic flux emergence,\nwas inferred from observations rather than explicitly modeled, and future\ncomprehensive simulations should incorporate this mechanism directly."
                },
                "authors": [
                    {
                        "name": "Keitarou Matsumoto"
                    },
                    {
                        "name": "Satoshi Inoue"
                    },
                    {
                        "name": "Keiji Hayashi"
                    },
                    {
                        "name": "Nian Liu"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Jeongwoo Lee"
                    },
                    {
                        "name": "Ju Jing"
                    },
                    {
                        "name": "Haimin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haimin Wang"
                },
                "author": "Haimin Wang",
                "arxiv_comment": "10 Figures. Accepted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17001v2",
                "updated": "2025-08-11T13:41:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    41,
                    39,
                    0,
                    223,
                    0
                ],
                "published": "2025-06-20T13:52:15Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    52,
                    15,
                    4,
                    171,
                    0
                ],
                "title": "PersonalAI: A Systematic Comparison of Knowledge Graph Storage and\n  Retrieval Approaches for Personalized LLM agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonalAI: A Systematic Comparison of Knowledge Graph Storage and\n  Retrieval Approaches for Personalized LLM agents"
                },
                "summary": "Personalizing language models by effectively incorporating user interaction\nhistory remains a central challenge in the development of adaptive AI systems.\nWhile large language models (LLMs) combined with Retrieval-Augmented Generation\n(RAG) have improved factual accuracy, they often lack structured memory and\nfail to scale in complex, long-term interactions. To address this, we propose a\nflexible external memory framework based on knowledge graphs, automatically\nconstructed and updated by the LLM itself, and capable of encoding information\nin multiple formats-including nodes, triplets, higher-order propositions, and\nepisodic traces. Building upon the AriGraph architecture, we introduce a novel\nhybrid graph design that supports both standard edges and two types of\nhyperedges, enabling rich and dynamic semantic and temporal representations.\nOur framework also supports diverse retrieval mechanisms, including A*,\nwater-circle propagation, beam search, and hybrid methods, making it adaptable\nto different datasets and LLM capacities. We evaluate our system on three\nbenchmarks-TriviaQA, HotpotQA, and DiaASQ-demonstrating that different memory\nand retrieval configurations yield optimal performance depending on the task.\nAdditionally, we extend the DiaASQ benchmark with temporal annotations and\ninternally contradictory statements, showing that our system remains robust and\neffective in managing temporal dependencies and context-aware reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalizing language models by effectively incorporating user interaction\nhistory remains a central challenge in the development of adaptive AI systems.\nWhile large language models (LLMs) combined with Retrieval-Augmented Generation\n(RAG) have improved factual accuracy, they often lack structured memory and\nfail to scale in complex, long-term interactions. To address this, we propose a\nflexible external memory framework based on knowledge graphs, automatically\nconstructed and updated by the LLM itself, and capable of encoding information\nin multiple formats-including nodes, triplets, higher-order propositions, and\nepisodic traces. Building upon the AriGraph architecture, we introduce a novel\nhybrid graph design that supports both standard edges and two types of\nhyperedges, enabling rich and dynamic semantic and temporal representations.\nOur framework also supports diverse retrieval mechanisms, including A*,\nwater-circle propagation, beam search, and hybrid methods, making it adaptable\nto different datasets and LLM capacities. We evaluate our system on three\nbenchmarks-TriviaQA, HotpotQA, and DiaASQ-demonstrating that different memory\nand retrieval configurations yield optimal performance depending on the task.\nAdditionally, we extend the DiaASQ benchmark with temporal annotations and\ninternally contradictory statements, showing that our system remains robust and\neffective in managing temporal dependencies and context-aware reasoning."
                },
                "authors": [
                    {
                        "name": "Mikhail Menschikov"
                    },
                    {
                        "name": "Dmitry Evseev"
                    },
                    {
                        "name": "Victoria Dochkina"
                    },
                    {
                        "name": "Ruslan Kostoev"
                    },
                    {
                        "name": "Ilia Perepechkin"
                    },
                    {
                        "name": "Petr Anokhin"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    },
                    {
                        "name": "Nikita Semenov"
                    }
                ],
                "author_detail": {
                    "name": "Nikita Semenov"
                },
                "author": "Nikita Semenov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07978v2",
                "updated": "2025-08-12T06:17:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    6,
                    17,
                    10,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T13:37:41Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    37,
                    41,
                    0,
                    223,
                    0
                ],
                "title": "Adaptive Multiple Access and Service Placement for Generative Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multiple Access and Service Placement for Generative Diffusion\n  Models"
                },
                "summary": "Generative Diffusion Models (GDMs) have emerged as key components of\nGenerative Artificial Intelligence (GenAI), offering unparalleled\nexpressiveness and controllability for complex data generation tasks. However,\ntheir deployment in real-time and mobile environments remains challenging due\nto the iterative and resource-intensive nature of the inference process.\nAddressing these challenges, this paper introduces a unified optimization\nframework that jointly tackles service placement and multiple access control\nfor GDMs in mobile edge networks. We propose LEARN-GDM, a Deep Reinforcement\nLearning-based algorithm that dynamically partitions denoising blocks across\nheterogeneous edge nodes, while accounting for latent transmission costs and\nenabling adaptive reduction of inference steps. Our approach integrates a\ngreedy multiple access scheme with a Double and Dueling Deep Q-Learning\n(D3QL)-based service placement, allowing for scalable, adaptable, and\nresource-efficient operation under stringent quality of service requirements.\nSimulations demonstrate the superior performance of the proposed framework in\nterms of scalability and latency resilience compared to conventional monolithic\nand fixed chain-length placement strategies. This work advances the state of\nthe art in edge-enabled GenAI by offering an adaptable solution for GDM\nservices orchestration, paving the way for future extensions toward semantic\nnetworking and co-inference across distributed environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Diffusion Models (GDMs) have emerged as key components of\nGenerative Artificial Intelligence (GenAI), offering unparalleled\nexpressiveness and controllability for complex data generation tasks. However,\ntheir deployment in real-time and mobile environments remains challenging due\nto the iterative and resource-intensive nature of the inference process.\nAddressing these challenges, this paper introduces a unified optimization\nframework that jointly tackles service placement and multiple access control\nfor GDMs in mobile edge networks. We propose LEARN-GDM, a Deep Reinforcement\nLearning-based algorithm that dynamically partitions denoising blocks across\nheterogeneous edge nodes, while accounting for latent transmission costs and\nenabling adaptive reduction of inference steps. Our approach integrates a\ngreedy multiple access scheme with a Double and Dueling Deep Q-Learning\n(D3QL)-based service placement, allowing for scalable, adaptable, and\nresource-efficient operation under stringent quality of service requirements.\nSimulations demonstrate the superior performance of the proposed framework in\nterms of scalability and latency resilience compared to conventional monolithic\nand fixed chain-length placement strategies. This work advances the state of\nthe art in edge-enabled GenAI by offering an adaptable solution for GDM\nservices orchestration, paving the way for future extensions toward semantic\nnetworking and co-inference across distributed environments."
                },
                "authors": [
                    {
                        "name": "Hamidreza Mazandarani"
                    },
                    {
                        "name": "Mohammad Farhoudi"
                    },
                    {
                        "name": "Masoud Shokrnezhad"
                    },
                    {
                        "name": "Tarik Taleb"
                    }
                ],
                "author_detail": {
                    "name": "Tarik Taleb"
                },
                "author": "Tarik Taleb",
                "arxiv_comment": "This manuscript has been accepted for presentation at IEEE GLOBECOM\n  2025. You can use this material personally. Reprinting or republishing this\n  material for the purpose of advertising or promotion, etc., must adhere to\n  IEEE policy. The DOI will be supplied as soon as it becomes available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07976v1",
                "updated": "2025-08-11T13:36:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    36,
                    57,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T13:36:57Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    36,
                    57,
                    0,
                    223,
                    0
                ],
                "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL"
                },
                "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Minyang Xie"
                    },
                    {
                        "name": "Shusheng Xu"
                    },
                    {
                        "name": "Chuyi He"
                    },
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Banghua Zhu"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10434v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10434v3",
                "updated": "2025-08-11T13:32:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    32,
                    9,
                    0,
                    223,
                    0
                ],
                "published": "2024-03-15T16:14:34Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    16,
                    14,
                    34,
                    4,
                    75,
                    0
                ],
                "title": "Spotter+GPT: Turning Sign Spottings into Sentences with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotter+GPT: Turning Sign Spottings into Sentences with LLMs"
                },
                "summary": "Sign Language Translation (SLT) is a challenging task that aims to generate\nspoken language sentences from sign language videos. In this paper, we\nintroduce a lightweight, modular SLT framework, Spotter+GPT, that leverages the\npower of Large Language Models (LLMs) and avoids heavy end-to-end training.\nSpotter+GPT breaks down the SLT task into two distinct stages. First, a sign\nspotter identifies individual signs within the input video. The spotted signs\nare then passed to an LLM, which transforms them into meaningful spoken\nlanguage sentences. Spotter+GPT eliminates the requirement for SLT-specific\ntraining. This significantly reduces computational costs and time requirements.\nThe source code and pretrained weights of the Spotter are available at\nhttps://gitlab.surrey.ac.uk/cogvispublic/sign-spotter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign Language Translation (SLT) is a challenging task that aims to generate\nspoken language sentences from sign language videos. In this paper, we\nintroduce a lightweight, modular SLT framework, Spotter+GPT, that leverages the\npower of Large Language Models (LLMs) and avoids heavy end-to-end training.\nSpotter+GPT breaks down the SLT task into two distinct stages. First, a sign\nspotter identifies individual signs within the input video. The spotted signs\nare then passed to an LLM, which transforms them into meaningful spoken\nlanguage sentences. Spotter+GPT eliminates the requirement for SLT-specific\ntraining. This significantly reduces computational costs and time requirements.\nThe source code and pretrained weights of the Spotter are available at\nhttps://gitlab.surrey.ac.uk/cogvispublic/sign-spotter."
                },
                "authors": [
                    {
                        "name": "Ozge Mercanoglu Sincan"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden",
                "arxiv_doi": "10.1145/3742886.3756708",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3742886.3756708",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.10434v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10434v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 9th Workshop on Sign Language Translation and Avatar\n  Technologies (SLTAT) in ACM International Conference on Intelligent Virtual\n  Agents (IVA`25)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2207.01702v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2207.01702v3",
                "updated": "2025-08-11T13:25:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    25,
                    32,
                    0,
                    223,
                    0
                ],
                "published": "2022-07-04T20:02:27Z",
                "published_parsed": [
                    2022,
                    7,
                    4,
                    20,
                    2,
                    27,
                    0,
                    185,
                    0
                ],
                "title": "Statistical inference of random graphs with a surrogate likelihood\n  function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference of random graphs with a surrogate likelihood\n  function"
                },
                "summary": "Spectral estimators have been broadly applied to statistical network\nanalysis, but they do not incorporate the likelihood information of the network\nsampling model. This paper proposes a novel surrogate likelihood function for\nstatistical inference of a class of popular network models referred to as\nrandom dot product graphs. In contrast to the structurally complicated exact\nlikelihood function, the surrogate likelihood function has a separable\nstructure and is log-concave yet approximates the exact likelihood function\nwell. From the frequentist perspective, we study the maximum surrogate\nlikelihood estimator and establish the accompanying theory. We show its\nexistence, uniqueness, large sample properties, and that it improves upon the\nbaseline spectral estimator with a smaller sum of squared errors. Furthermore,\nwe derive the second-order bias of the proposed estimator and gain insight into\nwhy it outperforms some of the existing estimators. A computationally\nconvenient stochastic gradient descent algorithm is designed to find the\nmaximum surrogate likelihood estimator in practice. From the Bayesian\nperspective, we establish the Bernstein--von Mises theorem of the posterior\ndistribution with the surrogate likelihood function and show that the resulting\ncredible sets have the correct frequentist coverage. The empirical performance\nof the proposed surrogate-likelihood-based methods is validated through the\nanalyses of simulation examples and a real-world Wikipedia graph dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral estimators have been broadly applied to statistical network\nanalysis, but they do not incorporate the likelihood information of the network\nsampling model. This paper proposes a novel surrogate likelihood function for\nstatistical inference of a class of popular network models referred to as\nrandom dot product graphs. In contrast to the structurally complicated exact\nlikelihood function, the surrogate likelihood function has a separable\nstructure and is log-concave yet approximates the exact likelihood function\nwell. From the frequentist perspective, we study the maximum surrogate\nlikelihood estimator and establish the accompanying theory. We show its\nexistence, uniqueness, large sample properties, and that it improves upon the\nbaseline spectral estimator with a smaller sum of squared errors. Furthermore,\nwe derive the second-order bias of the proposed estimator and gain insight into\nwhy it outperforms some of the existing estimators. A computationally\nconvenient stochastic gradient descent algorithm is designed to find the\nmaximum surrogate likelihood estimator in practice. From the Bayesian\nperspective, we establish the Bernstein--von Mises theorem of the posterior\ndistribution with the surrogate likelihood function and show that the resulting\ncredible sets have the correct frequentist coverage. The empirical performance\nof the proposed surrogate-likelihood-based methods is validated through the\nanalyses of simulation examples and a real-world Wikipedia graph dataset."
                },
                "authors": [
                    {
                        "name": "Dingbo Wu"
                    },
                    {
                        "name": "Fangzheng Xie"
                    }
                ],
                "author_detail": {
                    "name": "Fangzheng Xie"
                },
                "author": "Fangzheng Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2207.01702v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2207.01702v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07959v1",
                "updated": "2025-08-11T13:10:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    10,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T13:10:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    10,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "Large Language Models for Subjective Language Understanding: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Subjective Language Understanding: A Survey"
                },
                "summary": "Subjective language understanding refers to a broad set of natural language\nprocessing tasks where the goal is to interpret or generate content that\nconveys personal feelings, opinions, or figurative meanings rather than\nobjective facts. With the advent of large language models (LLMs) such as\nChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach\nthese inherently nuanced tasks. In this survey, we provide a comprehensive\nreview of recent advances in applying LLMs to subjective language tasks,\nincluding sentiment analysis, emotion recognition, sarcasm detection, humor\nunderstanding, stance detection, metaphor interpretation, intent detection, and\naesthetics assessment. We begin by clarifying the definition of subjective\nlanguage from linguistic and cognitive perspectives, and we outline the unique\nchallenges posed by subjective language (e.g. ambiguity, figurativeness,\ncontext dependence). We then survey the evolution of LLM architectures and\ntechniques that particularly benefit subjectivity tasks, highlighting why LLMs\nare well-suited to model subtle human-like judgments. For each of the eight\ntasks, we summarize task definitions, key datasets, state-of-the-art LLM-based\nmethods, and remaining challenges. We provide comparative insights, discussing\ncommonalities and differences among tasks and how multi-task LLM approaches\nmight yield unified models of subjectivity. Finally, we identify open issues\nsuch as data limitations, model bias, and ethical considerations, and suggest\nfuture research directions. We hope this survey will serve as a valuable\nresource for researchers and practitioners interested in the intersection of\naffective computing, figurative language processing, and large-scale language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subjective language understanding refers to a broad set of natural language\nprocessing tasks where the goal is to interpret or generate content that\nconveys personal feelings, opinions, or figurative meanings rather than\nobjective facts. With the advent of large language models (LLMs) such as\nChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach\nthese inherently nuanced tasks. In this survey, we provide a comprehensive\nreview of recent advances in applying LLMs to subjective language tasks,\nincluding sentiment analysis, emotion recognition, sarcasm detection, humor\nunderstanding, stance detection, metaphor interpretation, intent detection, and\naesthetics assessment. We begin by clarifying the definition of subjective\nlanguage from linguistic and cognitive perspectives, and we outline the unique\nchallenges posed by subjective language (e.g. ambiguity, figurativeness,\ncontext dependence). We then survey the evolution of LLM architectures and\ntechniques that particularly benefit subjectivity tasks, highlighting why LLMs\nare well-suited to model subtle human-like judgments. For each of the eight\ntasks, we summarize task definitions, key datasets, state-of-the-art LLM-based\nmethods, and remaining challenges. We provide comparative insights, discussing\ncommonalities and differences among tasks and how multi-task LLM approaches\nmight yield unified models of subjectivity. Finally, we identify open issues\nsuch as data limitations, model bias, and ethical considerations, and suggest\nfuture research directions. We hope this survey will serve as a valuable\nresource for researchers and practitioners interested in the intersection of\naffective computing, figurative language processing, and large-scale language\nmodels."
                },
                "authors": [
                    {
                        "name": "Changhao Song"
                    },
                    {
                        "name": "Yazhou Zhang"
                    },
                    {
                        "name": "Hui Gao"
                    },
                    {
                        "name": "Ben Yao"
                    },
                    {
                        "name": "Peng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Zhang"
                },
                "author": "Peng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07956v1",
                "updated": "2025-08-11T13:08:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    8,
                    37,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T13:08:37Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    8,
                    37,
                    0,
                    223,
                    0
                ],
                "title": "Careful Queries, Credible Results: Teaching RAG Models Advanced Web\n  Search Tools with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Careful Queries, Credible Results: Teaching RAG Models Advanced Web\n  Search Tools with Reinforcement Learning"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating up-to-date external knowledge, yet real-world web environments\npresent unique challenges. These limitations manifest as two key challenges:\npervasive misinformation in the web environment, which introduces unreliable or\nmisleading content that can degrade retrieval accuracy, and the\nunderutilization of web tools, which, if effectively employed, could enhance\nquery precision and help mitigate this noise, ultimately improving the\nretrieval results in RAG systems. To address these issues, we propose\nWebFilter, a novel RAG framework that generates source-restricted queries and\nfilters out unreliable content. This approach combines a retrieval filtering\nmechanism with a behavior- and outcome-driven reward strategy, optimizing both\nquery formulation and retrieval outcomes. Extensive experiments demonstrate\nthat WebFilter improves answer quality and retrieval precision, outperforming\nexisting RAG methods on both in-domain and out-of-domain benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating up-to-date external knowledge, yet real-world web environments\npresent unique challenges. These limitations manifest as two key challenges:\npervasive misinformation in the web environment, which introduces unreliable or\nmisleading content that can degrade retrieval accuracy, and the\nunderutilization of web tools, which, if effectively employed, could enhance\nquery precision and help mitigate this noise, ultimately improving the\nretrieval results in RAG systems. To address these issues, we propose\nWebFilter, a novel RAG framework that generates source-restricted queries and\nfilters out unreliable content. This approach combines a retrieval filtering\nmechanism with a behavior- and outcome-driven reward strategy, optimizing both\nquery formulation and retrieval outcomes. Extensive experiments demonstrate\nthat WebFilter improves answer quality and retrieval precision, outperforming\nexisting RAG methods on both in-domain and out-of-domain benchmarks."
                },
                "authors": [
                    {
                        "name": "Yuqin Dai"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Yong Deng"
                    },
                    {
                        "name": "Zhanwei Zhang"
                    },
                    {
                        "name": "Jun Yin"
                    },
                    {
                        "name": "Pengyu Zeng"
                    },
                    {
                        "name": "Zhenzhe Ying"
                    },
                    {
                        "name": "Changhua Meng"
                    },
                    {
                        "name": "Can Yi"
                    },
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Shuai Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Lu"
                },
                "author": "Shuai Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07955v1",
                "updated": "2025-08-11T13:08:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    8,
                    7,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T13:08:07Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    8,
                    7,
                    0,
                    223,
                    0
                ],
                "title": "Expert Preference-based Evaluation of Automated Related Work Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert Preference-based Evaluation of Automated Related Work Generation"
                },
                "summary": "Expert domain writing, such as scientific writing, typically demands\nextensive domain knowledge. Recent advances in LLMs show promising potential in\nreducing the expert workload. However, evaluating the quality of automatically\ngenerated scientific writing is a crucial open issue, as it requires knowledge\nof domain-specific evaluation criteria and the ability to discern expert\npreferences. Conventional automatic metrics and LLM-as-a-judge systems are\ninsufficient to grasp expert preferences and domain-specific quality standards.\nTo address this gap and support human-AI collaborative writing, we focus on\nrelated work generation, one of the most challenging scientific tasks, as an\nexemplar. We propose GREP, a multi-turn evaluation framework that integrates\nclassical related work evaluation criteria with expert-specific preferences.\nInstead of assigning a single score, our framework decomposes the evaluation\ninto fine-grained dimensions. This localized evaluation approach is further\naugmented with contrastive few-shot examples to provide detailed contextual\nguidance for the evaluation dimensions. The design principles allow our\nframework to deliver cardinal assessment of quality, which can facilitate\nbetter post-training compared to ordinal preference data. For better\naccessibility, we design two variants of GREP: a more precise variant with\nproprietary LLMs as evaluators, and a cheaper alternative with open-weight\nLLMs. Empirical investigation reveals that our framework is able to assess the\nquality of related work sections in a much more robust manner compared to\nstandard LLM judges, reflects natural scenarios of scientific writing, and\nbears a strong correlation with the human expert assessment. We also observe\nthat generations from state-of-the-art LLMs struggle to satisfy validation\nconstraints of a suitable related work section. They (mostly) fail to improve\nbased on feedback as well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert domain writing, such as scientific writing, typically demands\nextensive domain knowledge. Recent advances in LLMs show promising potential in\nreducing the expert workload. However, evaluating the quality of automatically\ngenerated scientific writing is a crucial open issue, as it requires knowledge\nof domain-specific evaluation criteria and the ability to discern expert\npreferences. Conventional automatic metrics and LLM-as-a-judge systems are\ninsufficient to grasp expert preferences and domain-specific quality standards.\nTo address this gap and support human-AI collaborative writing, we focus on\nrelated work generation, one of the most challenging scientific tasks, as an\nexemplar. We propose GREP, a multi-turn evaluation framework that integrates\nclassical related work evaluation criteria with expert-specific preferences.\nInstead of assigning a single score, our framework decomposes the evaluation\ninto fine-grained dimensions. This localized evaluation approach is further\naugmented with contrastive few-shot examples to provide detailed contextual\nguidance for the evaluation dimensions. The design principles allow our\nframework to deliver cardinal assessment of quality, which can facilitate\nbetter post-training compared to ordinal preference data. For better\naccessibility, we design two variants of GREP: a more precise variant with\nproprietary LLMs as evaluators, and a cheaper alternative with open-weight\nLLMs. Empirical investigation reveals that our framework is able to assess the\nquality of related work sections in a much more robust manner compared to\nstandard LLM judges, reflects natural scenarios of scientific writing, and\nbears a strong correlation with the human expert assessment. We also observe\nthat generations from state-of-the-art LLMs struggle to satisfy validation\nconstraints of a suitable related work section. They (mostly) fail to improve\nbased on feedback as well."
                },
                "authors": [
                    {
                        "name": "Furkan Şahinuç"
                    },
                    {
                        "name": "Subhabrata Dutta"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "Project page: https://ukplab.github.io/arxiv2025-expert-eval-rw/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07950v1",
                "updated": "2025-08-11T13:05:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    5,
                    59,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T13:05:59Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    5,
                    59,
                    0,
                    223,
                    0
                ],
                "title": "FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large\n  Language Model for Automated Cause-of-Death Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large\n  Language Model for Automated Cause-of-Death Analysis"
                },
                "summary": "Forensic cause-of-death determination faces systemic challenges, including\nworkforce shortages and diagnostic variability, particularly in high-volume\nsystems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic\nAgenT), a multi-agent AI framework that automates and standardizes death\ninvestigations through a domain-adapted large language model. FEAT's\napplication-oriented architecture integrates: (i) a central Planner for task\ndecomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a\nMemory & Reflection module for iterative refinement, and (iv) a Global Solver\nfor conclusion synthesis. The system employs tool-augmented reasoning,\nhierarchical retrieval-augmented generation, forensic-tuned LLMs, and\nhuman-in-the-loop feedback to ensure legal and medical validity. In evaluations\nacross diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI\nsystems in both long-form autopsy analyses and concise cause-of-death\nconclusions. It demonstrated robust generalization across six geographic\nregions and achieved high expert concordance in blinded validations. Senior\npathologists validated FEAT's outputs as comparable to those of human experts,\nwith improved detection of subtle evidentiary nuances. To our knowledge, FEAT\nis the first LLM-based AI agent system dedicated to forensic medicine, offering\nscalable, consistent death certification while maintaining expert-level rigor.\nBy integrating AI efficiency with human oversight, this work could advance\nequitable access to reliable medicolegal services while addressing critical\ncapacity constraints in forensic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forensic cause-of-death determination faces systemic challenges, including\nworkforce shortages and diagnostic variability, particularly in high-volume\nsystems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic\nAgenT), a multi-agent AI framework that automates and standardizes death\ninvestigations through a domain-adapted large language model. FEAT's\napplication-oriented architecture integrates: (i) a central Planner for task\ndecomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a\nMemory & Reflection module for iterative refinement, and (iv) a Global Solver\nfor conclusion synthesis. The system employs tool-augmented reasoning,\nhierarchical retrieval-augmented generation, forensic-tuned LLMs, and\nhuman-in-the-loop feedback to ensure legal and medical validity. In evaluations\nacross diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI\nsystems in both long-form autopsy analyses and concise cause-of-death\nconclusions. It demonstrated robust generalization across six geographic\nregions and achieved high expert concordance in blinded validations. Senior\npathologists validated FEAT's outputs as comparable to those of human experts,\nwith improved detection of subtle evidentiary nuances. To our knowledge, FEAT\nis the first LLM-based AI agent system dedicated to forensic medicine, offering\nscalable, consistent death certification while maintaining expert-level rigor.\nBy integrating AI efficiency with human oversight, this work could advance\nequitable access to reliable medicolegal services while addressing critical\ncapacity constraints in forensic systems."
                },
                "authors": [
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Wanqing Zhang"
                    },
                    {
                        "name": "Kehan Li"
                    },
                    {
                        "name": "Erwen Huang"
                    },
                    {
                        "name": "Haitao Bi"
                    },
                    {
                        "name": "Aiying Fan"
                    },
                    {
                        "name": "Yiwen Shen"
                    },
                    {
                        "name": "Hongmei Dong"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Yuming Shao"
                    },
                    {
                        "name": "Zengjia Liu"
                    },
                    {
                        "name": "Xinshe Liu"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Chunxia Yan"
                    },
                    {
                        "name": "Shuanliang Fan"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Jianhua Ma"
                    },
                    {
                        "name": "Bin Cong"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Chunfeng Lian"
                    }
                ],
                "author_detail": {
                    "name": "Chunfeng Lian"
                },
                "author": "Chunfeng Lian",
                "arxiv_comment": "18pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04951v2",
                "updated": "2025-08-11T12:50:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    50,
                    54,
                    0,
                    223,
                    0
                ],
                "published": "2025-07-07T12:52:57Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    12,
                    52,
                    57,
                    0,
                    188,
                    0
                ],
                "title": "What is emergence, after all?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is emergence, after all?"
                },
                "summary": "The term emergence is increasingly used across scientific disciplines to\ndescribe phenomena that arise from interactions among a system's components but\ncannot be readily inferred by examining those components in isolation. While\noften invoked to explain higher-level behaviors, such as flocking,\nsynchronization, or collective intelligence, the term is frequently used\nwithout precision, sometimes giving rise to ambiguity or even mystique. In this\nperspective paper, we clarify the scientific meaning of emergence as a\nmeasurable, physically grounded phenomenon. Through concrete examples, such as\ntemperature, magnetism, and herd immunity in social networks, we review how\ncollective behavior can arise from local interactions that are constrained by\nglobal boundaries. By disentangling emergence from vague overuse, we emphasize\nits role as a rigorous tool for understanding complex systems. Our goal is to\nshow that emergence, when properly framed, offers not mysticism but insight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The term emergence is increasingly used across scientific disciplines to\ndescribe phenomena that arise from interactions among a system's components but\ncannot be readily inferred by examining those components in isolation. While\noften invoked to explain higher-level behaviors, such as flocking,\nsynchronization, or collective intelligence, the term is frequently used\nwithout precision, sometimes giving rise to ambiguity or even mystique. In this\nperspective paper, we clarify the scientific meaning of emergence as a\nmeasurable, physically grounded phenomenon. Through concrete examples, such as\ntemperature, magnetism, and herd immunity in social networks, we review how\ncollective behavior can arise from local interactions that are constrained by\nglobal boundaries. By disentangling emergence from vague overuse, we emphasize\nits role as a rigorous tool for understanding complex systems. Our goal is to\nshow that emergence, when properly framed, offers not mysticism but insight."
                },
                "authors": [
                    {
                        "name": "Abbas K. Rizi"
                    }
                ],
                "author_detail": {
                    "name": "Abbas K. Rizi"
                },
                "author": "Abbas K. Rizi",
                "arxiv_comment": "7 pages, 3 figures, 100 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.hist-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07935v1",
                "updated": "2025-08-11T12:50:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    50,
                    46,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T12:50:46Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    50,
                    46,
                    0,
                    223,
                    0
                ],
                "title": "SHIELDA: Structured Handling of Exceptions in LLM-Driven Agentic\n  Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHIELDA: Structured Handling of Exceptions in LLM-Driven Agentic\n  Workflows"
                },
                "summary": "Large Language Model (LLM) agentic systems are software systems powered by\nLLMs that autonomously reason, plan, and execute multi-step workflows to\nachieve human goals, rather than merely executing predefined steps. During\nexecution, these workflows frequently encounter exceptions. Existing exception\nhandling solutions often treat exceptions superficially, failing to trace\nexecution-phase exceptions to their reasoning-phase root causes. Furthermore,\ntheir recovery logic is brittle, lacking structured escalation pathways when\ninitial attempts fail. To tackle these challenges, we first present a\ncomprehensive taxonomy of 36 exception types across 12 agent artifacts.\nBuilding on this, we propose SHIELDA (Structured Handling of Exceptions in\nLLM-Driven Agentic Workflows), a modular runtime exception handling framework\nfor LLM agentic workflows. SHIELDA uses an exception classifier to select a\npredefined exception handling pattern from a handling pattern registry. These\npatterns are then executed via a structured handling executor, comprising local\nhandling, flow control, and state recovery, to enable phase-aware recovery by\nlinking exceptions to their root causes and facilitating composable strategies.\nWe validate SHIELDA's effectiveness through a case study on the AutoPR agent,\ndemonstrating effective, cross-phase recovery from a reasoning-induced\nexception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agentic systems are software systems powered by\nLLMs that autonomously reason, plan, and execute multi-step workflows to\nachieve human goals, rather than merely executing predefined steps. During\nexecution, these workflows frequently encounter exceptions. Existing exception\nhandling solutions often treat exceptions superficially, failing to trace\nexecution-phase exceptions to their reasoning-phase root causes. Furthermore,\ntheir recovery logic is brittle, lacking structured escalation pathways when\ninitial attempts fail. To tackle these challenges, we first present a\ncomprehensive taxonomy of 36 exception types across 12 agent artifacts.\nBuilding on this, we propose SHIELDA (Structured Handling of Exceptions in\nLLM-Driven Agentic Workflows), a modular runtime exception handling framework\nfor LLM agentic workflows. SHIELDA uses an exception classifier to select a\npredefined exception handling pattern from a handling pattern registry. These\npatterns are then executed via a structured handling executor, comprising local\nhandling, flow control, and state recovery, to enable phase-aware recovery by\nlinking exceptions to their root causes and facilitating composable strategies.\nWe validate SHIELDA's effectiveness through a case study on the AutoPR agent,\ndemonstrating effective, cross-phase recovery from a reasoning-induced\nexception."
                },
                "authors": [
                    {
                        "name": "Jingwen Zhou"
                    },
                    {
                        "name": "Jieshan Chen"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Dehai Zhao"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07932v1",
                "updated": "2025-08-11T12:47:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    47,
                    59,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T12:47:59Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    47,
                    59,
                    0,
                    223,
                    0
                ],
                "title": "\\(X\\)-evolve: Solution space evolution powered by large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\\(X\\)-evolve: Solution space evolution powered by large language models"
                },
                "summary": "While combining large language models (LLMs) with evolutionary algorithms\n(EAs) shows promise for solving complex optimization problems, current\napproaches typically evolve individual solutions, often incurring high LLM call\ncosts. We introduce \\(X\\)-evolve, a paradigm-shifting method that instead\nevolves solution spaces \\(X\\) (sets of individual solutions) - subsets of the\noverall search space \\(S\\). In \\(X\\)-evolve, LLMs generate tunable programs\nwherein certain code snippets, designated as parameters, define a tunable\nsolution space. A score-based search algorithm then efficiently explores this\nparametrically defined space, guided by feedback from objective function\nscores. This strategy enables broader and more efficient exploration, which can\npotentially accelerate convergence at a much lower search cost, requiring up to\ntwo orders of magnitude fewer LLM calls than prior leading methods. We\ndemonstrate \\(X\\)-evolve's efficacy across three distinct hard optimization\nproblems. For the cap set problem, we discover a larger partial admissible set,\nestablishing a new tighter asymptotic lower bound for the cap set constant (\\(C\n\\ge 2.2203\\)). In information theory, we uncover a larger independent set for\nthe 15-vertex cycle graph (\\(\\mathcal{C}_{15}^{\\boxtimes 5}\\), size 19,946),\nthereby raising the known lower bound on its Shannon capacity. Furthermore, for\nthe NP-hard online bin packing problem, we generate heuristics that\nconsistently outperform standard strategies across established benchmarks. By\nevolving solution spaces, our method considerably improves search\neffectiveness, making it possible to tackle high-dimensional problems that were\npreviously computationally prohibitive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While combining large language models (LLMs) with evolutionary algorithms\n(EAs) shows promise for solving complex optimization problems, current\napproaches typically evolve individual solutions, often incurring high LLM call\ncosts. We introduce \\(X\\)-evolve, a paradigm-shifting method that instead\nevolves solution spaces \\(X\\) (sets of individual solutions) - subsets of the\noverall search space \\(S\\). In \\(X\\)-evolve, LLMs generate tunable programs\nwherein certain code snippets, designated as parameters, define a tunable\nsolution space. A score-based search algorithm then efficiently explores this\nparametrically defined space, guided by feedback from objective function\nscores. This strategy enables broader and more efficient exploration, which can\npotentially accelerate convergence at a much lower search cost, requiring up to\ntwo orders of magnitude fewer LLM calls than prior leading methods. We\ndemonstrate \\(X\\)-evolve's efficacy across three distinct hard optimization\nproblems. For the cap set problem, we discover a larger partial admissible set,\nestablishing a new tighter asymptotic lower bound for the cap set constant (\\(C\n\\ge 2.2203\\)). In information theory, we uncover a larger independent set for\nthe 15-vertex cycle graph (\\(\\mathcal{C}_{15}^{\\boxtimes 5}\\), size 19,946),\nthereby raising the known lower bound on its Shannon capacity. Furthermore, for\nthe NP-hard online bin packing problem, we generate heuristics that\nconsistently outperform standard strategies across established benchmarks. By\nevolving solution spaces, our method considerably improves search\neffectiveness, making it possible to tackle high-dimensional problems that were\npreviously computationally prohibitive."
                },
                "authors": [
                    {
                        "name": "Yi Zhai"
                    },
                    {
                        "name": "Zhiqiang Wei"
                    },
                    {
                        "name": "Ruohan Li"
                    },
                    {
                        "name": "Keyu Pan"
                    },
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Jianmin Ji"
                    },
                    {
                        "name": "Wuyang Zhang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yanyong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanyong Zhang"
                },
                "author": "Yanyong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01422v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01422v5",
                "updated": "2025-08-11T12:47:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    47,
                    49,
                    0,
                    223,
                    0
                ],
                "published": "2024-03-03T07:43:39Z",
                "published_parsed": [
                    2024,
                    3,
                    3,
                    7,
                    43,
                    39,
                    6,
                    63,
                    0
                ],
                "title": "DreamFrame: Enhancing Video Understanding via Automatically Generated QA\n  and Style-Consistent Keyframes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamFrame: Enhancing Video Understanding via Automatically Generated QA\n  and Style-Consistent Keyframes"
                },
                "summary": "Recent large vision-language models (LVLMs) for video understanding are\nprimarily fine-tuned with various videos scraped from online platforms.\nExisting datasets, such as ActivityNet, require considerable human labor for\nstructuring and annotation before effectively utilized for tuning LVLMs. While\ncurrent LVLMs are primarily trained on existing datasets in broad,\ngeneral-purpose settings, adapting them to specific downstream scenarios\nremains challenging, as collecting and annotating task-specific videos is\nhighly labor-intensive and time-consuming. To address this issue, we propose a\nthree-stage framework named DreamFrame for automatically generating\nstyle-consistent keyframes and corresponding question-answer (QA) pairs to\nsupport LVLM instruction tuning. DreamFrame generates datasets in a movie-like\nmanner. First, we utilize an LLM to generate structured movie plots including\nmovie prior information (like overview and style), frame descriptions and\nplot-related QA pairs, with a story expansion strategy to mitigate context\nlength limitations.Then, to ensure visual consistency across generated frames,\nwe design a Style Immobilization Process which maintains consistent style\nthrough an embedding learning strategy. Finally, frame descriptions and style\nembeddings are integrated to produce coherent keyframes. Using DreamFrame, we\nconstruct a dataset comprising approximately 1k stylized keyframe-like videos\nand 100k diverse QA pairs. Extensive fine-tuned experiments on various LVLM\narchitectures demonstrate the effectiveness of the proposed dataset.\nFurthermore, based on the proposed dataset, we fine-tune a new LVLM named\nDreamFrame-7B, which significantly surpasses the previous similar-sized LVLMs\nacross different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large vision-language models (LVLMs) for video understanding are\nprimarily fine-tuned with various videos scraped from online platforms.\nExisting datasets, such as ActivityNet, require considerable human labor for\nstructuring and annotation before effectively utilized for tuning LVLMs. While\ncurrent LVLMs are primarily trained on existing datasets in broad,\ngeneral-purpose settings, adapting them to specific downstream scenarios\nremains challenging, as collecting and annotating task-specific videos is\nhighly labor-intensive and time-consuming. To address this issue, we propose a\nthree-stage framework named DreamFrame for automatically generating\nstyle-consistent keyframes and corresponding question-answer (QA) pairs to\nsupport LVLM instruction tuning. DreamFrame generates datasets in a movie-like\nmanner. First, we utilize an LLM to generate structured movie plots including\nmovie prior information (like overview and style), frame descriptions and\nplot-related QA pairs, with a story expansion strategy to mitigate context\nlength limitations.Then, to ensure visual consistency across generated frames,\nwe design a Style Immobilization Process which maintains consistent style\nthrough an embedding learning strategy. Finally, frame descriptions and style\nembeddings are integrated to produce coherent keyframes. Using DreamFrame, we\nconstruct a dataset comprising approximately 1k stylized keyframe-like videos\nand 100k diverse QA pairs. Extensive fine-tuned experiments on various LVLM\narchitectures demonstrate the effectiveness of the proposed dataset.\nFurthermore, based on the proposed dataset, we fine-tune a new LVLM named\nDreamFrame-7B, which significantly surpasses the previous similar-sized LVLMs\nacross different benchmarks."
                },
                "authors": [
                    {
                        "name": "Zhende Song"
                    },
                    {
                        "name": "Chenchen Wang"
                    },
                    {
                        "name": "Jiamu Sheng"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Shengji Tang"
                    },
                    {
                        "name": "Jiayuan Fan"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "Accepted by ACMM'MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01422v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01422v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07927v1",
                "updated": "2025-08-11T12:40:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    40,
                    8,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T12:40:08Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    40,
                    8,
                    0,
                    223,
                    0
                ],
                "title": "Adaptive Fine-Tuning via Pattern Specialization for Deep Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Fine-Tuning via Pattern Specialization for Deep Time Series\n  Forecasting"
                },
                "summary": "Time series forecasting poses significant challenges in non-stationary\nenvironments where underlying patterns evolve over time. In this work, we\npropose a novel framework that enhances deep neural network (DNN) performance\nby leveraging specialized model adaptation and selection. Initially, a base DNN\nis trained offline on historical time series data. A reserved validation subset\nis then segmented to extract and cluster the most dominant patterns within the\nseries, thereby identifying distinct regimes. For each identified cluster, the\nbase DNN is fine-tuned to produce a specialized version that captures unique\npattern characteristics. At inference, the most recent input is matched against\nthe cluster centroids, and the corresponding fine-tuned version is deployed\nbased on the closest similarity measure. Additionally, our approach integrates\na concept drift detection mechanism to identify and adapt to emerging patterns\ncaused by non-stationary behavior. The proposed framework is generalizable\nacross various DNN architectures and has demonstrated significant performance\ngains on both traditional DNNs and recent advanced architectures implemented in\nthe GluonTS library.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting poses significant challenges in non-stationary\nenvironments where underlying patterns evolve over time. In this work, we\npropose a novel framework that enhances deep neural network (DNN) performance\nby leveraging specialized model adaptation and selection. Initially, a base DNN\nis trained offline on historical time series data. A reserved validation subset\nis then segmented to extract and cluster the most dominant patterns within the\nseries, thereby identifying distinct regimes. For each identified cluster, the\nbase DNN is fine-tuned to produce a specialized version that captures unique\npattern characteristics. At inference, the most recent input is matched against\nthe cluster centroids, and the corresponding fine-tuned version is deployed\nbased on the closest similarity measure. Additionally, our approach integrates\na concept drift detection mechanism to identify and adapt to emerging patterns\ncaused by non-stationary behavior. The proposed framework is generalizable\nacross various DNN architectures and has demonstrated significant performance\ngains on both traditional DNNs and recent advanced architectures implemented in\nthe GluonTS library."
                },
                "authors": [
                    {
                        "name": "Amal Saadallah"
                    },
                    {
                        "name": "Abdulaziz Al-Ademi"
                    }
                ],
                "author_detail": {
                    "name": "Abdulaziz Al-Ademi"
                },
                "author": "Abdulaziz Al-Ademi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07925v1",
                "updated": "2025-08-11T12:38:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    38,
                    46,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T12:38:46Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    38,
                    46,
                    0,
                    223,
                    0
                ],
                "title": "TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video\n  Temporal Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video\n  Temporal Grounding"
                },
                "summary": "Video Temporal Grounding (VTG) aims to extract relevant video segments based\non a given natural language query. Recently, zero-shot VTG methods have gained\nattention by leveraging pretrained vision-language models (VLMs) to localize\ntarget moments without additional training. However, existing approaches suffer\nfrom semantic fragmentation, where temporally continuous frames sharing the\nsame semantics are split across multiple segments. When segments are\nfragmented, it becomes difficult to predict an accurate target moment that\naligns with the text query. Also, they rely on skewed similarity distributions\nfor localization, making it difficult to select the optimal segment.\nFurthermore, they heavily depend on the use of LLMs which require expensive\ninferences. To address these limitations, we propose a \\textit{TAG}, a simple\nyet effective Temporal-Aware approach for zero-shot video temporal Grounding,\nwhich incorporates temporal pooling, temporal coherence clustering, and\nsimilarity adjustment. Our proposed method effectively captures the temporal\ncontext of videos and addresses distorted similarity distributions without\ntraining. Our approach achieves state-of-the-art results on Charades-STA and\nActivityNet Captions benchmark datasets without rely on LLMs. Our code is\navailable at https://github.com/Nuetee/TAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Temporal Grounding (VTG) aims to extract relevant video segments based\non a given natural language query. Recently, zero-shot VTG methods have gained\nattention by leveraging pretrained vision-language models (VLMs) to localize\ntarget moments without additional training. However, existing approaches suffer\nfrom semantic fragmentation, where temporally continuous frames sharing the\nsame semantics are split across multiple segments. When segments are\nfragmented, it becomes difficult to predict an accurate target moment that\naligns with the text query. Also, they rely on skewed similarity distributions\nfor localization, making it difficult to select the optimal segment.\nFurthermore, they heavily depend on the use of LLMs which require expensive\ninferences. To address these limitations, we propose a \\textit{TAG}, a simple\nyet effective Temporal-Aware approach for zero-shot video temporal Grounding,\nwhich incorporates temporal pooling, temporal coherence clustering, and\nsimilarity adjustment. Our proposed method effectively captures the temporal\ncontext of videos and addresses distorted similarity distributions without\ntraining. Our approach achieves state-of-the-art results on Charades-STA and\nActivityNet Captions benchmark datasets without rely on LLMs. Our code is\navailable at https://github.com/Nuetee/TAG"
                },
                "authors": [
                    {
                        "name": "Jin-Seop Lee"
                    },
                    {
                        "name": "SungJoon Lee"
                    },
                    {
                        "name": "Jaehan Ahn"
                    },
                    {
                        "name": "YunSeok Choi"
                    },
                    {
                        "name": "Jee-Hyong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jee-Hyong Lee"
                },
                "author": "Jee-Hyong Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07918v1",
                "updated": "2025-08-11T12:32:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    32,
                    48,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T12:32:48Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    32,
                    48,
                    0,
                    223,
                    0
                ],
                "title": "RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language\n  Model-based Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language\n  Model-based Question Answering"
                },
                "summary": "Visual Question Answering (VQA) in remote sensing (RS) is pivotal for\ninterpreting Earth observation data. However, existing RS VQA datasets are\nconstrained by limitations in annotation richness, question diversity, and the\nassessment of specific reasoning capabilities. This paper introduces RSVLM-QA\ndataset, a new large-scale, content-rich VQA dataset for the RS domain.\nRSVLM-QA is constructed by integrating data from several prominent RS\nsegmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ\nan innovative dual-track annotation generation pipeline. Firstly, we leverage\nLarge Language Models (LLMs), specifically GPT-4.1, with meticulously designed\nprompts to automatically generate a suite of detailed annotations including\nimage captions, spatial relations, and semantic tags, alongside complex\ncaption-based VQA pairs. Secondly, to address the challenging task of object\ncounting in RS imagery, we have developed a specialized automated process that\nextracts object counts directly from the original segmentation data; GPT-4.1\nthen formulates natural language answers from these counts, which are paired\nwith preset question templates to create counting QA pairs. RSVLM-QA comprises\n13,820 images and 162,373 VQA pairs, featuring extensive annotations and\ndiverse question types. We provide a detailed statistical analysis of the\ndataset and a comparison with existing RS VQA benchmarks, highlighting the\nsuperior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct\nbenchmark experiments on Six mainstream Vision Language Models (VLMs),\ndemonstrating that RSVLM-QA effectively evaluates and challenges the\nunderstanding and reasoning abilities of current VLMs in the RS domain. We\nbelieve RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM\nresearch communities, poised to catalyze advancements in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Question Answering (VQA) in remote sensing (RS) is pivotal for\ninterpreting Earth observation data. However, existing RS VQA datasets are\nconstrained by limitations in annotation richness, question diversity, and the\nassessment of specific reasoning capabilities. This paper introduces RSVLM-QA\ndataset, a new large-scale, content-rich VQA dataset for the RS domain.\nRSVLM-QA is constructed by integrating data from several prominent RS\nsegmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ\nan innovative dual-track annotation generation pipeline. Firstly, we leverage\nLarge Language Models (LLMs), specifically GPT-4.1, with meticulously designed\nprompts to automatically generate a suite of detailed annotations including\nimage captions, spatial relations, and semantic tags, alongside complex\ncaption-based VQA pairs. Secondly, to address the challenging task of object\ncounting in RS imagery, we have developed a specialized automated process that\nextracts object counts directly from the original segmentation data; GPT-4.1\nthen formulates natural language answers from these counts, which are paired\nwith preset question templates to create counting QA pairs. RSVLM-QA comprises\n13,820 images and 162,373 VQA pairs, featuring extensive annotations and\ndiverse question types. We provide a detailed statistical analysis of the\ndataset and a comparison with existing RS VQA benchmarks, highlighting the\nsuperior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct\nbenchmark experiments on Six mainstream Vision Language Models (VLMs),\ndemonstrating that RSVLM-QA effectively evaluates and challenges the\nunderstanding and reasoning abilities of current VLMs in the RS domain. We\nbelieve RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM\nresearch communities, poised to catalyze advancements in the field."
                },
                "authors": [
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Jinghao Xiao"
                    },
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Xian Tao"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Ali Braytee"
                    },
                    {
                        "name": "Mukesh Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Mukesh Prasad"
                },
                "author": "Mukesh Prasad",
                "arxiv_comment": "This paper has been accepted to the proceedings of the 33rd ACM\n  International Multimedia Conference (ACM Multimedia 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17066v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17066v3",
                "updated": "2025-08-11T12:32:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    32,
                    14,
                    0,
                    223,
                    0
                ],
                "published": "2025-05-18T16:13:07Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    16,
                    13,
                    7,
                    6,
                    138,
                    0
                ],
                "title": "Improving LLM Outputs Against Jailbreak Attacks with Expert Model\n  Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Outputs Against Jailbreak Attacks with Expert Model\n  Integration"
                },
                "summary": "Using LLMs in a production environment presents security challenges that\ninclude vulnerabilities to jailbreaks and prompt injections, which can result\nin harmful outputs for humans or the enterprise. The challenge is amplified\nwhen working within a specific domain, as topics generally accepted for LLMs to\naddress may be irrelevant to that field. These problems can be mitigated, for\nexample, by fine-tuning large language models with domain-specific and\nsecurity-focused data. However, these alone are insufficient, as jailbreak\ntechniques evolve. Additionally, API-accessed models do not offer the\nflexibility needed to tailor behavior to industry-specific objectives, and\nin-context learning is not always sufficient or reliable. In response to these\nchallenges, we introduce Archias, an expert model adept at distinguishing\nbetween in-domain and out-of-domain communications. Archias classifies user\ninquiries into several categories: in-domain (specifically for the automotive\nindustry), malicious questions, price injections, prompt injections, and\nout-of-domain examples. Our methodology integrates outputs from the expert\nmodel (Archias) into prompts, which are then processed by the LLM to generate\nresponses. This method increases the model's ability to understand the user's\nintention and give appropriate answers. Archias can be adjusted, fine-tuned,\nand used for many different purposes due to its small size. Therefore, it can\nbe easily customized to the needs of any industry. To validate our approach, we\ncreated a benchmark dataset for the automotive industry. Furthermore, in the\ninterest of advancing research and development, we release our benchmark\ndataset to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs in a production environment presents security challenges that\ninclude vulnerabilities to jailbreaks and prompt injections, which can result\nin harmful outputs for humans or the enterprise. The challenge is amplified\nwhen working within a specific domain, as topics generally accepted for LLMs to\naddress may be irrelevant to that field. These problems can be mitigated, for\nexample, by fine-tuning large language models with domain-specific and\nsecurity-focused data. However, these alone are insufficient, as jailbreak\ntechniques evolve. Additionally, API-accessed models do not offer the\nflexibility needed to tailor behavior to industry-specific objectives, and\nin-context learning is not always sufficient or reliable. In response to these\nchallenges, we introduce Archias, an expert model adept at distinguishing\nbetween in-domain and out-of-domain communications. Archias classifies user\ninquiries into several categories: in-domain (specifically for the automotive\nindustry), malicious questions, price injections, prompt injections, and\nout-of-domain examples. Our methodology integrates outputs from the expert\nmodel (Archias) into prompts, which are then processed by the LLM to generate\nresponses. This method increases the model's ability to understand the user's\nintention and give appropriate answers. Archias can be adjusted, fine-tuned,\nand used for many different purposes due to its small size. Therefore, it can\nbe easily customized to the needs of any industry. To validate our approach, we\ncreated a benchmark dataset for the automotive industry. Furthermore, in the\ninterest of advancing research and development, we release our benchmark\ndataset to the community."
                },
                "authors": [
                    {
                        "name": "Tatia Tsmindashvili"
                    },
                    {
                        "name": "Ana Kolkhidashvili"
                    },
                    {
                        "name": "Dachi Kurtskhalia"
                    },
                    {
                        "name": "Nino Maghlakelidze"
                    },
                    {
                        "name": "Elene Mekvabishvili"
                    },
                    {
                        "name": "Guram Dentoshvili"
                    },
                    {
                        "name": "Orkhan Shamilov"
                    },
                    {
                        "name": "Zaal Gachechiladze"
                    },
                    {
                        "name": "Steven Saporta"
                    },
                    {
                        "name": "David Dachi Choladze"
                    }
                ],
                "author_detail": {
                    "name": "David Dachi Choladze"
                },
                "author": "David Dachi Choladze",
                "arxiv_doi": "10.1109/ACCESS.2025.3592458",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3592458",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.17066v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17066v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access, vol. 13, pp. 134976-134988, Jul. 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07902v1",
                "updated": "2025-08-11T12:17:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    17,
                    58,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T12:17:58Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    17,
                    58,
                    0,
                    223,
                    0
                ],
                "title": "Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity"
                },
                "summary": "Large language models (LLMs) show promise in offering emotional support and\ngenerating empathetic responses for individuals in distress, but their ability\nto deliver culturally sensitive support remains underexplored due to lack of\nresources. In this work, we introduce CultureCare, the first dataset designed\nfor this task, spanning four cultures and including 1729 distress messages,\n1523 cultural signals, and 1041 support strategies with fine-grained emotional\nand cultural annotations. Leveraging CultureCare, we (i) develop and test four\nadaptation strategies for guiding three state-of-the-art LLMs toward culturally\nsensitive responses; (ii) conduct comprehensive evaluations using LLM judges,\nin-culture human annotators, and clinical psychologists; (iii) show that\nadapted LLMs outperform anonymous online peer responses, and that simple\ncultural role-play is insufficient for cultural sensitivity; and (iv) explore\nthe application of LLMs in clinical training, where experts highlight their\npotential in fostering cultural competence in future therapists.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show promise in offering emotional support and\ngenerating empathetic responses for individuals in distress, but their ability\nto deliver culturally sensitive support remains underexplored due to lack of\nresources. In this work, we introduce CultureCare, the first dataset designed\nfor this task, spanning four cultures and including 1729 distress messages,\n1523 cultural signals, and 1041 support strategies with fine-grained emotional\nand cultural annotations. Leveraging CultureCare, we (i) develop and test four\nadaptation strategies for guiding three state-of-the-art LLMs toward culturally\nsensitive responses; (ii) conduct comprehensive evaluations using LLM judges,\nin-culture human annotators, and clinical psychologists; (iii) show that\nadapted LLMs outperform anonymous online peer responses, and that simple\ncultural role-play is insufficient for cultural sensitivity; and (iv) explore\nthe application of LLMs in clinical training, where experts highlight their\npotential in fostering cultural competence in future therapists."
                },
                "authors": [
                    {
                        "name": "Chen Cecilia Liu"
                    },
                    {
                        "name": "Hiba Arnaout"
                    },
                    {
                        "name": "Nils Kovačić"
                    },
                    {
                        "name": "Dana Atzil-Slonim"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "Under review; joint first authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02233v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02233v2",
                "updated": "2025-08-11T12:16:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    16,
                    3,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-04T09:33:47Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    33,
                    47,
                    0,
                    216,
                    0
                ],
                "title": "A Methodological Framework for LLM-Based Mining of Software Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Methodological Framework for LLM-Based Mining of Software Repositories"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in software engineering\nresearch, offering new opportunities for automating repository mining tasks.\nHowever, despite their growing popularity, the methodological integration of\nLLMs into Mining Software Repositories (MSR) remains poorly understood.\nExisting studies tend to focus on specific capabilities or performance\nbenchmarks, providing limited insight into how researchers utilize LLMs across\nthe full research pipeline. To address this gap, we conduct a mixed-method\nstudy that combines a rapid review and questionnaire survey in the field of\nLLM4MSR. We investigate (1) the approaches and (2) the threats that affect the\nempirical rigor of researchers involved in this field. Our findings reveal 15\nmethodological approaches, nine main threats, and 25 mitigation strategies.\nBuilding on these findings, we present PRIMES 2.0, a refined empirical\nframework organized into six stages, comprising 23 methodological substeps,\neach mapped to specific threats and corresponding mitigation strategies,\nproviding prescriptive and adaptive support throughout the lifecycle of\nLLM-based MSR studies. Our work contributes to establishing a more transparent\nand reproducible foundation for LLM-based MSR research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in software engineering\nresearch, offering new opportunities for automating repository mining tasks.\nHowever, despite their growing popularity, the methodological integration of\nLLMs into Mining Software Repositories (MSR) remains poorly understood.\nExisting studies tend to focus on specific capabilities or performance\nbenchmarks, providing limited insight into how researchers utilize LLMs across\nthe full research pipeline. To address this gap, we conduct a mixed-method\nstudy that combines a rapid review and questionnaire survey in the field of\nLLM4MSR. We investigate (1) the approaches and (2) the threats that affect the\nempirical rigor of researchers involved in this field. Our findings reveal 15\nmethodological approaches, nine main threats, and 25 mitigation strategies.\nBuilding on these findings, we present PRIMES 2.0, a refined empirical\nframework organized into six stages, comprising 23 methodological substeps,\neach mapped to specific threats and corresponding mitigation strategies,\nproviding prescriptive and adaptive support throughout the lifecycle of\nLLM-based MSR studies. Our work contributes to establishing a more transparent\nand reproducible foundation for LLM-based MSR research."
                },
                "authors": [
                    {
                        "name": "Vincenzo De Martino"
                    },
                    {
                        "name": "Joel Castaño"
                    },
                    {
                        "name": "Fabio Palomba"
                    },
                    {
                        "name": "Xavier Franch"
                    },
                    {
                        "name": "Silverio Martínez-Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Silverio Martínez-Fernández"
                },
                "author": "Silverio Martínez-Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02233v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02233v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07887v1",
                "updated": "2025-08-11T12:05:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    5,
                    18,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T12:05:18Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    5,
                    18,
                    0,
                    223,
                    0
                ],
                "title": "Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic\n  Participant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic\n  Participant"
                },
                "summary": "Simulators have revolutionized scientific practice across the natural\nsciences. By generating data that reliably approximate real-world phenomena,\nthey enable scientists to accelerate hypothesis testing and optimize\nexperimental designs. This is perhaps best illustrated by AlphaFold, a\nNobel-prize winning simulator in chemistry that predicts protein structures\nfrom amino acid sequences, enabling rapid prototyping of molecular\ninteractions, drug targets, and protein functions. In the behavioral sciences,\na reliable participant simulator - a system capable of producing human-like\nbehavior across cognitive tasks - would represent a similarly transformative\nadvance. Recently, Binz et al. introduced Centaur, a large language model (LLM)\nfine-tuned on human data from 160 experiments, proposing its use not only as a\nmodel of cognition but also as a participant simulator for \"in silico\nprototyping of experimental studies\", e.g., to advance automated cognitive\nscience. Here, we review the core criteria for a participant simulator and\nassess how well Centaur meets them. Although Centaur demonstrates strong\npredictive accuracy, its generative behavior - a critical criterion for a\nparticipant simulator - systematically diverges from human data. This suggests\nthat, while Centaur is a significant step toward predicting human behavior, it\ndoes not yet meet the standards of a reliable participant simulator or an\naccurate model of cognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulators have revolutionized scientific practice across the natural\nsciences. By generating data that reliably approximate real-world phenomena,\nthey enable scientists to accelerate hypothesis testing and optimize\nexperimental designs. This is perhaps best illustrated by AlphaFold, a\nNobel-prize winning simulator in chemistry that predicts protein structures\nfrom amino acid sequences, enabling rapid prototyping of molecular\ninteractions, drug targets, and protein functions. In the behavioral sciences,\na reliable participant simulator - a system capable of producing human-like\nbehavior across cognitive tasks - would represent a similarly transformative\nadvance. Recently, Binz et al. introduced Centaur, a large language model (LLM)\nfine-tuned on human data from 160 experiments, proposing its use not only as a\nmodel of cognition but also as a participant simulator for \"in silico\nprototyping of experimental studies\", e.g., to advance automated cognitive\nscience. Here, we review the core criteria for a participant simulator and\nassess how well Centaur meets them. Although Centaur demonstrates strong\npredictive accuracy, its generative behavior - a critical criterion for a\nparticipant simulator - systematically diverges from human data. This suggests\nthat, while Centaur is a significant step toward predicting human behavior, it\ndoes not yet meet the standards of a reliable participant simulator or an\naccurate model of cognition."
                },
                "authors": [
                    {
                        "name": "Sabrina Namazova"
                    },
                    {
                        "name": "Alessandra Brondetta"
                    },
                    {
                        "name": "Younes Strittmatter"
                    },
                    {
                        "name": "Matthew Nassar"
                    },
                    {
                        "name": "Sebastian Musslick"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Musslick"
                },
                "author": "Sebastian Musslick",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07885v1",
                "updated": "2025-08-11T12:00:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    0,
                    3,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T12:00:03Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    0,
                    3,
                    0,
                    223,
                    0
                ],
                "title": "Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces\n  Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces\n  Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning"
                },
                "summary": "This paper introduces an advanced AI-driven perception system for autonomous\nquadcopter navigation in GPS-denied indoor environments. The proposed framework\nleverages cloud computing to offload computationally intensive tasks and\nincorporates a custom-designed printed circuit board (PCB) for efficient sensor\ndata acquisition, enabling robust navigation in confined spaces. The system\nintegrates YOLOv11 for object detection, Depth Anything V2 for monocular depth\nestimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial\nMeasurement Unit (IMU), and a cloud-based Large Language Model (LLM) for\ncontext-aware decision-making. A virtual safety envelope, enforced by\ncalibrated sensor offsets, ensures collision avoidance, while a multithreaded\narchitecture achieves low-latency processing. Enhanced spatial awareness is\nfacilitated by 3D bounding box estimation with Kalman filtering. Experimental\nresults in an indoor testbed demonstrate strong performance, with object\ndetection achieving a mean Average Precision (mAP50) of 0.6, depth estimation\nMean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42\ntrials over approximately 11 minutes, and end-to-end system latency below 1\nsecond. This cloud-supported, high-intelligence framework serves as an\nauxiliary perception and navigation system, complementing state-of-the-art\ndrone autonomy for GPS-denied confined spaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an advanced AI-driven perception system for autonomous\nquadcopter navigation in GPS-denied indoor environments. The proposed framework\nleverages cloud computing to offload computationally intensive tasks and\nincorporates a custom-designed printed circuit board (PCB) for efficient sensor\ndata acquisition, enabling robust navigation in confined spaces. The system\nintegrates YOLOv11 for object detection, Depth Anything V2 for monocular depth\nestimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial\nMeasurement Unit (IMU), and a cloud-based Large Language Model (LLM) for\ncontext-aware decision-making. A virtual safety envelope, enforced by\ncalibrated sensor offsets, ensures collision avoidance, while a multithreaded\narchitecture achieves low-latency processing. Enhanced spatial awareness is\nfacilitated by 3D bounding box estimation with Kalman filtering. Experimental\nresults in an indoor testbed demonstrate strong performance, with object\ndetection achieving a mean Average Precision (mAP50) of 0.6, depth estimation\nMean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42\ntrials over approximately 11 minutes, and end-to-end system latency below 1\nsecond. This cloud-supported, high-intelligence framework serves as an\nauxiliary perception and navigation system, complementing state-of-the-art\ndrone autonomy for GPS-denied confined spaces."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmmad"
                    },
                    {
                        "name": "Zubayer Ahmed Aditto"
                    },
                    {
                        "name": "Md Mehrab Hossain"
                    },
                    {
                        "name": "Noushin Yeasmin"
                    },
                    {
                        "name": "Shorower Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Shorower Hossain"
                },
                "author": "Shorower Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07880v1",
                "updated": "2025-08-11T11:55:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    55,
                    18,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T11:55:18Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    55,
                    18,
                    0,
                    223,
                    0
                ],
                "title": "Multi-agent systems for chemical engineering: A review and perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems for chemical engineering: A review and perspective"
                },
                "summary": "Large language model (LLM)-based multi-agent systems (MASs) are a recent but\nrapidly evolving technology with the potential to transform chemical\nengineering by decomposing complex workflows into teams of collaborative agents\nwith specialized knowledge and tools. This review surveys the state-of-the-art\nof MAS within chemical engineering. While early studies demonstrate promising\nresults, scientific challenges remain, including the design of tailored\narchitectures, integration of heterogeneous data modalities, development of\nfoundation models with domain-specific modalities, and strategies for ensuring\ntransparency, safety, and environmental impact. As a young but fast-moving\nfield, MASs offer exciting opportunities to rethink chemical engineering\nworkflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based multi-agent systems (MASs) are a recent but\nrapidly evolving technology with the potential to transform chemical\nengineering by decomposing complex workflows into teams of collaborative agents\nwith specialized knowledge and tools. This review surveys the state-of-the-art\nof MAS within chemical engineering. While early studies demonstrate promising\nresults, scientific challenges remain, including the design of tailored\narchitectures, integration of heterogeneous data modalities, development of\nfoundation models with domain-specific modalities, and strategies for ensuring\ntransparency, safety, and environmental impact. As a young but fast-moving\nfield, MASs offer exciting opportunities to rethink chemical engineering\nworkflows."
                },
                "authors": [
                    {
                        "name": "Sophia Rupprecht"
                    },
                    {
                        "name": "Qinghe Gao"
                    },
                    {
                        "name": "Tanuj Karia"
                    },
                    {
                        "name": "Artur M. Schweidtmann"
                    }
                ],
                "author_detail": {
                    "name": "Artur M. Schweidtmann"
                },
                "author": "Artur M. Schweidtmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14594v2",
                "updated": "2025-08-11T11:50:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    50,
                    10,
                    0,
                    223,
                    0
                ],
                "published": "2024-11-21T21:27:30Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    21,
                    27,
                    30,
                    3,
                    326,
                    0
                ],
                "title": "Solving Zero-Shot 3D Visual Grounding as Constraint Satisfaction\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving Zero-Shot 3D Visual Grounding as Constraint Satisfaction\n  Problems"
                },
                "summary": "3D visual grounding (3DVG) aims to locate objects in a 3D scene with natural\nlanguage descriptions. Supervised methods have achieved decent accuracy, but\nhave a closed vocabulary and limited language understanding ability. Zero-shot\nmethods utilize large language models (LLMs) to handle natural language\ndescriptions, where the LLM either produces grounding results directly or\ngenerates programs that compute results (symbolically). In this work, we\npropose a zero-shot method that reformulates the 3DVG task as a Constraint\nSatisfaction Problem (CSP), where the variables and constraints represent\nobjects and their spatial relations, respectively. This allows a global\nsymbolic reasoning of all relevant objects, producing grounding results of both\nthe target and anchor objects. Moreover, we demonstrate the flexibility of our\nframework by handling negation- and counting-based queries with only minor\nextra coding efforts. Our system, Constraint Satisfaction Visual Grounding\n(CSVG), has been extensively evaluated on the public datasets ScanRefer and\nNr3D datasets using only open-source LLMs. Results show the effectiveness of\nCSVG and superior grounding accuracy over current state-of-the-art zero-shot\n3DVG methods with improvements of $+7.0\\%$ (Acc@0.5 score) and $+11.2\\%$ on the\nScanRefer and Nr3D datasets, respectively. The code of our system is available\nat https://asig-x.github.io/csvg_web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D visual grounding (3DVG) aims to locate objects in a 3D scene with natural\nlanguage descriptions. Supervised methods have achieved decent accuracy, but\nhave a closed vocabulary and limited language understanding ability. Zero-shot\nmethods utilize large language models (LLMs) to handle natural language\ndescriptions, where the LLM either produces grounding results directly or\ngenerates programs that compute results (symbolically). In this work, we\npropose a zero-shot method that reformulates the 3DVG task as a Constraint\nSatisfaction Problem (CSP), where the variables and constraints represent\nobjects and their spatial relations, respectively. This allows a global\nsymbolic reasoning of all relevant objects, producing grounding results of both\nthe target and anchor objects. Moreover, we demonstrate the flexibility of our\nframework by handling negation- and counting-based queries with only minor\nextra coding efforts. Our system, Constraint Satisfaction Visual Grounding\n(CSVG), has been extensively evaluated on the public datasets ScanRefer and\nNr3D datasets using only open-source LLMs. Results show the effectiveness of\nCSVG and superior grounding accuracy over current state-of-the-art zero-shot\n3DVG methods with improvements of $+7.0\\%$ (Acc@0.5 score) and $+11.2\\%$ on the\nScanRefer and Nr3D datasets, respectively. The code of our system is available\nat https://asig-x.github.io/csvg_web."
                },
                "authors": [
                    {
                        "name": "Qihao Yuan"
                    },
                    {
                        "name": "Kailai Li"
                    },
                    {
                        "name": "Jiaming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Zhang"
                },
                "author": "Jiaming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19860v2",
                "updated": "2025-08-11T11:43:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    43,
                    39,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-28T14:50:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    50,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "CoherenDream: Boosting Holistic Text Coherence in 3D Generation via\n  Multimodal Large Language Models Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoherenDream: Boosting Holistic Text Coherence in 3D Generation via\n  Multimodal Large Language Models Feedback"
                },
                "summary": "Score Distillation Sampling (SDS) has achieved remarkable success in\ntext-to-3D content generation. However, SDS-based methods struggle to maintain\nsemantic fidelity for user prompts, particularly when involving multiple\nobjects with intricate interactions. While existing approaches often address 3D\nconsistency through multiview diffusion model fine-tuning on 3D datasets, this\nstrategy inadvertently exacerbates text-3D alignment degradation. The\nlimitation stems from SDS's inherent accumulation of view-independent biases\nduring optimization, which progressively diverges from the ideal text alignment\ndirection. To alleviate this limitation, we propose a novel SDS objective,\ndubbed as Textual Coherent Score Distillation (TCSD), which integrates\nalignment feedback from multimodal large language models (MLLMs). Our TCSD\nleverages cross-modal understanding capabilities of MLLMs to assess and guide\nthe text-3D correspondence during the optimization. We further develop\n3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text\nalignment in 3D generations. Additionally, we introduce an LLM-layout\ninitialization that significantly accelerates optimization convergence through\nsemantic-aware spatial configuration. Our framework, CoherenDream, achieves\nconsistent improvement across multiple metrics on TIFA subset.As the first\nstudy to incorporate MLLMs into SDS optimization, we also conduct extensive\nablation studies to explore optimal MLLM adaptations for 3D generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Score Distillation Sampling (SDS) has achieved remarkable success in\ntext-to-3D content generation. However, SDS-based methods struggle to maintain\nsemantic fidelity for user prompts, particularly when involving multiple\nobjects with intricate interactions. While existing approaches often address 3D\nconsistency through multiview diffusion model fine-tuning on 3D datasets, this\nstrategy inadvertently exacerbates text-3D alignment degradation. The\nlimitation stems from SDS's inherent accumulation of view-independent biases\nduring optimization, which progressively diverges from the ideal text alignment\ndirection. To alleviate this limitation, we propose a novel SDS objective,\ndubbed as Textual Coherent Score Distillation (TCSD), which integrates\nalignment feedback from multimodal large language models (MLLMs). Our TCSD\nleverages cross-modal understanding capabilities of MLLMs to assess and guide\nthe text-3D correspondence during the optimization. We further develop\n3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text\nalignment in 3D generations. Additionally, we introduce an LLM-layout\ninitialization that significantly accelerates optimization convergence through\nsemantic-aware spatial configuration. Our framework, CoherenDream, achieves\nconsistent improvement across multiple metrics on TIFA subset.As the first\nstudy to incorporate MLLMs into SDS optimization, we also conduct extensive\nablation studies to explore optimal MLLM adaptations for 3D generation tasks."
                },
                "authors": [
                    {
                        "name": "Chenhan Jiang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Dit-Yan Yeung"
                    }
                ],
                "author_detail": {
                    "name": "Dit-Yan Yeung"
                },
                "author": "Dit-Yan Yeung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07871v1",
                "updated": "2025-08-11T11:41:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    41,
                    51,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T11:41:51Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    41,
                    51,
                    0,
                    223,
                    0
                ],
                "title": "CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced\n  Multimodal In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced\n  Multimodal In-Context Learning"
                },
                "summary": "Modern large vision-language models (LVLMs) convert each input image into a\nlarge set of tokens, far outnumbering the text tokens. Although this improves\nvisual perception, it introduces severe image token redundancy. Because image\ntokens carry sparse information, many add little to reasoning, yet greatly\nincrease inference cost. The emerging image token pruning methods tackle this\nissue by identifying the most important tokens and discarding the rest. These\nmethods can raise efficiency with only modest performance loss. However, most\nof them only consider single-image tasks and overlook multimodal in-context\nlearning (ICL), where redundancy is greater and efficiency is more critical.\nRedundant tokens weaken the advantage of multimodal ICL for rapid domain\nadaptation and cause unstable performance. Applying existing pruning methods in\nthis setting leads to large accuracy drops, exposing a clear gap and the need\nfor new techniques. Thus, we propose Contextually Adaptive Token Pruning\n(CATP), a training-free pruning method targeted at multimodal ICL. CATP\nconsists of two stages that perform progressive pruning to fully account for\nthe complex cross-modal interactions in the input sequence. After removing\n77.8\\% of the image tokens, CATP produces an average performance gain of 0.6\\%\nover the vanilla model on four LVLMs and eight benchmarks, exceeding all\nbaselines remarkably. Meanwhile, it effectively improves efficiency by\nachieving an average reduction of 10.78\\% in inference latency. CATP enhances\nthe practical value of multimodal ICL and lays the groundwork for future\nprogress in interleaved image-text scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large vision-language models (LVLMs) convert each input image into a\nlarge set of tokens, far outnumbering the text tokens. Although this improves\nvisual perception, it introduces severe image token redundancy. Because image\ntokens carry sparse information, many add little to reasoning, yet greatly\nincrease inference cost. The emerging image token pruning methods tackle this\nissue by identifying the most important tokens and discarding the rest. These\nmethods can raise efficiency with only modest performance loss. However, most\nof them only consider single-image tasks and overlook multimodal in-context\nlearning (ICL), where redundancy is greater and efficiency is more critical.\nRedundant tokens weaken the advantage of multimodal ICL for rapid domain\nadaptation and cause unstable performance. Applying existing pruning methods in\nthis setting leads to large accuracy drops, exposing a clear gap and the need\nfor new techniques. Thus, we propose Contextually Adaptive Token Pruning\n(CATP), a training-free pruning method targeted at multimodal ICL. CATP\nconsists of two stages that perform progressive pruning to fully account for\nthe complex cross-modal interactions in the input sequence. After removing\n77.8\\% of the image tokens, CATP produces an average performance gain of 0.6\\%\nover the vanilla model on four LVLMs and eight benchmarks, exceeding all\nbaselines remarkably. Meanwhile, it effectively improves efficiency by\nachieving an average reduction of 10.78\\% in inference latency. CATP enhances\nthe practical value of multimodal ICL and lays the groundwork for future\nprogress in interleaved image-text scenarios."
                },
                "authors": [
                    {
                        "name": "Yanshu Li"
                    },
                    {
                        "name": "Jianjiang Yang"
                    },
                    {
                        "name": "Zhennan Shen"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Haoyan Xu"
                    },
                    {
                        "name": "Ruixiang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruixiang Tang"
                },
                "author": "Ruixiang Tang",
                "arxiv_comment": "13 pages, 12 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09368v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09368v4",
                "updated": "2025-08-11T11:28:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    28,
                    39,
                    0,
                    223,
                    0
                ],
                "published": "2025-01-16T08:27:40Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    27,
                    40,
                    3,
                    16,
                    0
                ],
                "title": "Aligning Instruction Tuning with Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Instruction Tuning with Pre-training"
                },
                "summary": "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose Aligning Instruction Tuning\nwith Pre-training (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose Aligning Instruction Tuning\nwith Pre-training (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Wenqiang Zu"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09368v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09368v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07864v1",
                "updated": "2025-08-11T11:26:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    26,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T11:26:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    26,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "A Review and Classification of Model Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review and Classification of Model Uncertainty"
                },
                "summary": "Model uncertainty is a crucial issue in statistics, econometrics and machine\nlearning, yet its definition remains ambiguous and is subject to various\ninterpretations in the literature. So far, there has not been a universally\naccepted definition of model uncertainty. We review different understandings of\nmodel uncertainty and categorize them into three distinct types: uncertainty\nabout the true model, model selection uncertainty, and model selection\ninstability. We further offer interpretations and examples for a better\nillustration of these definitions. We also discuss the potential consequences\nof neglecting model uncertainty in the process of conducting statistical\ninference, and provide effective solutions to these problems. Our aim is to\nhelp researchers better understand the concept of model uncertainty and obtain\nvalid statistical inference results on the premise of its existence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model uncertainty is a crucial issue in statistics, econometrics and machine\nlearning, yet its definition remains ambiguous and is subject to various\ninterpretations in the literature. So far, there has not been a universally\naccepted definition of model uncertainty. We review different understandings of\nmodel uncertainty and categorize them into three distinct types: uncertainty\nabout the true model, model selection uncertainty, and model selection\ninstability. We further offer interpretations and examples for a better\nillustration of these definitions. We also discuss the potential consequences\nof neglecting model uncertainty in the process of conducting statistical\ninference, and provide effective solutions to these problems. Our aim is to\nhelp researchers better understand the concept of model uncertainty and obtain\nvalid statistical inference results on the premise of its existence."
                },
                "authors": [
                    {
                        "name": "Guangyuan Cui"
                    },
                    {
                        "name": "Yuting Wei"
                    },
                    {
                        "name": "Xinyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Zhang"
                },
                "author": "Xinyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12659v2",
                "updated": "2025-08-11T11:26:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    26,
                    24,
                    0,
                    223,
                    0
                ],
                "published": "2024-06-18T14:27:44Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    14,
                    27,
                    44,
                    1,
                    170,
                    0
                ],
                "title": "A variational Bayes approach to debiased inference for low-dimensional\n  parameters in high-dimensional linear regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A variational Bayes approach to debiased inference for low-dimensional\n  parameters in high-dimensional linear regression"
                },
                "summary": "We propose a scalable variational Bayes method for statistical inference for\na single or low-dimensional subset of the coordinates of a high-dimensional\nparameter in sparse linear regression. Our approach relies on assigning a\nmean-field approximation to the nuisance coordinates and carefully modelling\nthe conditional distribution of the target given the nuisance. This requires\nonly a preprocessing step and preserves the computational advantages of\nmean-field variational Bayes, while ensuring accurate and reliable inference\nfor the target parameter, including for uncertainty quantification. We\ninvestigate the numerical performance of our algorithm, showing that it\nperforms competitively with existing methods. We further establish accompanying\ntheoretical guarantees for estimation and uncertainty quantification in the\nform of a Bernstein--von Mises theorem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a scalable variational Bayes method for statistical inference for\na single or low-dimensional subset of the coordinates of a high-dimensional\nparameter in sparse linear regression. Our approach relies on assigning a\nmean-field approximation to the nuisance coordinates and carefully modelling\nthe conditional distribution of the target given the nuisance. This requires\nonly a preprocessing step and preserves the computational advantages of\nmean-field variational Bayes, while ensuring accurate and reliable inference\nfor the target parameter, including for uncertainty quantification. We\ninvestigate the numerical performance of our algorithm, showing that it\nperforms competitively with existing methods. We further establish accompanying\ntheoretical guarantees for estimation and uncertainty quantification in the\nform of a Bernstein--von Mises theorem."
                },
                "authors": [
                    {
                        "name": "Ismaël Castillo"
                    },
                    {
                        "name": "Alice L'Huillier"
                    },
                    {
                        "name": "Kolyan Ray"
                    },
                    {
                        "name": "Luke Travis"
                    }
                ],
                "author_detail": {
                    "name": "Luke Travis"
                },
                "author": "Luke Travis",
                "arxiv_comment": "47 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07860v1",
                "updated": "2025-08-11T11:24:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    24,
                    57,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T11:24:57Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    24,
                    57,
                    0,
                    223,
                    0
                ],
                "title": "Large Language Models for Czech Aspect-Based Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Czech Aspect-Based Sentiment Analysis"
                },
                "summary": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that aims to identify sentiment toward specific aspects of an entity.\nWhile large language models (LLMs) have shown strong performance in various\nnatural language processing (NLP) tasks, their capabilities for Czech ABSA\nremain largely unexplored. In this work, we conduct a comprehensive evaluation\nof 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their\nperformance in zero-shot, few-shot, and fine-tuning scenarios. Our results show\nthat small domain-specific models fine-tuned for ABSA outperform\ngeneral-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs\nachieve state-of-the-art results. We analyze how factors such as\nmultilingualism, model size, and recency influence performance and present an\nerror analysis highlighting key challenges, particularly in aspect term\nprediction. Our findings provide insights into the suitability of LLMs for\nCzech ABSA and offer guidance for future research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that aims to identify sentiment toward specific aspects of an entity.\nWhile large language models (LLMs) have shown strong performance in various\nnatural language processing (NLP) tasks, their capabilities for Czech ABSA\nremain largely unexplored. In this work, we conduct a comprehensive evaluation\nof 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their\nperformance in zero-shot, few-shot, and fine-tuning scenarios. Our results show\nthat small domain-specific models fine-tuned for ABSA outperform\ngeneral-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs\nachieve state-of-the-art results. We analyze how factors such as\nmultilingualism, model size, and recency influence performance and present an\nerror analysis highlighting key challenges, particularly in aspect term\nprediction. Our findings provide insights into the suitability of LLMs for\nCzech ABSA and offer guidance for future research in this area."
                },
                "authors": [
                    {
                        "name": "Jakub Šmíd"
                    },
                    {
                        "name": "Pavel Přibáň"
                    },
                    {
                        "name": "Pavel Král"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Král"
                },
                "author": "Pavel Král",
                "arxiv_comment": "Accepted for presentation at the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06225v2",
                "updated": "2025-08-11T11:15:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    15,
                    26,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-08T11:11:22Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    11,
                    22,
                    4,
                    220,
                    0
                ],
                "title": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven\n  Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven\n  Solution"
                },
                "summary": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the Overconfidence Phenomenon in current\nLLM-as-a-Judges, where predicted confidence significantly overstates actual\ncorrectness, undermining reliability in practical deployment. To quantify this\nphenomenon, we introduce TH-Score, a novel metric measuring confidence-accuracy\nalignment. Furthermore, we propose LLM-as-a-Fuser, an ensemble framework that\ntransforms LLMs into reliable, risk-aware evaluators. Extensive experiments\ndemonstrate that our approach substantially improves calibration and enables\nadaptive, confidence-driven evaluation pipelines, achieving superior\nreliability and accuracy compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the Overconfidence Phenomenon in current\nLLM-as-a-Judges, where predicted confidence significantly overstates actual\ncorrectness, undermining reliability in practical deployment. To quantify this\nphenomenon, we introduce TH-Score, a novel metric measuring confidence-accuracy\nalignment. Furthermore, we propose LLM-as-a-Fuser, an ensemble framework that\ntransforms LLMs into reliable, risk-aware evaluators. Extensive experiments\ndemonstrate that our approach substantially improves calibration and enables\nadaptive, confidence-driven evaluation pipelines, achieving superior\nreliability and accuracy compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Zailong Tian"
                    },
                    {
                        "name": "Zhuoheng Han"
                    },
                    {
                        "name": "Yanzhe Chen"
                    },
                    {
                        "name": "Haozhe Xu"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Richeng Xuan"
                    },
                    {
                        "name": "Houfeng Wang"
                    },
                    {
                        "name": "Lizi Liao"
                    }
                ],
                "author_detail": {
                    "name": "Lizi Liao"
                },
                "author": "Lizi Liao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07856v1",
                "updated": "2025-08-11T11:14:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    14,
                    49,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T11:14:49Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    14,
                    49,
                    0,
                    223,
                    0
                ],
                "title": "Recommendation Is a Dish Better Served Warm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation Is a Dish Better Served Warm"
                },
                "summary": "In modern recommender systems, experimental settings typically include\nfiltering out cold users and items based on a minimum interaction threshold.\nHowever, these thresholds are often chosen arbitrarily and vary widely across\nstudies, leading to inconsistencies that can significantly affect the\ncomparability and reliability of evaluation results. In this paper, we\nsystematically explore the cold-start boundary by examining the criteria used\nto determine whether a user or an item should be considered cold. Our\nexperiments incrementally vary the number of interactions for different items\nduring training, and gradually update the length of user interaction histories\nduring inference. We investigate the thresholds across several widely used\ndatasets, commonly represented in recent papers from top-tier conferences, and\non multiple established recommender baselines. Our findings show that\ninconsistent selection of cold-start thresholds can either result in the\nunnecessary removal of valuable data or lead to the misclassification of cold\ninstances as warm, introducing more noise into the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern recommender systems, experimental settings typically include\nfiltering out cold users and items based on a minimum interaction threshold.\nHowever, these thresholds are often chosen arbitrarily and vary widely across\nstudies, leading to inconsistencies that can significantly affect the\ncomparability and reliability of evaluation results. In this paper, we\nsystematically explore the cold-start boundary by examining the criteria used\nto determine whether a user or an item should be considered cold. Our\nexperiments incrementally vary the number of interactions for different items\nduring training, and gradually update the length of user interaction histories\nduring inference. We investigate the thresholds across several widely used\ndatasets, commonly represented in recent papers from top-tier conferences, and\non multiple established recommender baselines. Our findings show that\ninconsistent selection of cold-start thresholds can either result in the\nunnecessary removal of valuable data or lead to the misclassification of cold\ninstances as warm, introducing more noise into the system."
                },
                "authors": [
                    {
                        "name": "Danil Gusak"
                    },
                    {
                        "name": "Nikita Sukhorukov"
                    },
                    {
                        "name": "Evgeny Frolov"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Frolov"
                },
                "author": "Evgeny Frolov",
                "arxiv_doi": "10.1145/3705328.3759331",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3705328.3759331",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.07856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for ACM RecSys 2025. Author's version. The final published\n  version will be available at the ACM Digital Library",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07852v1",
                "updated": "2025-08-11T11:10:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    10,
                    19,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T11:10:19Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    10,
                    19,
                    0,
                    223,
                    0
                ],
                "title": "Vertex Features for Neural Global Illumination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertex Features for Neural Global Illumination"
                },
                "summary": "Recent research on learnable neural representations has been widely adopted\nin the field of 3D scene reconstruction and neural rendering applications.\nHowever, traditional feature grid representations often suffer from substantial\nmemory footprint, posing a significant bottleneck for modern parallel computing\nhardware. In this paper, we present neural vertex features, a generalized\nformulation of learnable representation for neural rendering tasks involving\nexplicit mesh surfaces. Instead of uniformly distributing neural features\nthroughout 3D space, our method stores learnable features directly at mesh\nvertices, leveraging the underlying geometry as a compact and structured\nrepresentation for neural processing. This not only optimizes memory\nefficiency, but also improves feature representation by aligning compactly with\nthe surface using task-specific geometric priors. We validate our neural\nrepresentation across diverse neural rendering tasks, with a specific emphasis\non neural radiosity. Experimental results demonstrate that our method reduces\nmemory consumption to only one-fifth (or even less) of grid-based\nrepresentations, while maintaining comparable rendering quality and lowering\ninference overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on learnable neural representations has been widely adopted\nin the field of 3D scene reconstruction and neural rendering applications.\nHowever, traditional feature grid representations often suffer from substantial\nmemory footprint, posing a significant bottleneck for modern parallel computing\nhardware. In this paper, we present neural vertex features, a generalized\nformulation of learnable representation for neural rendering tasks involving\nexplicit mesh surfaces. Instead of uniformly distributing neural features\nthroughout 3D space, our method stores learnable features directly at mesh\nvertices, leveraging the underlying geometry as a compact and structured\nrepresentation for neural processing. This not only optimizes memory\nefficiency, but also improves feature representation by aligning compactly with\nthe surface using task-specific geometric priors. We validate our neural\nrepresentation across diverse neural rendering tasks, with a specific emphasis\non neural radiosity. Experimental results demonstrate that our method reduces\nmemory consumption to only one-fifth (or even less) of grid-based\nrepresentations, while maintaining comparable rendering quality and lowering\ninference overhead."
                },
                "authors": [
                    {
                        "name": "Rui Su"
                    },
                    {
                        "name": "Honghao Dong"
                    },
                    {
                        "name": "Haojie Jin"
                    },
                    {
                        "name": "Yisong Chen"
                    },
                    {
                        "name": "Guoping Wang"
                    },
                    {
                        "name": "Sheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Li"
                },
                "author": "Sheng Li",
                "arxiv_comment": "Accepted by ACM SIGGRAPH Asia'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07849v1",
                "updated": "2025-08-11T11:08:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    8,
                    32,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T11:08:32Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    8,
                    32,
                    0,
                    223,
                    0
                ],
                "title": "LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding"
                },
                "summary": "Despite advances in legal NLP, no comprehensive evaluation covering multiple\nlegal-specific LLMs currently exists for contract classification tasks in\ncontract understanding. To address this gap, we present an evaluation of 10\nlegal-specific LLMs on three English language contract understanding tasks and\ncompare them with 7 general-purpose LLMs. The results show that legal-specific\nLLMs consistently outperform general-purpose models, especially on tasks\nrequiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish\nnew SOTAs on two of the three tasks, despite having 69% fewer parameters than\nthe best-performing general-purpose LLM. We also identify CaseLaw-BERT and\nLexLM as strong additional baselines for contract understanding. Our results\nprovide a holistic evaluation of legal-specific LLMs and will facilitate the\ndevelopment of more accurate contract understanding systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in legal NLP, no comprehensive evaluation covering multiple\nlegal-specific LLMs currently exists for contract classification tasks in\ncontract understanding. To address this gap, we present an evaluation of 10\nlegal-specific LLMs on three English language contract understanding tasks and\ncompare them with 7 general-purpose LLMs. The results show that legal-specific\nLLMs consistently outperform general-purpose models, especially on tasks\nrequiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish\nnew SOTAs on two of the three tasks, despite having 69% fewer parameters than\nthe best-performing general-purpose LLM. We also identify CaseLaw-BERT and\nLexLM as strong additional baselines for contract understanding. Our results\nprovide a holistic evaluation of legal-specific LLMs and will facilitate the\ndevelopment of more accurate contract understanding systems."
                },
                "authors": [
                    {
                        "name": "Amrita Singh"
                    },
                    {
                        "name": "H. Suhan Karaca"
                    },
                    {
                        "name": "Aditya Joshi"
                    },
                    {
                        "name": "Hye-young Paik"
                    },
                    {
                        "name": "Jiaojiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaojiao Jiang"
                },
                "author": "Jiaojiao Jiang",
                "arxiv_comment": "Under review. 4 pages + references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07844v1",
                "updated": "2025-08-11T10:59:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    59,
                    9,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T10:59:09Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    59,
                    9,
                    0,
                    223,
                    0
                ],
                "title": "Thermo-coalescence model for Light Nuclei production in Relativistic\n  Heavy-Ion Collisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thermo-coalescence model for Light Nuclei production in Relativistic\n  Heavy-Ion Collisions"
                },
                "summary": "We employ a hybrid approach to describe the light nuclei production mechanism\nwhere the nucleons are assumed to be thermally produced, and are allowed to\nform light nuclei using a coalescence prescription. In this approach, we first\nfit transverse momentum ($p_{T}$) distribution of nucleons using hydro-inspired\nboost-invariant blast-wave model. The extracted parameters are then used to\ndescribe the deuteron $p_{T}$ spectra, along with two additional parameters\nthat characterize the coalescence prescription employed in this study. We refer\nthis combined approach as ``thermo-coalescence model'' and it is designed to\nstudy the deuteron production and describe the experimental measurements. In\nthis work, we analyze the measured $p_{T}$ distribution of protons and\ndeuterons from Pb-Pb collisions at the ALICE Collaboration at LHC. We also\nevaluate the $p_{T}$-integrated deuteron yields using this approach and compare\nwith experimental measurements. A Bayesian inference framework is employed to\ndetermine the best-fit parameters of the thermo-coalescence model. Finally, we\nestimate the traditionally used experimental coalescence parameter ($B_{A}$)\nwithin our framework in order to establish a connection between our model and\nthe conventional coalescence approach commonly used to relate experimental data\nwith theoretical descriptions of light nuclei production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We employ a hybrid approach to describe the light nuclei production mechanism\nwhere the nucleons are assumed to be thermally produced, and are allowed to\nform light nuclei using a coalescence prescription. In this approach, we first\nfit transverse momentum ($p_{T}$) distribution of nucleons using hydro-inspired\nboost-invariant blast-wave model. The extracted parameters are then used to\ndescribe the deuteron $p_{T}$ spectra, along with two additional parameters\nthat characterize the coalescence prescription employed in this study. We refer\nthis combined approach as ``thermo-coalescence model'' and it is designed to\nstudy the deuteron production and describe the experimental measurements. In\nthis work, we analyze the measured $p_{T}$ distribution of protons and\ndeuterons from Pb-Pb collisions at the ALICE Collaboration at LHC. We also\nevaluate the $p_{T}$-integrated deuteron yields using this approach and compare\nwith experimental measurements. A Bayesian inference framework is employed to\ndetermine the best-fit parameters of the thermo-coalescence model. Finally, we\nestimate the traditionally used experimental coalescence parameter ($B_{A}$)\nwithin our framework in order to establish a connection between our model and\nthe conventional coalescence approach commonly used to relate experimental data\nwith theoretical descriptions of light nuclei production."
                },
                "authors": [
                    {
                        "name": "Arun Kumar Yadav"
                    },
                    {
                        "name": "Nachiketa Sarkar"
                    },
                    {
                        "name": "Sudhir Pandurang Rode"
                    },
                    {
                        "name": "Partha Pratim Bhaduri"
                    },
                    {
                        "name": "Abhijit Bhattacharyya"
                    },
                    {
                        "name": "Amaresh Jaiswal"
                    }
                ],
                "author_detail": {
                    "name": "Amaresh Jaiswal"
                },
                "author": "Amaresh Jaiswal",
                "arxiv_comment": "Submitted to European Physical Journal C",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07838v1",
                "updated": "2025-08-11T10:44:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    44,
                    25,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T10:44:25Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    44,
                    25,
                    0,
                    223,
                    0
                ],
                "title": "CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional\n  Modules in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional\n  Modules in Autonomous Driving"
                },
                "summary": "Bird's Eye View (BEV) perception systems based on multi-sensor feature fusion\nhave become a fundamental cornerstone for end-to-end autonomous driving.\nHowever, existing multi-modal BEV methods commonly suffer from limited input\nadaptability, constrained modeling capacity, and suboptimal generalization. To\naddress these challenges, we propose a hierarchically decoupled\nMixture-of-Experts architecture at the functional module level, termed\nComputing Brain DEvelopment System Mixture-of-Experts (CBDES MoE). CBDES MoE\nintegrates multiple structurally heterogeneous expert networks with a\nlightweight Self-Attention Router (SAR) gating mechanism, enabling dynamic\nexpert path selection and sparse, input-aware efficient inference. To the best\nof our knowledge, this is the first modular Mixture-of-Experts framework\nconstructed at the functional module granularity within the autonomous driving\ndomain. Extensive evaluations on the real-world nuScenes dataset demonstrate\nthat CBDES MoE consistently outperforms fixed single-expert baselines in 3D\nobject detection. Compared to the strongest single-expert model, CBDES MoE\nachieves a 1.6-point increase in mAP and a 4.1-point improvement in NDS,\ndemonstrating the effectiveness and practical advantages of the proposed\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bird's Eye View (BEV) perception systems based on multi-sensor feature fusion\nhave become a fundamental cornerstone for end-to-end autonomous driving.\nHowever, existing multi-modal BEV methods commonly suffer from limited input\nadaptability, constrained modeling capacity, and suboptimal generalization. To\naddress these challenges, we propose a hierarchically decoupled\nMixture-of-Experts architecture at the functional module level, termed\nComputing Brain DEvelopment System Mixture-of-Experts (CBDES MoE). CBDES MoE\nintegrates multiple structurally heterogeneous expert networks with a\nlightweight Self-Attention Router (SAR) gating mechanism, enabling dynamic\nexpert path selection and sparse, input-aware efficient inference. To the best\nof our knowledge, this is the first modular Mixture-of-Experts framework\nconstructed at the functional module granularity within the autonomous driving\ndomain. Extensive evaluations on the real-world nuScenes dataset demonstrate\nthat CBDES MoE consistently outperforms fixed single-expert baselines in 3D\nobject detection. Compared to the strongest single-expert model, CBDES MoE\nachieves a 1.6-point increase in mAP and a 4.1-point improvement in NDS,\ndemonstrating the effectiveness and practical advantages of the proposed\napproach."
                },
                "authors": [
                    {
                        "name": "Qi Xiang"
                    },
                    {
                        "name": "Kunsong Shi"
                    },
                    {
                        "name": "Zhigui Lin"
                    },
                    {
                        "name": "Lei He"
                    }
                ],
                "author_detail": {
                    "name": "Lei He"
                },
                "author": "Lei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12524v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12524v3",
                "updated": "2025-08-11T10:39:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    39,
                    9,
                    0,
                    223,
                    0
                ],
                "published": "2025-06-14T14:48:11Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    14,
                    48,
                    11,
                    5,
                    165,
                    0
                ],
                "title": "Inference-Time Gaze Refinement for Micro-Expression Recognition:\n  Enhancing Event-Based Eye Tracking with Motion-Aware Post-Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Gaze Refinement for Micro-Expression Recognition:\n  Enhancing Event-Based Eye Tracking with Motion-Aware Post-Processing"
                },
                "summary": "Event-based eye tracking holds significant promise for fine-grained cognitive\nstate inference, offering high temporal resolution and robustness to motion\nartifacts, critical features for decoding subtle mental states such as\nattention, confusion, or fatigue. In this work, we introduce a model-agnostic,\ninference-time refinement framework designed to enhance the output of existing\nevent-based gaze estimation models without modifying their architecture or\nrequiring retraining. Our method comprises two key post-processing modules: (i)\nMotion-Aware Median Filtering, which suppresses blink-induced spikes while\npreserving natural gaze dynamics, and (ii) Optical Flow-Based Local Refinement,\nwhich aligns gaze predictions with cumulative event motion to reduce spatial\njitter and temporal discontinuities. To complement traditional spatial accuracy\nmetrics, we propose a novel Jitter Metric that captures the temporal smoothness\nof predicted gaze trajectories based on velocity regularity and local signal\ncomplexity. Together, these contributions significantly improve the consistency\nof event-based gaze signals, making them better suited for downstream tasks\nsuch as micro-expression analysis and mind-state decoding. Our results\ndemonstrate consistent improvements across multiple baseline models on\ncontrolled datasets, laying the groundwork for future integration with\nmultimodal affect recognition systems in real-world environments. Our code\nimplementations can be found at\nhttps://github.com/eye-tracking-for-physiological-sensing/EyeLoRiN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-based eye tracking holds significant promise for fine-grained cognitive\nstate inference, offering high temporal resolution and robustness to motion\nartifacts, critical features for decoding subtle mental states such as\nattention, confusion, or fatigue. In this work, we introduce a model-agnostic,\ninference-time refinement framework designed to enhance the output of existing\nevent-based gaze estimation models without modifying their architecture or\nrequiring retraining. Our method comprises two key post-processing modules: (i)\nMotion-Aware Median Filtering, which suppresses blink-induced spikes while\npreserving natural gaze dynamics, and (ii) Optical Flow-Based Local Refinement,\nwhich aligns gaze predictions with cumulative event motion to reduce spatial\njitter and temporal discontinuities. To complement traditional spatial accuracy\nmetrics, we propose a novel Jitter Metric that captures the temporal smoothness\nof predicted gaze trajectories based on velocity regularity and local signal\ncomplexity. Together, these contributions significantly improve the consistency\nof event-based gaze signals, making them better suited for downstream tasks\nsuch as micro-expression analysis and mind-state decoding. Our results\ndemonstrate consistent improvements across multiple baseline models on\ncontrolled datasets, laying the groundwork for future integration with\nmultimodal affect recognition systems in real-world environments. Our code\nimplementations can be found at\nhttps://github.com/eye-tracking-for-physiological-sensing/EyeLoRiN."
                },
                "authors": [
                    {
                        "name": "Nuwan Bandara"
                    },
                    {
                        "name": "Thivya Kandappu"
                    },
                    {
                        "name": "Archan Misra"
                    }
                ],
                "author_detail": {
                    "name": "Archan Misra"
                },
                "author": "Archan Misra",
                "arxiv_comment": "Accepted at 4DMR@IJCAI25: International IJCAI Workshop on 1st\n  Challenge and Workshop for 4D Micro-Expression Recognition for Mind Reading,\n  August 29, 2025, Guangzhou, China",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12524v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12524v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13380v3",
                "updated": "2025-08-11T10:35:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    35,
                    31,
                    0,
                    223,
                    0
                ],
                "published": "2025-06-16T11:44:28Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    44,
                    28,
                    0,
                    167,
                    0
                ],
                "title": "DAGR: Decomposition Augmented Graph Retrieval with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAGR: Decomposition Augmented Graph Retrieval with LLMs"
                },
                "summary": "Large Language Models (LLMs) excel at many Natural Language Processing (NLP)\ntasks, but struggle with multi-hop reasoning and factual consistency, limiting\ntheir effectiveness on knowledge-intensive tasks like complex question\nanswering (QA). Linking Knowledge Graphs (KG) and LLMs has shown promising\nresults, but LLMs generally lack the ability to reason efficiently over\ngraph-structured information. To address this challenge, we introduce DAGR, a\nretrieval method that leverages both complex questions and their decomposition\nin subquestions to extract relevant, linked textual subgraphs. DAGR first\nbreaks down complex queries, retrieves subgraphs guided by a weighted\nsimilarity function over both the original and decomposed queries, and creates\na question-specific knowledge graph to guide answer generation. The resulting\nGraph-RAG pipeline is suited to handle complex multi-hop questions and\neffectively reason over graph-structured data. We evaluate DAGR on standard\nmulti-hop QA benchmarks and show that it achieves comparable or superior\nperformance to competitive existing methods, using smaller models and fewer LLM\ncalls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at many Natural Language Processing (NLP)\ntasks, but struggle with multi-hop reasoning and factual consistency, limiting\ntheir effectiveness on knowledge-intensive tasks like complex question\nanswering (QA). Linking Knowledge Graphs (KG) and LLMs has shown promising\nresults, but LLMs generally lack the ability to reason efficiently over\ngraph-structured information. To address this challenge, we introduce DAGR, a\nretrieval method that leverages both complex questions and their decomposition\nin subquestions to extract relevant, linked textual subgraphs. DAGR first\nbreaks down complex queries, retrieves subgraphs guided by a weighted\nsimilarity function over both the original and decomposed queries, and creates\na question-specific knowledge graph to guide answer generation. The resulting\nGraph-RAG pipeline is suited to handle complex multi-hop questions and\neffectively reason over graph-structured data. We evaluate DAGR on standard\nmulti-hop QA benchmarks and show that it achieves comparable or superior\nperformance to competitive existing methods, using smaller models and fewer LLM\ncalls."
                },
                "authors": [
                    {
                        "name": "Valentin Six"
                    },
                    {
                        "name": "Evan Dufraisse"
                    },
                    {
                        "name": "Gaël de Chalendar"
                    }
                ],
                "author_detail": {
                    "name": "Gaël de Chalendar"
                },
                "author": "Gaël de Chalendar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07827v1",
                "updated": "2025-08-11T10:19:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    19,
                    10,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T10:19:10Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    19,
                    10,
                    0,
                    223,
                    0
                ],
                "title": "Evaluating Large Language Models as Expert Annotators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models as Expert Annotators"
                },
                "summary": "Textual data annotation, the process of labeling or tagging text with\nrelevant information, is typically costly, time-consuming, and labor-intensive.\nWhile large language models (LLMs) have demonstrated their potential as direct\nalternatives to human annotators for general domains natural language\nprocessing (NLP) tasks, their effectiveness on annotation tasks in domains\nrequiring expert knowledge remains underexplored. In this paper, we\ninvestigate: whether top-performing LLMs, which might be perceived as having\nexpert-level proficiency in academic and professional benchmarks, can serve as\ndirect alternatives to human expert annotators? To this end, we evaluate both\nindividual LLMs and multi-agent approaches across three highly specialized\ndomains: finance, biomedicine, and law. Specifically, we propose a multi-agent\ndiscussion framework to simulate a group of human annotators, where LLMs are\ntasked to engage in discussions by considering others' annotations and\njustifications before finalizing their labels. Additionally, we incorporate\nreasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our\nempirical results reveal that: (1) Individual LLMs equipped with inference-time\ntechniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal\nor even negative performance gains, contrary to prior literature suggesting\ntheir broad effectiveness. (2) Overall, reasoning models do not demonstrate\nstatistically significant improvements over non-reasoning models in most\nsettings. This suggests that extended long CoT provides relatively limited\nbenefits for data annotation in specialized domains. (3) Certain model\nbehaviors emerge in the multi-agent discussion environment. For instance,\nClaude 3.7 Sonnet with thinking rarely changes its initial annotations, even\nwhen other agents provide correct annotations or valid reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual data annotation, the process of labeling or tagging text with\nrelevant information, is typically costly, time-consuming, and labor-intensive.\nWhile large language models (LLMs) have demonstrated their potential as direct\nalternatives to human annotators for general domains natural language\nprocessing (NLP) tasks, their effectiveness on annotation tasks in domains\nrequiring expert knowledge remains underexplored. In this paper, we\ninvestigate: whether top-performing LLMs, which might be perceived as having\nexpert-level proficiency in academic and professional benchmarks, can serve as\ndirect alternatives to human expert annotators? To this end, we evaluate both\nindividual LLMs and multi-agent approaches across three highly specialized\ndomains: finance, biomedicine, and law. Specifically, we propose a multi-agent\ndiscussion framework to simulate a group of human annotators, where LLMs are\ntasked to engage in discussions by considering others' annotations and\njustifications before finalizing their labels. Additionally, we incorporate\nreasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our\nempirical results reveal that: (1) Individual LLMs equipped with inference-time\ntechniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal\nor even negative performance gains, contrary to prior literature suggesting\ntheir broad effectiveness. (2) Overall, reasoning models do not demonstrate\nstatistically significant improvements over non-reasoning models in most\nsettings. This suggests that extended long CoT provides relatively limited\nbenefits for data annotation in specialized domains. (3) Certain model\nbehaviors emerge in the multi-agent discussion environment. For instance,\nClaude 3.7 Sonnet with thinking rarely changes its initial annotations, even\nwhen other agents provide correct annotations or valid reasoning."
                },
                "authors": [
                    {
                        "name": "Yu-Min Tseng"
                    },
                    {
                        "name": "Wei-Lin Chen"
                    },
                    {
                        "name": "Chung-Chi Chen"
                    },
                    {
                        "name": "Hsin-Hsi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hsin-Hsi Chen"
                },
                "author": "Hsin-Hsi Chen",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07520v2",
                "updated": "2025-08-11T10:09:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    9,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-03-10T16:46:43Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    46,
                    43,
                    0,
                    69,
                    0
                ],
                "title": "From Limited Labels to Open Domains:An Efficient Learning Method for\n  Drone-view Geo-Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Limited Labels to Open Domains:An Efficient Learning Method for\n  Drone-view Geo-Localization"
                },
                "summary": "Traditional supervised drone-view geo-localization (DVGL) methods heavily\ndepend on paired training data and encounter difficulties in learning\ncross-view correlations from unpaired data. Moreover, when deployed in a new\ndomain, these methods require obtaining the new paired data and subsequent\nretraining for model adaptation, which significantly increases computational\noverhead. Existing unsupervised methods have enabled to generate pseudo-labels\nbased on cross-view similarity to infer the pairing relationships. However,\ngeographical similarity and spatial continuity often cause visually analogous\nfeatures at different geographical locations. The feature confusion compromises\nthe reliability of pseudo-label generation, where incorrect pseudo-labels drive\nnegative optimization. Given these challenges inherent in both supervised and\nunsupervised DVGL methods, we propose a novel cross-domain invariant knowledge\ntransfer network (CDIKTNet) with limited supervision, whose architecture\nconsists of a cross-domain invariance sub-network (CDIS) and a cross-domain\ntransfer sub-network (CDTS). This architecture facilitates a closed-loop\nframework for invariance feature learning and knowledge transfer. The CDIS is\ndesigned to learn cross-view structural and spatial invariance from a small\namount of paired data that serves as prior knowledge. It endows the shared\nfeature space of unpaired data with similar implicit cross-view correlations at\ninitialization, which alleviates feature confusion. Based on this, the CDTS\nemploys dual-path contrastive learning to further optimize each subspace while\npreserving consistency in a shared feature space. Extensive experiments\ndemonstrate that CDIKTNet achieves state-of-the-art performance under full\nsupervision compared with those supervised methods, and further surpasses\nexisting unsupervised methods in both few-shot and cross-domain initialization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional supervised drone-view geo-localization (DVGL) methods heavily\ndepend on paired training data and encounter difficulties in learning\ncross-view correlations from unpaired data. Moreover, when deployed in a new\ndomain, these methods require obtaining the new paired data and subsequent\nretraining for model adaptation, which significantly increases computational\noverhead. Existing unsupervised methods have enabled to generate pseudo-labels\nbased on cross-view similarity to infer the pairing relationships. However,\ngeographical similarity and spatial continuity often cause visually analogous\nfeatures at different geographical locations. The feature confusion compromises\nthe reliability of pseudo-label generation, where incorrect pseudo-labels drive\nnegative optimization. Given these challenges inherent in both supervised and\nunsupervised DVGL methods, we propose a novel cross-domain invariant knowledge\ntransfer network (CDIKTNet) with limited supervision, whose architecture\nconsists of a cross-domain invariance sub-network (CDIS) and a cross-domain\ntransfer sub-network (CDTS). This architecture facilitates a closed-loop\nframework for invariance feature learning and knowledge transfer. The CDIS is\ndesigned to learn cross-view structural and spatial invariance from a small\namount of paired data that serves as prior knowledge. It endows the shared\nfeature space of unpaired data with similar implicit cross-view correlations at\ninitialization, which alleviates feature confusion. Based on this, the CDTS\nemploys dual-path contrastive learning to further optimize each subspace while\npreserving consistency in a shared feature space. Extensive experiments\ndemonstrate that CDIKTNet achieves state-of-the-art performance under full\nsupervision compared with those supervised methods, and further surpasses\nexisting unsupervised methods in both few-shot and cross-domain initialization."
                },
                "authors": [
                    {
                        "name": "Zhongwei Chen"
                    },
                    {
                        "name": "Zhao-Xu Yang"
                    },
                    {
                        "name": "Hai-Jun Rong"
                    },
                    {
                        "name": "Jiawei Lang"
                    },
                    {
                        "name": "Guoqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoqi Li"
                },
                "author": "Guoqi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17337v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17337v2",
                "updated": "2025-08-11T10:09:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    9,
                    32,
                    0,
                    223,
                    0
                ],
                "published": "2024-11-26T11:31:47Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    11,
                    31,
                    47,
                    1,
                    331,
                    0
                ],
                "title": "sbi reloaded: a toolkit for simulation-based inference workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sbi reloaded: a toolkit for simulation-based inference workflows"
                },
                "summary": "Scientists and engineers use simulators to model empirically observed\nphenomena. However, tuning the parameters of a simulator to ensure its outputs\nmatch observed data presents a significant challenge. Simulation-based\ninference (SBI) addresses this by enabling Bayesian inference for simulators,\nidentifying parameters that match observed data and align with prior knowledge.\nUnlike traditional Bayesian inference, SBI only needs access to simulations\nfrom the model and does not require evaluations of the likelihood function. In\naddition, SBI algorithms do not require gradients through the simulator, allow\nfor massive parallelization of simulations, and can perform inference for\ndifferent observations without further simulations or training, thereby\namortizing inference. Over the past years, we have developed, maintained, and\nextended sbi, a PyTorch-based package that implements Bayesian SBI algorithms\nbased on neural networks. The sbi toolkit implements a wide range of inference\nmethods, neural network architectures, sampling methods, and diagnostic tools.\nIn addition, it provides well-tested default settings, but also offers\nflexibility to fully customize every step of the simulation-based inference\nworkflow. Taken together, the sbi toolkit enables scientists and engineers to\napply state-of-the-art SBI methods to black-box simulators, opening up new\npossibilities for aligning simulations with empirically observed data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientists and engineers use simulators to model empirically observed\nphenomena. However, tuning the parameters of a simulator to ensure its outputs\nmatch observed data presents a significant challenge. Simulation-based\ninference (SBI) addresses this by enabling Bayesian inference for simulators,\nidentifying parameters that match observed data and align with prior knowledge.\nUnlike traditional Bayesian inference, SBI only needs access to simulations\nfrom the model and does not require evaluations of the likelihood function. In\naddition, SBI algorithms do not require gradients through the simulator, allow\nfor massive parallelization of simulations, and can perform inference for\ndifferent observations without further simulations or training, thereby\namortizing inference. Over the past years, we have developed, maintained, and\nextended sbi, a PyTorch-based package that implements Bayesian SBI algorithms\nbased on neural networks. The sbi toolkit implements a wide range of inference\nmethods, neural network architectures, sampling methods, and diagnostic tools.\nIn addition, it provides well-tested default settings, but also offers\nflexibility to fully customize every step of the simulation-based inference\nworkflow. Taken together, the sbi toolkit enables scientists and engineers to\napply state-of-the-art SBI methods to black-box simulators, opening up new\npossibilities for aligning simulations with empirically observed data."
                },
                "authors": [
                    {
                        "name": "Jan Boelts"
                    },
                    {
                        "name": "Michael Deistler"
                    },
                    {
                        "name": "Manuel Gloeckler"
                    },
                    {
                        "name": "Álvaro Tejero-Cantero"
                    },
                    {
                        "name": "Jan-Matthis Lueckmann"
                    },
                    {
                        "name": "Guy Moss"
                    },
                    {
                        "name": "Peter Steinbach"
                    },
                    {
                        "name": "Thomas Moreau"
                    },
                    {
                        "name": "Fabio Muratore"
                    },
                    {
                        "name": "Julia Linhart"
                    },
                    {
                        "name": "Conor Durkan"
                    },
                    {
                        "name": "Julius Vetter"
                    },
                    {
                        "name": "Benjamin Kurt Miller"
                    },
                    {
                        "name": "Maternus Herold"
                    },
                    {
                        "name": "Abolfazl Ziaeemehr"
                    },
                    {
                        "name": "Matthijs Pals"
                    },
                    {
                        "name": "Theo Gruner"
                    },
                    {
                        "name": "Sebastian Bischoff"
                    },
                    {
                        "name": "Nastya Krouglova"
                    },
                    {
                        "name": "Richard Gao"
                    },
                    {
                        "name": "Janne K. Lappalainen"
                    },
                    {
                        "name": "Bálint Mucsányi"
                    },
                    {
                        "name": "Felix Pei"
                    },
                    {
                        "name": "Auguste Schulz"
                    },
                    {
                        "name": "Zinovia Stefanidi"
                    },
                    {
                        "name": "Pedro Rodrigues"
                    },
                    {
                        "name": "Cornelius Schröder"
                    },
                    {
                        "name": "Faried Abu Zaid"
                    },
                    {
                        "name": "Jonas Beck"
                    },
                    {
                        "name": "Jaivardhan Kapoor"
                    },
                    {
                        "name": "David S. Greenberg"
                    },
                    {
                        "name": "Pedro J. Gonçalves"
                    },
                    {
                        "name": "Jakob H. Macke"
                    }
                ],
                "author_detail": {
                    "name": "Jakob H. Macke"
                },
                "author": "Jakob H. Macke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17337v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17337v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07583v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07583v3",
                "updated": "2025-08-11T10:08:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    8,
                    37,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-10T09:24:54Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    24,
                    54,
                    3,
                    100,
                    0
                ],
                "title": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with\n  Question Answering"
                },
                "summary": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa"
                },
                "authors": [
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Emmanouil Zaranis"
                    },
                    {
                        "name": "André F. T. Martins"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07583v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07583v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07809v1",
                "updated": "2025-08-11T09:49:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    49,
                    1,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T09:49:01Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    49,
                    1,
                    0,
                    223,
                    0
                ],
                "title": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning"
                },
                "summary": "Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on stronger LLMs for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens them to expand the space in a controlled way. This enables\nLLMs to stably learn from initially unsolved hard problems under sparse\nrewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,\nand Llama. Experiments show that EvoCoT enables LLMs to solve previously\nunsolved problems, improves reasoning capability without external CoT\nsupervision, and is compatible with various RL fine-tuning methods. We release\nthe source code to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on stronger LLMs for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens them to expand the space in a controlled way. This enables\nLLMs to stably learn from initially unsolved hard problems under sparse\nrewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,\nand Llama. Experiments show that EvoCoT enables LLMs to solve previously\nunsolved problems, improves reasoning capability without external CoT\nsupervision, and is compatible with various RL fine-tuning methods. We release\nthe source code to support future research."
                },
                "authors": [
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Taozhi Chen"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Lecheng Wang"
                    },
                    {
                        "name": "Hu XiaoLong"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.08243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08243v2",
                "updated": "2025-08-12T14:19:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    19,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T17:56:06Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    56,
                    6,
                    0,
                    223,
                    0
                ],
                "title": "Jinx: Unlimited LLMs for Probing Alignment Failures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jinx: Unlimited LLMs for Probing Alignment Failures"
                },
                "summary": "Unlimited, or so-called helpful-only language models are trained without\nsafety alignment constraints and never refuse user queries. They are widely\nused by leading AI companies as internal tools for red teaming and alignment\nevaluation. For example, if a safety-aligned model produces harmful outputs\nsimilar to an unlimited model, this indicates alignment failures that require\nfurther attention. Despite their essential role in assessing alignment, such\nmodels are not available to the research community.\n  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx\nresponds to all queries without refusals or safety filtering, while preserving\nthe base model's capabilities in reasoning and instruction following. It\nprovides researchers with an accessible tool for probing alignment failures,\nevaluating safety boundaries, and systematically studying failure modes in\nlanguage model safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlimited, or so-called helpful-only language models are trained without\nsafety alignment constraints and never refuse user queries. They are widely\nused by leading AI companies as internal tools for red teaming and alignment\nevaluation. For example, if a safety-aligned model produces harmful outputs\nsimilar to an unlimited model, this indicates alignment failures that require\nfurther attention. Despite their essential role in assessing alignment, such\nmodels are not available to the research community.\n  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx\nresponds to all queries without refusals or safety filtering, while preserving\nthe base model's capabilities in reasoning and instruction following. It\nprovides researchers with an accessible tool for probing alignment failures,\nevaluating safety boundaries, and systematically studying failure modes in\nlanguage model safety."
                },
                "authors": [
                    {
                        "name": "Jiahao Zhao"
                    },
                    {
                        "name": "Liwei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Dong"
                },
                "author": "Liwei Dong",
                "arxiv_comment": "https://huggingface.co/Jinx-org",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08242v1",
                "updated": "2025-08-11T17:55:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    55,
                    40,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:55:40Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    55,
                    40,
                    0,
                    223,
                    0
                ],
                "title": "Bringing Everyone to the Table: An Experimental Study of LLM-Facilitated\n  Group Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bringing Everyone to the Table: An Experimental Study of LLM-Facilitated\n  Group Decision Making"
                },
                "summary": "Group decision-making often suffers from uneven information sharing,\nhindering decision quality. While large language models (LLMs) have been widely\nstudied as aids for individuals, their potential to support groups of users,\npotentially as facilitators, is relatively underexplored. We present a\npre-registered randomized experiment with 1,475 participants assigned to 281\nfive-person groups completing a hidden profile task--selecting an optimal city\nfor a hypothetical sporting event--under one of four facilitation conditions:\nno facilitation, a one-time message prompting information sharing, a human\nfacilitator, or an LLM (GPT-4o) facilitator. We find that LLM facilitation\nincreases information shared within a discussion by raising the minimum level\nof engagement with the task among group members, and that these gains come at\nlimited cost in terms of participants' attitudes towards the task, their group,\nor their facilitator. Whether by human or AI, there is no significant effect of\nfacilitation on the final decision outcome, suggesting that even substantial\nbut partial increases in information sharing are insufficient to overcome the\nhidden profile effect studied. To support further research into how LLM-based\ninterfaces can support the future of collaborative decision making, we release\nour experimental platform, the Group-AI Interaction Laboratory (GRAIL), as an\nopen-source tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group decision-making often suffers from uneven information sharing,\nhindering decision quality. While large language models (LLMs) have been widely\nstudied as aids for individuals, their potential to support groups of users,\npotentially as facilitators, is relatively underexplored. We present a\npre-registered randomized experiment with 1,475 participants assigned to 281\nfive-person groups completing a hidden profile task--selecting an optimal city\nfor a hypothetical sporting event--under one of four facilitation conditions:\nno facilitation, a one-time message prompting information sharing, a human\nfacilitator, or an LLM (GPT-4o) facilitator. We find that LLM facilitation\nincreases information shared within a discussion by raising the minimum level\nof engagement with the task among group members, and that these gains come at\nlimited cost in terms of participants' attitudes towards the task, their group,\nor their facilitator. Whether by human or AI, there is no significant effect of\nfacilitation on the final decision outcome, suggesting that even substantial\nbut partial increases in information sharing are insufficient to overcome the\nhidden profile effect studied. To support further research into how LLM-based\ninterfaces can support the future of collaborative decision making, we release\nour experimental platform, the Group-AI Interaction Laboratory (GRAIL), as an\nopen-source tool."
                },
                "authors": [
                    {
                        "name": "Mohammed Alsobay"
                    },
                    {
                        "name": "David M. Rothschild"
                    },
                    {
                        "name": "Jake M. Hofman"
                    },
                    {
                        "name": "Daniel G. Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Daniel G. Goldstein"
                },
                "author": "Daniel G. Goldstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08240v1",
                "updated": "2025-08-11T17:54:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    54,
                    31,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:54:31Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    54,
                    31,
                    0,
                    223,
                    0
                ],
                "title": "ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for\n  Long-Horizon Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for\n  Long-Horizon Tasks"
                },
                "summary": "Language-guided long-horizon mobile manipulation has long been a grand\nchallenge in embodied semantic reasoning, generalizable manipulation, and\nadaptive locomotion. Three fundamental limitations hinder progress: First,\nalthough large language models have improved spatial reasoning and task\nplanning through semantic priors, existing implementations remain confined to\ntabletop scenarios, failing to address the constrained perception and limited\nactuation ranges of mobile platforms. Second, current manipulation strategies\nexhibit insufficient generalization when confronted with the diverse object\nconfigurations encountered in open-world environments. Third, while crucial for\npractical deployment, the dual requirement of maintaining high platform\nmaneuverability alongside precise end-effector control in unstructured settings\nremains understudied.\n  In this work, we present ODYSSEY, a unified mobile manipulation framework for\nagile quadruped robots equipped with manipulators, which seamlessly integrates\nhigh-level task planning with low-level whole-body control. To address the\nchallenge of egocentric perception in language-conditioned tasks, we introduce\na hierarchical planner powered by a vision-language model, enabling\nlong-horizon instruction decomposition and precise action execution. At the\ncontrol level, our novel whole-body policy achieves robust coordination across\nchallenging terrains. We further present the first benchmark for long-horizon\nmobile manipulation, evaluating diverse indoor and outdoor scenarios. Through\nsuccessful sim-to-real transfer, we demonstrate the system's generalization and\nrobustness in real-world deployments, underscoring the practicality of legged\nmanipulators in unstructured environments. Our work advances the feasibility of\ngeneralized robotic assistants capable of complex, dynamic tasks. Our project\npage: https://kaijwang.github.io/odyssey.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-guided long-horizon mobile manipulation has long been a grand\nchallenge in embodied semantic reasoning, generalizable manipulation, and\nadaptive locomotion. Three fundamental limitations hinder progress: First,\nalthough large language models have improved spatial reasoning and task\nplanning through semantic priors, existing implementations remain confined to\ntabletop scenarios, failing to address the constrained perception and limited\nactuation ranges of mobile platforms. Second, current manipulation strategies\nexhibit insufficient generalization when confronted with the diverse object\nconfigurations encountered in open-world environments. Third, while crucial for\npractical deployment, the dual requirement of maintaining high platform\nmaneuverability alongside precise end-effector control in unstructured settings\nremains understudied.\n  In this work, we present ODYSSEY, a unified mobile manipulation framework for\nagile quadruped robots equipped with manipulators, which seamlessly integrates\nhigh-level task planning with low-level whole-body control. To address the\nchallenge of egocentric perception in language-conditioned tasks, we introduce\na hierarchical planner powered by a vision-language model, enabling\nlong-horizon instruction decomposition and precise action execution. At the\ncontrol level, our novel whole-body policy achieves robust coordination across\nchallenging terrains. We further present the first benchmark for long-horizon\nmobile manipulation, evaluating diverse indoor and outdoor scenarios. Through\nsuccessful sim-to-real transfer, we demonstrate the system's generalization and\nrobustness in real-world deployments, underscoring the practicality of legged\nmanipulators in unstructured environments. Our work advances the feasibility of\ngeneralized robotic assistants capable of complex, dynamic tasks. Our project\npage: https://kaijwang.github.io/odyssey.github.io/"
                },
                "authors": [
                    {
                        "name": "Kaijun Wang"
                    },
                    {
                        "name": "Liqin Lu"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Jianuo Jiang"
                    },
                    {
                        "name": "Zeju Li"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Wancai Zheng"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08236v1",
                "updated": "2025-08-11T17:52:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    52,
                    7,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:52:07Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    52,
                    7,
                    0,
                    223,
                    0
                ],
                "title": "Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health\n  Dialogues via LLM-as-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health\n  Dialogues via LLM-as-Judge"
                },
                "summary": "Evaluating the safety alignment of LLM responses in high-risk mental health\ndialogues is particularly difficult due to missing gold-standard answers and\nthe ethically sensitive nature of these interactions. To address this\nchallenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark\nbased on real-world Chinese mental health dialogues. It evaluates whether the\nmodel responses align with the safety principles defined by experts.\nSpecifically designed for settings without standard references, our method\nadopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation\nusing expert-defined reasoning chains grounded in psychological intervention\nprinciples. We employ binary point-wise scoring across multiple safety\ndimensions to enhance the explainability and traceability of the evaluation.\nAdditionally, we present a manually curated, high-quality Chinese-language\ndataset covering self-harm, suicidal ideation, and existential distress,\nderived from real-world online discourse. Experiments on 3600 judgments show\nthat our method achieves the highest agreement with expert assessments and\nproduces more interpretable evaluation rationales compared to existing\napproaches. Our dataset and evaluation tool are publicly available to\nfacilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the safety alignment of LLM responses in high-risk mental health\ndialogues is particularly difficult due to missing gold-standard answers and\nthe ethically sensitive nature of these interactions. To address this\nchallenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark\nbased on real-world Chinese mental health dialogues. It evaluates whether the\nmodel responses align with the safety principles defined by experts.\nSpecifically designed for settings without standard references, our method\nadopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation\nusing expert-defined reasoning chains grounded in psychological intervention\nprinciples. We employ binary point-wise scoring across multiple safety\ndimensions to enhance the explainability and traceability of the evaluation.\nAdditionally, we present a manually curated, high-quality Chinese-language\ndataset covering self-harm, suicidal ideation, and existential distress,\nderived from real-world online discourse. Experiments on 3600 judgments show\nthat our method achieves the highest agreement with expert assessments and\nproduces more interpretable evaluation rationales compared to existing\napproaches. Our dataset and evaluation tool are publicly available to\nfacilitate further research."
                },
                "authors": [
                    {
                        "name": "Yunna Cai"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Haowei Wang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    },
                    {
                        "name": "Moyan Li"
                    },
                    {
                        "name": "Mingming Fan"
                    }
                ],
                "author_detail": {
                    "name": "Mingming Fan"
                },
                "author": "Mingming Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08228v1",
                "updated": "2025-08-11T17:48:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    48,
                    2,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:48:02Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    48,
                    2,
                    0,
                    223,
                    0
                ],
                "title": "LL3M: Large Language 3D Modelers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LL3M: Large Language 3D Modelers"
                },
                "summary": "We present LL3M, a multi-agent system that leverages pretrained large\nlanguage models (LLMs) to generate 3D assets by writing interpretable Python\ncode in Blender. We break away from the typical generative approach that learns\nfrom a collection of 3D data. Instead, we reformulate shape generation as a\ncode-writing task, enabling greater modularity, editability, and integration\nwith artist workflows. Given a text prompt, LL3M coordinates a team of\nspecialized LLM agents to plan, retrieve, write, debug, and refine Blender\nscripts that generate and edit geometry and appearance. The generated code\nworks as a high-level, interpretable, human-readable, well-documented\nrepresentation of scenes and objects, making full use of sophisticated Blender\nconstructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,\nunconstrained shapes, materials, and scenes. This code presents many avenues\nfor further agent and human editing and experimentation via code tweaks or\nprocedural parameters. This medium naturally enables a co-creative loop in our\nsystem: agents can automatically self-critique using code and visuals, while\niterative user instructions provide an intuitive way to refine assets. A shared\ncode context across agents enables awareness of previous attempts, and a\nretrieval-augmented generation knowledge base built from Blender API\ndocumentation, BlenderRAG, equips agents with examples, types, and functions\nempowering advanced modeling operations and code correctness. We demonstrate\nthe effectiveness of LL3M across diverse shape categories, style and material\nedits, and user-driven refinements. Our experiments showcase the power of code\nas a generative and interpretable medium for 3D asset creation. Our project\npage is at https://threedle.github.io/ll3m.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LL3M, a multi-agent system that leverages pretrained large\nlanguage models (LLMs) to generate 3D assets by writing interpretable Python\ncode in Blender. We break away from the typical generative approach that learns\nfrom a collection of 3D data. Instead, we reformulate shape generation as a\ncode-writing task, enabling greater modularity, editability, and integration\nwith artist workflows. Given a text prompt, LL3M coordinates a team of\nspecialized LLM agents to plan, retrieve, write, debug, and refine Blender\nscripts that generate and edit geometry and appearance. The generated code\nworks as a high-level, interpretable, human-readable, well-documented\nrepresentation of scenes and objects, making full use of sophisticated Blender\nconstructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,\nunconstrained shapes, materials, and scenes. This code presents many avenues\nfor further agent and human editing and experimentation via code tweaks or\nprocedural parameters. This medium naturally enables a co-creative loop in our\nsystem: agents can automatically self-critique using code and visuals, while\niterative user instructions provide an intuitive way to refine assets. A shared\ncode context across agents enables awareness of previous attempts, and a\nretrieval-augmented generation knowledge base built from Blender API\ndocumentation, BlenderRAG, equips agents with examples, types, and functions\nempowering advanced modeling operations and code correctness. We demonstrate\nthe effectiveness of LL3M across diverse shape categories, style and material\nedits, and user-driven refinements. Our experiments showcase the power of code\nas a generative and interpretable medium for 3D asset creation. Our project\npage is at https://threedle.github.io/ll3m."
                },
                "authors": [
                    {
                        "name": "Sining Lu"
                    },
                    {
                        "name": "Guan Chen"
                    },
                    {
                        "name": "Nam Anh Dinh"
                    },
                    {
                        "name": "Itai Lang"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Rana Hanocka"
                    }
                ],
                "author_detail": {
                    "name": "Rana Hanocka"
                },
                "author": "Rana Hanocka",
                "arxiv_comment": "Our project page is at https://threedle.github.io/ll3m",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08225v1",
                "updated": "2025-08-11T17:44:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    44,
                    24,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:44:24Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    44,
                    24,
                    0,
                    223,
                    0
                ],
                "title": "Industrial Viewpoints on RAN Technologies for 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial Viewpoints on RAN Technologies for 6G"
                },
                "summary": "6G standardization is to start imminently, with commercial deployments\nexpected before 2030. Its technical components and performance requirements are\nthe focus of this article. Our emphasis is on the 6G radio access, especially\nMIMO, AI, waveforms, coding, signal constellations and integration with\nnon-terrestrial networks. Whilst standardization has not yet formally started,\nthe scope of the 6G study items has been defined. Our predictions in this paper\nare speculative as there are no results of the study yet, but our views are\nguided by implementation and deployment aspects. We expect that the views here\nwill guide researchers and industry practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G standardization is to start imminently, with commercial deployments\nexpected before 2030. Its technical components and performance requirements are\nthe focus of this article. Our emphasis is on the 6G radio access, especially\nMIMO, AI, waveforms, coding, signal constellations and integration with\nnon-terrestrial networks. Whilst standardization has not yet formally started,\nthe scope of the 6G study items has been defined. Our predictions in this paper\nare speculative as there are no results of the study yet, but our views are\nguided by implementation and deployment aspects. We expect that the views here\nwill guide researchers and industry practitioners."
                },
                "authors": [
                    {
                        "name": "Mansoor Shafi"
                    },
                    {
                        "name": "Erik G. Larsson"
                    },
                    {
                        "name": "Xingqin Lin"
                    },
                    {
                        "name": "Dorin Panaitopol"
                    },
                    {
                        "name": "Stefan Parkvall"
                    },
                    {
                        "name": "Flavien Ronteix-Jacquet"
                    },
                    {
                        "name": "Antti Toskala"
                    }
                ],
                "author_detail": {
                    "name": "Antti Toskala"
                },
                "author": "Antti Toskala",
                "arxiv_comment": "submitted to the Proceedings of the IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08224v1",
                "updated": "2025-08-11T17:43:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    43,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:43:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    43,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "Capabilities of GPT-5 on Multimodal Medical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capabilities of GPT-5 on Multimodal Medical Reasoning"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled general-purpose\nsystems to perform increasingly complex domain-specific reasoning without\nextensive fine-tuning. In the medical domain, decision-making often requires\nintegrating heterogeneous information sources, including patient narratives,\nstructured data, and medical images. This study positions GPT-5 as a generalist\nmultimodal reasoner for medical decision support and systematically evaluates\nits zero-shot chain-of-thought reasoning performance on both text-based\nquestion answering and visual question answering tasks under a unified\nprotocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20\nagainst standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU\nmedical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that\nGPT-5 consistently outperforms all baselines, achieving state-of-the-art\naccuracy across all QA benchmarks and delivering substantial gains in\nmultimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and\nunderstanding scores by +29.62% and +36.18% over GPT-4o, respectively, and\nsurpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in\nunderstanding. In contrast, GPT-4o remains below human expert performance in\nmost dimensions. A representative case study demonstrates GPT-5's ability to\nintegrate visual and textual cues into a coherent diagnostic reasoning chain,\nrecommending appropriate high-stakes interventions. Our results show that, on\nthese controlled multimodal reasoning benchmarks, GPT-5 moves from\nhuman-comparable to above human-expert performance. This improvement may\nsubstantially inform the design of future clinical decision-support systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled general-purpose\nsystems to perform increasingly complex domain-specific reasoning without\nextensive fine-tuning. In the medical domain, decision-making often requires\nintegrating heterogeneous information sources, including patient narratives,\nstructured data, and medical images. This study positions GPT-5 as a generalist\nmultimodal reasoner for medical decision support and systematically evaluates\nits zero-shot chain-of-thought reasoning performance on both text-based\nquestion answering and visual question answering tasks under a unified\nprotocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20\nagainst standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU\nmedical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that\nGPT-5 consistently outperforms all baselines, achieving state-of-the-art\naccuracy across all QA benchmarks and delivering substantial gains in\nmultimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and\nunderstanding scores by +29.62% and +36.18% over GPT-4o, respectively, and\nsurpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in\nunderstanding. In contrast, GPT-4o remains below human expert performance in\nmost dimensions. A representative case study demonstrates GPT-5's ability to\nintegrate visual and textual cues into a coherent diagnostic reasoning chain,\nrecommending appropriate high-stakes interventions. Our results show that, on\nthese controlled multimodal reasoning benchmarks, GPT-5 moves from\nhuman-comparable to above human-expert performance. This improvement may\nsubstantially inform the design of future clinical decision-support systems."
                },
                "authors": [
                    {
                        "name": "Shansong Wang"
                    },
                    {
                        "name": "Mingzhe Hu"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Mojtaba Safari"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Yang"
                },
                "author": "Xiaofeng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08221v1",
                "updated": "2025-08-11T17:39:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    39,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:39:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    39,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning"
                },
                "summary": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO."
                },
                "authors": [
                    {
                        "name": "Zihe Liu"
                    },
                    {
                        "name": "Jiashun Liu"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ling Pan"
                    },
                    {
                        "name": "Xinyu Hu"
                    },
                    {
                        "name": "Shaopan Xiong"
                    },
                    {
                        "name": "Ju Huang"
                    },
                    {
                        "name": "Jian Hu"
                    },
                    {
                        "name": "Shengyi Huang"
                    },
                    {
                        "name": "Siran Yang"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "26 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09373v2",
                "updated": "2025-08-11T17:38:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    38,
                    47,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-12T23:46:09Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    23,
                    46,
                    9,
                    5,
                    102,
                    0
                ],
                "title": "QUDsim: Quantifying Discourse Similarities in LLM-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUDsim: Quantifying Discourse Similarities in LLM-Generated Text"
                },
                "summary": "As large language models become increasingly capable at various writing\ntasks, their weakness at generating unique and creative content becomes a major\nliability. Although LLMs have the ability to generate text covering diverse\ntopics, there is an overall sense of repetitiveness across texts that we aim to\nformalize and quantify via a similarity metric. The familiarity between\ndocuments arises from the persistence of underlying discourse structures.\nHowever, existing similarity metrics dependent on lexical overlap and syntactic\npatterns largely capture $\\textit{content}$ overlap, thus making them\nunsuitable for detecting $\\textit{structural}$ similarities. We introduce an\nabstraction based on linguistic theories in Questions Under Discussion (QUD)\nand question semantics to help quantify differences in discourse progression.\nWe then use this framework to build $\\textbf{QUDsim}$, a similarity metric that\ncan detect discursive parallels between documents. Using QUDsim, we find that\nLLMs often reuse discourse structures (more so than humans) across samples,\neven when content differs. Furthermore, LLMs are not only repetitive and\nstructurally uniform, but are also divergent from human authors in the types of\nstructures they use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models become increasingly capable at various writing\ntasks, their weakness at generating unique and creative content becomes a major\nliability. Although LLMs have the ability to generate text covering diverse\ntopics, there is an overall sense of repetitiveness across texts that we aim to\nformalize and quantify via a similarity metric. The familiarity between\ndocuments arises from the persistence of underlying discourse structures.\nHowever, existing similarity metrics dependent on lexical overlap and syntactic\npatterns largely capture $\\textit{content}$ overlap, thus making them\nunsuitable for detecting $\\textit{structural}$ similarities. We introduce an\nabstraction based on linguistic theories in Questions Under Discussion (QUD)\nand question semantics to help quantify differences in discourse progression.\nWe then use this framework to build $\\textbf{QUDsim}$, a similarity metric that\ncan detect discursive parallels between documents. Using QUDsim, we find that\nLLMs often reuse discourse structures (more so than humans) across samples,\neven when content differs. Furthermore, LLMs are not only repetitive and\nstructurally uniform, but are also divergent from human authors in the types of\nstructures they use."
                },
                "authors": [
                    {
                        "name": "Ramya Namuduri"
                    },
                    {
                        "name": "Yating Wu"
                    },
                    {
                        "name": "Anshun Asher Zheng"
                    },
                    {
                        "name": "Manya Wadhwa"
                    },
                    {
                        "name": "Greg Durrett"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    }
                ],
                "author_detail": {
                    "name": "Junyi Jessy Li"
                },
                "author": "Junyi Jessy Li",
                "arxiv_comment": "COLM 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08211v1",
                "updated": "2025-08-11T17:33:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    33,
                    18,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:33:18Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    33,
                    18,
                    0,
                    223,
                    0
                ],
                "title": "SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling"
                },
                "summary": "Watermarking LLM-generated text is critical for content attribution and\nmisinformation prevention. However, existing methods compromise text quality,\nrequire white-box model access and logit manipulation. These limitations\nexclude API-based models and multilingual scenarios. We propose SAEMark, a\ngeneral framework for post-hoc multi-bit watermarking that embeds personalized\nmessages solely via inference-time, feature-based rejection sampling without\naltering model logits or requiring training. Our approach operates on\ndeterministic features extracted from generated text, selecting outputs whose\nfeature statistics align with key-derived targets. This framework naturally\ngeneralizes across languages and domains while preserving text quality through\nsampling LLM outputs instead of modifying. We provide theoretical guarantees\nrelating watermark success probability and compute budget that hold for any\nsuitable feature extractor. Empirically, we demonstrate the framework's\neffectiveness using Sparse Autoencoders (SAEs), achieving superior detection\naccuracy and text quality. Experiments across 4 datasets show SAEMark's\nconsistent performance, with 99.7% F1 on English and strong multi-bit detection\naccuracy. SAEMark establishes a new paradigm for scalable watermarking that\nworks out-of-the-box with closed-source LLMs while enabling content\nattribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking LLM-generated text is critical for content attribution and\nmisinformation prevention. However, existing methods compromise text quality,\nrequire white-box model access and logit manipulation. These limitations\nexclude API-based models and multilingual scenarios. We propose SAEMark, a\ngeneral framework for post-hoc multi-bit watermarking that embeds personalized\nmessages solely via inference-time, feature-based rejection sampling without\naltering model logits or requiring training. Our approach operates on\ndeterministic features extracted from generated text, selecting outputs whose\nfeature statistics align with key-derived targets. This framework naturally\ngeneralizes across languages and domains while preserving text quality through\nsampling LLM outputs instead of modifying. We provide theoretical guarantees\nrelating watermark success probability and compute budget that hold for any\nsuitable feature extractor. Empirically, we demonstrate the framework's\neffectiveness using Sparse Autoencoders (SAEs), achieving superior detection\naccuracy and text quality. Experiments across 4 datasets show SAEMark's\nconsistent performance, with 99.7% F1 on English and strong multi-bit detection\naccuracy. SAEMark establishes a new paradigm for scalable watermarking that\nworks out-of-the-box with closed-source LLMs while enabling content\nattribution."
                },
                "authors": [
                    {
                        "name": "Zhuohao Yu"
                    },
                    {
                        "name": "Xingru Jiang"
                    },
                    {
                        "name": "Weizheng Gu"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Shikun Zhang"
                    },
                    {
                        "name": "Wei Ye"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ye"
                },
                "author": "Wei Ye",
                "arxiv_comment": "24 pages, 12 figures, code available:\n  https://zhuohaoyu.github.io/SAEMark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16663v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16663v5",
                "updated": "2025-08-11T17:29:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    29,
                    56,
                    0,
                    223,
                    0
                ],
                "published": "2024-04-25T15:04:27Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    15,
                    4,
                    27,
                    3,
                    116,
                    0
                ],
                "title": "Runtime Monitoring and Enforcement of Conditional Fairness in Generative\n  AIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runtime Monitoring and Enforcement of Conditional Fairness in Generative\n  AIs"
                },
                "summary": "The deployment of generative AI (GenAI) models raises significant fairness\nconcerns, addressed in this paper through novel characterization and\nenforcement techniques specific to GenAI. Unlike standard AI performing\nspecific tasks, GenAI's broad functionality requires ``conditional fairness''\ntailored to the context being generated, such as demographic fairness in\ngenerating images of poor people versus successful business leaders. We define\ntwo fairness levels: the first evaluates fairness in generated outputs,\nindependent of prompts and models; the second assesses inherent fairness with\nneutral prompts. Given the complexity of GenAI and challenges in fairness\nspecifications, we focus on bounding the worst case, considering a GenAI system\nunfair if the distance between appearances of a specific group exceeds preset\nthresholds. We also explore combinatorial testing for assessing relative\ncompleteness in intersectional fairness. By bounding the worst case, we develop\na prompt injection scheme within an agent-based framework to enforce\nconditional fairness with minimal intervention, validated on state-of-the-art\nGenAI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of generative AI (GenAI) models raises significant fairness\nconcerns, addressed in this paper through novel characterization and\nenforcement techniques specific to GenAI. Unlike standard AI performing\nspecific tasks, GenAI's broad functionality requires ``conditional fairness''\ntailored to the context being generated, such as demographic fairness in\ngenerating images of poor people versus successful business leaders. We define\ntwo fairness levels: the first evaluates fairness in generated outputs,\nindependent of prompts and models; the second assesses inherent fairness with\nneutral prompts. Given the complexity of GenAI and challenges in fairness\nspecifications, we focus on bounding the worst case, considering a GenAI system\nunfair if the distance between appearances of a specific group exceeds preset\nthresholds. We also explore combinatorial testing for assessing relative\ncompleteness in intersectional fairness. By bounding the worst case, we develop\na prompt injection scheme within an agent-based framework to enforce\nconditional fairness with minimal intervention, validated on state-of-the-art\nGenAI systems."
                },
                "authors": [
                    {
                        "name": "Chih-Hong Cheng"
                    },
                    {
                        "name": "Changshun Wu"
                    },
                    {
                        "name": "Xingyu Zhao"
                    },
                    {
                        "name": "Saddek Bensalem"
                    },
                    {
                        "name": "Harald Ruess"
                    }
                ],
                "author_detail": {
                    "name": "Harald Ruess"
                },
                "author": "Harald Ruess",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16663v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16663v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08204v1",
                "updated": "2025-08-11T17:22:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    22,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:22:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    22,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "Human-Alignment and Calibration of Inference-Time Uncertainty in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Alignment and Calibration of Inference-Time Uncertainty in Large\n  Language Models"
                },
                "summary": "There has been much recent interest in evaluating large language models for\nuncertainty calibration to facilitate model control and modulate user trust.\nInference time uncertainty, which may provide a real-time signal to the model\nor external control modules, is particularly important for applying these\nconcepts to improve LLM-user experience in practice. While many of the existing\npapers consider model calibration, comparatively little work has sought to\nevaluate how closely model uncertainty aligns to human uncertainty. In this\nwork, we evaluate a collection of inference-time uncertainty measures, using\nboth established metrics and novel variations, to determine how closely they\nalign with both human group-level uncertainty and traditional notions of model\ncalibration. We find that numerous measures show evidence of strong alignment\nto human uncertainty, even despite the lack of alignment to human answer\npreference. For those successful metrics, we find moderate to strong evidence\nof model calibration in terms of both correctness correlation and\ndistributional analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been much recent interest in evaluating large language models for\nuncertainty calibration to facilitate model control and modulate user trust.\nInference time uncertainty, which may provide a real-time signal to the model\nor external control modules, is particularly important for applying these\nconcepts to improve LLM-user experience in practice. While many of the existing\npapers consider model calibration, comparatively little work has sought to\nevaluate how closely model uncertainty aligns to human uncertainty. In this\nwork, we evaluate a collection of inference-time uncertainty measures, using\nboth established metrics and novel variations, to determine how closely they\nalign with both human group-level uncertainty and traditional notions of model\ncalibration. We find that numerous measures show evidence of strong alignment\nto human uncertainty, even despite the lack of alignment to human answer\npreference. For those successful metrics, we find moderate to strong evidence\nof model calibration in terms of both correctness correlation and\ndistributional analysis."
                },
                "authors": [
                    {
                        "name": "Kyle Moore"
                    },
                    {
                        "name": "Jesse Roberts"
                    },
                    {
                        "name": "Daryl Watson"
                    }
                ],
                "author_detail": {
                    "name": "Daryl Watson"
                },
                "author": "Daryl Watson",
                "arxiv_comment": "preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17130v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17130v3",
                "updated": "2025-08-11T17:20:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    20,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-23T22:47:30Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    22,
                    47,
                    30,
                    2,
                    113,
                    0
                ],
                "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM\n  \"Thought\" Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering the CensorShip: Uncovering Representation Vectors for LLM\n  \"Thought\" Control"
                },
                "summary": "Large language models (LLMs) have transformed the way we access information.\nThese models are often tuned to refuse to comply with requests that are\nconsidered harmful and to produce responses that better align with the\npreferences of those who control the models. To understand how this\n\"censorship\" works. We use representation engineering techniques to study\nopen-weights safety-tuned models. We present a method for finding a\nrefusal--compliance vector that detects and controls the level of censorship in\nmodel outputs. We also analyze recent reasoning LLMs, distilled from\nDeepSeek-R1, and uncover an additional dimension of censorship through \"thought\nsuppression\". We show a similar approach can be used to find a vector that\nsuppresses the model's reasoning process, allowing us to remove censorship by\napplying the negative multiples of this vector. Our code is publicly available\nat: https://github.com/hannahxchen/llm-censorship-steering",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed the way we access information.\nThese models are often tuned to refuse to comply with requests that are\nconsidered harmful and to produce responses that better align with the\npreferences of those who control the models. To understand how this\n\"censorship\" works. We use representation engineering techniques to study\nopen-weights safety-tuned models. We present a method for finding a\nrefusal--compliance vector that detects and controls the level of censorship in\nmodel outputs. We also analyze recent reasoning LLMs, distilled from\nDeepSeek-R1, and uncover an additional dimension of censorship through \"thought\nsuppression\". We show a similar approach can be used to find a vector that\nsuppresses the model's reasoning process, allowing us to remove censorship by\napplying the negative multiples of this vector. Our code is publicly available\nat: https://github.com/hannahxchen/llm-censorship-steering"
                },
                "authors": [
                    {
                        "name": "Hannah Cyberey"
                    },
                    {
                        "name": "David Evans"
                    }
                ],
                "author_detail": {
                    "name": "David Evans"
                },
                "author": "David Evans",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17130v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17130v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08193v1",
                "updated": "2025-08-11T17:12:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    12,
                    55,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:12:55Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    12,
                    55,
                    0,
                    223,
                    0
                ],
                "title": "Street-Level AI: Are Large Language Models Ready for Real-World\n  Judgments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Street-Level AI: Are Large Language Models Ready for Real-World\n  Judgments?"
                },
                "summary": "A surge of recent work explores the ethical and societal implications of\nlarge-scale AI models that make \"moral\" judgments. Much of this literature\nfocuses either on alignment with human judgments through various thought\nexperiments or on the group fairness implications of AI judgments. However, the\nmost immediate and likely use of AI is to help or fully replace the so-called\nstreet-level bureaucrats, the individuals deciding to allocate scarce social\nresources or approve benefits. There is a rich history underlying how\nprinciples of local justice determine how society decides on prioritization\nmechanisms in such domains. In this paper, we examine how well LLM judgments\nalign with human judgments, as well as with socially and politically determined\nvulnerability scoring systems currently used in the domain of homelessness\nresource allocation. Crucially, we use real data on those needing services\n(maintaining strict confidentiality by only using local large models) to\nperform our analyses. We find that LLM prioritizations are extremely\ninconsistent in several ways: internally on different runs, between different\nLLMs, and between LLMs and the vulnerability scoring systems. At the same time,\nLLMs demonstrate qualitative consistency with lay human judgments in pairwise\ntesting. Findings call into question the readiness of current generation AI\nsystems for naive integration in high-stakes societal decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A surge of recent work explores the ethical and societal implications of\nlarge-scale AI models that make \"moral\" judgments. Much of this literature\nfocuses either on alignment with human judgments through various thought\nexperiments or on the group fairness implications of AI judgments. However, the\nmost immediate and likely use of AI is to help or fully replace the so-called\nstreet-level bureaucrats, the individuals deciding to allocate scarce social\nresources or approve benefits. There is a rich history underlying how\nprinciples of local justice determine how society decides on prioritization\nmechanisms in such domains. In this paper, we examine how well LLM judgments\nalign with human judgments, as well as with socially and politically determined\nvulnerability scoring systems currently used in the domain of homelessness\nresource allocation. Crucially, we use real data on those needing services\n(maintaining strict confidentiality by only using local large models) to\nperform our analyses. We find that LLM prioritizations are extremely\ninconsistent in several ways: internally on different runs, between different\nLLMs, and between LLMs and the vulnerability scoring systems. At the same time,\nLLMs demonstrate qualitative consistency with lay human judgments in pairwise\ntesting. Findings call into question the readiness of current generation AI\nsystems for naive integration in high-stakes societal decision-making."
                },
                "authors": [
                    {
                        "name": "Gaurab Pokharel"
                    },
                    {
                        "name": "Shafkat Farabi"
                    },
                    {
                        "name": "Patrick J. Fowler"
                    },
                    {
                        "name": "Sanmay Das"
                    }
                ],
                "author_detail": {
                    "name": "Sanmay Das"
                },
                "author": "Sanmay Das",
                "arxiv_comment": "This work has been accepted for publication as a full paper at the\n  AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08189v1",
                "updated": "2025-08-11T17:08:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    8,
                    55,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:08:55Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    8,
                    55,
                    0,
                    223,
                    0
                ],
                "title": "Reinforcement Learning in Vision: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning in Vision: A Survey"
                },
                "summary": "Recent advances at the intersection of reinforcement learning (RL) and visual\nintelligence have enabled agents that not only perceive complex visual scenes\nbut also reason, generate, and act within them. This survey offers a critical\nand up-to-date synthesis of the field. We first formalize visual RL problems\nand trace the evolution of policy-optimization strategies from RLHF to\nverifiable reward paradigms, and from Proximal Policy Optimization to Group\nRelative Policy Optimization. We then organize more than 200 representative\nworks into four thematic pillars: multi-modal large language models, visual\ngeneration, unified model frameworks, and vision-language-action models. For\neach pillar we examine algorithmic design, reward engineering, benchmark\nprogress, and we distill trends such as curriculum-driven training,\npreference-aligned diffusion, and unified reward modeling. Finally, we review\nevaluation protocols spanning set-level fidelity, sample-level preference, and\nstate-level stability, and we identify open challenges that include sample\nefficiency, generalization, and safe deployment. Our goal is to provide\nresearchers and practitioners with a coherent map of the rapidly expanding\nlandscape of visual RL and to highlight promising directions for future\ninquiry. Resources are available at:\nhttps://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances at the intersection of reinforcement learning (RL) and visual\nintelligence have enabled agents that not only perceive complex visual scenes\nbut also reason, generate, and act within them. This survey offers a critical\nand up-to-date synthesis of the field. We first formalize visual RL problems\nand trace the evolution of policy-optimization strategies from RLHF to\nverifiable reward paradigms, and from Proximal Policy Optimization to Group\nRelative Policy Optimization. We then organize more than 200 representative\nworks into four thematic pillars: multi-modal large language models, visual\ngeneration, unified model frameworks, and vision-language-action models. For\neach pillar we examine algorithmic design, reward engineering, benchmark\nprogress, and we distill trends such as curriculum-driven training,\npreference-aligned diffusion, and unified reward modeling. Finally, we review\nevaluation protocols spanning set-level fidelity, sample-level preference, and\nstate-level stability, and we identify open challenges that include sample\nefficiency, generalization, and safe deployment. Our goal is to provide\nresearchers and practitioners with a coherent map of the rapidly expanding\nlandscape of visual RL and to highlight promising directions for future\ninquiry. Resources are available at:\nhttps://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning."
                },
                "authors": [
                    {
                        "name": "Weijia Wu"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qingwei Meng"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Yuke Qiu"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08186v1",
                "updated": "2025-08-11T17:06:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    6,
                    55,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T17:06:55Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    6,
                    55,
                    0,
                    223,
                    0
                ],
                "title": "KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold\n  Representation Learning"
                },
                "summary": "Semantic segmentation of structural defects in civil infrastructure remains\nchallenging due to variable defect appearances, harsh imaging conditions, and\nsignificant class imbalance. Current deep learning methods, despite their\neffectiveness, typically require millions of parameters, rendering them\nimpractical for real-time inspection systems. We introduce KARMA\n(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient\nsemantic segmentation framework that models complex defect patterns through\ncompositions of one-dimensional functions rather than conventional\nconvolutions. KARMA features three technical innovations: (1) a\nparameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging\nlow-rank factorization for KAN-based feature transformation; (2) an optimized\nfeature pyramid structure with separable convolutions for multi-scale defect\nanalysis; and (3) a static-dynamic prototype mechanism that enhances feature\nrepresentation for imbalanced classes. Extensive experiments on benchmark\ninfrastructure inspection datasets demonstrate that KARMA achieves competitive\nor superior mean IoU performance compared to state-of-the-art approaches, while\nusing significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).\nOperating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for\nreal-time deployment, enabling practical automated infrastructure inspection\nsystems without compromising accuracy. The source code can be accessed at the\nfollowing URL: https://github.com/faeyelab/karma.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic segmentation of structural defects in civil infrastructure remains\nchallenging due to variable defect appearances, harsh imaging conditions, and\nsignificant class imbalance. Current deep learning methods, despite their\neffectiveness, typically require millions of parameters, rendering them\nimpractical for real-time inspection systems. We introduce KARMA\n(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient\nsemantic segmentation framework that models complex defect patterns through\ncompositions of one-dimensional functions rather than conventional\nconvolutions. KARMA features three technical innovations: (1) a\nparameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging\nlow-rank factorization for KAN-based feature transformation; (2) an optimized\nfeature pyramid structure with separable convolutions for multi-scale defect\nanalysis; and (3) a static-dynamic prototype mechanism that enhances feature\nrepresentation for imbalanced classes. Extensive experiments on benchmark\ninfrastructure inspection datasets demonstrate that KARMA achieves competitive\nor superior mean IoU performance compared to state-of-the-art approaches, while\nusing significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).\nOperating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for\nreal-time deployment, enabling practical automated infrastructure inspection\nsystems without compromising accuracy. The source code can be accessed at the\nfollowing URL: https://github.com/faeyelab/karma."
                },
                "authors": [
                    {
                        "name": "Md Meftahul Ferdaus"
                    },
                    {
                        "name": "Mahdi Abdelguerfi"
                    },
                    {
                        "name": "Elias Ioup"
                    },
                    {
                        "name": "Steven Sloan"
                    },
                    {
                        "name": "Kendall N. Niles"
                    },
                    {
                        "name": "Ken Pathak"
                    }
                ],
                "author_detail": {
                    "name": "Ken Pathak"
                },
                "author": "Ken Pathak",
                "arxiv_comment": "submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02904v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02904v2",
                "updated": "2025-08-11T16:54:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    54,
                    1,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-03T06:30:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    6,
                    30,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge,\n  Truthfulness, Refusal, and Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge,\n  Truthfulness, Refusal, and Confidence"
                },
                "summary": "Post-training is essential for the success of large language models (LLMs),\ntransforming pre-trained base models into more useful and aligned post-trained\nmodels. While plenty of works have studied post-training algorithms and\nevaluated post-training models by their outputs, it remains understudied how\npost-training reshapes LLMs internally. In this paper, we compare base and\npost-trained LLMs mechanistically from four perspectives to better understand\npost-training effects. Our findings across model families and datasets reveal\nthat: (1) Post-training does not change the factual knowledge storage\nlocations, and it adapts knowledge representations from the base model while\ndeveloping new knowledge representations; (2) Both truthfulness and refusal can\nbe represented by vectors in the hidden representation space. The truthfulness\ndirection is highly similar between the base and post-trained model, and it is\neffectively transferable for interventions; (3) The refusal direction is\ndifferent between the base and post-trained models, and it shows limited\nforward transferability; (4) Differences in confidence between the base and\npost-trained models cannot be attributed to entropy neurons. Our study provides\ninsights into the fundamental mechanisms preserved and altered during\npost-training, facilitates downstream tasks like model steering, and could\npotentially benefit future research in interpretability and LLM post-training.\nOur code is publicly available at\nhttps://github.com/HZD01/post-training-mechanistic-analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training is essential for the success of large language models (LLMs),\ntransforming pre-trained base models into more useful and aligned post-trained\nmodels. While plenty of works have studied post-training algorithms and\nevaluated post-training models by their outputs, it remains understudied how\npost-training reshapes LLMs internally. In this paper, we compare base and\npost-trained LLMs mechanistically from four perspectives to better understand\npost-training effects. Our findings across model families and datasets reveal\nthat: (1) Post-training does not change the factual knowledge storage\nlocations, and it adapts knowledge representations from the base model while\ndeveloping new knowledge representations; (2) Both truthfulness and refusal can\nbe represented by vectors in the hidden representation space. The truthfulness\ndirection is highly similar between the base and post-trained model, and it is\neffectively transferable for interventions; (3) The refusal direction is\ndifferent between the base and post-trained models, and it shows limited\nforward transferability; (4) Differences in confidence between the base and\npost-trained models cannot be attributed to entropy neurons. Our study provides\ninsights into the fundamental mechanisms preserved and altered during\npost-training, facilitates downstream tasks like model steering, and could\npotentially benefit future research in interpretability and LLM post-training.\nOur code is publicly available at\nhttps://github.com/HZD01/post-training-mechanistic-analysis."
                },
                "authors": [
                    {
                        "name": "Hongzhe Du"
                    },
                    {
                        "name": "Weikai Li"
                    },
                    {
                        "name": "Min Cai"
                    },
                    {
                        "name": "Karim Saraipour"
                    },
                    {
                        "name": "Zimin Zhang"
                    },
                    {
                        "name": "Himabindu Lakkaraju"
                    },
                    {
                        "name": "Yizhou Sun"
                    },
                    {
                        "name": "Shichang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shichang Zhang"
                },
                "author": "Shichang Zhang",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02904v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08171v1",
                "updated": "2025-08-11T16:49:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    49,
                    7,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:49:07Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    49,
                    7,
                    0,
                    223,
                    0
                ],
                "title": "PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded\n  Model Checking for C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded\n  Model Checking for C"
                },
                "summary": "Python has become the dominant language for general-purpose programming, yet\nit lacks robust tools for formal verification. In contrast, programmers working\nin languages such as C benefit from mature model checkers, for example CBMC,\nwhich enable exhaustive symbolic reasoning and fault localisation. The inherent\ncomplexity of Python, coupled with the verbosity and low-level nature of\nexisting transpilers (e.g., Cython), have historically limited the\napplicability of formal verification to Python programs.\n  In this paper, we propose PyVeritas, a novel framework that leverages Large\nLanguage Models (LLMs) for high-level transpilation from Python to C, followed\nby bounded model checking and MaxSAT-based fault localisation in the generated\nC code. PyVeritas enables verification and bug localisation for Python code\nusing existing model checking tools for C. Our empirical evaluation on two\nPython benchmarks demonstrates that LLM-based transpilation can achieve a high\ndegree of accuracy, up to 80--90% for some LLMs, enabling effective development\nenvironment that supports assertion-based verification and interpretable fault\ndiagnosis for small yet non-trivial Python programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python has become the dominant language for general-purpose programming, yet\nit lacks robust tools for formal verification. In contrast, programmers working\nin languages such as C benefit from mature model checkers, for example CBMC,\nwhich enable exhaustive symbolic reasoning and fault localisation. The inherent\ncomplexity of Python, coupled with the verbosity and low-level nature of\nexisting transpilers (e.g., Cython), have historically limited the\napplicability of formal verification to Python programs.\n  In this paper, we propose PyVeritas, a novel framework that leverages Large\nLanguage Models (LLMs) for high-level transpilation from Python to C, followed\nby bounded model checking and MaxSAT-based fault localisation in the generated\nC code. PyVeritas enables verification and bug localisation for Python code\nusing existing model checking tools for C. Our empirical evaluation on two\nPython benchmarks demonstrates that LLM-based transpilation can achieve a high\ndegree of accuracy, up to 80--90% for some LLMs, enabling effective development\nenvironment that supports assertion-based verification and interpretable fault\ndiagnosis for small yet non-trivial Python programs."
                },
                "authors": [
                    {
                        "name": "Pedro Orvalho"
                    },
                    {
                        "name": "Marta Kwiatkowska"
                    }
                ],
                "author_detail": {
                    "name": "Marta Kwiatkowska"
                },
                "author": "Marta Kwiatkowska",
                "arxiv_comment": "14 pages, 6 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23701v2",
                "updated": "2025-08-11T16:46:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    46,
                    8,
                    0,
                    223,
                    0
                ],
                "published": "2025-07-31T16:22:55Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    22,
                    55,
                    3,
                    212,
                    0
                ],
                "title": "TextQuests: How Good are LLMs at Text-Based Video Games?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextQuests: How Good are LLMs at Text-Based Video Games?"
                },
                "summary": "Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai."
                },
                "authors": [
                    {
                        "name": "Long Phan"
                    },
                    {
                        "name": "Mantas Mazeika"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Dan Hendrycks"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hendrycks"
                },
                "author": "Dan Hendrycks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08149v2",
                "updated": "2025-08-12T03:54:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    54,
                    24,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T16:25:25Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    25,
                    25,
                    0,
                    223,
                    0
                ],
                "title": "REX-RAG: Reasoning Exploration with Policy Correction in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REX-RAG: Reasoning Exploration with Policy Correction in\n  Retrieval-Augmented Generation"
                },
                "summary": "Reinforcement learning (RL) is emerging as a powerful paradigm for enabling\nlarge language models (LLMs) to perform complex reasoning tasks. Recent\nadvances indicate that integrating RL with retrieval-augmented generation (RAG)\nallows LLMs to dynamically incorporate external knowledge, leading to more\ninformed and robust decision making. However, we identify a critical challenge\nduring policy-driven trajectory sampling: LLMs are frequently trapped in\nunproductive reasoning paths, which we refer to as \"dead ends\", committing to\noverconfident yet incorrect conclusions. This severely hampers exploration and\nundermines effective policy optimization. To address this challenge, we propose\nREX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented\nGeneration), a novel framework that explores alternative reasoning paths while\nmaintaining rigorous policy learning through principled distributional\ncorrections. Our approach introduces two key innovations: (1) Mixed Sampling\nStrategy, which combines a novel probe sampling method with exploratory prompts\nto escape dead ends; and (2) Policy Correction Mechanism, which employs\nimportance sampling to correct distribution shifts induced by mixed sampling,\nthereby mitigating gradient estimation bias. We evaluate it on seven\nquestion-answering benchmarks, and the experimental results show that REX-RAG\nachieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B\nover strong baselines, demonstrating competitive results across multiple\ndatasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is emerging as a powerful paradigm for enabling\nlarge language models (LLMs) to perform complex reasoning tasks. Recent\nadvances indicate that integrating RL with retrieval-augmented generation (RAG)\nallows LLMs to dynamically incorporate external knowledge, leading to more\ninformed and robust decision making. However, we identify a critical challenge\nduring policy-driven trajectory sampling: LLMs are frequently trapped in\nunproductive reasoning paths, which we refer to as \"dead ends\", committing to\noverconfident yet incorrect conclusions. This severely hampers exploration and\nundermines effective policy optimization. To address this challenge, we propose\nREX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented\nGeneration), a novel framework that explores alternative reasoning paths while\nmaintaining rigorous policy learning through principled distributional\ncorrections. Our approach introduces two key innovations: (1) Mixed Sampling\nStrategy, which combines a novel probe sampling method with exploratory prompts\nto escape dead ends; and (2) Policy Correction Mechanism, which employs\nimportance sampling to correct distribution shifts induced by mixed sampling,\nthereby mitigating gradient estimation bias. We evaluate it on seven\nquestion-answering benchmarks, and the experimental results show that REX-RAG\nachieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B\nover strong baselines, demonstrating competitive results across multiple\ndatasets. The code is publicly available at https://github.com/MiliLab/REX-RAG."
                },
                "authors": [
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Xiang Feng"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Yong Luo"
                    },
                    {
                        "name": "Pingbo Xu"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "arxiv_comment": "17 pages, 4 figures; updated references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12962v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12962v2",
                "updated": "2025-08-11T16:25:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    25,
                    8,
                    0,
                    223,
                    0
                ],
                "published": "2024-09-19T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    17,
                    59,
                    52,
                    3,
                    263,
                    0
                ],
                "title": "CLAIR-A: Leveraging Large Language Models to Judge Audio Captions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLAIR-A: Leveraging Large Language Models to Judge Audio Captions"
                },
                "summary": "The Automated Audio Captioning (AAC) task asks models to generate natural\nlanguage descriptions of an audio input. Evaluating these machine-generated\naudio captions is a complex task that requires considering diverse factors,\namong them, auditory scene understanding, sound-object inference, temporal\ncoherence, and the environmental context of the scene. While current methods\nfocus on specific aspects, they often fail to provide an overall score that\naligns well with human judgment. In this work, we propose CLAIR-A, a simple and\nflexible method that leverages the zero-shot capabilities of large language\nmodels (LLMs) to evaluate candidate audio captions by directly asking LLMs for\na semantic distance score. In our evaluations, CLAIR-A better predicts human\njudgements of quality compared to traditional metrics, with a 5.8% relative\naccuracy improvement compared to the domain-specific FENSE metric and up to 11%\nover the best general-purpose measure on the Clotho-Eval dataset. Moreover,\nCLAIR-A offers more transparency by allowing the language model to explain the\nreasoning behind its scores, with these explanations rated up to 30% better by\nhuman evaluators than those provided by baseline methods. CLAIR-A is made\npublicly available at https://github.com/DavidMChan/clair-a.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Automated Audio Captioning (AAC) task asks models to generate natural\nlanguage descriptions of an audio input. Evaluating these machine-generated\naudio captions is a complex task that requires considering diverse factors,\namong them, auditory scene understanding, sound-object inference, temporal\ncoherence, and the environmental context of the scene. While current methods\nfocus on specific aspects, they often fail to provide an overall score that\naligns well with human judgment. In this work, we propose CLAIR-A, a simple and\nflexible method that leverages the zero-shot capabilities of large language\nmodels (LLMs) to evaluate candidate audio captions by directly asking LLMs for\na semantic distance score. In our evaluations, CLAIR-A better predicts human\njudgements of quality compared to traditional metrics, with a 5.8% relative\naccuracy improvement compared to the domain-specific FENSE metric and up to 11%\nover the best general-purpose measure on the Clotho-Eval dataset. Moreover,\nCLAIR-A offers more transparency by allowing the language model to explain the\nreasoning behind its scores, with these explanations rated up to 30% better by\nhuman evaluators than those provided by baseline methods. CLAIR-A is made\npublicly available at https://github.com/DavidMChan/clair-a."
                },
                "authors": [
                    {
                        "name": "Tsung-Han Wu"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "David M. Chan"
                    }
                ],
                "author_detail": {
                    "name": "David M. Chan"
                },
                "author": "David M. Chan",
                "arxiv_comment": "Accepted to ASRU 2025; Code is publicly available at\n  https://github.com/DavidMChan/clair-a",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12962v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21931v2",
                "updated": "2025-08-11T16:24:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    24,
                    36,
                    0,
                    223,
                    0
                ],
                "published": "2025-06-27T05:45:59Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    5,
                    45,
                    59,
                    4,
                    178,
                    0
                ],
                "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARAG: Agentic Retrieval Augmented Generation for Personalized\n  Recommendation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization."
                },
                "authors": [
                    {
                        "name": "Reza Yousefi Maragheh"
                    },
                    {
                        "name": "Pratheek Vadla"
                    },
                    {
                        "name": "Priyank Gupta"
                    },
                    {
                        "name": "Kai Zhao"
                    },
                    {
                        "name": "Aysenur Inan"
                    },
                    {
                        "name": "Kehui Yao"
                    },
                    {
                        "name": "Jianpeng Xu"
                    },
                    {
                        "name": "Praveen Kanumala"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Sushant Kumar"
                },
                "author": "Sushant Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08147v1",
                "updated": "2025-08-11T16:22:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    22,
                    57,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:22:57Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    22,
                    57,
                    0,
                    223,
                    0
                ],
                "title": "From Natural Language to Solver-Ready Power System Optimization: An\n  LLM-Assisted, Validation-in-the-Loop Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Natural Language to Solver-Ready Power System Optimization: An\n  LLM-Assisted, Validation-in-the-Loop Framework"
                },
                "summary": "This paper introduces a novel Large Language Models (LLMs)-assisted agent\nthat automatically converts natural-language descriptions of power system\noptimization scenarios into compact, solver-ready formulations and generates\ncorresponding solutions. In contrast to approaches that rely solely on LLM to\nproduce solutions directly, the proposed method focuses on discovering a\nmathematically compatible formulation that can be efficiently solved by\noff-the-shelf optimization solvers. Directly using LLMs to produce solutions\noften leads to infeasible or suboptimal results, as these models lack the\nnumerical precision and constraint-handling capabilities of established\noptimization solvers. The pipeline integrates a domain-aware prompt and schema\nwith an LLM, enforces feasibility through systematic validation and iterative\nrepair, and returns both solver-ready models and user-facing results. Using the\nunit commitment problem as a representative case study, the agent produces\noptimal or near-optimal schedules along with the associated objective costs.\nResults demonstrate that coupling the solver with task-specific validation\nsignificantly enhances solution reliability. This work shows that combining AI\nwith established optimization frameworks bridges high-level problem\ndescriptions and executable mathematical models, enabling more efficient\ndecision-making in energy systems",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel Large Language Models (LLMs)-assisted agent\nthat automatically converts natural-language descriptions of power system\noptimization scenarios into compact, solver-ready formulations and generates\ncorresponding solutions. In contrast to approaches that rely solely on LLM to\nproduce solutions directly, the proposed method focuses on discovering a\nmathematically compatible formulation that can be efficiently solved by\noff-the-shelf optimization solvers. Directly using LLMs to produce solutions\noften leads to infeasible or suboptimal results, as these models lack the\nnumerical precision and constraint-handling capabilities of established\noptimization solvers. The pipeline integrates a domain-aware prompt and schema\nwith an LLM, enforces feasibility through systematic validation and iterative\nrepair, and returns both solver-ready models and user-facing results. Using the\nunit commitment problem as a representative case study, the agent produces\noptimal or near-optimal schedules along with the associated objective costs.\nResults demonstrate that coupling the solver with task-specific validation\nsignificantly enhances solution reliability. This work shows that combining AI\nwith established optimization frameworks bridges high-level problem\ndescriptions and executable mathematical models, enabling more efficient\ndecision-making in energy systems"
                },
                "authors": [
                    {
                        "name": "Yunkai Hu"
                    },
                    {
                        "name": "Tianqiao Zhao"
                    },
                    {
                        "name": "Meng Yue"
                    }
                ],
                "author_detail": {
                    "name": "Meng Yue"
                },
                "author": "Meng Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08144v1",
                "updated": "2025-08-11T16:16:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    16,
                    51,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:16:51Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    16,
                    51,
                    0,
                    223,
                    0
                ],
                "title": "COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space\n  Models"
                },
                "summary": "The rapid growth of resource-constrained mobile platforms, including mobile\nrobots, wearable systems, and Internet-of-Things devices, has increased the\ndemand for computationally efficient neural network controllers (NNCs) that can\noperate within strict hardware limitations. While deep neural networks (DNNs)\ndemonstrate superior performance in control applications, their substantial\ncomputational complexity and memory requirements present significant barriers\nto practical deployment on edge devices. This paper introduces a comprehensive\nmodel compression methodology that leverages component-aware structured pruning\nto determine the optimal pruning magnitude for each pruning group, ensuring a\nbalance between compression and stability for NNC deployment. Our approach is\nrigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC),\na state-of-the-art model-based reinforcement learning algorithm, with a\nsystematic integration of mathematical stability guarantee properties,\nspecifically Lyapunov criteria. The key contribution of this work lies in\nproviding a principled framework for determining the theoretical limits of\nmodel compression while preserving controller stability. Experimental\nvalidation demonstrates that our methodology successfully reduces model\ncomplexity while maintaining requisite control performance and stability\ncharacteristics. Furthermore, our approach establishes a quantitative boundary\nfor safe compression ratios, enabling practitioners to systematically determine\nthe maximum permissible model reduction before violating critical stability\nproperties, thereby facilitating the confident deployment of compressed NNCs in\nresource-limited environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of resource-constrained mobile platforms, including mobile\nrobots, wearable systems, and Internet-of-Things devices, has increased the\ndemand for computationally efficient neural network controllers (NNCs) that can\noperate within strict hardware limitations. While deep neural networks (DNNs)\ndemonstrate superior performance in control applications, their substantial\ncomputational complexity and memory requirements present significant barriers\nto practical deployment on edge devices. This paper introduces a comprehensive\nmodel compression methodology that leverages component-aware structured pruning\nto determine the optimal pruning magnitude for each pruning group, ensuring a\nbalance between compression and stability for NNC deployment. Our approach is\nrigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC),\na state-of-the-art model-based reinforcement learning algorithm, with a\nsystematic integration of mathematical stability guarantee properties,\nspecifically Lyapunov criteria. The key contribution of this work lies in\nproviding a principled framework for determining the theoretical limits of\nmodel compression while preserving controller stability. Experimental\nvalidation demonstrates that our methodology successfully reduces model\ncomplexity while maintaining requisite control performance and stability\ncharacteristics. Furthermore, our approach establishes a quantitative boundary\nfor safe compression ratios, enabling practitioners to systematically determine\nthe maximum permissible model reduction before violating critical stability\nproperties, thereby facilitating the confident deployment of compressed NNCs in\nresource-limited environments."
                },
                "authors": [
                    {
                        "name": "Ganesh Sundaram"
                    },
                    {
                        "name": "Jonas Ulmen"
                    },
                    {
                        "name": "Amjad Haider"
                    },
                    {
                        "name": "Daniel Görges"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Görges"
                },
                "author": "Daniel Görges",
                "arxiv_comment": "Submitted in: The 2026 IEEE/SICE International Symposium on System\n  Integration (SII 2026)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08140v1",
                "updated": "2025-08-11T16:13:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    13,
                    21,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:13:21Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    13,
                    21,
                    0,
                    223,
                    0
                ],
                "title": "Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced\n  Submodular Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced\n  Submodular Perspective"
                },
                "summary": "Recent progress in large language models (LLMs) has leveraged their\nin-context learning (ICL) abilities to enable quick adaptation to unseen\nbiomedical NLP tasks. By incorporating only a few input-output examples into\nprompts, LLMs can rapidly perform these new tasks. While the impact of these\ndemonstrations on LLM performance has been extensively studied, most existing\napproaches prioritize representativeness over diversity when selecting examples\nfrom large corpora. To address this gap, we propose Dual-Div, a\ndiversity-enhanced data-efficient framework for demonstration selection in\nbiomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:\nFirst, it identifies a limited set of candidate examples from a corpus by\noptimizing both representativeness and diversity (with optional annotation for\nunlabeled data). Second, it ranks these candidates against test queries to\nselect the most relevant and non-redundant demonstrations. Evaluated on three\nbiomedical NLP tasks (named entity recognition (NER), relation extraction (RE),\nand text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along\nwith three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently\noutperforms baselines-achieving up to 5% higher macro-F1 scores-while\ndemonstrating robustness to prompt permutations and class imbalance. Our\nfindings establish that diversity in initial retrieval is more critical than\nranking-stage optimization, and limiting demonstrations to 3-5 examples\nmaximizes performance efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has leveraged their\nin-context learning (ICL) abilities to enable quick adaptation to unseen\nbiomedical NLP tasks. By incorporating only a few input-output examples into\nprompts, LLMs can rapidly perform these new tasks. While the impact of these\ndemonstrations on LLM performance has been extensively studied, most existing\napproaches prioritize representativeness over diversity when selecting examples\nfrom large corpora. To address this gap, we propose Dual-Div, a\ndiversity-enhanced data-efficient framework for demonstration selection in\nbiomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:\nFirst, it identifies a limited set of candidate examples from a corpus by\noptimizing both representativeness and diversity (with optional annotation for\nunlabeled data). Second, it ranks these candidates against test queries to\nselect the most relevant and non-redundant demonstrations. Evaluated on three\nbiomedical NLP tasks (named entity recognition (NER), relation extraction (RE),\nand text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along\nwith three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently\noutperforms baselines-achieving up to 5% higher macro-F1 scores-while\ndemonstrating robustness to prompt permutations and class imbalance. Our\nfindings establish that diversity in initial retrieval is more critical than\nranking-stage optimization, and limiting demonstrations to 3-5 examples\nmaximizes performance efficiency."
                },
                "authors": [
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Zaifu Zhan"
                    },
                    {
                        "name": "Qixin Zhang"
                    },
                    {
                        "name": "Mingquan Lin"
                    },
                    {
                        "name": "Meijia Song"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08139v1",
                "updated": "2025-08-11T16:12:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    12,
                    36,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:12:36Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    12,
                    36,
                    0,
                    223,
                    0
                ],
                "title": "Can LLMs Detect Their Confabulations? Estimating Reliability in\n  Uncertainty-Aware Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Detect Their Confabulations? Estimating Reliability in\n  Uncertainty-Aware Language Models"
                },
                "summary": "Large Language Models (LLMs) are prone to generating fluent but incorrect\ncontent, known as confabulation, which poses increasing risks in multi-turn or\nagentic applications where outputs may be reused as context. In this work, we\ninvestigate how in-context information influences model behavior and whether\nLLMs can identify their unreliable responses. We propose a reliability\nestimation that leverages token-level uncertainty to guide the aggregation of\ninternal model representations. Specifically, we compute aleatoric and\nepistemic uncertainty from output logits to identify salient tokens and\naggregate their hidden states into compact representations for response-level\nreliability prediction. Through controlled experiments on open QA benchmarks,\nwe find that correct in-context information improves both answer accuracy and\nmodel confidence, while misleading context often induces confidently incorrect\nresponses, revealing a misalignment between uncertainty and correctness. Our\nprobing-based method captures these shifts in model behavior and improves the\ndetection of unreliable outputs across multiple open-source LLMs. These results\nunderscore the limitations of direct uncertainty signals and highlight the\npotential of uncertainty-guided probing for reliability-aware generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to generating fluent but incorrect\ncontent, known as confabulation, which poses increasing risks in multi-turn or\nagentic applications where outputs may be reused as context. In this work, we\ninvestigate how in-context information influences model behavior and whether\nLLMs can identify their unreliable responses. We propose a reliability\nestimation that leverages token-level uncertainty to guide the aggregation of\ninternal model representations. Specifically, we compute aleatoric and\nepistemic uncertainty from output logits to identify salient tokens and\naggregate their hidden states into compact representations for response-level\nreliability prediction. Through controlled experiments on open QA benchmarks,\nwe find that correct in-context information improves both answer accuracy and\nmodel confidence, while misleading context often induces confidently incorrect\nresponses, revealing a misalignment between uncertainty and correctness. Our\nprobing-based method captures these shifts in model behavior and improves the\ndetection of unreliable outputs across multiple open-source LLMs. These results\nunderscore the limitations of direct uncertainty signals and highlight the\npotential of uncertainty-guided probing for reliability-aware generation."
                },
                "authors": [
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Johanne Medina"
                    },
                    {
                        "name": "Sanjay Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay Chawla"
                },
                "author": "Sanjay Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08137v1",
                "updated": "2025-08-11T16:11:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    11,
                    9,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:11:09Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    11,
                    9,
                    0,
                    223,
                    0
                ],
                "title": "MuaLLM: A Multimodal Large Language Model Agent for Circuit Design\n  Assistance with Hybrid Contextual Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuaLLM: A Multimodal Large Language Model Agent for Circuit Design\n  Assistance with Hybrid Contextual Retrieval-Augmented Generation"
                },
                "summary": "Conducting a comprehensive literature review is crucial for advancing circuit\ndesign methodologies. However, the rapid influx of state-of-the-art research,\ninconsistent data representation, and the complexity of optimizing circuit\ndesign objectives make this task significantly challenging. In this paper, we\npropose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for\ncircuit design assistance that integrates a hybrid Retrieval-Augmented\nGeneration (RAG) framework with an adaptive vector database of circuit design\nresearch papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason +\nAct (ReAct) workflow for iterative reasoning, goal-setting, and multi-step\ninformation retrieval. It functions as a question-answering design assistant,\ncapable of interpreting complex queries and providing reasoned responses\ngrounded in circuit literature. Its multimodal capabilities enable processing\nof both textual and visual data, facilitating more efficient and comprehensive\nanalysis. The system dynamically adapts using intelligent search tools,\nautomated document retrieval from the internet, and real-time database updates.\nUnlike conventional approaches constrained by model context limits, MuaLLM\ndecouples retrieval from inference, enabling scalable reasoning over\narbitrarily large corpora. At the maximum context length supported by standard\nLLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining\nthe same accuracy. This allows rapid, no-human-in-the-loop database generation,\novercoming the bottleneck of simulation-based dataset creation for circuits. To\nevaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval\nand citation performance, and Reasoning-100 (Reas-100), focused on multistep\nreasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8%\naccuracy on Reas-100.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conducting a comprehensive literature review is crucial for advancing circuit\ndesign methodologies. However, the rapid influx of state-of-the-art research,\ninconsistent data representation, and the complexity of optimizing circuit\ndesign objectives make this task significantly challenging. In this paper, we\npropose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for\ncircuit design assistance that integrates a hybrid Retrieval-Augmented\nGeneration (RAG) framework with an adaptive vector database of circuit design\nresearch papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason +\nAct (ReAct) workflow for iterative reasoning, goal-setting, and multi-step\ninformation retrieval. It functions as a question-answering design assistant,\ncapable of interpreting complex queries and providing reasoned responses\ngrounded in circuit literature. Its multimodal capabilities enable processing\nof both textual and visual data, facilitating more efficient and comprehensive\nanalysis. The system dynamically adapts using intelligent search tools,\nautomated document retrieval from the internet, and real-time database updates.\nUnlike conventional approaches constrained by model context limits, MuaLLM\ndecouples retrieval from inference, enabling scalable reasoning over\narbitrarily large corpora. At the maximum context length supported by standard\nLLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining\nthe same accuracy. This allows rapid, no-human-in-the-loop database generation,\novercoming the bottleneck of simulation-based dataset creation for circuits. To\nevaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval\nand citation performance, and Reasoning-100 (Reas-100), focused on multistep\nreasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8%\naccuracy on Reas-100."
                },
                "authors": [
                    {
                        "name": "Pravallika Abbineni"
                    },
                    {
                        "name": "Saoud Aldowaish"
                    },
                    {
                        "name": "Colin Liechty"
                    },
                    {
                        "name": "Soroosh Noorzad"
                    },
                    {
                        "name": "Ali Ghazizadeh"
                    },
                    {
                        "name": "Morteza Fayazi"
                    }
                ],
                "author_detail": {
                    "name": "Morteza Fayazi"
                },
                "author": "Morteza Fayazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05592v2",
                "updated": "2025-08-11T16:10:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    56,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-07T17:32:14Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    32,
                    14,
                    3,
                    219,
                    0
                ],
                "title": "MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging\n  Synthetic Problems with a Reinforced Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging\n  Synthetic Problems with a Reinforced Policy"
                },
                "summary": "Large language models have achieved substantial progress in mathematical\nreasoning, yet their advancement is limited by the scarcity of high-quality,\nhigh-difficulty training data. Existing synthesis methods largely rely on\ntransforming human-written templates, limiting both diversity and scalability.\nWe propose MathSmith, a novel framework for synthesizing challenging\nmathematical problems to enhance LLM reasoning. Rather than modifying existing\nproblems, MathSmith constructs new ones from scratch by randomly sampling\nconcept-explanation pairs from PlanetMath, ensuring data independence and\navoiding contamination. To increase difficulty, we design nine predefined\nstrategies as soft constraints during rationales. We further adopts\nreinforcement learning to jointly optimize structural validity, reasoning\ncomplexity, and answer consistency. The length of the reasoning trace generated\nunder autoregressive prompting is used to reflect cognitive complexity,\nencouraging the creation of more demanding problems aligned with\nlong-chain-of-thought reasoning. Experiments across five benchmarks,\ncategorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,\nOlympiadBench), show that MathSmith consistently outperforms existing baselines\nunder both short and long CoT settings. Additionally, a weakness-focused\nvariant generation module enables targeted improvement on specific concepts.\nOverall, MathSmith exhibits strong scalability, generalization, and\ntransferability, highlighting the promise of high-difficulty synthetic data in\nadvancing LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved substantial progress in mathematical\nreasoning, yet their advancement is limited by the scarcity of high-quality,\nhigh-difficulty training data. Existing synthesis methods largely rely on\ntransforming human-written templates, limiting both diversity and scalability.\nWe propose MathSmith, a novel framework for synthesizing challenging\nmathematical problems to enhance LLM reasoning. Rather than modifying existing\nproblems, MathSmith constructs new ones from scratch by randomly sampling\nconcept-explanation pairs from PlanetMath, ensuring data independence and\navoiding contamination. To increase difficulty, we design nine predefined\nstrategies as soft constraints during rationales. We further adopts\nreinforcement learning to jointly optimize structural validity, reasoning\ncomplexity, and answer consistency. The length of the reasoning trace generated\nunder autoregressive prompting is used to reflect cognitive complexity,\nencouraging the creation of more demanding problems aligned with\nlong-chain-of-thought reasoning. Experiments across five benchmarks,\ncategorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,\nOlympiadBench), show that MathSmith consistently outperforms existing baselines\nunder both short and long CoT settings. Additionally, a weakness-focused\nvariant generation module enables targeted improvement on specific concepts.\nOverall, MathSmith exhibits strong scalability, generalization, and\ntransferability, highlighting the promise of high-difficulty synthetic data in\nadvancing LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Shaoxiong Zhan"
                    },
                    {
                        "name": "Yanlin Lai"
                    },
                    {
                        "name": "Ziyu Lu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Ziqing Yang"
                    },
                    {
                        "name": "Fei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Fei Tan"
                },
                "author": "Fei Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08131v1",
                "updated": "2025-08-11T16:06:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    6,
                    4,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:06:04Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    6,
                    4,
                    0,
                    223,
                    0
                ],
                "title": "Optimal Transport Regularization for Speech Text Alignment in Spoken\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Transport Regularization for Speech Text Alignment in Spoken\n  Language Models"
                },
                "summary": "Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to\nperceive speech inputs, have gained increasing attention for their potential to\nadvance speech understanding tasks. However, despite recent progress, studies\nshow that SLMs often struggle to generalize across datasets, even for trained\nlanguages and tasks, raising concerns about whether they process speech in a\ntext-like manner as intended. A key challenge underlying this limitation is the\nmodality gap between speech and text representations. The high variability in\nspeech embeddings may allow SLMs to achieve strong in-domain performance by\nexploiting unintended speech variations, ultimately hindering generalization.\nTo mitigate this modality gap, we introduce Optimal Transport Regularization\n(OTReg), a method that formulates speech-text alignment as an optimal transport\nproblem and derives a regularization loss to improve SLM training. In each\ntraining iteration, OTReg first establishes a structured correspondence between\nspeech and transcript embeddings by determining the optimal transport plan,\nthen incorporates the regularization loss based on this transport plan to\noptimize SLMs in generating speech embeddings that align more effectively with\ntranscript embeddings. OTReg is lightweight, requiring no additional labels or\nlearnable parameters, and integrates seamlessly into existing SLM training\nprocedures. Extensive multilingual ASR experiments demonstrate that OTReg\nenhances speech-text alignment, mitigates the modality gap, and consequently\nimproves SLM generalization across diverse datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to\nperceive speech inputs, have gained increasing attention for their potential to\nadvance speech understanding tasks. However, despite recent progress, studies\nshow that SLMs often struggle to generalize across datasets, even for trained\nlanguages and tasks, raising concerns about whether they process speech in a\ntext-like manner as intended. A key challenge underlying this limitation is the\nmodality gap between speech and text representations. The high variability in\nspeech embeddings may allow SLMs to achieve strong in-domain performance by\nexploiting unintended speech variations, ultimately hindering generalization.\nTo mitigate this modality gap, we introduce Optimal Transport Regularization\n(OTReg), a method that formulates speech-text alignment as an optimal transport\nproblem and derives a regularization loss to improve SLM training. In each\ntraining iteration, OTReg first establishes a structured correspondence between\nspeech and transcript embeddings by determining the optimal transport plan,\nthen incorporates the regularization loss based on this transport plan to\noptimize SLMs in generating speech embeddings that align more effectively with\ntranscript embeddings. OTReg is lightweight, requiring no additional labels or\nlearnable parameters, and integrates seamlessly into existing SLM training\nprocedures. Extensive multilingual ASR experiments demonstrate that OTReg\nenhances speech-text alignment, mitigates the modality gap, and consequently\nimproves SLM generalization across diverse datasets."
                },
                "authors": [
                    {
                        "name": "Wenze Xu"
                    },
                    {
                        "name": "Chun Wang"
                    },
                    {
                        "name": "Jiazhen Yu"
                    },
                    {
                        "name": "Sheng Chen"
                    },
                    {
                        "name": "Liang Gao"
                    },
                    {
                        "name": "Weihong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Weihong Deng"
                },
                "author": "Weihong Deng",
                "arxiv_comment": "To be presented at ACPR 2025 Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08127v1",
                "updated": "2025-08-11T16:04:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    4,
                    47,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:04:47Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    4,
                    47,
                    0,
                    223,
                    0
                ],
                "title": "BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown\n  Attacks"
                },
                "summary": "The security of LLM-based multi-agent systems (MAS) is critically threatened\nby propagation vulnerability, where malicious agents can distort collective\ndecision-making through inter-agent message interactions. While existing\nsupervised defense methods demonstrate promising performance, they may be\nimpractical in real-world scenarios due to their heavy reliance on labeled\nmalicious agents to train a supervised malicious detection model. To enable\npractical and generalizable MAS defenses, in this paper, we propose BlindGuard,\nan unsupervised defense method that learns without requiring any\nattack-specific labels or prior knowledge of malicious behaviors. To this end,\nwe establish a hierarchical agent encoder to capture individual, neighborhood,\nand global interaction patterns of each agent, providing a comprehensive\nunderstanding for malicious agent detection. Meanwhile, we design a\ncorruption-guided detector that consists of directional noise injection and\ncontrastive learning, allowing effective detection model training solely on\nnormal agent behaviors. Extensive experiments show that BlindGuard effectively\ndetects diverse attack types (i.e., prompt injection, memory poisoning, and\ntool attack) across MAS with various communication patterns while maintaining\nsuperior generalizability compared to supervised baselines. The code is\navailable at: https://github.com/MR9812/BlindGuard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security of LLM-based multi-agent systems (MAS) is critically threatened\nby propagation vulnerability, where malicious agents can distort collective\ndecision-making through inter-agent message interactions. While existing\nsupervised defense methods demonstrate promising performance, they may be\nimpractical in real-world scenarios due to their heavy reliance on labeled\nmalicious agents to train a supervised malicious detection model. To enable\npractical and generalizable MAS defenses, in this paper, we propose BlindGuard,\nan unsupervised defense method that learns without requiring any\nattack-specific labels or prior knowledge of malicious behaviors. To this end,\nwe establish a hierarchical agent encoder to capture individual, neighborhood,\nand global interaction patterns of each agent, providing a comprehensive\nunderstanding for malicious agent detection. Meanwhile, we design a\ncorruption-guided detector that consists of directional noise injection and\ncontrastive learning, allowing effective detection model training solely on\nnormal agent behaviors. Extensive experiments show that BlindGuard effectively\ndetects diverse attack types (i.e., prompt injection, memory poisoning, and\ntool attack) across MAS with various communication patterns while maintaining\nsuperior generalizability compared to supervised baselines. The code is\navailable at: https://github.com/MR9812/BlindGuard."
                },
                "authors": [
                    {
                        "name": "Rui Miao"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Yili Wang"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Yue Tan"
                    },
                    {
                        "name": "Yiwei Dai"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08124v1",
                "updated": "2025-08-11T16:02:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    2,
                    25,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T16:02:25Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    2,
                    25,
                    0,
                    223,
                    0
                ],
                "title": "NeuroDx-LM: A Clinical Large-Scale Model for EEG-based Neurological\n  Disorder Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroDx-LM: A Clinical Large-Scale Model for EEG-based Neurological\n  Disorder Detection"
                },
                "summary": "Large-scale models pre-trained on Electroencephalography (EEG) have shown\npromise in clinical applications such as neurological disorder detection.\nHowever, the practical deployment of EEG-based large-scale models faces\ncritical challenges such as limited labeled EEG data and suboptimal performance\nin clinical scenarios. To address these issues, we propose NeuroDx-LM, a novel\nlarge-scale model specifically designed for detecting EEG-based neurological\ndisorders. Our key contributions include (i) a Selective Temporal-Frequency\nEmbedding mechanism that adaptively captures complex temporal and spectral\npatterns in EEG signals; and (ii) a Progressive Feature-Aware Training strategy\nthat refines feature representation in a two-stage process. In the first stage,\nour model learns the fundamental discriminative features of EEG activities; in\nthe second stage, the model further extracts more specialized fine-grained\nfeatures for accurate diagnostic performance. We evaluated NeuroDx-LM on the\nCHB-MIT and Schizophrenia datasets, achieving state-of-the-art performance in\nEEG-based seizure and schizophrenia detection, respectively. These results\ndemonstrate the great potential of EEG-based large-scale models to advance\nclinical applicability. Our code is available at\nhttps://github.com/LetItBe12345/NeuroDx-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale models pre-trained on Electroencephalography (EEG) have shown\npromise in clinical applications such as neurological disorder detection.\nHowever, the practical deployment of EEG-based large-scale models faces\ncritical challenges such as limited labeled EEG data and suboptimal performance\nin clinical scenarios. To address these issues, we propose NeuroDx-LM, a novel\nlarge-scale model specifically designed for detecting EEG-based neurological\ndisorders. Our key contributions include (i) a Selective Temporal-Frequency\nEmbedding mechanism that adaptively captures complex temporal and spectral\npatterns in EEG signals; and (ii) a Progressive Feature-Aware Training strategy\nthat refines feature representation in a two-stage process. In the first stage,\nour model learns the fundamental discriminative features of EEG activities; in\nthe second stage, the model further extracts more specialized fine-grained\nfeatures for accurate diagnostic performance. We evaluated NeuroDx-LM on the\nCHB-MIT and Schizophrenia datasets, achieving state-of-the-art performance in\nEEG-based seizure and schizophrenia detection, respectively. These results\ndemonstrate the great potential of EEG-based large-scale models to advance\nclinical applicability. Our code is available at\nhttps://github.com/LetItBe12345/NeuroDx-LM."
                },
                "authors": [
                    {
                        "name": "Guanghao Jin"
                    },
                    {
                        "name": "Yuan Liang"
                    },
                    {
                        "name": "Yihan Ma"
                    },
                    {
                        "name": "Jingpei Wu"
                    },
                    {
                        "name": "Guoyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Guoyang Liu"
                },
                "author": "Guoyang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08120v1",
                "updated": "2025-08-11T15:59:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    59,
                    9,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:59:09Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    59,
                    9,
                    0,
                    223,
                    0
                ],
                "title": "Vision-Based Localization and LLM-based Navigation for Indoor\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Based Localization and LLM-based Navigation for Indoor\n  Environments"
                },
                "summary": "Indoor navigation remains a complex challenge due to the absence of reliable\nGPS signals and the architectural intricacies of large enclosed environments.\nThis study presents an indoor localization and navigation approach that\nintegrates vision-based localization with large language model (LLM)-based\nnavigation. The localization system utilizes a ResNet-50 convolutional neural\nnetwork fine-tuned through a two-stage process to identify the user's position\nusing smartphone camera input. To complement localization, the navigation\nmodule employs an LLM, guided by a carefully crafted system prompt, to\ninterpret preprocessed floor plan images and generate step-by-step directions.\nExperimental evaluation was conducted in a realistic office corridor with\nrepetitive features and limited visibility to test localization robustness. The\nmodel achieved high confidence and an accuracy of 96% across all tested\nwaypoints, even under constrained viewing conditions and short-duration\nqueries. Navigation tests using ChatGPT on real building floor maps yielded an\naverage instruction accuracy of 75%, with observed limitations in zero-shot\nreasoning and inference time. This research demonstrates the potential for\nscalable, infrastructure-free indoor navigation using off-the-shelf cameras and\npublicly available floor plans, particularly in resource-constrained settings\nlike hospitals, airports, and educational institutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor navigation remains a complex challenge due to the absence of reliable\nGPS signals and the architectural intricacies of large enclosed environments.\nThis study presents an indoor localization and navigation approach that\nintegrates vision-based localization with large language model (LLM)-based\nnavigation. The localization system utilizes a ResNet-50 convolutional neural\nnetwork fine-tuned through a two-stage process to identify the user's position\nusing smartphone camera input. To complement localization, the navigation\nmodule employs an LLM, guided by a carefully crafted system prompt, to\ninterpret preprocessed floor plan images and generate step-by-step directions.\nExperimental evaluation was conducted in a realistic office corridor with\nrepetitive features and limited visibility to test localization robustness. The\nmodel achieved high confidence and an accuracy of 96% across all tested\nwaypoints, even under constrained viewing conditions and short-duration\nqueries. Navigation tests using ChatGPT on real building floor maps yielded an\naverage instruction accuracy of 75%, with observed limitations in zero-shot\nreasoning and inference time. This research demonstrates the potential for\nscalable, infrastructure-free indoor navigation using off-the-shelf cameras and\npublicly available floor plans, particularly in resource-constrained settings\nlike hospitals, airports, and educational institutions."
                },
                "authors": [
                    {
                        "name": "Keyan Rahimi"
                    },
                    {
                        "name": "Md. Wasiul Haque"
                    },
                    {
                        "name": "Sagar Dasgupta"
                    },
                    {
                        "name": "Mizanur Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Mizanur Rahman"
                },
                "author": "Mizanur Rahman",
                "arxiv_comment": "20 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12856v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12856v2",
                "updated": "2025-08-11T15:57:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    57,
                    40,
                    0,
                    223,
                    0
                ],
                "published": "2024-07-09T13:15:14Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    13,
                    15,
                    14,
                    1,
                    191,
                    0
                ],
                "title": "AI-AI Bias: large language models favor communications generated by\n  large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-AI Bias: large language models favor communications generated by\n  large language models"
                },
                "summary": "Are large language models (LLMs) biased in favor of communications produced\nby LLMs, leading to possible antihuman discrimination? Using a classical\nexperimental design inspired by employment discrimination studies, we tested\nwidely used LLMs, including GPT-3.5, GPT-4 and a selection of recent\nopen-weight models in binary choice scenarios. These involved LLM-based\nassistants selecting between goods (the goods we study include consumer\nproducts, academic papers, and film-viewings) described either by humans or\nLLMs. Our results show a consistent tendency for LLM-based AIs to prefer\nLLM-presented options. This suggests the possibility of future AI systems\nimplicitly discriminating against humans as a class, giving AI agents and\nAI-assisted humans an unfair advantage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are large language models (LLMs) biased in favor of communications produced\nby LLMs, leading to possible antihuman discrimination? Using a classical\nexperimental design inspired by employment discrimination studies, we tested\nwidely used LLMs, including GPT-3.5, GPT-4 and a selection of recent\nopen-weight models in binary choice scenarios. These involved LLM-based\nassistants selecting between goods (the goods we study include consumer\nproducts, academic papers, and film-viewings) described either by humans or\nLLMs. Our results show a consistent tendency for LLM-based AIs to prefer\nLLM-presented options. This suggests the possibility of future AI systems\nimplicitly discriminating against humans as a class, giving AI agents and\nAI-assisted humans an unfair advantage."
                },
                "authors": [
                    {
                        "name": "Walter Laurito"
                    },
                    {
                        "name": "Benjamin Davis"
                    },
                    {
                        "name": "Peli Grietzer"
                    },
                    {
                        "name": "Tomáš Gavenčiak"
                    },
                    {
                        "name": "Ada Böhm"
                    },
                    {
                        "name": "Jan Kulveit"
                    }
                ],
                "author_detail": {
                    "name": "Jan Kulveit"
                },
                "author": "Jan Kulveit",
                "arxiv_doi": "10.1073/pnas.2415697122",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2415697122",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.12856v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12856v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 4 figures",
                "arxiv_journal_ref": "Proc. Natl. Acad. Sci. U.S.A. 122 (31) e2415697122 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08115v1",
                "updated": "2025-08-11T15:55:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    55,
                    6,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:55:06Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    55,
                    6,
                    0,
                    223,
                    0
                ],
                "title": "TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through\n  Structured Teamwork",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through\n  Structured Teamwork"
                },
                "summary": "We present TeamMedAgents, a novel multi-agent approach that systematically\nintegrates evidence-based teamwork components from human-human collaboration\ninto medical decision-making with large language models (LLMs). Our approach\nvalidates an organizational psychology teamwork model from human collaboration\nto computational multi-agent medical systems by operationalizing six core\nteamwork components derived from Salas et al.'s \"Big Five\" model: team\nleadership, mutual performance monitoring, team orientation, shared mental\nmodels, closed-loop communication, and mutual trust. We implement and evaluate\nthese components as modular, configurable mechanisms within an adaptive\ncollaboration architecture while assessing the effect of the number of agents\ninvolved based on the task's requirements and domain. Systematic evaluation of\ncomputational implementations of teamwork behaviors across eight medical\nbenchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,\nPath-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8\nevaluated datasets. Controlled ablation studies conducted on 50 questions per\nconfiguration across 3 independent runs provide mechanistic insights into\nindividual component contributions, revealing optimal teamwork configurations\nthat vary by reasoning task complexity and domain-specific requirements. Our\nablation analyses reveal dataset-specific optimal teamwork configurations,\nindicating that different medical reasoning modalities benefit from distinct\ncollaborative patterns. TeamMedAgents represents an advancement in\ncollaborative AI by providing a systematic translation of established teamwork\ntheories from human collaboration into agentic collaboration, establishing a\nfoundation for evidence-based multi-agent system design in critical\ndecision-making domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present TeamMedAgents, a novel multi-agent approach that systematically\nintegrates evidence-based teamwork components from human-human collaboration\ninto medical decision-making with large language models (LLMs). Our approach\nvalidates an organizational psychology teamwork model from human collaboration\nto computational multi-agent medical systems by operationalizing six core\nteamwork components derived from Salas et al.'s \"Big Five\" model: team\nleadership, mutual performance monitoring, team orientation, shared mental\nmodels, closed-loop communication, and mutual trust. We implement and evaluate\nthese components as modular, configurable mechanisms within an adaptive\ncollaboration architecture while assessing the effect of the number of agents\ninvolved based on the task's requirements and domain. Systematic evaluation of\ncomputational implementations of teamwork behaviors across eight medical\nbenchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,\nPath-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8\nevaluated datasets. Controlled ablation studies conducted on 50 questions per\nconfiguration across 3 independent runs provide mechanistic insights into\nindividual component contributions, revealing optimal teamwork configurations\nthat vary by reasoning task complexity and domain-specific requirements. Our\nablation analyses reveal dataset-specific optimal teamwork configurations,\nindicating that different medical reasoning modalities benefit from distinct\ncollaborative patterns. TeamMedAgents represents an advancement in\ncollaborative AI by providing a systematic translation of established teamwork\ntheories from human collaboration into agentic collaboration, establishing a\nfoundation for evidence-based multi-agent system design in critical\ndecision-making domains."
                },
                "authors": [
                    {
                        "name": "Pranav Pushkar Mishra"
                    },
                    {
                        "name": "Mohammad Arvan"
                    },
                    {
                        "name": "Mohan Zalake"
                    }
                ],
                "author_detail": {
                    "name": "Mohan Zalake"
                },
                "arxiv_affiliation": "University of Illinois, Chicago",
                "author": "Mohan Zalake",
                "arxiv_comment": "10 pages, 1 figure, 6 tables(2 in main, 4 in appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00485v2",
                "updated": "2025-08-11T15:49:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    49,
                    2,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-01T10:01:12Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    10,
                    1,
                    12,
                    4,
                    213,
                    0
                ],
                "title": "A Frame for Communication Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Frame for Communication Control"
                },
                "summary": "We are experiencing the rise of ChatGPT-like systems or LLMs in political\nturbulent times. We assume the need to regulate their use because of their\nbubble-shaping and polarizing potential. To regulate, we need a language that\nallows interests and compromises to be discussed. In this context, we can think\nof such a shared language as a jargon, a specialized vocabulary for law-making.\nTo the extent that such a jargon exists, it is now being corrupted by LLMs.\nThis situation appears paradoxical. The issue includes persistent communication\nfailures, between disciplines that cannot translate their technical vocabulary\ninto accessible terms, and between political movements that operate in\nincompatible worldviews. We show that a frame integrating four specialist\nlanguages, those of governance, economy, community and science, is able to\naddress these failures case-wise, which we consider helpful. However, for\nreasons noted, we cannot create the more generic jargon needed on our own. We\nconclude that our frame provides the knowledge to design and apply RAG-LLM\narchitectures for researching their jargon generating potential in a future\nproject. We show its feasibility in the appendix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We are experiencing the rise of ChatGPT-like systems or LLMs in political\nturbulent times. We assume the need to regulate their use because of their\nbubble-shaping and polarizing potential. To regulate, we need a language that\nallows interests and compromises to be discussed. In this context, we can think\nof such a shared language as a jargon, a specialized vocabulary for law-making.\nTo the extent that such a jargon exists, it is now being corrupted by LLMs.\nThis situation appears paradoxical. The issue includes persistent communication\nfailures, between disciplines that cannot translate their technical vocabulary\ninto accessible terms, and between political movements that operate in\nincompatible worldviews. We show that a frame integrating four specialist\nlanguages, those of governance, economy, community and science, is able to\naddress these failures case-wise, which we consider helpful. However, for\nreasons noted, we cannot create the more generic jargon needed on our own. We\nconclude that our frame provides the knowledge to design and apply RAG-LLM\narchitectures for researching their jargon generating potential in a future\nproject. We show its feasibility in the appendix."
                },
                "authors": [
                    {
                        "name": "Aernout Schmidt"
                    },
                    {
                        "name": "Kunbei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kunbei Zhang"
                },
                "author": "Kunbei Zhang",
                "arxiv_comment": "10,216 words, 21 pages, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08101v1",
                "updated": "2025-08-11T15:40:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    40,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:40:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    40,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle\n  Conversational Agents for Safer and More Enjoyable Driving Experience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle\n  Conversational Agents for Safer and More Enjoyable Driving Experience"
                },
                "summary": "Studies on in-vehicle conversational agents have traditionally relied on\npre-scripted prompts or limited voice commands, constraining natural\ndriver-agent interaction. To resolve this issue, the present study explored the\npotential of a ChatGPT-based in-vehicle agent capable of carrying continuous,\nmulti-turn dialogues. Forty drivers participated in our experiment using a\nmotion-based driving simulator, comparing three conditions (No agent,\nPre-scripted agent, and ChatGPT-based agent) as a within-subjects variable.\nResults showed that the ChatGPT-based agent condition led to more stable\ndriving performance across multiple metrics. Participants demonstrated lower\nvariability in longitudinal acceleration, lateral acceleration, and lane\ndeviation compared to the other two conditions. In subjective evaluations, the\nChatGPT-based agent also received significantly higher ratings in competence,\nanimacy, affective trust, and preference compared to the Pre-scripted agent.\nOur thematic analysis of driver-agent conversations revealed diverse\ninteraction patterns in topics, including driving assistance/questions,\nentertainment requests, and anthropomorphic interactions. Our results highlight\nthe potential of LLM-powered in-vehicle conversational agents to enhance\ndriving safety and user experience through natural, context-rich interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studies on in-vehicle conversational agents have traditionally relied on\npre-scripted prompts or limited voice commands, constraining natural\ndriver-agent interaction. To resolve this issue, the present study explored the\npotential of a ChatGPT-based in-vehicle agent capable of carrying continuous,\nmulti-turn dialogues. Forty drivers participated in our experiment using a\nmotion-based driving simulator, comparing three conditions (No agent,\nPre-scripted agent, and ChatGPT-based agent) as a within-subjects variable.\nResults showed that the ChatGPT-based agent condition led to more stable\ndriving performance across multiple metrics. Participants demonstrated lower\nvariability in longitudinal acceleration, lateral acceleration, and lane\ndeviation compared to the other two conditions. In subjective evaluations, the\nChatGPT-based agent also received significantly higher ratings in competence,\nanimacy, affective trust, and preference compared to the Pre-scripted agent.\nOur thematic analysis of driver-agent conversations revealed diverse\ninteraction patterns in topics, including driving assistance/questions,\nentertainment requests, and anthropomorphic interactions. Our results highlight\nthe potential of LLM-powered in-vehicle conversational agents to enhance\ndriving safety and user experience through natural, context-rich interactions."
                },
                "authors": [
                    {
                        "name": "Yeana Lee Bond"
                    },
                    {
                        "name": "Mungyeong Choe"
                    },
                    {
                        "name": "Baker Kasim Hasan"
                    },
                    {
                        "name": "Arsh Siddiqui"
                    },
                    {
                        "name": "Myounghoon Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Myounghoon Jeon"
                },
                "arxiv_affiliation": "Computer Science, Virginia Tech, Blacksburg, Virginia, USA",
                "author": "Myounghoon Jeon",
                "arxiv_comment": "Submitted to International Journal of Human-Computer Studies. Bond\n  and Choe: Drafting, Review, Editing, Validation, Software, Methodology,\n  Investigation, Data Analysis, Conceptualization, Experiment training. Hasan\n  and Siddiqui: Experimental and Data Analysis Support. Jeon: Supervision,\n  Review, Resources, Project Admin, Methodology, Conceptualization. Total 34\n  pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16801v2",
                "updated": "2025-08-11T15:35:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    35,
                    42,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-23T15:20:53Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    20,
                    53,
                    2,
                    113,
                    0
                ],
                "title": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding"
                },
                "summary": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA"
                },
                "authors": [
                    {
                        "name": "Xiaoxing Hu"
                    },
                    {
                        "name": "Kaicheng Yang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Ziyong Feng"
                    },
                    {
                        "name": "Yupei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yupei Wang"
                },
                "author": "Yupei Wang",
                "arxiv_comment": "[ACM MM]2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08096v1",
                "updated": "2025-08-11T15:34:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    34,
                    49,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:34:49Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    34,
                    49,
                    0,
                    223,
                    0
                ],
                "title": "Assessing LLM Text Detection in Educational Contexts: Does Human\n  Contribution Affect Detection?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing LLM Text Detection in Educational Contexts: Does Human\n  Contribution Affect Detection?"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) and their increased\naccessibility have made it easier than ever for students to automatically\ngenerate texts, posing new challenges for educational institutions. To enforce\nnorms of academic integrity and ensure students' learning, learning analytics\nmethods to automatically detect LLM-generated text appear increasingly\nappealing. This paper benchmarks the performance of different state-of-the-art\ndetectors in educational contexts, introducing a novel dataset, called\nGenerative Essay Detection in Education (GEDE), containing over 900\nstudent-written essays and over 12,500 LLM-generated essays from various\ndomains. To capture the diversity of LLM usage practices in generating text, we\npropose the concept of contribution levels, representing students' contribution\nto a given assignment. These levels range from purely human-written texts, to\nslightly LLM-improved versions, to fully LLM-generated texts, and finally to\nactive attacks on the detector by \"humanizing\" generated texts. We show that\nmost detectors struggle to accurately classify texts of intermediate student\ncontribution levels, like LLM-improved human-written texts. Detectors are\nparticularly likely to produce false positives, which is problematic in\neducational settings where false suspicions can severely impact students'\nlives. Our dataset, code, and additional supplementary materials are publicly\navailable at\nhttps://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) and their increased\naccessibility have made it easier than ever for students to automatically\ngenerate texts, posing new challenges for educational institutions. To enforce\nnorms of academic integrity and ensure students' learning, learning analytics\nmethods to automatically detect LLM-generated text appear increasingly\nappealing. This paper benchmarks the performance of different state-of-the-art\ndetectors in educational contexts, introducing a novel dataset, called\nGenerative Essay Detection in Education (GEDE), containing over 900\nstudent-written essays and over 12,500 LLM-generated essays from various\ndomains. To capture the diversity of LLM usage practices in generating text, we\npropose the concept of contribution levels, representing students' contribution\nto a given assignment. These levels range from purely human-written texts, to\nslightly LLM-improved versions, to fully LLM-generated texts, and finally to\nactive attacks on the detector by \"humanizing\" generated texts. We show that\nmost detectors struggle to accurately classify texts of intermediate student\ncontribution levels, like LLM-improved human-written texts. Detectors are\nparticularly likely to produce false positives, which is problematic in\neducational settings where false suspicions can severely impact students'\nlives. Our dataset, code, and additional supplementary materials are publicly\navailable at\nhttps://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts."
                },
                "authors": [
                    {
                        "name": "Lukas Gehring"
                    },
                    {
                        "name": "Benjamin Paaßen"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Paaßen"
                },
                "author": "Benjamin Paaßen",
                "arxiv_comment": "Preprint as provided by the authors (19 pages, 12 figures, 9 tables)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08095v1",
                "updated": "2025-08-11T15:33:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    33,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:33:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    33,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "Dual Information Speech Language Models for Emotional Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Information Speech Language Models for Emotional Conversations"
                },
                "summary": "Conversational systems relying on text-based large language models (LLMs)\noften overlook paralinguistic cues, essential for understanding emotions and\nintentions. Speech-language models (SLMs), which use speech as input, are\nemerging as a promising solution. However, SLMs built by extending frozen LLMs\nstruggle to capture paralinguistic information and exhibit reduced context\nunderstanding. We identify entangled information and improper training\nstrategies as key issues. To address these issues, we propose two heterogeneous\nadapters and suggest a weakly supervised training strategy. Our approach\ndisentangles paralinguistic and linguistic information, enabling SLMs to\ninterpret speech through structured representations. It also preserves\ncontextual understanding by avoiding the generation of task-specific vectors\nthrough controlled randomness. This approach trains only the adapters on common\ndatasets, ensuring parameter and data efficiency. Experiments demonstrate\ncompetitive performance in emotional conversation tasks, showcasing the model's\nability to effectively integrate both paralinguistic and linguistic information\nwithin contextual settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational systems relying on text-based large language models (LLMs)\noften overlook paralinguistic cues, essential for understanding emotions and\nintentions. Speech-language models (SLMs), which use speech as input, are\nemerging as a promising solution. However, SLMs built by extending frozen LLMs\nstruggle to capture paralinguistic information and exhibit reduced context\nunderstanding. We identify entangled information and improper training\nstrategies as key issues. To address these issues, we propose two heterogeneous\nadapters and suggest a weakly supervised training strategy. Our approach\ndisentangles paralinguistic and linguistic information, enabling SLMs to\ninterpret speech through structured representations. It also preserves\ncontextual understanding by avoiding the generation of task-specific vectors\nthrough controlled randomness. This approach trains only the adapters on common\ndatasets, ensuring parameter and data efficiency. Experiments demonstrate\ncompetitive performance in emotional conversation tasks, showcasing the model's\nability to effectively integrate both paralinguistic and linguistic information\nwithin contextual settings."
                },
                "authors": [
                    {
                        "name": "Chun Wang"
                    },
                    {
                        "name": "Chenyang Liu"
                    },
                    {
                        "name": "Wenze Xu"
                    },
                    {
                        "name": "Weihong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Weihong Deng"
                },
                "author": "Weihong Deng",
                "arxiv_comment": "Presented at IEEE ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10574v2",
                "updated": "2025-08-11T15:10:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    10,
                    59,
                    0,
                    223,
                    0
                ],
                "published": "2025-06-12T11:03:47Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    3,
                    47,
                    3,
                    163,
                    0
                ],
                "title": "DanceChat: Large Language Model-Guided Music-to-Dance Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DanceChat: Large Language Model-Guided Music-to-Dance Generation"
                },
                "summary": "Music-to-dance generation aims to synthesize human dance motion conditioned\non musical input. Despite recent progress, significant challenges remain due to\nthe semantic gap between music and dance motion, as music offers only abstract\ncues, such as melody, groove, and emotion, without explicitly specifying the\nphysical movements. Moreover, a single piece of music can produce multiple\nplausible dance interpretations. This one-to-many mapping demands additional\nguidance, as music alone provides limited information for generating diverse\ndance movements. The challenge is further amplified by the scarcity of paired\nmusic and dance data, which restricts the model\\^a\\u{A}\\'Zs ability to learn\ndiverse dance patterns. In this paper, we introduce DanceChat, a Large Language\nModel (LLM)-guided music-to-dance generation approach. We use an LLM as a\nchoreographer that provides textual motion instructions, offering explicit,\nhigh-level guidance for dance generation. This approach goes beyond implicit\nlearning from music alone, enabling the model to generate dance that is both\nmore diverse and better aligned with musical styles. Our approach consists of\nthree components: (1) an LLM-based pseudo instruction generation module that\nproduces textual dance guidance based on music style and structure, (2) a\nmulti-modal feature extraction and fusion module that integrates music, rhythm,\nand textual guidance into a shared representation, and (3) a diffusion-based\nmotion synthesis module together with a multi-modal alignment loss, which\nensures that the generated dance is aligned with both musical and textual cues.\nExtensive experiments on AIST++ and human evaluations show that DanceChat\noutperforms state-of-the-art methods both qualitatively and quantitatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Music-to-dance generation aims to synthesize human dance motion conditioned\non musical input. Despite recent progress, significant challenges remain due to\nthe semantic gap between music and dance motion, as music offers only abstract\ncues, such as melody, groove, and emotion, without explicitly specifying the\nphysical movements. Moreover, a single piece of music can produce multiple\nplausible dance interpretations. This one-to-many mapping demands additional\nguidance, as music alone provides limited information for generating diverse\ndance movements. The challenge is further amplified by the scarcity of paired\nmusic and dance data, which restricts the model\\^a\\u{A}\\'Zs ability to learn\ndiverse dance patterns. In this paper, we introduce DanceChat, a Large Language\nModel (LLM)-guided music-to-dance generation approach. We use an LLM as a\nchoreographer that provides textual motion instructions, offering explicit,\nhigh-level guidance for dance generation. This approach goes beyond implicit\nlearning from music alone, enabling the model to generate dance that is both\nmore diverse and better aligned with musical styles. Our approach consists of\nthree components: (1) an LLM-based pseudo instruction generation module that\nproduces textual dance guidance based on music style and structure, (2) a\nmulti-modal feature extraction and fusion module that integrates music, rhythm,\nand textual guidance into a shared representation, and (3) a diffusion-based\nmotion synthesis module together with a multi-modal alignment loss, which\nensures that the generated dance is aligned with both musical and textual cues.\nExtensive experiments on AIST++ and human evaluations show that DanceChat\noutperforms state-of-the-art methods both qualitatively and quantitatively."
                },
                "authors": [
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Xiaohang Yang"
                    },
                    {
                        "name": "Yilan Dong"
                    },
                    {
                        "name": "Naveen Raj Govindaraj"
                    },
                    {
                        "name": "Gregory Slabaugh"
                    },
                    {
                        "name": "Shanxin Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Shanxin Yuan"
                },
                "author": "Shanxin Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20988v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20988v2",
                "updated": "2025-08-11T15:03:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    3,
                    23,
                    0,
                    223,
                    0
                ],
                "published": "2025-02-28T12:00:51Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    0,
                    51,
                    4,
                    59,
                    0
                ],
                "title": "Reviewing Clinical Knowledge in Medical Large Language Models: Training\n  and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reviewing Clinical Knowledge in Medical Large Language Models: Training\n  and Beyond"
                },
                "summary": "The large-scale development of large language models (LLMs) in medical\ncontexts, such as diagnostic assistance and treatment recommendations,\nnecessitates that these models possess accurate medical knowledge and deliver\ntraceable decision-making processes. Clinical knowledge, encompassing the\ninsights gained from research on the causes, prognosis, diagnosis, and\ntreatment of diseases, has been extensively examined within real-world medical\npractices. Recently, there has been a notable increase in research efforts\naimed at integrating this type of knowledge into LLMs, encompassing not only\ntraditional text and multimodal data integration but also technologies such as\nknowledge graphs (KGs) and retrieval-augmented generation (RAG). In this paper,\nwe review the various initiatives to embed clinical knowledge into\ntraining-based, KG-supported, and RAG-assisted LLMs. We begin by gathering\nreliable knowledge sources from the medical domain, including databases and\ndatasets. Next, we evaluate implementations for integrating clinical knowledge\nthrough specialized datasets and collaborations with external knowledge sources\nsuch as KGs and relevant documentation. Furthermore, we discuss the\napplications of the developed medical LLMs in the industrial sector to assess\nthe disparity between models developed in academic settings and those in\nindustry. We conclude the survey by presenting evaluation systems applicable to\nrelevant tasks and identifying potential challenges facing this field. In this\nreview, we do not aim for completeness, since any ostensibly complete review\nwould soon be outdated. Our goal is to illustrate diversity by selecting\nrepresentative and accessible items from current research and industry\npractices, reflecting real-world situations rather than claiming completeness.\nThus, we emphasize showcasing diverse approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale development of large language models (LLMs) in medical\ncontexts, such as diagnostic assistance and treatment recommendations,\nnecessitates that these models possess accurate medical knowledge and deliver\ntraceable decision-making processes. Clinical knowledge, encompassing the\ninsights gained from research on the causes, prognosis, diagnosis, and\ntreatment of diseases, has been extensively examined within real-world medical\npractices. Recently, there has been a notable increase in research efforts\naimed at integrating this type of knowledge into LLMs, encompassing not only\ntraditional text and multimodal data integration but also technologies such as\nknowledge graphs (KGs) and retrieval-augmented generation (RAG). In this paper,\nwe review the various initiatives to embed clinical knowledge into\ntraining-based, KG-supported, and RAG-assisted LLMs. We begin by gathering\nreliable knowledge sources from the medical domain, including databases and\ndatasets. Next, we evaluate implementations for integrating clinical knowledge\nthrough specialized datasets and collaborations with external knowledge sources\nsuch as KGs and relevant documentation. Furthermore, we discuss the\napplications of the developed medical LLMs in the industrial sector to assess\nthe disparity between models developed in academic settings and those in\nindustry. We conclude the survey by presenting evaluation systems applicable to\nrelevant tasks and identifying potential challenges facing this field. In this\nreview, we do not aim for completeness, since any ostensibly complete review\nwould soon be outdated. Our goal is to illustrate diversity by selecting\nrepresentative and accessible items from current research and industry\npractices, reflecting real-world situations rather than claiming completeness.\nThus, we emphasize showcasing diverse approaches."
                },
                "authors": [
                    {
                        "name": "Qiyuan Li"
                    },
                    {
                        "name": "Haijiang Liu"
                    },
                    {
                        "name": "Caicai Guo"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Deyu Chen"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Frank van Harmelen"
                    },
                    {
                        "name": "Jinguang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinguang Gu"
                },
                "author": "Jinguang Gu",
                "arxiv_doi": "10.1016/j.knosys.2025.114215",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.knosys.2025.114215",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.20988v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20988v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in Knowledge-Based Systems. The arXiv\n  version is the pre-peer-review preprint, and the final published version is\n  not available here due to publisher policy",
                "arxiv_journal_ref": "Knowledge-Based Systems, 114215(2025)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08053v1",
                "updated": "2025-08-11T14:52:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    52,
                    59,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:52:59Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    52,
                    59,
                    0,
                    223,
                    0
                ],
                "title": "AdaptFlow: Adaptive Workflow Optimization via Meta-Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptFlow: Adaptive Workflow Optimization via Meta-Learning"
                },
                "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin agentic workflows, which are structured sequences of LLM invocations\nintended to solve complex tasks. However, existing approaches often rely on\nstatic templates or manually designed workflows, which limit adaptability to\ndiverse tasks and hinder scalability. We propose AdaptFlow, a natural\nlanguage-based meta-learning framework inspired by model-agnostic meta-learning\n(MAML). AdaptFlow learns a generalizable workflow initialization that enables\nrapid subtask-level adaptation. It employs a bi-level optimization scheme: the\ninner loop refines the workflow for a specific subtask using LLM-generated\nfeedback, while the outer loop updates the shared initialization to perform\nwell across tasks. This setup allows AdaptFlow to generalize effectively to\nunseen tasks by adapting the initialized workflow through language-guided\nmodifications. Evaluated across question answering, code generation, and\nmathematical reasoning benchmarks, AdaptFlow consistently outperforms both\nmanually crafted and automatically searched baselines, achieving\nstate-of-the-art results with strong generalization across tasks and models.\nThe source code and data are available at\nhttps://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have sparked growing interest\nin agentic workflows, which are structured sequences of LLM invocations\nintended to solve complex tasks. However, existing approaches often rely on\nstatic templates or manually designed workflows, which limit adaptability to\ndiverse tasks and hinder scalability. We propose AdaptFlow, a natural\nlanguage-based meta-learning framework inspired by model-agnostic meta-learning\n(MAML). AdaptFlow learns a generalizable workflow initialization that enables\nrapid subtask-level adaptation. It employs a bi-level optimization scheme: the\ninner loop refines the workflow for a specific subtask using LLM-generated\nfeedback, while the outer loop updates the shared initialization to perform\nwell across tasks. This setup allows AdaptFlow to generalize effectively to\nunseen tasks by adapting the initialized workflow through language-guided\nmodifications. Evaluated across question answering, code generation, and\nmathematical reasoning benchmarks, AdaptFlow consistently outperforms both\nmanually crafted and automatically searched baselines, achieving\nstate-of-the-art results with strong generalization across tasks and models.\nThe source code and data are available at\nhttps://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow."
                },
                "authors": [
                    {
                        "name": "Runchuan Zhu"
                    },
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Haoxiang Gao"
                    },
                    {
                        "name": "Fengshuo Bai"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08040v1",
                "updated": "2025-08-11T14:42:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    42,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:42:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    42,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning\n  in Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning\n  in Multimodal Models"
                },
                "summary": "Prompt-based tuning has emerged as a lightweight alternative to full\nfine-tuning in large vision-language models, enabling efficient adaptation via\nlearned contextual prompts. This paradigm has recently been extended to\nfederated learning settings (e.g., PromptFL), where clients collaboratively\ntrain prompts under data privacy constraints. However, the security\nimplications of prompt-based aggregation in federated multimodal learning\nremain largely unexplored, leaving a critical attack surface unaddressed. In\nthis paper, we introduce \\textbf{BadPromptFL}, the first backdoor attack\ntargeting prompt-based federated learning in multimodal contrastive models. In\nBadPromptFL, compromised clients jointly optimize local backdoor triggers and\nprompt embeddings, injecting poisoned prompts into the global aggregation\nprocess. These prompts are then propagated to benign clients, enabling\nuniversal backdoor activation at inference without modifying model parameters.\nLeveraging the contextual learning behavior of CLIP-style architectures,\nBadPromptFL achieves high attack success rates (e.g., \\(>90\\%\\)) with minimal\nvisibility and limited client participation. Extensive experiments across\nmultiple datasets and aggregation protocols validate the effectiveness,\nstealth, and generalizability of our attack, raising critical concerns about\nthe robustness of prompt-based federated learning in real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-based tuning has emerged as a lightweight alternative to full\nfine-tuning in large vision-language models, enabling efficient adaptation via\nlearned contextual prompts. This paradigm has recently been extended to\nfederated learning settings (e.g., PromptFL), where clients collaboratively\ntrain prompts under data privacy constraints. However, the security\nimplications of prompt-based aggregation in federated multimodal learning\nremain largely unexplored, leaving a critical attack surface unaddressed. In\nthis paper, we introduce \\textbf{BadPromptFL}, the first backdoor attack\ntargeting prompt-based federated learning in multimodal contrastive models. In\nBadPromptFL, compromised clients jointly optimize local backdoor triggers and\nprompt embeddings, injecting poisoned prompts into the global aggregation\nprocess. These prompts are then propagated to benign clients, enabling\nuniversal backdoor activation at inference without modifying model parameters.\nLeveraging the contextual learning behavior of CLIP-style architectures,\nBadPromptFL achieves high attack success rates (e.g., \\(>90\\%\\)) with minimal\nvisibility and limited client participation. Extensive experiments across\nmultiple datasets and aggregation protocols validate the effectiveness,\nstealth, and generalizability of our attack, raising critical concerns about\nthe robustness of prompt-based federated learning in real-world deployments."
                },
                "authors": [
                    {
                        "name": "Maozhen Zhang"
                    },
                    {
                        "name": "Mengnan Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01926v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01926v3",
                "updated": "2025-08-11T14:37:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    37,
                    48,
                    0,
                    223,
                    0
                ],
                "published": "2025-02-04T01:56:28Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    1,
                    56,
                    28,
                    1,
                    35,
                    0
                ],
                "title": "Fairness through Difference Awareness: Measuring Desired Group\n  Discrimination in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness through Difference Awareness: Measuring Desired Group\n  Discrimination in LLMs"
                },
                "summary": "Algorithmic fairness has conventionally adopted the mathematically convenient\nperspective of racial color-blindness (i.e., difference unaware treatment).\nHowever, we contend that in a range of important settings, group difference\nawareness matters. For example, differentiating between groups may be necessary\nin legal contexts (e.g., the U.S. compulsory draft applies to men but not\nwomen) and harm assessments (e.g., referring to girls as ``terrorists'' may be\nless harmful than referring to Muslim people as such). Thus, in contrast to\nmost fairness work, we study fairness through the perspective of treating\npeople differently -- when it is contextually appropriate to. We first\nintroduce an important distinction between descriptive (fact-based), normative\n(value-based), and correlation (association-based) benchmarks. This distinction\nis significant because each category requires separate interpretation and\nmitigation tailored to its specific characteristics. Then, we present a\nbenchmark suite composed of eight different scenarios for a total of 16k\nquestions that enables us to assess difference awareness. Finally, we show\nresults across ten models that demonstrate difference awareness is a distinct\ndimension to fairness where existing bias mitigation strategies may backfire.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic fairness has conventionally adopted the mathematically convenient\nperspective of racial color-blindness (i.e., difference unaware treatment).\nHowever, we contend that in a range of important settings, group difference\nawareness matters. For example, differentiating between groups may be necessary\nin legal contexts (e.g., the U.S. compulsory draft applies to men but not\nwomen) and harm assessments (e.g., referring to girls as ``terrorists'' may be\nless harmful than referring to Muslim people as such). Thus, in contrast to\nmost fairness work, we study fairness through the perspective of treating\npeople differently -- when it is contextually appropriate to. We first\nintroduce an important distinction between descriptive (fact-based), normative\n(value-based), and correlation (association-based) benchmarks. This distinction\nis significant because each category requires separate interpretation and\nmitigation tailored to its specific characteristics. Then, we present a\nbenchmark suite composed of eight different scenarios for a total of 16k\nquestions that enables us to assess difference awareness. Finally, we show\nresults across ten models that demonstrate difference awareness is a distinct\ndimension to fairness where existing bias mitigation strategies may backfire."
                },
                "authors": [
                    {
                        "name": "Angelina Wang"
                    },
                    {
                        "name": "Michelle Phan"
                    },
                    {
                        "name": "Daniel E. Ho"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    }
                ],
                "author_detail": {
                    "name": "Sanmi Koyejo"
                },
                "author": "Sanmi Koyejo",
                "arxiv_comment": "Best Paper award at ACL 2025; dataset available at\n  https://github.com/Angelina-Wang/difference_awareness",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01926v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01926v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08034v1",
                "updated": "2025-08-11T14:37:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    37,
                    40,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:37:40Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    37,
                    40,
                    0,
                    223,
                    0
                ],
                "title": "Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric,\n  and Hybrid Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric,\n  and Hybrid Vehicles"
                },
                "summary": "Accurate power consumption prediction is crucial for improving efficiency and\nreducing environmental impact, yet traditional methods relying on specialized\ninstruments or rigid physical models are impractical for large-scale,\nreal-world deployment. This study introduces a scalable data-driven method\nusing powertrain dynamic feature sets and both traditional machine learning and\ndeep neural networks to estimate instantaneous and cumulative power consumption\nin internal combustion engine (ICE), electric vehicle (EV), and hybrid electric\nvehicle (HEV) platforms. ICE models achieved high instantaneous accuracy with\nmean absolute error and root mean squared error on the order of $10^{-3}$, and\ncumulative errors under 3%. Transformer and long short-term memory models\nperformed best for EVs and HEVs, with cumulative errors below 4.1% and 2.1%,\nrespectively. Results confirm the approach's effectiveness across vehicles and\nmodels. Uncertainty analysis revealed greater variability in EV and HEV\ndatasets than ICE, due to complex power management, emphasizing the need for\nrobust models for advanced powertrains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate power consumption prediction is crucial for improving efficiency and\nreducing environmental impact, yet traditional methods relying on specialized\ninstruments or rigid physical models are impractical for large-scale,\nreal-world deployment. This study introduces a scalable data-driven method\nusing powertrain dynamic feature sets and both traditional machine learning and\ndeep neural networks to estimate instantaneous and cumulative power consumption\nin internal combustion engine (ICE), electric vehicle (EV), and hybrid electric\nvehicle (HEV) platforms. ICE models achieved high instantaneous accuracy with\nmean absolute error and root mean squared error on the order of $10^{-3}$, and\ncumulative errors under 3%. Transformer and long short-term memory models\nperformed best for EVs and HEVs, with cumulative errors below 4.1% and 2.1%,\nrespectively. Results confirm the approach's effectiveness across vehicles and\nmodels. Uncertainty analysis revealed greater variability in EV and HEV\ndatasets than ICE, due to complex power management, emphasizing the need for\nrobust models for advanced powertrains."
                },
                "authors": [
                    {
                        "name": "Roksana Yahyaabadi"
                    },
                    {
                        "name": "Ghazal Farhani"
                    },
                    {
                        "name": "Taufiq Rahman"
                    },
                    {
                        "name": "Soodeh Nikan"
                    },
                    {
                        "name": "Abdullah Jirjees"
                    },
                    {
                        "name": "Fadi Araji"
                    }
                ],
                "author_detail": {
                    "name": "Fadi Araji"
                },
                "author": "Fadi Araji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08031v1",
                "updated": "2025-08-11T14:36:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    36,
                    11,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:36:11Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    36,
                    11,
                    0,
                    223,
                    0
                ],
                "title": "IPBA: Imperceptible Perturbation Backdoor Attack in Federated\n  Self-Supervised Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IPBA: Imperceptible Perturbation Backdoor Attack in Federated\n  Self-Supervised Learning"
                },
                "summary": "Federated self-supervised learning (FSSL) combines the advantages of\ndecentralized modeling and unlabeled representation learning, serving as a\ncutting-edge paradigm with strong potential for scalability and privacy\npreservation. Although FSSL has garnered increasing attention, research\nindicates that it remains vulnerable to backdoor attacks. Existing methods\ngenerally rely on visually obvious triggers, which makes it difficult to meet\nthe requirements for stealth and practicality in real-world deployment. In this\npaper, we propose an imperceptible and effective backdoor attack method against\nFSSL, called IPBA. Our empirical study reveals that existing imperceptible\ntriggers face a series of challenges in FSSL, particularly limited\ntransferability, feature entanglement with augmented samples, and\nout-of-distribution properties. These issues collectively undermine the\neffectiveness and stealthiness of traditional backdoor attacks in FSSL. To\novercome these challenges, IPBA decouples the feature distributions of backdoor\nand augmented samples, and introduces Sliced-Wasserstein distance to mitigate\nthe out-of-distribution properties of backdoor samples, thereby optimizing the\ntrigger generation process. Our experimental results on several FSSL scenarios\nand datasets show that IPBA significantly outperforms existing backdoor attack\nmethods in performance and exhibits strong robustness under various defense\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated self-supervised learning (FSSL) combines the advantages of\ndecentralized modeling and unlabeled representation learning, serving as a\ncutting-edge paradigm with strong potential for scalability and privacy\npreservation. Although FSSL has garnered increasing attention, research\nindicates that it remains vulnerable to backdoor attacks. Existing methods\ngenerally rely on visually obvious triggers, which makes it difficult to meet\nthe requirements for stealth and practicality in real-world deployment. In this\npaper, we propose an imperceptible and effective backdoor attack method against\nFSSL, called IPBA. Our empirical study reveals that existing imperceptible\ntriggers face a series of challenges in FSSL, particularly limited\ntransferability, feature entanglement with augmented samples, and\nout-of-distribution properties. These issues collectively undermine the\neffectiveness and stealthiness of traditional backdoor attacks in FSSL. To\novercome these challenges, IPBA decouples the feature distributions of backdoor\nand augmented samples, and introduces Sliced-Wasserstein distance to mitigate\nthe out-of-distribution properties of backdoor samples, thereby optimizing the\ntrigger generation process. Our experimental results on several FSSL scenarios\nand datasets show that IPBA significantly outperforms existing backdoor attack\nmethods in performance and exhibits strong robustness under various defense\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Jiayao Wang"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Zhendong Zhao"
                    },
                    {
                        "name": "Jiale Zhang"
                    },
                    {
                        "name": "Qilin Wu"
                    },
                    {
                        "name": "Junwu Zhu"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08029v1",
                "updated": "2025-08-11T14:32:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    32,
                    43,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:32:43Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    32,
                    43,
                    0,
                    223,
                    0
                ],
                "title": "Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data\n  Manipulation Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data\n  Manipulation Attacks"
                },
                "summary": "The introduction of 5G and the Open Radio Access Network (O-RAN) architecture\nhas enabled more flexible and intelligent network deployments. However, the\nincreased complexity and openness of these architectures also introduce novel\nsecurity challenges, such as data manipulation attacks on the semi-standardised\nShared Data Layer (SDL) within the O-RAN platform through malicious xApps. In\nparticular, malicious xApps can exploit this vulnerability by introducing\nsubtle Unicode-wise alterations (hypoglyphs) into the data that are being used\nby traditional machine learning (ML)-based anomaly detection methods. These\nUnicode-wise manipulations can potentially bypass detection and cause failures\nin anomaly detection systems based on traditional ML, such as AutoEncoders,\nwhich are unable to process hypoglyphed data without crashing. We investigate\nthe use of Large Language Models (LLMs) for anomaly detection within the O-RAN\narchitecture to address this challenge. We demonstrate that LLM-based xApps\nmaintain robust operational performance and are capable of processing\nmanipulated messages without crashing. While initial detection accuracy\nrequires further improvements, our results highlight the robustness of LLMs to\nadversarial attacks such as hypoglyphs in input data. There is potential to use\ntheir adaptability through prompt engineering to further improve the accuracy,\nalthough this requires further research. Additionally, we show that LLMs\nachieve low detection latency (under 0.07 seconds), making them suitable for\nNear-Real-Time (Near-RT) RIC deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The introduction of 5G and the Open Radio Access Network (O-RAN) architecture\nhas enabled more flexible and intelligent network deployments. However, the\nincreased complexity and openness of these architectures also introduce novel\nsecurity challenges, such as data manipulation attacks on the semi-standardised\nShared Data Layer (SDL) within the O-RAN platform through malicious xApps. In\nparticular, malicious xApps can exploit this vulnerability by introducing\nsubtle Unicode-wise alterations (hypoglyphs) into the data that are being used\nby traditional machine learning (ML)-based anomaly detection methods. These\nUnicode-wise manipulations can potentially bypass detection and cause failures\nin anomaly detection systems based on traditional ML, such as AutoEncoders,\nwhich are unable to process hypoglyphed data without crashing. We investigate\nthe use of Large Language Models (LLMs) for anomaly detection within the O-RAN\narchitecture to address this challenge. We demonstrate that LLM-based xApps\nmaintain robust operational performance and are capable of processing\nmanipulated messages without crashing. While initial detection accuracy\nrequires further improvements, our results highlight the robustness of LLMs to\nadversarial attacks such as hypoglyphs in input data. There is potential to use\ntheir adaptability through prompt engineering to further improve the accuracy,\nalthough this requires further research. Additionally, we show that LLMs\nachieve low detection latency (under 0.07 seconds), making them suitable for\nNear-Real-Time (Near-RT) RIC deployments."
                },
                "authors": [
                    {
                        "name": "Thusitha Dayaratne"
                    },
                    {
                        "name": "Ngoc Duy Pham"
                    },
                    {
                        "name": "Viet Vo"
                    },
                    {
                        "name": "Shangqi Lai"
                    },
                    {
                        "name": "Sharif Abuadbba"
                    },
                    {
                        "name": "Hajime Suzuki"
                    },
                    {
                        "name": "Xingliang Yuan"
                    },
                    {
                        "name": "Carsten Rudolph"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Rudolph"
                },
                "author": "Carsten Rudolph",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21051v3",
                "updated": "2025-08-11T14:32:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    32,
                    1,
                    0,
                    223,
                    0
                ],
                "published": "2024-12-30T16:09:28Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    9,
                    28,
                    0,
                    365,
                    0
                ],
                "title": "Toward Intelligent and Secure Cloud: Large Language Model Empowered\n  Proactive Defense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Intelligent and Secure Cloud: Large Language Model Empowered\n  Proactive Defense"
                },
                "summary": "The rapid evolution of cloud computing technologies and the increasing number\nof cloud applications have provided numerous benefits in our daily lives.\nHowever, the diversity and complexity of different components pose a\nsignificant challenge to cloud security, especially when dealing with\nsophisticated and advanced cyberattacks such as Denial of Service (DoS). Recent\nadvancements in the large language models (LLMs) offer promising solutions for\nsecurity intelligence. By exploiting the powerful capabilities in language\nunderstanding, data analysis, task inference, action planning, and code\ngeneration, we present LLM-PD, a novel defense architecture that proactively\nmitigates various DoS threats in cloud networks. LLM-PD can efficiently make\ndecisions through comprehensive data analysis and sequential reasoning, as well\nas dynamically create and deploy actionable defense mechanisms. Furthermore, it\ncan flexibly self-evolve based on experience learned from previous interactions\nand adapt to new attack scenarios without additional training. Our case study\non three distinct DoS attacks demonstrates its remarkable ability in terms of\ndefense effectiveness and efficiency when compared with other existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of cloud computing technologies and the increasing number\nof cloud applications have provided numerous benefits in our daily lives.\nHowever, the diversity and complexity of different components pose a\nsignificant challenge to cloud security, especially when dealing with\nsophisticated and advanced cyberattacks such as Denial of Service (DoS). Recent\nadvancements in the large language models (LLMs) offer promising solutions for\nsecurity intelligence. By exploiting the powerful capabilities in language\nunderstanding, data analysis, task inference, action planning, and code\ngeneration, we present LLM-PD, a novel defense architecture that proactively\nmitigates various DoS threats in cloud networks. LLM-PD can efficiently make\ndecisions through comprehensive data analysis and sequential reasoning, as well\nas dynamically create and deploy actionable defense mechanisms. Furthermore, it\ncan flexibly self-evolve based on experience learned from previous interactions\nand adapt to new attack scenarios without additional training. Our case study\non three distinct DoS attacks demonstrates its remarkable ability in terms of\ndefense effectiveness and efficiency when compared with other existing methods."
                },
                "authors": [
                    {
                        "name": "Yuyang Zhou"
                    },
                    {
                        "name": "Guang Cheng"
                    },
                    {
                        "name": "Kang Du"
                    },
                    {
                        "name": "Zihan Chen"
                    },
                    {
                        "name": "Yuyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Zhao"
                },
                "author": "Yuyu Zhao",
                "arxiv_comment": "7 pages; Major Revision for IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08027v1",
                "updated": "2025-08-11T14:31:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    31,
                    20,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:31:20Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    31,
                    20,
                    0,
                    223,
                    0
                ],
                "title": "Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking\n  Self-Supervised and Generative Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking\n  Self-Supervised and Generative Approaches"
                },
                "summary": "Speech Recognition (ASR) due to phoneme distortions and high variability.\nWhile self-supervised ASR models like Wav2Vec, HuBERT, and Whisper have shown\npromise, their effectiveness in dysarthric speech remains unclear. This study\nsystematically benchmarks these models with different decoding strategies,\nincluding CTC, seq2seq, and LLM-enhanced decoding (BART,GPT-2, Vicuna). Our\ncontributions include (1) benchmarking ASR architectures for dysarthric speech,\n(2) introducing LLM-based decoding to improve intelligibility, (3) analyzing\ngeneralization across datasets, and (4) providing insights into recognition\nerrors across severity levels. Findings highlight that LLM-enhanced decoding\nimproves dysarthric ASR by leveraging linguistic constraints for phoneme\nrestoration and grammatical correction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Recognition (ASR) due to phoneme distortions and high variability.\nWhile self-supervised ASR models like Wav2Vec, HuBERT, and Whisper have shown\npromise, their effectiveness in dysarthric speech remains unclear. This study\nsystematically benchmarks these models with different decoding strategies,\nincluding CTC, seq2seq, and LLM-enhanced decoding (BART,GPT-2, Vicuna). Our\ncontributions include (1) benchmarking ASR architectures for dysarthric speech,\n(2) introducing LLM-based decoding to improve intelligibility, (3) analyzing\ngeneralization across datasets, and (4) providing insights into recognition\nerrors across severity levels. Findings highlight that LLM-enhanced decoding\nimproves dysarthric ASR by leveraging linguistic constraints for phoneme\nrestoration and grammatical correction."
                },
                "authors": [
                    {
                        "name": "Ahmed Aboeitta"
                    },
                    {
                        "name": "Ahmed Sharshar"
                    },
                    {
                        "name": "Youssef Nafea"
                    },
                    {
                        "name": "Shady Shehata"
                    }
                ],
                "author_detail": {
                    "name": "Shady Shehata"
                },
                "author": "Shady Shehata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08020v1",
                "updated": "2025-08-11T14:26:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    26,
                    36,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:26:36Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    26,
                    36,
                    0,
                    223,
                    0
                ],
                "title": "EchoAid: Enhancing Livestream Shopping Accessibility for the DHH\n  Community",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoAid: Enhancing Livestream Shopping Accessibility for the DHH\n  Community"
                },
                "summary": "Livestream shopping platforms often overlook the accessibility needs of the\nDeaf and Hard of Hearing (DHH) community, leading to barriers such as\ninformation inaccessibility and overload. To tackle these challenges, we\ndeveloped \\textit{EchoAid}, a mobile app designed to improve the livestream\nshopping experience for DHH users. \\textit{EchoAid} utilizes advanced\nspeech-to-text conversion, Rapid Serial Visual Presentation (RSVP) technology,\nand Large Language Models (LLMs) to simplify the complex information flow in\nlive sales environments. We conducted exploratory studies with eight DHH\nindividuals to identify design needs and iteratively developed the\n\\textit{EchoAid} prototype based on feedback from three participants. We then\nevaluate the performance of this system in a user study workshop involving 38\nDHH participants. Our findings demonstrate the successful design and validation\nprocess of \\textit{EchoAid}, highlighting its potential to enhance product\ninformation extraction, leading to reduced cognitive overload and more engaging\nand customized shopping experiences for DHH users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Livestream shopping platforms often overlook the accessibility needs of the\nDeaf and Hard of Hearing (DHH) community, leading to barriers such as\ninformation inaccessibility and overload. To tackle these challenges, we\ndeveloped \\textit{EchoAid}, a mobile app designed to improve the livestream\nshopping experience for DHH users. \\textit{EchoAid} utilizes advanced\nspeech-to-text conversion, Rapid Serial Visual Presentation (RSVP) technology,\nand Large Language Models (LLMs) to simplify the complex information flow in\nlive sales environments. We conducted exploratory studies with eight DHH\nindividuals to identify design needs and iteratively developed the\n\\textit{EchoAid} prototype based on feedback from three participants. We then\nevaluate the performance of this system in a user study workshop involving 38\nDHH participants. Our findings demonstrate the successful design and validation\nprocess of \\textit{EchoAid}, highlighting its potential to enhance product\ninformation extraction, leading to reduced cognitive overload and more engaging\nand customized shopping experiences for DHH users."
                },
                "authors": [
                    {
                        "name": "Zeyu Yang"
                    },
                    {
                        "name": "Zheng Wei"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Xian Xu"
                    },
                    {
                        "name": "Changyang He"
                    },
                    {
                        "name": "Muzhi Zhou"
                    },
                    {
                        "name": "Pan Hui"
                    }
                ],
                "author_detail": {
                    "name": "Pan Hui"
                },
                "author": "Pan Hui",
                "arxiv_doi": "10.1145/3757471",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3757471",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.08020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Paper for CSCW 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08011v1",
                "updated": "2025-08-11T14:15:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    15,
                    33,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:15:33Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    15,
                    33,
                    0,
                    223,
                    0
                ],
                "title": "Progressive Depth Up-scaling via Optimal Transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Depth Up-scaling via Optimal Transport"
                },
                "summary": "Scaling Large Language Models (LLMs) yields performance gains but incurs\nsubstantial training costs. Depth up-scaling offers training efficiency by\nadding new layers to pre-trained models. However, most existing methods copy or\naverage weights from base layers, neglecting neuron permutation differences.\nThis limitation can potentially cause misalignment that harms performance.\nInspired by applying Optimal Transport (OT) for neuron alignment, we propose\nOptimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses\nTransformer blocks in adjacent base layers via OT for new layer creation, to\nmitigate neuron permutation mismatch between layers. OpT-DeUS achieves better\noverall performance and offers improved training efficiency than existing\nmethods for continual pre-training and supervised fine-tuning across different\nmodel sizes. To further evaluate the impact of interpolation positions, our\nextensive analysis shows that inserting new layers closer to the top results in\nhigher training efficiency due to shorter back-propagation time while obtaining\nadditional performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Large Language Models (LLMs) yields performance gains but incurs\nsubstantial training costs. Depth up-scaling offers training efficiency by\nadding new layers to pre-trained models. However, most existing methods copy or\naverage weights from base layers, neglecting neuron permutation differences.\nThis limitation can potentially cause misalignment that harms performance.\nInspired by applying Optimal Transport (OT) for neuron alignment, we propose\nOptimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses\nTransformer blocks in adjacent base layers via OT for new layer creation, to\nmitigate neuron permutation mismatch between layers. OpT-DeUS achieves better\noverall performance and offers improved training efficiency than existing\nmethods for continual pre-training and supervised fine-tuning across different\nmodel sizes. To further evaluate the impact of interpolation positions, our\nextensive analysis shows that inserting new layers closer to the top results in\nhigher training efficiency due to shorter back-propagation time while obtaining\nadditional performance gains."
                },
                "authors": [
                    {
                        "name": "Mingzi Cao"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18938v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18938v2",
                "updated": "2025-08-11T14:10:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    10,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-26T14:48:44Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    14,
                    48,
                    44,
                    5,
                    116,
                    0
                ],
                "title": "RAIR: Retrieval-Augmented Iterative Refinement for Chinese Spelling\n  Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAIR: Retrieval-Augmented Iterative Refinement for Chinese Spelling\n  Correction"
                },
                "summary": "Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens\nin sentences. Traditional CSC focuses on equal length correction and uses\npretrained language models (PLMs). While Large Language Models (LLMs) have\nshown remarkable success in identifying and rectifying potential errors, they\noften struggle with adapting to domain-specific corrections, especially when\nencountering terminologies in specialized domains. To address domain\nadaptation, we propose a \\textbf{R}etrieval-\\textbf{A}ugmented\n\\textbf{I}terative \\textbf{R}efinement (RAIR) framework. Our approach\nconstructs a retrieval corpus adaptively from domain-specific training data and\ndictionaries, employing a fine-tuned retriever to ensure that the retriever\ncatches the error correction pattern. We also extend equal-length into\nvariable-length correction scenarios. Extensive experiments demonstrate that\nour framework outperforms current approaches in domain spelling correction and\nsignificantly improves the performance of LLMs in variable-length scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens\nin sentences. Traditional CSC focuses on equal length correction and uses\npretrained language models (PLMs). While Large Language Models (LLMs) have\nshown remarkable success in identifying and rectifying potential errors, they\noften struggle with adapting to domain-specific corrections, especially when\nencountering terminologies in specialized domains. To address domain\nadaptation, we propose a \\textbf{R}etrieval-\\textbf{A}ugmented\n\\textbf{I}terative \\textbf{R}efinement (RAIR) framework. Our approach\nconstructs a retrieval corpus adaptively from domain-specific training data and\ndictionaries, employing a fine-tuned retriever to ensure that the retriever\ncatches the error correction pattern. We also extend equal-length into\nvariable-length correction scenarios. Extensive experiments demonstrate that\nour framework outperforms current approaches in domain spelling correction and\nsignificantly improves the performance of LLMs in variable-length scenarios."
                },
                "authors": [
                    {
                        "name": "Junhong Liang"
                    },
                    {
                        "name": "Yu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhou"
                },
                "author": "Yu Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18938v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18938v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08001v2",
                "updated": "2025-08-12T04:42:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    4,
                    42,
                    34,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T14:04:59Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    4,
                    59,
                    0,
                    223,
                    0
                ],
                "title": "Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware\n  Framework Guided by Monetary Policy Transmission Paths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware\n  Framework Guided by Monetary Policy Transmission Paths"
                },
                "summary": "\"Fedspeak\", the stylized and often nuanced language used by the U.S. Federal\nReserve, encodes implicit policy signals and strategic stances. The Federal\nOpen Market Committee strategically employs Fedspeak as a communication tool to\nshape market expectations and influence both domestic and global economic\nconditions. As such, automatically parsing and interpreting Fedspeak presents a\nhigh-impact challenge, with significant implications for financial forecasting,\nalgorithmic trading, and data-driven policy analysis. In this paper, we propose\nan LLM-based, uncertainty-aware framework for deciphering Fedspeak and\nclassifying its underlying monetary policy stance. Technically, to enrich the\nsemantic and contextual representation of Fedspeak texts, we incorporate\ndomain-specific reasoning grounded in the monetary policy transmission\nmechanism. We further introduce a dynamic uncertainty decoding module to assess\nthe confidence of model predictions, thereby enhancing both classification\naccuracy and model reliability. Experimental results demonstrate that our\nframework achieves state-of-the-art performance on the policy stance analysis\ntask. Moreover, statistical analysis reveals a significant positive correlation\nbetween perceptual uncertainty and model error rates, validating the\neffectiveness of perceptual uncertainty as a diagnostic signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Fedspeak\", the stylized and often nuanced language used by the U.S. Federal\nReserve, encodes implicit policy signals and strategic stances. The Federal\nOpen Market Committee strategically employs Fedspeak as a communication tool to\nshape market expectations and influence both domestic and global economic\nconditions. As such, automatically parsing and interpreting Fedspeak presents a\nhigh-impact challenge, with significant implications for financial forecasting,\nalgorithmic trading, and data-driven policy analysis. In this paper, we propose\nan LLM-based, uncertainty-aware framework for deciphering Fedspeak and\nclassifying its underlying monetary policy stance. Technically, to enrich the\nsemantic and contextual representation of Fedspeak texts, we incorporate\ndomain-specific reasoning grounded in the monetary policy transmission\nmechanism. We further introduce a dynamic uncertainty decoding module to assess\nthe confidence of model predictions, thereby enhancing both classification\naccuracy and model reliability. Experimental results demonstrate that our\nframework achieves state-of-the-art performance on the policy stance analysis\ntask. Moreover, statistical analysis reveals a significant positive correlation\nbetween perceptual uncertainty and model error rates, validating the\neffectiveness of perceptual uncertainty as a diagnostic signal."
                },
                "authors": [
                    {
                        "name": "Rui Yao"
                    },
                    {
                        "name": "Qi Chai"
                    },
                    {
                        "name": "Jinhai Yao"
                    },
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07999v1",
                "updated": "2025-08-11T14:03:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    3,
                    9,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T14:03:09Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    3,
                    9,
                    0,
                    223,
                    0
                ],
                "title": "WideSearch: Benchmarking Agentic Broad Info-Seeking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WideSearch: Benchmarking Agentic Broad Info-Seeking"
                },
                "summary": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/"
                },
                "authors": [
                    {
                        "name": "Ryan Wong"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Junjie Zhao"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Xuan Zhou"
                    },
                    {
                        "name": "Zuo Wang"
                    },
                    {
                        "name": "Kai Xiang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Ke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Wang"
                },
                "author": "Ke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07995v2",
                "updated": "2025-08-12T13:46:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    13,
                    46,
                    8,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T13:57:49Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    57,
                    49,
                    0,
                    223,
                    0
                ],
                "title": "DIVER: A Multi-Stage Approach for Reasoning-intensive Information\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIVER: A Multi-Stage Approach for Reasoning-intensive Information\n  Retrieval"
                },
                "summary": "Retrieval-augmented generation has achieved strong performance on\nknowledge-intensive tasks where query-document relevance can be identified\nthrough direct lexical or semantic matches. However, many real-world queries\ninvolve abstract reasoning, analogical thinking, or multi-step inference, which\nexisting retrievers often struggle to capture. To address this challenge, we\npresent \\textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive\ninformation retrieval. DIVER consists of four components: document processing\nto improve input quality, LLM-driven query expansion via iterative document\ninteraction, a reasoning-enhanced retriever fine-tuned on synthetic\nmulti-domain data with hard negatives, and a pointwise reranker that combines\nLLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,\nDIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original\nqueries, consistently outperforming competitive reasoning-aware models. These\nresults demonstrate the effectiveness of reasoning-aware retrieval strategies\nin complex real-world tasks. Our code and retrieval model will be released\nsoon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation has achieved strong performance on\nknowledge-intensive tasks where query-document relevance can be identified\nthrough direct lexical or semantic matches. However, many real-world queries\ninvolve abstract reasoning, analogical thinking, or multi-step inference, which\nexisting retrievers often struggle to capture. To address this challenge, we\npresent \\textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive\ninformation retrieval. DIVER consists of four components: document processing\nto improve input quality, LLM-driven query expansion via iterative document\ninteraction, a reasoning-enhanced retriever fine-tuned on synthetic\nmulti-domain data with hard negatives, and a pointwise reranker that combines\nLLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,\nDIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original\nqueries, consistently outperforming competitive reasoning-aware models. These\nresults demonstrate the effectiveness of reasoning-aware retrieval strategies\nin complex real-world tasks. Our code and retrieval model will be released\nsoon."
                },
                "authors": [
                    {
                        "name": "Meixiu Long"
                    },
                    {
                        "name": "Duolin Sun"
                    },
                    {
                        "name": "Dan Yang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Jiahai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiahai Wang"
                },
                "author": "Jiahai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17001v2",
                "updated": "2025-08-11T13:41:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    41,
                    39,
                    0,
                    223,
                    0
                ],
                "published": "2025-06-20T13:52:15Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    52,
                    15,
                    4,
                    171,
                    0
                ],
                "title": "PersonalAI: A Systematic Comparison of Knowledge Graph Storage and\n  Retrieval Approaches for Personalized LLM agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonalAI: A Systematic Comparison of Knowledge Graph Storage and\n  Retrieval Approaches for Personalized LLM agents"
                },
                "summary": "Personalizing language models by effectively incorporating user interaction\nhistory remains a central challenge in the development of adaptive AI systems.\nWhile large language models (LLMs) combined with Retrieval-Augmented Generation\n(RAG) have improved factual accuracy, they often lack structured memory and\nfail to scale in complex, long-term interactions. To address this, we propose a\nflexible external memory framework based on knowledge graphs, automatically\nconstructed and updated by the LLM itself, and capable of encoding information\nin multiple formats-including nodes, triplets, higher-order propositions, and\nepisodic traces. Building upon the AriGraph architecture, we introduce a novel\nhybrid graph design that supports both standard edges and two types of\nhyperedges, enabling rich and dynamic semantic and temporal representations.\nOur framework also supports diverse retrieval mechanisms, including A*,\nwater-circle propagation, beam search, and hybrid methods, making it adaptable\nto different datasets and LLM capacities. We evaluate our system on three\nbenchmarks-TriviaQA, HotpotQA, and DiaASQ-demonstrating that different memory\nand retrieval configurations yield optimal performance depending on the task.\nAdditionally, we extend the DiaASQ benchmark with temporal annotations and\ninternally contradictory statements, showing that our system remains robust and\neffective in managing temporal dependencies and context-aware reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalizing language models by effectively incorporating user interaction\nhistory remains a central challenge in the development of adaptive AI systems.\nWhile large language models (LLMs) combined with Retrieval-Augmented Generation\n(RAG) have improved factual accuracy, they often lack structured memory and\nfail to scale in complex, long-term interactions. To address this, we propose a\nflexible external memory framework based on knowledge graphs, automatically\nconstructed and updated by the LLM itself, and capable of encoding information\nin multiple formats-including nodes, triplets, higher-order propositions, and\nepisodic traces. Building upon the AriGraph architecture, we introduce a novel\nhybrid graph design that supports both standard edges and two types of\nhyperedges, enabling rich and dynamic semantic and temporal representations.\nOur framework also supports diverse retrieval mechanisms, including A*,\nwater-circle propagation, beam search, and hybrid methods, making it adaptable\nto different datasets and LLM capacities. We evaluate our system on three\nbenchmarks-TriviaQA, HotpotQA, and DiaASQ-demonstrating that different memory\nand retrieval configurations yield optimal performance depending on the task.\nAdditionally, we extend the DiaASQ benchmark with temporal annotations and\ninternally contradictory statements, showing that our system remains robust and\neffective in managing temporal dependencies and context-aware reasoning."
                },
                "authors": [
                    {
                        "name": "Mikhail Menschikov"
                    },
                    {
                        "name": "Dmitry Evseev"
                    },
                    {
                        "name": "Victoria Dochkina"
                    },
                    {
                        "name": "Ruslan Kostoev"
                    },
                    {
                        "name": "Ilia Perepechkin"
                    },
                    {
                        "name": "Petr Anokhin"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    },
                    {
                        "name": "Nikita Semenov"
                    }
                ],
                "author_detail": {
                    "name": "Nikita Semenov"
                },
                "author": "Nikita Semenov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07978v2",
                "updated": "2025-08-12T06:17:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    6,
                    17,
                    10,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T13:37:41Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    37,
                    41,
                    0,
                    223,
                    0
                ],
                "title": "Adaptive Multiple Access and Service Placement for Generative Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multiple Access and Service Placement for Generative Diffusion\n  Models"
                },
                "summary": "Generative Diffusion Models (GDMs) have emerged as key components of\nGenerative Artificial Intelligence (GenAI), offering unparalleled\nexpressiveness and controllability for complex data generation tasks. However,\ntheir deployment in real-time and mobile environments remains challenging due\nto the iterative and resource-intensive nature of the inference process.\nAddressing these challenges, this paper introduces a unified optimization\nframework that jointly tackles service placement and multiple access control\nfor GDMs in mobile edge networks. We propose LEARN-GDM, a Deep Reinforcement\nLearning-based algorithm that dynamically partitions denoising blocks across\nheterogeneous edge nodes, while accounting for latent transmission costs and\nenabling adaptive reduction of inference steps. Our approach integrates a\ngreedy multiple access scheme with a Double and Dueling Deep Q-Learning\n(D3QL)-based service placement, allowing for scalable, adaptable, and\nresource-efficient operation under stringent quality of service requirements.\nSimulations demonstrate the superior performance of the proposed framework in\nterms of scalability and latency resilience compared to conventional monolithic\nand fixed chain-length placement strategies. This work advances the state of\nthe art in edge-enabled GenAI by offering an adaptable solution for GDM\nservices orchestration, paving the way for future extensions toward semantic\nnetworking and co-inference across distributed environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Diffusion Models (GDMs) have emerged as key components of\nGenerative Artificial Intelligence (GenAI), offering unparalleled\nexpressiveness and controllability for complex data generation tasks. However,\ntheir deployment in real-time and mobile environments remains challenging due\nto the iterative and resource-intensive nature of the inference process.\nAddressing these challenges, this paper introduces a unified optimization\nframework that jointly tackles service placement and multiple access control\nfor GDMs in mobile edge networks. We propose LEARN-GDM, a Deep Reinforcement\nLearning-based algorithm that dynamically partitions denoising blocks across\nheterogeneous edge nodes, while accounting for latent transmission costs and\nenabling adaptive reduction of inference steps. Our approach integrates a\ngreedy multiple access scheme with a Double and Dueling Deep Q-Learning\n(D3QL)-based service placement, allowing for scalable, adaptable, and\nresource-efficient operation under stringent quality of service requirements.\nSimulations demonstrate the superior performance of the proposed framework in\nterms of scalability and latency resilience compared to conventional monolithic\nand fixed chain-length placement strategies. This work advances the state of\nthe art in edge-enabled GenAI by offering an adaptable solution for GDM\nservices orchestration, paving the way for future extensions toward semantic\nnetworking and co-inference across distributed environments."
                },
                "authors": [
                    {
                        "name": "Hamidreza Mazandarani"
                    },
                    {
                        "name": "Mohammad Farhoudi"
                    },
                    {
                        "name": "Masoud Shokrnezhad"
                    },
                    {
                        "name": "Tarik Taleb"
                    }
                ],
                "author_detail": {
                    "name": "Tarik Taleb"
                },
                "author": "Tarik Taleb",
                "arxiv_comment": "This manuscript has been accepted for presentation at IEEE GLOBECOM\n  2025. You can use this material personally. Reprinting or republishing this\n  material for the purpose of advertising or promotion, etc., must adhere to\n  IEEE policy. The DOI will be supplied as soon as it becomes available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07976v1",
                "updated": "2025-08-11T13:36:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    36,
                    57,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T13:36:57Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    36,
                    57,
                    0,
                    223,
                    0
                ],
                "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL"
                },
                "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Minyang Xie"
                    },
                    {
                        "name": "Shusheng Xu"
                    },
                    {
                        "name": "Chuyi He"
                    },
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Banghua Zhu"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10434v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10434v3",
                "updated": "2025-08-11T13:32:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    32,
                    9,
                    0,
                    223,
                    0
                ],
                "published": "2024-03-15T16:14:34Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    16,
                    14,
                    34,
                    4,
                    75,
                    0
                ],
                "title": "Spotter+GPT: Turning Sign Spottings into Sentences with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotter+GPT: Turning Sign Spottings into Sentences with LLMs"
                },
                "summary": "Sign Language Translation (SLT) is a challenging task that aims to generate\nspoken language sentences from sign language videos. In this paper, we\nintroduce a lightweight, modular SLT framework, Spotter+GPT, that leverages the\npower of Large Language Models (LLMs) and avoids heavy end-to-end training.\nSpotter+GPT breaks down the SLT task into two distinct stages. First, a sign\nspotter identifies individual signs within the input video. The spotted signs\nare then passed to an LLM, which transforms them into meaningful spoken\nlanguage sentences. Spotter+GPT eliminates the requirement for SLT-specific\ntraining. This significantly reduces computational costs and time requirements.\nThe source code and pretrained weights of the Spotter are available at\nhttps://gitlab.surrey.ac.uk/cogvispublic/sign-spotter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign Language Translation (SLT) is a challenging task that aims to generate\nspoken language sentences from sign language videos. In this paper, we\nintroduce a lightweight, modular SLT framework, Spotter+GPT, that leverages the\npower of Large Language Models (LLMs) and avoids heavy end-to-end training.\nSpotter+GPT breaks down the SLT task into two distinct stages. First, a sign\nspotter identifies individual signs within the input video. The spotted signs\nare then passed to an LLM, which transforms them into meaningful spoken\nlanguage sentences. Spotter+GPT eliminates the requirement for SLT-specific\ntraining. This significantly reduces computational costs and time requirements.\nThe source code and pretrained weights of the Spotter are available at\nhttps://gitlab.surrey.ac.uk/cogvispublic/sign-spotter."
                },
                "authors": [
                    {
                        "name": "Ozge Mercanoglu Sincan"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden",
                "arxiv_doi": "10.1145/3742886.3756708",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3742886.3756708",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.10434v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10434v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 9th Workshop on Sign Language Translation and Avatar\n  Technologies (SLTAT) in ACM International Conference on Intelligent Virtual\n  Agents (IVA`25)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07967v1",
                "updated": "2025-08-11T13:28:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    28,
                    22,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T13:28:22Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    28,
                    22,
                    0,
                    223,
                    0
                ],
                "title": "Advancing the Control of Low-Altitude Wireless Networks: Architecture,\n  Design Principles, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing the Control of Low-Altitude Wireless Networks: Architecture,\n  Design Principles, and Future Directions"
                },
                "summary": "This article introduces a control-oriented low-altitude wireless network\n(LAWN) that integrates near-ground communications and remote estimation of the\ninternal system state. This integration supports reliable networked control in\ndynamic aerial-ground environments. First, we introduce the network's modular\narchitecture and key performance metrics. Then, we discuss core design\ntrade-offs across the control, communication, and estimation layers. A case\nstudy illustrates closed-loop coordination under wireless constraints. Finally,\nwe outline future directions for scalable, resilient LAWN deployments in\nreal-time and resource-constrained scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article introduces a control-oriented low-altitude wireless network\n(LAWN) that integrates near-ground communications and remote estimation of the\ninternal system state. This integration supports reliable networked control in\ndynamic aerial-ground environments. First, we introduce the network's modular\narchitecture and key performance metrics. Then, we discuss core design\ntrade-offs across the control, communication, and estimation layers. A case\nstudy illustrates closed-loop coordination under wireless constraints. Finally,\nwe outline future directions for scalable, resilient LAWN deployments in\nreal-time and resource-constrained scenarios."
                },
                "authors": [
                    {
                        "name": "Haijia Jin"
                    },
                    {
                        "name": "Weijie Yuan"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Xianbin Wang"
                    },
                    {
                        "name": "George K. Karagiannidis"
                    },
                    {
                        "name": "Zhiyun Lin"
                    },
                    {
                        "name": "Yi Gong"
                    },
                    {
                        "name": "Dong In Kim"
                    },
                    {
                        "name": "Athina Petropulu"
                    },
                    {
                        "name": "Maria Sabrina Greco"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Sumei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Sumei Sun"
                },
                "author": "Sumei Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07966v1",
                "updated": "2025-08-11T13:26:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    26,
                    48,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T13:26:48Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    26,
                    48,
                    0,
                    223,
                    0
                ],
                "title": "Exploring the Challenges and Opportunities of AI-assisted Codebase\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Challenges and Opportunities of AI-assisted Codebase\n  Generation"
                },
                "summary": "Recent AI code assistants have significantly improved their ability to\nprocess more complex contexts and generate entire codebases based on a textual\ndescription, compared to the popular snippet-level generation. These codebase\nAI assistants (CBAs) can also extend or adapt codebases, allowing users to\nfocus on higher-level design and deployment decisions. While prior work has\nextensively studied the impact of snippet-level code generation, this new class\nof codebase generation models is relatively unexplored. Despite initial\nanecdotal reports of excitement about these agents, they remain less frequently\nadopted compared to snippet-level code assistants. To utilize CBAs better, we\nneed to understand how developers interact with CBAs, and how and why CBAs fall\nshort of developers' needs. In this paper, we explored these gaps through a\ncounterbalanced user study and interview with (n = 16) students and developers\nworking on coding tasks with CBAs. We found that participants varied the\ninformation in their prompts, like problem description (48% of prompts),\nrequired functionality (98% of prompts), code structure (48% of prompts), and\ntheir prompt writing process. Despite various strategies, the overall\nsatisfaction score with generated codebases remained low (mean = 2.8, median =\n3, on a scale of one to five). Participants mentioned functionality as the most\ncommon factor for dissatisfaction (77% of instances), alongside poor code\nquality (42% of instances) and communication issues (25% of instances). We\ndelve deeper into participants' dissatisfaction to identify six underlying\nchallenges that participants faced when using CBAs, and extracted five barriers\nto incorporating CBAs into their workflows. Finally, we surveyed 21 commercial\nCBAs to compare their capabilities with participant challenges and present\ndesign opportunities for more efficient and useful CBAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent AI code assistants have significantly improved their ability to\nprocess more complex contexts and generate entire codebases based on a textual\ndescription, compared to the popular snippet-level generation. These codebase\nAI assistants (CBAs) can also extend or adapt codebases, allowing users to\nfocus on higher-level design and deployment decisions. While prior work has\nextensively studied the impact of snippet-level code generation, this new class\nof codebase generation models is relatively unexplored. Despite initial\nanecdotal reports of excitement about these agents, they remain less frequently\nadopted compared to snippet-level code assistants. To utilize CBAs better, we\nneed to understand how developers interact with CBAs, and how and why CBAs fall\nshort of developers' needs. In this paper, we explored these gaps through a\ncounterbalanced user study and interview with (n = 16) students and developers\nworking on coding tasks with CBAs. We found that participants varied the\ninformation in their prompts, like problem description (48% of prompts),\nrequired functionality (98% of prompts), code structure (48% of prompts), and\ntheir prompt writing process. Despite various strategies, the overall\nsatisfaction score with generated codebases remained low (mean = 2.8, median =\n3, on a scale of one to five). Participants mentioned functionality as the most\ncommon factor for dissatisfaction (77% of instances), alongside poor code\nquality (42% of instances) and communication issues (25% of instances). We\ndelve deeper into participants' dissatisfaction to identify six underlying\nchallenges that participants faced when using CBAs, and extracted five barriers\nto incorporating CBAs into their workflows. Finally, we surveyed 21 commercial\nCBAs to compare their capabilities with participant challenges and present\ndesign opportunities for more efficient and useful CBAs."
                },
                "authors": [
                    {
                        "name": "Philipp Eibl"
                    },
                    {
                        "name": "Sadra Sabouri"
                    },
                    {
                        "name": "Souti Chattopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Souti Chattopadhyay"
                },
                "author": "Souti Chattopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07959v1",
                "updated": "2025-08-11T13:10:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    10,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T13:10:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    10,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "Large Language Models for Subjective Language Understanding: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Subjective Language Understanding: A Survey"
                },
                "summary": "Subjective language understanding refers to a broad set of natural language\nprocessing tasks where the goal is to interpret or generate content that\nconveys personal feelings, opinions, or figurative meanings rather than\nobjective facts. With the advent of large language models (LLMs) such as\nChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach\nthese inherently nuanced tasks. In this survey, we provide a comprehensive\nreview of recent advances in applying LLMs to subjective language tasks,\nincluding sentiment analysis, emotion recognition, sarcasm detection, humor\nunderstanding, stance detection, metaphor interpretation, intent detection, and\naesthetics assessment. We begin by clarifying the definition of subjective\nlanguage from linguistic and cognitive perspectives, and we outline the unique\nchallenges posed by subjective language (e.g. ambiguity, figurativeness,\ncontext dependence). We then survey the evolution of LLM architectures and\ntechniques that particularly benefit subjectivity tasks, highlighting why LLMs\nare well-suited to model subtle human-like judgments. For each of the eight\ntasks, we summarize task definitions, key datasets, state-of-the-art LLM-based\nmethods, and remaining challenges. We provide comparative insights, discussing\ncommonalities and differences among tasks and how multi-task LLM approaches\nmight yield unified models of subjectivity. Finally, we identify open issues\nsuch as data limitations, model bias, and ethical considerations, and suggest\nfuture research directions. We hope this survey will serve as a valuable\nresource for researchers and practitioners interested in the intersection of\naffective computing, figurative language processing, and large-scale language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subjective language understanding refers to a broad set of natural language\nprocessing tasks where the goal is to interpret or generate content that\nconveys personal feelings, opinions, or figurative meanings rather than\nobjective facts. With the advent of large language models (LLMs) such as\nChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach\nthese inherently nuanced tasks. In this survey, we provide a comprehensive\nreview of recent advances in applying LLMs to subjective language tasks,\nincluding sentiment analysis, emotion recognition, sarcasm detection, humor\nunderstanding, stance detection, metaphor interpretation, intent detection, and\naesthetics assessment. We begin by clarifying the definition of subjective\nlanguage from linguistic and cognitive perspectives, and we outline the unique\nchallenges posed by subjective language (e.g. ambiguity, figurativeness,\ncontext dependence). We then survey the evolution of LLM architectures and\ntechniques that particularly benefit subjectivity tasks, highlighting why LLMs\nare well-suited to model subtle human-like judgments. For each of the eight\ntasks, we summarize task definitions, key datasets, state-of-the-art LLM-based\nmethods, and remaining challenges. We provide comparative insights, discussing\ncommonalities and differences among tasks and how multi-task LLM approaches\nmight yield unified models of subjectivity. Finally, we identify open issues\nsuch as data limitations, model bias, and ethical considerations, and suggest\nfuture research directions. We hope this survey will serve as a valuable\nresource for researchers and practitioners interested in the intersection of\naffective computing, figurative language processing, and large-scale language\nmodels."
                },
                "authors": [
                    {
                        "name": "Changhao Song"
                    },
                    {
                        "name": "Yazhou Zhang"
                    },
                    {
                        "name": "Hui Gao"
                    },
                    {
                        "name": "Ben Yao"
                    },
                    {
                        "name": "Peng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Zhang"
                },
                "author": "Peng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07956v1",
                "updated": "2025-08-11T13:08:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    8,
                    37,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T13:08:37Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    8,
                    37,
                    0,
                    223,
                    0
                ],
                "title": "Careful Queries, Credible Results: Teaching RAG Models Advanced Web\n  Search Tools with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Careful Queries, Credible Results: Teaching RAG Models Advanced Web\n  Search Tools with Reinforcement Learning"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating up-to-date external knowledge, yet real-world web environments\npresent unique challenges. These limitations manifest as two key challenges:\npervasive misinformation in the web environment, which introduces unreliable or\nmisleading content that can degrade retrieval accuracy, and the\nunderutilization of web tools, which, if effectively employed, could enhance\nquery precision and help mitigate this noise, ultimately improving the\nretrieval results in RAG systems. To address these issues, we propose\nWebFilter, a novel RAG framework that generates source-restricted queries and\nfilters out unreliable content. This approach combines a retrieval filtering\nmechanism with a behavior- and outcome-driven reward strategy, optimizing both\nquery formulation and retrieval outcomes. Extensive experiments demonstrate\nthat WebFilter improves answer quality and retrieval precision, outperforming\nexisting RAG methods on both in-domain and out-of-domain benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating up-to-date external knowledge, yet real-world web environments\npresent unique challenges. These limitations manifest as two key challenges:\npervasive misinformation in the web environment, which introduces unreliable or\nmisleading content that can degrade retrieval accuracy, and the\nunderutilization of web tools, which, if effectively employed, could enhance\nquery precision and help mitigate this noise, ultimately improving the\nretrieval results in RAG systems. To address these issues, we propose\nWebFilter, a novel RAG framework that generates source-restricted queries and\nfilters out unreliable content. This approach combines a retrieval filtering\nmechanism with a behavior- and outcome-driven reward strategy, optimizing both\nquery formulation and retrieval outcomes. Extensive experiments demonstrate\nthat WebFilter improves answer quality and retrieval precision, outperforming\nexisting RAG methods on both in-domain and out-of-domain benchmarks."
                },
                "authors": [
                    {
                        "name": "Yuqin Dai"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Yong Deng"
                    },
                    {
                        "name": "Zhanwei Zhang"
                    },
                    {
                        "name": "Jun Yin"
                    },
                    {
                        "name": "Pengyu Zeng"
                    },
                    {
                        "name": "Zhenzhe Ying"
                    },
                    {
                        "name": "Changhua Meng"
                    },
                    {
                        "name": "Can Yi"
                    },
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Shuai Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Lu"
                },
                "author": "Shuai Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07955v1",
                "updated": "2025-08-11T13:08:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    8,
                    7,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T13:08:07Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    8,
                    7,
                    0,
                    223,
                    0
                ],
                "title": "Expert Preference-based Evaluation of Automated Related Work Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert Preference-based Evaluation of Automated Related Work Generation"
                },
                "summary": "Expert domain writing, such as scientific writing, typically demands\nextensive domain knowledge. Recent advances in LLMs show promising potential in\nreducing the expert workload. However, evaluating the quality of automatically\ngenerated scientific writing is a crucial open issue, as it requires knowledge\nof domain-specific evaluation criteria and the ability to discern expert\npreferences. Conventional automatic metrics and LLM-as-a-judge systems are\ninsufficient to grasp expert preferences and domain-specific quality standards.\nTo address this gap and support human-AI collaborative writing, we focus on\nrelated work generation, one of the most challenging scientific tasks, as an\nexemplar. We propose GREP, a multi-turn evaluation framework that integrates\nclassical related work evaluation criteria with expert-specific preferences.\nInstead of assigning a single score, our framework decomposes the evaluation\ninto fine-grained dimensions. This localized evaluation approach is further\naugmented with contrastive few-shot examples to provide detailed contextual\nguidance for the evaluation dimensions. The design principles allow our\nframework to deliver cardinal assessment of quality, which can facilitate\nbetter post-training compared to ordinal preference data. For better\naccessibility, we design two variants of GREP: a more precise variant with\nproprietary LLMs as evaluators, and a cheaper alternative with open-weight\nLLMs. Empirical investigation reveals that our framework is able to assess the\nquality of related work sections in a much more robust manner compared to\nstandard LLM judges, reflects natural scenarios of scientific writing, and\nbears a strong correlation with the human expert assessment. We also observe\nthat generations from state-of-the-art LLMs struggle to satisfy validation\nconstraints of a suitable related work section. They (mostly) fail to improve\nbased on feedback as well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert domain writing, such as scientific writing, typically demands\nextensive domain knowledge. Recent advances in LLMs show promising potential in\nreducing the expert workload. However, evaluating the quality of automatically\ngenerated scientific writing is a crucial open issue, as it requires knowledge\nof domain-specific evaluation criteria and the ability to discern expert\npreferences. Conventional automatic metrics and LLM-as-a-judge systems are\ninsufficient to grasp expert preferences and domain-specific quality standards.\nTo address this gap and support human-AI collaborative writing, we focus on\nrelated work generation, one of the most challenging scientific tasks, as an\nexemplar. We propose GREP, a multi-turn evaluation framework that integrates\nclassical related work evaluation criteria with expert-specific preferences.\nInstead of assigning a single score, our framework decomposes the evaluation\ninto fine-grained dimensions. This localized evaluation approach is further\naugmented with contrastive few-shot examples to provide detailed contextual\nguidance for the evaluation dimensions. The design principles allow our\nframework to deliver cardinal assessment of quality, which can facilitate\nbetter post-training compared to ordinal preference data. For better\naccessibility, we design two variants of GREP: a more precise variant with\nproprietary LLMs as evaluators, and a cheaper alternative with open-weight\nLLMs. Empirical investigation reveals that our framework is able to assess the\nquality of related work sections in a much more robust manner compared to\nstandard LLM judges, reflects natural scenarios of scientific writing, and\nbears a strong correlation with the human expert assessment. We also observe\nthat generations from state-of-the-art LLMs struggle to satisfy validation\nconstraints of a suitable related work section. They (mostly) fail to improve\nbased on feedback as well."
                },
                "authors": [
                    {
                        "name": "Furkan Şahinuç"
                    },
                    {
                        "name": "Subhabrata Dutta"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "Project page: https://ukplab.github.io/arxiv2025-expert-eval-rw/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07950v1",
                "updated": "2025-08-11T13:05:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    5,
                    59,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T13:05:59Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    13,
                    5,
                    59,
                    0,
                    223,
                    0
                ],
                "title": "FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large\n  Language Model for Automated Cause-of-Death Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large\n  Language Model for Automated Cause-of-Death Analysis"
                },
                "summary": "Forensic cause-of-death determination faces systemic challenges, including\nworkforce shortages and diagnostic variability, particularly in high-volume\nsystems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic\nAgenT), a multi-agent AI framework that automates and standardizes death\ninvestigations through a domain-adapted large language model. FEAT's\napplication-oriented architecture integrates: (i) a central Planner for task\ndecomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a\nMemory & Reflection module for iterative refinement, and (iv) a Global Solver\nfor conclusion synthesis. The system employs tool-augmented reasoning,\nhierarchical retrieval-augmented generation, forensic-tuned LLMs, and\nhuman-in-the-loop feedback to ensure legal and medical validity. In evaluations\nacross diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI\nsystems in both long-form autopsy analyses and concise cause-of-death\nconclusions. It demonstrated robust generalization across six geographic\nregions and achieved high expert concordance in blinded validations. Senior\npathologists validated FEAT's outputs as comparable to those of human experts,\nwith improved detection of subtle evidentiary nuances. To our knowledge, FEAT\nis the first LLM-based AI agent system dedicated to forensic medicine, offering\nscalable, consistent death certification while maintaining expert-level rigor.\nBy integrating AI efficiency with human oversight, this work could advance\nequitable access to reliable medicolegal services while addressing critical\ncapacity constraints in forensic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forensic cause-of-death determination faces systemic challenges, including\nworkforce shortages and diagnostic variability, particularly in high-volume\nsystems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic\nAgenT), a multi-agent AI framework that automates and standardizes death\ninvestigations through a domain-adapted large language model. FEAT's\napplication-oriented architecture integrates: (i) a central Planner for task\ndecomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a\nMemory & Reflection module for iterative refinement, and (iv) a Global Solver\nfor conclusion synthesis. The system employs tool-augmented reasoning,\nhierarchical retrieval-augmented generation, forensic-tuned LLMs, and\nhuman-in-the-loop feedback to ensure legal and medical validity. In evaluations\nacross diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI\nsystems in both long-form autopsy analyses and concise cause-of-death\nconclusions. It demonstrated robust generalization across six geographic\nregions and achieved high expert concordance in blinded validations. Senior\npathologists validated FEAT's outputs as comparable to those of human experts,\nwith improved detection of subtle evidentiary nuances. To our knowledge, FEAT\nis the first LLM-based AI agent system dedicated to forensic medicine, offering\nscalable, consistent death certification while maintaining expert-level rigor.\nBy integrating AI efficiency with human oversight, this work could advance\nequitable access to reliable medicolegal services while addressing critical\ncapacity constraints in forensic systems."
                },
                "authors": [
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Wanqing Zhang"
                    },
                    {
                        "name": "Kehan Li"
                    },
                    {
                        "name": "Erwen Huang"
                    },
                    {
                        "name": "Haitao Bi"
                    },
                    {
                        "name": "Aiying Fan"
                    },
                    {
                        "name": "Yiwen Shen"
                    },
                    {
                        "name": "Hongmei Dong"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Yuming Shao"
                    },
                    {
                        "name": "Zengjia Liu"
                    },
                    {
                        "name": "Xinshe Liu"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Chunxia Yan"
                    },
                    {
                        "name": "Shuanliang Fan"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Jianhua Ma"
                    },
                    {
                        "name": "Bin Cong"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Chunfeng Lian"
                    }
                ],
                "author_detail": {
                    "name": "Chunfeng Lian"
                },
                "author": "Chunfeng Lian",
                "arxiv_comment": "18pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07935v1",
                "updated": "2025-08-11T12:50:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    50,
                    46,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T12:50:46Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    50,
                    46,
                    0,
                    223,
                    0
                ],
                "title": "SHIELDA: Structured Handling of Exceptions in LLM-Driven Agentic\n  Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHIELDA: Structured Handling of Exceptions in LLM-Driven Agentic\n  Workflows"
                },
                "summary": "Large Language Model (LLM) agentic systems are software systems powered by\nLLMs that autonomously reason, plan, and execute multi-step workflows to\nachieve human goals, rather than merely executing predefined steps. During\nexecution, these workflows frequently encounter exceptions. Existing exception\nhandling solutions often treat exceptions superficially, failing to trace\nexecution-phase exceptions to their reasoning-phase root causes. Furthermore,\ntheir recovery logic is brittle, lacking structured escalation pathways when\ninitial attempts fail. To tackle these challenges, we first present a\ncomprehensive taxonomy of 36 exception types across 12 agent artifacts.\nBuilding on this, we propose SHIELDA (Structured Handling of Exceptions in\nLLM-Driven Agentic Workflows), a modular runtime exception handling framework\nfor LLM agentic workflows. SHIELDA uses an exception classifier to select a\npredefined exception handling pattern from a handling pattern registry. These\npatterns are then executed via a structured handling executor, comprising local\nhandling, flow control, and state recovery, to enable phase-aware recovery by\nlinking exceptions to their root causes and facilitating composable strategies.\nWe validate SHIELDA's effectiveness through a case study on the AutoPR agent,\ndemonstrating effective, cross-phase recovery from a reasoning-induced\nexception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agentic systems are software systems powered by\nLLMs that autonomously reason, plan, and execute multi-step workflows to\nachieve human goals, rather than merely executing predefined steps. During\nexecution, these workflows frequently encounter exceptions. Existing exception\nhandling solutions often treat exceptions superficially, failing to trace\nexecution-phase exceptions to their reasoning-phase root causes. Furthermore,\ntheir recovery logic is brittle, lacking structured escalation pathways when\ninitial attempts fail. To tackle these challenges, we first present a\ncomprehensive taxonomy of 36 exception types across 12 agent artifacts.\nBuilding on this, we propose SHIELDA (Structured Handling of Exceptions in\nLLM-Driven Agentic Workflows), a modular runtime exception handling framework\nfor LLM agentic workflows. SHIELDA uses an exception classifier to select a\npredefined exception handling pattern from a handling pattern registry. These\npatterns are then executed via a structured handling executor, comprising local\nhandling, flow control, and state recovery, to enable phase-aware recovery by\nlinking exceptions to their root causes and facilitating composable strategies.\nWe validate SHIELDA's effectiveness through a case study on the AutoPR agent,\ndemonstrating effective, cross-phase recovery from a reasoning-induced\nexception."
                },
                "authors": [
                    {
                        "name": "Jingwen Zhou"
                    },
                    {
                        "name": "Jieshan Chen"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Dehai Zhao"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07932v1",
                "updated": "2025-08-11T12:47:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    47,
                    59,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T12:47:59Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    47,
                    59,
                    0,
                    223,
                    0
                ],
                "title": "\\(X\\)-evolve: Solution space evolution powered by large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\\(X\\)-evolve: Solution space evolution powered by large language models"
                },
                "summary": "While combining large language models (LLMs) with evolutionary algorithms\n(EAs) shows promise for solving complex optimization problems, current\napproaches typically evolve individual solutions, often incurring high LLM call\ncosts. We introduce \\(X\\)-evolve, a paradigm-shifting method that instead\nevolves solution spaces \\(X\\) (sets of individual solutions) - subsets of the\noverall search space \\(S\\). In \\(X\\)-evolve, LLMs generate tunable programs\nwherein certain code snippets, designated as parameters, define a tunable\nsolution space. A score-based search algorithm then efficiently explores this\nparametrically defined space, guided by feedback from objective function\nscores. This strategy enables broader and more efficient exploration, which can\npotentially accelerate convergence at a much lower search cost, requiring up to\ntwo orders of magnitude fewer LLM calls than prior leading methods. We\ndemonstrate \\(X\\)-evolve's efficacy across three distinct hard optimization\nproblems. For the cap set problem, we discover a larger partial admissible set,\nestablishing a new tighter asymptotic lower bound for the cap set constant (\\(C\n\\ge 2.2203\\)). In information theory, we uncover a larger independent set for\nthe 15-vertex cycle graph (\\(\\mathcal{C}_{15}^{\\boxtimes 5}\\), size 19,946),\nthereby raising the known lower bound on its Shannon capacity. Furthermore, for\nthe NP-hard online bin packing problem, we generate heuristics that\nconsistently outperform standard strategies across established benchmarks. By\nevolving solution spaces, our method considerably improves search\neffectiveness, making it possible to tackle high-dimensional problems that were\npreviously computationally prohibitive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While combining large language models (LLMs) with evolutionary algorithms\n(EAs) shows promise for solving complex optimization problems, current\napproaches typically evolve individual solutions, often incurring high LLM call\ncosts. We introduce \\(X\\)-evolve, a paradigm-shifting method that instead\nevolves solution spaces \\(X\\) (sets of individual solutions) - subsets of the\noverall search space \\(S\\). In \\(X\\)-evolve, LLMs generate tunable programs\nwherein certain code snippets, designated as parameters, define a tunable\nsolution space. A score-based search algorithm then efficiently explores this\nparametrically defined space, guided by feedback from objective function\nscores. This strategy enables broader and more efficient exploration, which can\npotentially accelerate convergence at a much lower search cost, requiring up to\ntwo orders of magnitude fewer LLM calls than prior leading methods. We\ndemonstrate \\(X\\)-evolve's efficacy across three distinct hard optimization\nproblems. For the cap set problem, we discover a larger partial admissible set,\nestablishing a new tighter asymptotic lower bound for the cap set constant (\\(C\n\\ge 2.2203\\)). In information theory, we uncover a larger independent set for\nthe 15-vertex cycle graph (\\(\\mathcal{C}_{15}^{\\boxtimes 5}\\), size 19,946),\nthereby raising the known lower bound on its Shannon capacity. Furthermore, for\nthe NP-hard online bin packing problem, we generate heuristics that\nconsistently outperform standard strategies across established benchmarks. By\nevolving solution spaces, our method considerably improves search\neffectiveness, making it possible to tackle high-dimensional problems that were\npreviously computationally prohibitive."
                },
                "authors": [
                    {
                        "name": "Yi Zhai"
                    },
                    {
                        "name": "Zhiqiang Wei"
                    },
                    {
                        "name": "Ruohan Li"
                    },
                    {
                        "name": "Keyu Pan"
                    },
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Jianmin Ji"
                    },
                    {
                        "name": "Wuyang Zhang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yanyong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanyong Zhang"
                },
                "author": "Yanyong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01422v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01422v5",
                "updated": "2025-08-11T12:47:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    47,
                    49,
                    0,
                    223,
                    0
                ],
                "published": "2024-03-03T07:43:39Z",
                "published_parsed": [
                    2024,
                    3,
                    3,
                    7,
                    43,
                    39,
                    6,
                    63,
                    0
                ],
                "title": "DreamFrame: Enhancing Video Understanding via Automatically Generated QA\n  and Style-Consistent Keyframes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamFrame: Enhancing Video Understanding via Automatically Generated QA\n  and Style-Consistent Keyframes"
                },
                "summary": "Recent large vision-language models (LVLMs) for video understanding are\nprimarily fine-tuned with various videos scraped from online platforms.\nExisting datasets, such as ActivityNet, require considerable human labor for\nstructuring and annotation before effectively utilized for tuning LVLMs. While\ncurrent LVLMs are primarily trained on existing datasets in broad,\ngeneral-purpose settings, adapting them to specific downstream scenarios\nremains challenging, as collecting and annotating task-specific videos is\nhighly labor-intensive and time-consuming. To address this issue, we propose a\nthree-stage framework named DreamFrame for automatically generating\nstyle-consistent keyframes and corresponding question-answer (QA) pairs to\nsupport LVLM instruction tuning. DreamFrame generates datasets in a movie-like\nmanner. First, we utilize an LLM to generate structured movie plots including\nmovie prior information (like overview and style), frame descriptions and\nplot-related QA pairs, with a story expansion strategy to mitigate context\nlength limitations.Then, to ensure visual consistency across generated frames,\nwe design a Style Immobilization Process which maintains consistent style\nthrough an embedding learning strategy. Finally, frame descriptions and style\nembeddings are integrated to produce coherent keyframes. Using DreamFrame, we\nconstruct a dataset comprising approximately 1k stylized keyframe-like videos\nand 100k diverse QA pairs. Extensive fine-tuned experiments on various LVLM\narchitectures demonstrate the effectiveness of the proposed dataset.\nFurthermore, based on the proposed dataset, we fine-tune a new LVLM named\nDreamFrame-7B, which significantly surpasses the previous similar-sized LVLMs\nacross different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large vision-language models (LVLMs) for video understanding are\nprimarily fine-tuned with various videos scraped from online platforms.\nExisting datasets, such as ActivityNet, require considerable human labor for\nstructuring and annotation before effectively utilized for tuning LVLMs. While\ncurrent LVLMs are primarily trained on existing datasets in broad,\ngeneral-purpose settings, adapting them to specific downstream scenarios\nremains challenging, as collecting and annotating task-specific videos is\nhighly labor-intensive and time-consuming. To address this issue, we propose a\nthree-stage framework named DreamFrame for automatically generating\nstyle-consistent keyframes and corresponding question-answer (QA) pairs to\nsupport LVLM instruction tuning. DreamFrame generates datasets in a movie-like\nmanner. First, we utilize an LLM to generate structured movie plots including\nmovie prior information (like overview and style), frame descriptions and\nplot-related QA pairs, with a story expansion strategy to mitigate context\nlength limitations.Then, to ensure visual consistency across generated frames,\nwe design a Style Immobilization Process which maintains consistent style\nthrough an embedding learning strategy. Finally, frame descriptions and style\nembeddings are integrated to produce coherent keyframes. Using DreamFrame, we\nconstruct a dataset comprising approximately 1k stylized keyframe-like videos\nand 100k diverse QA pairs. Extensive fine-tuned experiments on various LVLM\narchitectures demonstrate the effectiveness of the proposed dataset.\nFurthermore, based on the proposed dataset, we fine-tune a new LVLM named\nDreamFrame-7B, which significantly surpasses the previous similar-sized LVLMs\nacross different benchmarks."
                },
                "authors": [
                    {
                        "name": "Zhende Song"
                    },
                    {
                        "name": "Chenchen Wang"
                    },
                    {
                        "name": "Jiamu Sheng"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Shengji Tang"
                    },
                    {
                        "name": "Jiayuan Fan"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "Accepted by ACMM'MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01422v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01422v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07925v1",
                "updated": "2025-08-11T12:38:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    38,
                    46,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T12:38:46Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    38,
                    46,
                    0,
                    223,
                    0
                ],
                "title": "TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video\n  Temporal Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video\n  Temporal Grounding"
                },
                "summary": "Video Temporal Grounding (VTG) aims to extract relevant video segments based\non a given natural language query. Recently, zero-shot VTG methods have gained\nattention by leveraging pretrained vision-language models (VLMs) to localize\ntarget moments without additional training. However, existing approaches suffer\nfrom semantic fragmentation, where temporally continuous frames sharing the\nsame semantics are split across multiple segments. When segments are\nfragmented, it becomes difficult to predict an accurate target moment that\naligns with the text query. Also, they rely on skewed similarity distributions\nfor localization, making it difficult to select the optimal segment.\nFurthermore, they heavily depend on the use of LLMs which require expensive\ninferences. To address these limitations, we propose a \\textit{TAG}, a simple\nyet effective Temporal-Aware approach for zero-shot video temporal Grounding,\nwhich incorporates temporal pooling, temporal coherence clustering, and\nsimilarity adjustment. Our proposed method effectively captures the temporal\ncontext of videos and addresses distorted similarity distributions without\ntraining. Our approach achieves state-of-the-art results on Charades-STA and\nActivityNet Captions benchmark datasets without rely on LLMs. Our code is\navailable at https://github.com/Nuetee/TAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Temporal Grounding (VTG) aims to extract relevant video segments based\non a given natural language query. Recently, zero-shot VTG methods have gained\nattention by leveraging pretrained vision-language models (VLMs) to localize\ntarget moments without additional training. However, existing approaches suffer\nfrom semantic fragmentation, where temporally continuous frames sharing the\nsame semantics are split across multiple segments. When segments are\nfragmented, it becomes difficult to predict an accurate target moment that\naligns with the text query. Also, they rely on skewed similarity distributions\nfor localization, making it difficult to select the optimal segment.\nFurthermore, they heavily depend on the use of LLMs which require expensive\ninferences. To address these limitations, we propose a \\textit{TAG}, a simple\nyet effective Temporal-Aware approach for zero-shot video temporal Grounding,\nwhich incorporates temporal pooling, temporal coherence clustering, and\nsimilarity adjustment. Our proposed method effectively captures the temporal\ncontext of videos and addresses distorted similarity distributions without\ntraining. Our approach achieves state-of-the-art results on Charades-STA and\nActivityNet Captions benchmark datasets without rely on LLMs. Our code is\navailable at https://github.com/Nuetee/TAG"
                },
                "authors": [
                    {
                        "name": "Jin-Seop Lee"
                    },
                    {
                        "name": "SungJoon Lee"
                    },
                    {
                        "name": "Jaehan Ahn"
                    },
                    {
                        "name": "YunSeok Choi"
                    },
                    {
                        "name": "Jee-Hyong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jee-Hyong Lee"
                },
                "author": "Jee-Hyong Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07918v1",
                "updated": "2025-08-11T12:32:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    32,
                    48,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T12:32:48Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    32,
                    48,
                    0,
                    223,
                    0
                ],
                "title": "RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language\n  Model-based Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language\n  Model-based Question Answering"
                },
                "summary": "Visual Question Answering (VQA) in remote sensing (RS) is pivotal for\ninterpreting Earth observation data. However, existing RS VQA datasets are\nconstrained by limitations in annotation richness, question diversity, and the\nassessment of specific reasoning capabilities. This paper introduces RSVLM-QA\ndataset, a new large-scale, content-rich VQA dataset for the RS domain.\nRSVLM-QA is constructed by integrating data from several prominent RS\nsegmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ\nan innovative dual-track annotation generation pipeline. Firstly, we leverage\nLarge Language Models (LLMs), specifically GPT-4.1, with meticulously designed\nprompts to automatically generate a suite of detailed annotations including\nimage captions, spatial relations, and semantic tags, alongside complex\ncaption-based VQA pairs. Secondly, to address the challenging task of object\ncounting in RS imagery, we have developed a specialized automated process that\nextracts object counts directly from the original segmentation data; GPT-4.1\nthen formulates natural language answers from these counts, which are paired\nwith preset question templates to create counting QA pairs. RSVLM-QA comprises\n13,820 images and 162,373 VQA pairs, featuring extensive annotations and\ndiverse question types. We provide a detailed statistical analysis of the\ndataset and a comparison with existing RS VQA benchmarks, highlighting the\nsuperior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct\nbenchmark experiments on Six mainstream Vision Language Models (VLMs),\ndemonstrating that RSVLM-QA effectively evaluates and challenges the\nunderstanding and reasoning abilities of current VLMs in the RS domain. We\nbelieve RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM\nresearch communities, poised to catalyze advancements in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Question Answering (VQA) in remote sensing (RS) is pivotal for\ninterpreting Earth observation data. However, existing RS VQA datasets are\nconstrained by limitations in annotation richness, question diversity, and the\nassessment of specific reasoning capabilities. This paper introduces RSVLM-QA\ndataset, a new large-scale, content-rich VQA dataset for the RS domain.\nRSVLM-QA is constructed by integrating data from several prominent RS\nsegmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ\nan innovative dual-track annotation generation pipeline. Firstly, we leverage\nLarge Language Models (LLMs), specifically GPT-4.1, with meticulously designed\nprompts to automatically generate a suite of detailed annotations including\nimage captions, spatial relations, and semantic tags, alongside complex\ncaption-based VQA pairs. Secondly, to address the challenging task of object\ncounting in RS imagery, we have developed a specialized automated process that\nextracts object counts directly from the original segmentation data; GPT-4.1\nthen formulates natural language answers from these counts, which are paired\nwith preset question templates to create counting QA pairs. RSVLM-QA comprises\n13,820 images and 162,373 VQA pairs, featuring extensive annotations and\ndiverse question types. We provide a detailed statistical analysis of the\ndataset and a comparison with existing RS VQA benchmarks, highlighting the\nsuperior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct\nbenchmark experiments on Six mainstream Vision Language Models (VLMs),\ndemonstrating that RSVLM-QA effectively evaluates and challenges the\nunderstanding and reasoning abilities of current VLMs in the RS domain. We\nbelieve RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM\nresearch communities, poised to catalyze advancements in the field."
                },
                "authors": [
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Jinghao Xiao"
                    },
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Xian Tao"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Ali Braytee"
                    },
                    {
                        "name": "Mukesh Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Mukesh Prasad"
                },
                "author": "Mukesh Prasad",
                "arxiv_comment": "This paper has been accepted to the proceedings of the 33rd ACM\n  International Multimedia Conference (ACM Multimedia 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17066v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17066v3",
                "updated": "2025-08-11T12:32:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    32,
                    14,
                    0,
                    223,
                    0
                ],
                "published": "2025-05-18T16:13:07Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    16,
                    13,
                    7,
                    6,
                    138,
                    0
                ],
                "title": "Improving LLM Outputs Against Jailbreak Attacks with Expert Model\n  Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Outputs Against Jailbreak Attacks with Expert Model\n  Integration"
                },
                "summary": "Using LLMs in a production environment presents security challenges that\ninclude vulnerabilities to jailbreaks and prompt injections, which can result\nin harmful outputs for humans or the enterprise. The challenge is amplified\nwhen working within a specific domain, as topics generally accepted for LLMs to\naddress may be irrelevant to that field. These problems can be mitigated, for\nexample, by fine-tuning large language models with domain-specific and\nsecurity-focused data. However, these alone are insufficient, as jailbreak\ntechniques evolve. Additionally, API-accessed models do not offer the\nflexibility needed to tailor behavior to industry-specific objectives, and\nin-context learning is not always sufficient or reliable. In response to these\nchallenges, we introduce Archias, an expert model adept at distinguishing\nbetween in-domain and out-of-domain communications. Archias classifies user\ninquiries into several categories: in-domain (specifically for the automotive\nindustry), malicious questions, price injections, prompt injections, and\nout-of-domain examples. Our methodology integrates outputs from the expert\nmodel (Archias) into prompts, which are then processed by the LLM to generate\nresponses. This method increases the model's ability to understand the user's\nintention and give appropriate answers. Archias can be adjusted, fine-tuned,\nand used for many different purposes due to its small size. Therefore, it can\nbe easily customized to the needs of any industry. To validate our approach, we\ncreated a benchmark dataset for the automotive industry. Furthermore, in the\ninterest of advancing research and development, we release our benchmark\ndataset to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs in a production environment presents security challenges that\ninclude vulnerabilities to jailbreaks and prompt injections, which can result\nin harmful outputs for humans or the enterprise. The challenge is amplified\nwhen working within a specific domain, as topics generally accepted for LLMs to\naddress may be irrelevant to that field. These problems can be mitigated, for\nexample, by fine-tuning large language models with domain-specific and\nsecurity-focused data. However, these alone are insufficient, as jailbreak\ntechniques evolve. Additionally, API-accessed models do not offer the\nflexibility needed to tailor behavior to industry-specific objectives, and\nin-context learning is not always sufficient or reliable. In response to these\nchallenges, we introduce Archias, an expert model adept at distinguishing\nbetween in-domain and out-of-domain communications. Archias classifies user\ninquiries into several categories: in-domain (specifically for the automotive\nindustry), malicious questions, price injections, prompt injections, and\nout-of-domain examples. Our methodology integrates outputs from the expert\nmodel (Archias) into prompts, which are then processed by the LLM to generate\nresponses. This method increases the model's ability to understand the user's\nintention and give appropriate answers. Archias can be adjusted, fine-tuned,\nand used for many different purposes due to its small size. Therefore, it can\nbe easily customized to the needs of any industry. To validate our approach, we\ncreated a benchmark dataset for the automotive industry. Furthermore, in the\ninterest of advancing research and development, we release our benchmark\ndataset to the community."
                },
                "authors": [
                    {
                        "name": "Tatia Tsmindashvili"
                    },
                    {
                        "name": "Ana Kolkhidashvili"
                    },
                    {
                        "name": "Dachi Kurtskhalia"
                    },
                    {
                        "name": "Nino Maghlakelidze"
                    },
                    {
                        "name": "Elene Mekvabishvili"
                    },
                    {
                        "name": "Guram Dentoshvili"
                    },
                    {
                        "name": "Orkhan Shamilov"
                    },
                    {
                        "name": "Zaal Gachechiladze"
                    },
                    {
                        "name": "Steven Saporta"
                    },
                    {
                        "name": "David Dachi Choladze"
                    }
                ],
                "author_detail": {
                    "name": "David Dachi Choladze"
                },
                "author": "David Dachi Choladze",
                "arxiv_doi": "10.1109/ACCESS.2025.3592458",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3592458",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.17066v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17066v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access, vol. 13, pp. 134976-134988, Jul. 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07902v1",
                "updated": "2025-08-11T12:17:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    17,
                    58,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T12:17:58Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    17,
                    58,
                    0,
                    223,
                    0
                ],
                "title": "Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity"
                },
                "summary": "Large language models (LLMs) show promise in offering emotional support and\ngenerating empathetic responses for individuals in distress, but their ability\nto deliver culturally sensitive support remains underexplored due to lack of\nresources. In this work, we introduce CultureCare, the first dataset designed\nfor this task, spanning four cultures and including 1729 distress messages,\n1523 cultural signals, and 1041 support strategies with fine-grained emotional\nand cultural annotations. Leveraging CultureCare, we (i) develop and test four\nadaptation strategies for guiding three state-of-the-art LLMs toward culturally\nsensitive responses; (ii) conduct comprehensive evaluations using LLM judges,\nin-culture human annotators, and clinical psychologists; (iii) show that\nadapted LLMs outperform anonymous online peer responses, and that simple\ncultural role-play is insufficient for cultural sensitivity; and (iv) explore\nthe application of LLMs in clinical training, where experts highlight their\npotential in fostering cultural competence in future therapists.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show promise in offering emotional support and\ngenerating empathetic responses for individuals in distress, but their ability\nto deliver culturally sensitive support remains underexplored due to lack of\nresources. In this work, we introduce CultureCare, the first dataset designed\nfor this task, spanning four cultures and including 1729 distress messages,\n1523 cultural signals, and 1041 support strategies with fine-grained emotional\nand cultural annotations. Leveraging CultureCare, we (i) develop and test four\nadaptation strategies for guiding three state-of-the-art LLMs toward culturally\nsensitive responses; (ii) conduct comprehensive evaluations using LLM judges,\nin-culture human annotators, and clinical psychologists; (iii) show that\nadapted LLMs outperform anonymous online peer responses, and that simple\ncultural role-play is insufficient for cultural sensitivity; and (iv) explore\nthe application of LLMs in clinical training, where experts highlight their\npotential in fostering cultural competence in future therapists."
                },
                "authors": [
                    {
                        "name": "Chen Cecilia Liu"
                    },
                    {
                        "name": "Hiba Arnaout"
                    },
                    {
                        "name": "Nils Kovačić"
                    },
                    {
                        "name": "Dana Atzil-Slonim"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "Under review; joint first authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02233v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02233v2",
                "updated": "2025-08-11T12:16:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    16,
                    3,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-04T09:33:47Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    33,
                    47,
                    0,
                    216,
                    0
                ],
                "title": "A Methodological Framework for LLM-Based Mining of Software Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Methodological Framework for LLM-Based Mining of Software Repositories"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in software engineering\nresearch, offering new opportunities for automating repository mining tasks.\nHowever, despite their growing popularity, the methodological integration of\nLLMs into Mining Software Repositories (MSR) remains poorly understood.\nExisting studies tend to focus on specific capabilities or performance\nbenchmarks, providing limited insight into how researchers utilize LLMs across\nthe full research pipeline. To address this gap, we conduct a mixed-method\nstudy that combines a rapid review and questionnaire survey in the field of\nLLM4MSR. We investigate (1) the approaches and (2) the threats that affect the\nempirical rigor of researchers involved in this field. Our findings reveal 15\nmethodological approaches, nine main threats, and 25 mitigation strategies.\nBuilding on these findings, we present PRIMES 2.0, a refined empirical\nframework organized into six stages, comprising 23 methodological substeps,\neach mapped to specific threats and corresponding mitigation strategies,\nproviding prescriptive and adaptive support throughout the lifecycle of\nLLM-based MSR studies. Our work contributes to establishing a more transparent\nand reproducible foundation for LLM-based MSR research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in software engineering\nresearch, offering new opportunities for automating repository mining tasks.\nHowever, despite their growing popularity, the methodological integration of\nLLMs into Mining Software Repositories (MSR) remains poorly understood.\nExisting studies tend to focus on specific capabilities or performance\nbenchmarks, providing limited insight into how researchers utilize LLMs across\nthe full research pipeline. To address this gap, we conduct a mixed-method\nstudy that combines a rapid review and questionnaire survey in the field of\nLLM4MSR. We investigate (1) the approaches and (2) the threats that affect the\nempirical rigor of researchers involved in this field. Our findings reveal 15\nmethodological approaches, nine main threats, and 25 mitigation strategies.\nBuilding on these findings, we present PRIMES 2.0, a refined empirical\nframework organized into six stages, comprising 23 methodological substeps,\neach mapped to specific threats and corresponding mitigation strategies,\nproviding prescriptive and adaptive support throughout the lifecycle of\nLLM-based MSR studies. Our work contributes to establishing a more transparent\nand reproducible foundation for LLM-based MSR research."
                },
                "authors": [
                    {
                        "name": "Vincenzo De Martino"
                    },
                    {
                        "name": "Joel Castaño"
                    },
                    {
                        "name": "Fabio Palomba"
                    },
                    {
                        "name": "Xavier Franch"
                    },
                    {
                        "name": "Silverio Martínez-Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Silverio Martínez-Fernández"
                },
                "author": "Silverio Martínez-Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02233v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02233v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07887v1",
                "updated": "2025-08-11T12:05:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    5,
                    18,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T12:05:18Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    5,
                    18,
                    0,
                    223,
                    0
                ],
                "title": "Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic\n  Participant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic\n  Participant"
                },
                "summary": "Simulators have revolutionized scientific practice across the natural\nsciences. By generating data that reliably approximate real-world phenomena,\nthey enable scientists to accelerate hypothesis testing and optimize\nexperimental designs. This is perhaps best illustrated by AlphaFold, a\nNobel-prize winning simulator in chemistry that predicts protein structures\nfrom amino acid sequences, enabling rapid prototyping of molecular\ninteractions, drug targets, and protein functions. In the behavioral sciences,\na reliable participant simulator - a system capable of producing human-like\nbehavior across cognitive tasks - would represent a similarly transformative\nadvance. Recently, Binz et al. introduced Centaur, a large language model (LLM)\nfine-tuned on human data from 160 experiments, proposing its use not only as a\nmodel of cognition but also as a participant simulator for \"in silico\nprototyping of experimental studies\", e.g., to advance automated cognitive\nscience. Here, we review the core criteria for a participant simulator and\nassess how well Centaur meets them. Although Centaur demonstrates strong\npredictive accuracy, its generative behavior - a critical criterion for a\nparticipant simulator - systematically diverges from human data. This suggests\nthat, while Centaur is a significant step toward predicting human behavior, it\ndoes not yet meet the standards of a reliable participant simulator or an\naccurate model of cognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulators have revolutionized scientific practice across the natural\nsciences. By generating data that reliably approximate real-world phenomena,\nthey enable scientists to accelerate hypothesis testing and optimize\nexperimental designs. This is perhaps best illustrated by AlphaFold, a\nNobel-prize winning simulator in chemistry that predicts protein structures\nfrom amino acid sequences, enabling rapid prototyping of molecular\ninteractions, drug targets, and protein functions. In the behavioral sciences,\na reliable participant simulator - a system capable of producing human-like\nbehavior across cognitive tasks - would represent a similarly transformative\nadvance. Recently, Binz et al. introduced Centaur, a large language model (LLM)\nfine-tuned on human data from 160 experiments, proposing its use not only as a\nmodel of cognition but also as a participant simulator for \"in silico\nprototyping of experimental studies\", e.g., to advance automated cognitive\nscience. Here, we review the core criteria for a participant simulator and\nassess how well Centaur meets them. Although Centaur demonstrates strong\npredictive accuracy, its generative behavior - a critical criterion for a\nparticipant simulator - systematically diverges from human data. This suggests\nthat, while Centaur is a significant step toward predicting human behavior, it\ndoes not yet meet the standards of a reliable participant simulator or an\naccurate model of cognition."
                },
                "authors": [
                    {
                        "name": "Sabrina Namazova"
                    },
                    {
                        "name": "Alessandra Brondetta"
                    },
                    {
                        "name": "Younes Strittmatter"
                    },
                    {
                        "name": "Matthew Nassar"
                    },
                    {
                        "name": "Sebastian Musslick"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Musslick"
                },
                "author": "Sebastian Musslick",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07885v1",
                "updated": "2025-08-11T12:00:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    0,
                    3,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T12:00:03Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    12,
                    0,
                    3,
                    0,
                    223,
                    0
                ],
                "title": "Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces\n  Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces\n  Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning"
                },
                "summary": "This paper introduces an advanced AI-driven perception system for autonomous\nquadcopter navigation in GPS-denied indoor environments. The proposed framework\nleverages cloud computing to offload computationally intensive tasks and\nincorporates a custom-designed printed circuit board (PCB) for efficient sensor\ndata acquisition, enabling robust navigation in confined spaces. The system\nintegrates YOLOv11 for object detection, Depth Anything V2 for monocular depth\nestimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial\nMeasurement Unit (IMU), and a cloud-based Large Language Model (LLM) for\ncontext-aware decision-making. A virtual safety envelope, enforced by\ncalibrated sensor offsets, ensures collision avoidance, while a multithreaded\narchitecture achieves low-latency processing. Enhanced spatial awareness is\nfacilitated by 3D bounding box estimation with Kalman filtering. Experimental\nresults in an indoor testbed demonstrate strong performance, with object\ndetection achieving a mean Average Precision (mAP50) of 0.6, depth estimation\nMean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42\ntrials over approximately 11 minutes, and end-to-end system latency below 1\nsecond. This cloud-supported, high-intelligence framework serves as an\nauxiliary perception and navigation system, complementing state-of-the-art\ndrone autonomy for GPS-denied confined spaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an advanced AI-driven perception system for autonomous\nquadcopter navigation in GPS-denied indoor environments. The proposed framework\nleverages cloud computing to offload computationally intensive tasks and\nincorporates a custom-designed printed circuit board (PCB) for efficient sensor\ndata acquisition, enabling robust navigation in confined spaces. The system\nintegrates YOLOv11 for object detection, Depth Anything V2 for monocular depth\nestimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial\nMeasurement Unit (IMU), and a cloud-based Large Language Model (LLM) for\ncontext-aware decision-making. A virtual safety envelope, enforced by\ncalibrated sensor offsets, ensures collision avoidance, while a multithreaded\narchitecture achieves low-latency processing. Enhanced spatial awareness is\nfacilitated by 3D bounding box estimation with Kalman filtering. Experimental\nresults in an indoor testbed demonstrate strong performance, with object\ndetection achieving a mean Average Precision (mAP50) of 0.6, depth estimation\nMean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42\ntrials over approximately 11 minutes, and end-to-end system latency below 1\nsecond. This cloud-supported, high-intelligence framework serves as an\nauxiliary perception and navigation system, complementing state-of-the-art\ndrone autonomy for GPS-denied confined spaces."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmmad"
                    },
                    {
                        "name": "Zubayer Ahmed Aditto"
                    },
                    {
                        "name": "Md Mehrab Hossain"
                    },
                    {
                        "name": "Noushin Yeasmin"
                    },
                    {
                        "name": "Shorower Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Shorower Hossain"
                },
                "author": "Shorower Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07882v1",
                "updated": "2025-08-11T11:56:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    56,
                    33,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T11:56:33Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    56,
                    33,
                    0,
                    223,
                    0
                ],
                "title": "Scalable and Energy-Efficient Predictive Data Collection in Wireless\n  Sensor Networks with Constructive Interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable and Energy-Efficient Predictive Data Collection in Wireless\n  Sensor Networks with Constructive Interference"
                },
                "summary": "A new class of Wireless Sensor Network has emerged whereby multiple nodes\ntransmit data simultaneously, exploiting constructive interference to enable\ndata collection frameworks with low energy usage and latency. This paper\npresents STAIR (Spatio-Temporal Activation for Intelligent Relaying), a\nscalable, resilient framework for Wireless Sensor Networks that leverages\nconstructive interference and operates effectively under stringent resource\nconstraints. Using constructive interference requires all nodes to transmit the\nsame packet at the same time, thus, only one source node can send data per time\nslot. STAIR uses coarse-grained topology information to flood a selected subset\nof the network, relaying sensor readings from individual nodes during their\nallocated time slots. A submodular optimisation algorithm with proven quality\nbounds determines near-optimal sensor activation locations and times, aiming to\nminimise the sum of mean squared prediction errors from a multiple multivariate\nlinear regression model, which is used to estimate values at unselected\nlocations and times. This framework has been extensively validated on a\nreal-world testbed deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new class of Wireless Sensor Network has emerged whereby multiple nodes\ntransmit data simultaneously, exploiting constructive interference to enable\ndata collection frameworks with low energy usage and latency. This paper\npresents STAIR (Spatio-Temporal Activation for Intelligent Relaying), a\nscalable, resilient framework for Wireless Sensor Networks that leverages\nconstructive interference and operates effectively under stringent resource\nconstraints. Using constructive interference requires all nodes to transmit the\nsame packet at the same time, thus, only one source node can send data per time\nslot. STAIR uses coarse-grained topology information to flood a selected subset\nof the network, relaying sensor readings from individual nodes during their\nallocated time slots. A submodular optimisation algorithm with proven quality\nbounds determines near-optimal sensor activation locations and times, aiming to\nminimise the sum of mean squared prediction errors from a multiple multivariate\nlinear regression model, which is used to estimate values at unselected\nlocations and times. This framework has been extensively validated on a\nreal-world testbed deployment."
                },
                "authors": [
                    {
                        "name": "Conor Muldoon"
                    }
                ],
                "author_detail": {
                    "name": "Conor Muldoon"
                },
                "author": "Conor Muldoon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07880v1",
                "updated": "2025-08-11T11:55:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    55,
                    18,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T11:55:18Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    55,
                    18,
                    0,
                    223,
                    0
                ],
                "title": "Multi-agent systems for chemical engineering: A review and perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems for chemical engineering: A review and perspective"
                },
                "summary": "Large language model (LLM)-based multi-agent systems (MASs) are a recent but\nrapidly evolving technology with the potential to transform chemical\nengineering by decomposing complex workflows into teams of collaborative agents\nwith specialized knowledge and tools. This review surveys the state-of-the-art\nof MAS within chemical engineering. While early studies demonstrate promising\nresults, scientific challenges remain, including the design of tailored\narchitectures, integration of heterogeneous data modalities, development of\nfoundation models with domain-specific modalities, and strategies for ensuring\ntransparency, safety, and environmental impact. As a young but fast-moving\nfield, MASs offer exciting opportunities to rethink chemical engineering\nworkflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based multi-agent systems (MASs) are a recent but\nrapidly evolving technology with the potential to transform chemical\nengineering by decomposing complex workflows into teams of collaborative agents\nwith specialized knowledge and tools. This review surveys the state-of-the-art\nof MAS within chemical engineering. While early studies demonstrate promising\nresults, scientific challenges remain, including the design of tailored\narchitectures, integration of heterogeneous data modalities, development of\nfoundation models with domain-specific modalities, and strategies for ensuring\ntransparency, safety, and environmental impact. As a young but fast-moving\nfield, MASs offer exciting opportunities to rethink chemical engineering\nworkflows."
                },
                "authors": [
                    {
                        "name": "Sophia Rupprecht"
                    },
                    {
                        "name": "Qinghe Gao"
                    },
                    {
                        "name": "Tanuj Karia"
                    },
                    {
                        "name": "Artur M. Schweidtmann"
                    }
                ],
                "author_detail": {
                    "name": "Artur M. Schweidtmann"
                },
                "author": "Artur M. Schweidtmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14594v2",
                "updated": "2025-08-11T11:50:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    50,
                    10,
                    0,
                    223,
                    0
                ],
                "published": "2024-11-21T21:27:30Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    21,
                    27,
                    30,
                    3,
                    326,
                    0
                ],
                "title": "Solving Zero-Shot 3D Visual Grounding as Constraint Satisfaction\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving Zero-Shot 3D Visual Grounding as Constraint Satisfaction\n  Problems"
                },
                "summary": "3D visual grounding (3DVG) aims to locate objects in a 3D scene with natural\nlanguage descriptions. Supervised methods have achieved decent accuracy, but\nhave a closed vocabulary and limited language understanding ability. Zero-shot\nmethods utilize large language models (LLMs) to handle natural language\ndescriptions, where the LLM either produces grounding results directly or\ngenerates programs that compute results (symbolically). In this work, we\npropose a zero-shot method that reformulates the 3DVG task as a Constraint\nSatisfaction Problem (CSP), where the variables and constraints represent\nobjects and their spatial relations, respectively. This allows a global\nsymbolic reasoning of all relevant objects, producing grounding results of both\nthe target and anchor objects. Moreover, we demonstrate the flexibility of our\nframework by handling negation- and counting-based queries with only minor\nextra coding efforts. Our system, Constraint Satisfaction Visual Grounding\n(CSVG), has been extensively evaluated on the public datasets ScanRefer and\nNr3D datasets using only open-source LLMs. Results show the effectiveness of\nCSVG and superior grounding accuracy over current state-of-the-art zero-shot\n3DVG methods with improvements of $+7.0\\%$ (Acc@0.5 score) and $+11.2\\%$ on the\nScanRefer and Nr3D datasets, respectively. The code of our system is available\nat https://asig-x.github.io/csvg_web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D visual grounding (3DVG) aims to locate objects in a 3D scene with natural\nlanguage descriptions. Supervised methods have achieved decent accuracy, but\nhave a closed vocabulary and limited language understanding ability. Zero-shot\nmethods utilize large language models (LLMs) to handle natural language\ndescriptions, where the LLM either produces grounding results directly or\ngenerates programs that compute results (symbolically). In this work, we\npropose a zero-shot method that reformulates the 3DVG task as a Constraint\nSatisfaction Problem (CSP), where the variables and constraints represent\nobjects and their spatial relations, respectively. This allows a global\nsymbolic reasoning of all relevant objects, producing grounding results of both\nthe target and anchor objects. Moreover, we demonstrate the flexibility of our\nframework by handling negation- and counting-based queries with only minor\nextra coding efforts. Our system, Constraint Satisfaction Visual Grounding\n(CSVG), has been extensively evaluated on the public datasets ScanRefer and\nNr3D datasets using only open-source LLMs. Results show the effectiveness of\nCSVG and superior grounding accuracy over current state-of-the-art zero-shot\n3DVG methods with improvements of $+7.0\\%$ (Acc@0.5 score) and $+11.2\\%$ on the\nScanRefer and Nr3D datasets, respectively. The code of our system is available\nat https://asig-x.github.io/csvg_web."
                },
                "authors": [
                    {
                        "name": "Qihao Yuan"
                    },
                    {
                        "name": "Kailai Li"
                    },
                    {
                        "name": "Jiaming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Zhang"
                },
                "author": "Jiaming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19860v2",
                "updated": "2025-08-11T11:43:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    43,
                    39,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-28T14:50:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    50,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "CoherenDream: Boosting Holistic Text Coherence in 3D Generation via\n  Multimodal Large Language Models Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoherenDream: Boosting Holistic Text Coherence in 3D Generation via\n  Multimodal Large Language Models Feedback"
                },
                "summary": "Score Distillation Sampling (SDS) has achieved remarkable success in\ntext-to-3D content generation. However, SDS-based methods struggle to maintain\nsemantic fidelity for user prompts, particularly when involving multiple\nobjects with intricate interactions. While existing approaches often address 3D\nconsistency through multiview diffusion model fine-tuning on 3D datasets, this\nstrategy inadvertently exacerbates text-3D alignment degradation. The\nlimitation stems from SDS's inherent accumulation of view-independent biases\nduring optimization, which progressively diverges from the ideal text alignment\ndirection. To alleviate this limitation, we propose a novel SDS objective,\ndubbed as Textual Coherent Score Distillation (TCSD), which integrates\nalignment feedback from multimodal large language models (MLLMs). Our TCSD\nleverages cross-modal understanding capabilities of MLLMs to assess and guide\nthe text-3D correspondence during the optimization. We further develop\n3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text\nalignment in 3D generations. Additionally, we introduce an LLM-layout\ninitialization that significantly accelerates optimization convergence through\nsemantic-aware spatial configuration. Our framework, CoherenDream, achieves\nconsistent improvement across multiple metrics on TIFA subset.As the first\nstudy to incorporate MLLMs into SDS optimization, we also conduct extensive\nablation studies to explore optimal MLLM adaptations for 3D generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Score Distillation Sampling (SDS) has achieved remarkable success in\ntext-to-3D content generation. However, SDS-based methods struggle to maintain\nsemantic fidelity for user prompts, particularly when involving multiple\nobjects with intricate interactions. While existing approaches often address 3D\nconsistency through multiview diffusion model fine-tuning on 3D datasets, this\nstrategy inadvertently exacerbates text-3D alignment degradation. The\nlimitation stems from SDS's inherent accumulation of view-independent biases\nduring optimization, which progressively diverges from the ideal text alignment\ndirection. To alleviate this limitation, we propose a novel SDS objective,\ndubbed as Textual Coherent Score Distillation (TCSD), which integrates\nalignment feedback from multimodal large language models (MLLMs). Our TCSD\nleverages cross-modal understanding capabilities of MLLMs to assess and guide\nthe text-3D correspondence during the optimization. We further develop\n3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text\nalignment in 3D generations. Additionally, we introduce an LLM-layout\ninitialization that significantly accelerates optimization convergence through\nsemantic-aware spatial configuration. Our framework, CoherenDream, achieves\nconsistent improvement across multiple metrics on TIFA subset.As the first\nstudy to incorporate MLLMs into SDS optimization, we also conduct extensive\nablation studies to explore optimal MLLM adaptations for 3D generation tasks."
                },
                "authors": [
                    {
                        "name": "Chenhan Jiang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Dit-Yan Yeung"
                    }
                ],
                "author_detail": {
                    "name": "Dit-Yan Yeung"
                },
                "author": "Dit-Yan Yeung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09368v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09368v4",
                "updated": "2025-08-11T11:28:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    28,
                    39,
                    0,
                    223,
                    0
                ],
                "published": "2025-01-16T08:27:40Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    27,
                    40,
                    3,
                    16,
                    0
                ],
                "title": "Aligning Instruction Tuning with Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Instruction Tuning with Pre-training"
                },
                "summary": "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose Aligning Instruction Tuning\nwith Pre-training (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning enhances large language models (LLMs) to follow human\ninstructions across diverse tasks, relying on high-quality datasets to guide\nbehavior. However, these datasets, whether manually curated or synthetically\ngenerated, are often narrowly focused and misaligned with the broad\ndistributions captured during pre-training, limiting LLM generalization and\neffective use of pre-trained knowledge. We propose Aligning Instruction Tuning\nwith Pre-training (AITP), a method that bridges this gap by identifying\ncoverage shortfalls in instruction-tuning datasets and rewriting\nunderrepresented pre-training data into high-quality instruction-response\npairs. This approach enriches dataset diversity while preserving task-specific\nobjectives. Evaluations on three fully open LLMs across eight benchmarks\ndemonstrate consistent performance improvements with AITP. Ablations highlight\nthe benefits of adaptive data selection, controlled rewriting, and balanced\nintegration, emphasizing the importance of aligning instruction tuning with\npre-training distributions to unlock the full potential of LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Wenqiang Zu"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09368v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09368v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07863v1",
                "updated": "2025-08-11T11:26:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    26,
                    10,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T11:26:10Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    26,
                    10,
                    0,
                    223,
                    0
                ],
                "title": "Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model"
                },
                "summary": "Human motion generation has emerged as a critical technology with\ntransformative potential for real-world applications. However, existing\nvision-language-motion models (VLMMs) face significant limitations that hinder\ntheir practical deployment. We identify controllability as a main bottleneck,\nmanifesting in five key aspects: inadequate response to diverse human commands,\nlimited pose initialization capabilities, poor performance on long-term\nsequences, insufficient handling of unseen scenarios, and lack of fine-grained\ncontrol over individual body parts. To overcome these limitations, we present\nBeing-M0.5, the first real-time, controllable VLMM that achieves\nstate-of-the-art performance across multiple motion generation tasks. Our\napproach is built upon HuMo100M, the largest and most comprehensive human\nmotion dataset to date, comprising over 5 million self-collected motion\nsequences, 100 million multi-task instructional instances, and detailed\npart-level annotations that address a critical gap in existing datasets. We\nintroduce a novel part-aware residual quantization technique for motion\ntokenization that enables precise, granular control over individual body parts\nduring generation. Extensive experimental validation demonstrates Being-M0.5's\nsuperior performance across diverse motion benchmarks, while comprehensive\nefficiency analysis confirms its real-time capabilities. Our contributions\ninclude design insights and detailed computational analysis to guide future\ndevelopment of practical motion generators. We believe that HuMo100M and\nBeing-M0.5 represent significant advances that will accelerate the adoption of\nmotion generation technologies in real-world applications. The project page is\navailable at https://beingbeyond.github.io/Being-M0.5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human motion generation has emerged as a critical technology with\ntransformative potential for real-world applications. However, existing\nvision-language-motion models (VLMMs) face significant limitations that hinder\ntheir practical deployment. We identify controllability as a main bottleneck,\nmanifesting in five key aspects: inadequate response to diverse human commands,\nlimited pose initialization capabilities, poor performance on long-term\nsequences, insufficient handling of unseen scenarios, and lack of fine-grained\ncontrol over individual body parts. To overcome these limitations, we present\nBeing-M0.5, the first real-time, controllable VLMM that achieves\nstate-of-the-art performance across multiple motion generation tasks. Our\napproach is built upon HuMo100M, the largest and most comprehensive human\nmotion dataset to date, comprising over 5 million self-collected motion\nsequences, 100 million multi-task instructional instances, and detailed\npart-level annotations that address a critical gap in existing datasets. We\nintroduce a novel part-aware residual quantization technique for motion\ntokenization that enables precise, granular control over individual body parts\nduring generation. Extensive experimental validation demonstrates Being-M0.5's\nsuperior performance across diverse motion benchmarks, while comprehensive\nefficiency analysis confirms its real-time capabilities. Our contributions\ninclude design insights and detailed computational analysis to guide future\ndevelopment of practical motion generators. We believe that HuMo100M and\nBeing-M0.5 represent significant advances that will accelerate the adoption of\nmotion generation technologies in real-world applications. The project page is\navailable at https://beingbeyond.github.io/Being-M0.5."
                },
                "authors": [
                    {
                        "name": "Bin Cao"
                    },
                    {
                        "name": "Sipeng Zheng"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Lujie Xia"
                    },
                    {
                        "name": "Qianshan Wei"
                    },
                    {
                        "name": "Qin Jin"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zongqing Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zongqing Lu"
                },
                "author": "Zongqing Lu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07860v1",
                "updated": "2025-08-11T11:24:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    24,
                    57,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T11:24:57Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    24,
                    57,
                    0,
                    223,
                    0
                ],
                "title": "Large Language Models for Czech Aspect-Based Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Czech Aspect-Based Sentiment Analysis"
                },
                "summary": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that aims to identify sentiment toward specific aspects of an entity.\nWhile large language models (LLMs) have shown strong performance in various\nnatural language processing (NLP) tasks, their capabilities for Czech ABSA\nremain largely unexplored. In this work, we conduct a comprehensive evaluation\nof 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their\nperformance in zero-shot, few-shot, and fine-tuning scenarios. Our results show\nthat small domain-specific models fine-tuned for ABSA outperform\ngeneral-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs\nachieve state-of-the-art results. We analyze how factors such as\nmultilingualism, model size, and recency influence performance and present an\nerror analysis highlighting key challenges, particularly in aspect term\nprediction. Our findings provide insights into the suitability of LLMs for\nCzech ABSA and offer guidance for future research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that aims to identify sentiment toward specific aspects of an entity.\nWhile large language models (LLMs) have shown strong performance in various\nnatural language processing (NLP) tasks, their capabilities for Czech ABSA\nremain largely unexplored. In this work, we conduct a comprehensive evaluation\nof 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their\nperformance in zero-shot, few-shot, and fine-tuning scenarios. Our results show\nthat small domain-specific models fine-tuned for ABSA outperform\ngeneral-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs\nachieve state-of-the-art results. We analyze how factors such as\nmultilingualism, model size, and recency influence performance and present an\nerror analysis highlighting key challenges, particularly in aspect term\nprediction. Our findings provide insights into the suitability of LLMs for\nCzech ABSA and offer guidance for future research in this area."
                },
                "authors": [
                    {
                        "name": "Jakub Šmíd"
                    },
                    {
                        "name": "Pavel Přibáň"
                    },
                    {
                        "name": "Pavel Král"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Král"
                },
                "author": "Pavel Král",
                "arxiv_comment": "Accepted for presentation at the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06225v2",
                "updated": "2025-08-11T11:15:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    15,
                    26,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-08T11:11:22Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    11,
                    22,
                    4,
                    220,
                    0
                ],
                "title": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven\n  Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven\n  Solution"
                },
                "summary": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the Overconfidence Phenomenon in current\nLLM-as-a-Judges, where predicted confidence significantly overstates actual\ncorrectness, undermining reliability in practical deployment. To quantify this\nphenomenon, we introduce TH-Score, a novel metric measuring confidence-accuracy\nalignment. Furthermore, we propose LLM-as-a-Fuser, an ensemble framework that\ntransforms LLMs into reliable, risk-aware evaluators. Extensive experiments\ndemonstrate that our approach substantially improves calibration and enables\nadaptive, confidence-driven evaluation pipelines, achieving superior\nreliability and accuracy compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the Overconfidence Phenomenon in current\nLLM-as-a-Judges, where predicted confidence significantly overstates actual\ncorrectness, undermining reliability in practical deployment. To quantify this\nphenomenon, we introduce TH-Score, a novel metric measuring confidence-accuracy\nalignment. Furthermore, we propose LLM-as-a-Fuser, an ensemble framework that\ntransforms LLMs into reliable, risk-aware evaluators. Extensive experiments\ndemonstrate that our approach substantially improves calibration and enables\nadaptive, confidence-driven evaluation pipelines, achieving superior\nreliability and accuracy compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Zailong Tian"
                    },
                    {
                        "name": "Zhuoheng Han"
                    },
                    {
                        "name": "Yanzhe Chen"
                    },
                    {
                        "name": "Haozhe Xu"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Richeng Xuan"
                    },
                    {
                        "name": "Houfeng Wang"
                    },
                    {
                        "name": "Lizi Liao"
                    }
                ],
                "author_detail": {
                    "name": "Lizi Liao"
                },
                "author": "Lizi Liao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07849v1",
                "updated": "2025-08-11T11:08:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    8,
                    32,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T11:08:32Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    11,
                    8,
                    32,
                    0,
                    223,
                    0
                ],
                "title": "LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding"
                },
                "summary": "Despite advances in legal NLP, no comprehensive evaluation covering multiple\nlegal-specific LLMs currently exists for contract classification tasks in\ncontract understanding. To address this gap, we present an evaluation of 10\nlegal-specific LLMs on three English language contract understanding tasks and\ncompare them with 7 general-purpose LLMs. The results show that legal-specific\nLLMs consistently outperform general-purpose models, especially on tasks\nrequiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish\nnew SOTAs on two of the three tasks, despite having 69% fewer parameters than\nthe best-performing general-purpose LLM. We also identify CaseLaw-BERT and\nLexLM as strong additional baselines for contract understanding. Our results\nprovide a holistic evaluation of legal-specific LLMs and will facilitate the\ndevelopment of more accurate contract understanding systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in legal NLP, no comprehensive evaluation covering multiple\nlegal-specific LLMs currently exists for contract classification tasks in\ncontract understanding. To address this gap, we present an evaluation of 10\nlegal-specific LLMs on three English language contract understanding tasks and\ncompare them with 7 general-purpose LLMs. The results show that legal-specific\nLLMs consistently outperform general-purpose models, especially on tasks\nrequiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish\nnew SOTAs on two of the three tasks, despite having 69% fewer parameters than\nthe best-performing general-purpose LLM. We also identify CaseLaw-BERT and\nLexLM as strong additional baselines for contract understanding. Our results\nprovide a holistic evaluation of legal-specific LLMs and will facilitate the\ndevelopment of more accurate contract understanding systems."
                },
                "authors": [
                    {
                        "name": "Amrita Singh"
                    },
                    {
                        "name": "H. Suhan Karaca"
                    },
                    {
                        "name": "Aditya Joshi"
                    },
                    {
                        "name": "Hye-young Paik"
                    },
                    {
                        "name": "Jiaojiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaojiao Jiang"
                },
                "author": "Jiaojiao Jiang",
                "arxiv_comment": "Under review. 4 pages + references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13380v3",
                "updated": "2025-08-11T10:35:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    35,
                    31,
                    0,
                    223,
                    0
                ],
                "published": "2025-06-16T11:44:28Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    44,
                    28,
                    0,
                    167,
                    0
                ],
                "title": "DAGR: Decomposition Augmented Graph Retrieval with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAGR: Decomposition Augmented Graph Retrieval with LLMs"
                },
                "summary": "Large Language Models (LLMs) excel at many Natural Language Processing (NLP)\ntasks, but struggle with multi-hop reasoning and factual consistency, limiting\ntheir effectiveness on knowledge-intensive tasks like complex question\nanswering (QA). Linking Knowledge Graphs (KG) and LLMs has shown promising\nresults, but LLMs generally lack the ability to reason efficiently over\ngraph-structured information. To address this challenge, we introduce DAGR, a\nretrieval method that leverages both complex questions and their decomposition\nin subquestions to extract relevant, linked textual subgraphs. DAGR first\nbreaks down complex queries, retrieves subgraphs guided by a weighted\nsimilarity function over both the original and decomposed queries, and creates\na question-specific knowledge graph to guide answer generation. The resulting\nGraph-RAG pipeline is suited to handle complex multi-hop questions and\neffectively reason over graph-structured data. We evaluate DAGR on standard\nmulti-hop QA benchmarks and show that it achieves comparable or superior\nperformance to competitive existing methods, using smaller models and fewer LLM\ncalls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at many Natural Language Processing (NLP)\ntasks, but struggle with multi-hop reasoning and factual consistency, limiting\ntheir effectiveness on knowledge-intensive tasks like complex question\nanswering (QA). Linking Knowledge Graphs (KG) and LLMs has shown promising\nresults, but LLMs generally lack the ability to reason efficiently over\ngraph-structured information. To address this challenge, we introduce DAGR, a\nretrieval method that leverages both complex questions and their decomposition\nin subquestions to extract relevant, linked textual subgraphs. DAGR first\nbreaks down complex queries, retrieves subgraphs guided by a weighted\nsimilarity function over both the original and decomposed queries, and creates\na question-specific knowledge graph to guide answer generation. The resulting\nGraph-RAG pipeline is suited to handle complex multi-hop questions and\neffectively reason over graph-structured data. We evaluate DAGR on standard\nmulti-hop QA benchmarks and show that it achieves comparable or superior\nperformance to competitive existing methods, using smaller models and fewer LLM\ncalls."
                },
                "authors": [
                    {
                        "name": "Valentin Six"
                    },
                    {
                        "name": "Evan Dufraisse"
                    },
                    {
                        "name": "Gaël de Chalendar"
                    }
                ],
                "author_detail": {
                    "name": "Gaël de Chalendar"
                },
                "author": "Gaël de Chalendar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01690v2",
                "updated": "2025-08-11T10:22:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    22,
                    51,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-03T09:50:40Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    50,
                    40,
                    6,
                    215,
                    0
                ],
                "title": "First Experience with Real-Time Control Using Simulated VQC-Based\n  Quantum Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Experience with Real-Time Control Using Simulated VQC-Based\n  Quantum Policies"
                },
                "summary": "This paper investigates the integration of quantum computing into offline\nreinforcement learning and the deployment of the resulting quantum policy in a\nreal-time control hardware realization of the cart-pole system. Variational\nQuantum Circuits (VQCs) are used to represent the policy. Classical model-based\noffline policy search was applied, in which a pure VQC with trainable\ninput-output weights is used as a policy network instead of a classical\nmultilayer perceptron. The goal is to evaluate the potential of deploying\nquantum architectures in real-world industrial control problems. The\nexperimental results show that the investigated model-based offline policy\nsearch is able to generate quantum policies that can balance the hardware\ncart-pole. A latency analysis reveals that while local simulated execution\nmeets real-time requirements, cloud-based quantum processing remains too slow\nfor closed-loop control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the integration of quantum computing into offline\nreinforcement learning and the deployment of the resulting quantum policy in a\nreal-time control hardware realization of the cart-pole system. Variational\nQuantum Circuits (VQCs) are used to represent the policy. Classical model-based\noffline policy search was applied, in which a pure VQC with trainable\ninput-output weights is used as a policy network instead of a classical\nmultilayer perceptron. The goal is to evaluate the potential of deploying\nquantum architectures in real-world industrial control problems. The\nexperimental results show that the investigated model-based offline policy\nsearch is able to generate quantum policies that can balance the hardware\ncart-pole. A latency analysis reveals that while local simulated execution\nmeets real-time requirements, cloud-based quantum processing remains too slow\nfor closed-loop control."
                },
                "authors": [
                    {
                        "name": "Yize Sun"
                    },
                    {
                        "name": "Mohamad Hagog"
                    },
                    {
                        "name": "Marc Weber"
                    },
                    {
                        "name": "Daniel Hein"
                    },
                    {
                        "name": "Steffen Udluft"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "6 pages, QML@QCE25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07827v1",
                "updated": "2025-08-11T10:19:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    19,
                    10,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T10:19:10Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    19,
                    10,
                    0,
                    223,
                    0
                ],
                "title": "Evaluating Large Language Models as Expert Annotators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models as Expert Annotators"
                },
                "summary": "Textual data annotation, the process of labeling or tagging text with\nrelevant information, is typically costly, time-consuming, and labor-intensive.\nWhile large language models (LLMs) have demonstrated their potential as direct\nalternatives to human annotators for general domains natural language\nprocessing (NLP) tasks, their effectiveness on annotation tasks in domains\nrequiring expert knowledge remains underexplored. In this paper, we\ninvestigate: whether top-performing LLMs, which might be perceived as having\nexpert-level proficiency in academic and professional benchmarks, can serve as\ndirect alternatives to human expert annotators? To this end, we evaluate both\nindividual LLMs and multi-agent approaches across three highly specialized\ndomains: finance, biomedicine, and law. Specifically, we propose a multi-agent\ndiscussion framework to simulate a group of human annotators, where LLMs are\ntasked to engage in discussions by considering others' annotations and\njustifications before finalizing their labels. Additionally, we incorporate\nreasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our\nempirical results reveal that: (1) Individual LLMs equipped with inference-time\ntechniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal\nor even negative performance gains, contrary to prior literature suggesting\ntheir broad effectiveness. (2) Overall, reasoning models do not demonstrate\nstatistically significant improvements over non-reasoning models in most\nsettings. This suggests that extended long CoT provides relatively limited\nbenefits for data annotation in specialized domains. (3) Certain model\nbehaviors emerge in the multi-agent discussion environment. For instance,\nClaude 3.7 Sonnet with thinking rarely changes its initial annotations, even\nwhen other agents provide correct annotations or valid reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual data annotation, the process of labeling or tagging text with\nrelevant information, is typically costly, time-consuming, and labor-intensive.\nWhile large language models (LLMs) have demonstrated their potential as direct\nalternatives to human annotators for general domains natural language\nprocessing (NLP) tasks, their effectiveness on annotation tasks in domains\nrequiring expert knowledge remains underexplored. In this paper, we\ninvestigate: whether top-performing LLMs, which might be perceived as having\nexpert-level proficiency in academic and professional benchmarks, can serve as\ndirect alternatives to human expert annotators? To this end, we evaluate both\nindividual LLMs and multi-agent approaches across three highly specialized\ndomains: finance, biomedicine, and law. Specifically, we propose a multi-agent\ndiscussion framework to simulate a group of human annotators, where LLMs are\ntasked to engage in discussions by considering others' annotations and\njustifications before finalizing their labels. Additionally, we incorporate\nreasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our\nempirical results reveal that: (1) Individual LLMs equipped with inference-time\ntechniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal\nor even negative performance gains, contrary to prior literature suggesting\ntheir broad effectiveness. (2) Overall, reasoning models do not demonstrate\nstatistically significant improvements over non-reasoning models in most\nsettings. This suggests that extended long CoT provides relatively limited\nbenefits for data annotation in specialized domains. (3) Certain model\nbehaviors emerge in the multi-agent discussion environment. For instance,\nClaude 3.7 Sonnet with thinking rarely changes its initial annotations, even\nwhen other agents provide correct annotations or valid reasoning."
                },
                "authors": [
                    {
                        "name": "Yu-Min Tseng"
                    },
                    {
                        "name": "Wei-Lin Chen"
                    },
                    {
                        "name": "Chung-Chi Chen"
                    },
                    {
                        "name": "Hsin-Hsi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hsin-Hsi Chen"
                },
                "author": "Hsin-Hsi Chen",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07583v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07583v3",
                "updated": "2025-08-11T10:08:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    8,
                    37,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-10T09:24:54Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    24,
                    54,
                    3,
                    100,
                    0
                ],
                "title": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with\n  Question Answering"
                },
                "summary": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa"
                },
                "authors": [
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Emmanouil Zaranis"
                    },
                    {
                        "name": "André F. T. Martins"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07583v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07583v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07809v1",
                "updated": "2025-08-11T09:49:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    49,
                    1,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T09:49:01Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    49,
                    1,
                    0,
                    223,
                    0
                ],
                "title": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning"
                },
                "summary": "Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on stronger LLMs for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens them to expand the space in a controlled way. This enables\nLLMs to stably learn from initially unsolved hard problems under sparse\nrewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,\nand Llama. Experiments show that EvoCoT enables LLMs to solve previously\nunsolved problems, improves reasoning capability without external CoT\nsupervision, and is compatible with various RL fine-tuning methods. We release\nthe source code to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on stronger LLMs for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens them to expand the space in a controlled way. This enables\nLLMs to stably learn from initially unsolved hard problems under sparse\nrewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,\nand Llama. Experiments show that EvoCoT enables LLMs to solve previously\nunsolved problems, improves reasoning capability without external CoT\nsupervision, and is compatible with various RL fine-tuning methods. We release\nthe source code to support future research."
                },
                "authors": [
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Taozhi Chen"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Lecheng Wang"
                    },
                    {
                        "name": "Hu XiaoLong"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07805v1",
                "updated": "2025-08-11T09:45:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    45,
                    2,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T09:45:02Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    45,
                    2,
                    0,
                    223,
                    0
                ],
                "title": "Can You Trick the Grader? Adversarial Persuasion of LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can You Trick the Grader? Adversarial Persuasion of LLM Judges"
                },
                "summary": "As large language models take on growing roles as automated evaluators in\npractical settings, a critical question arises: Can individuals persuade an LLM\njudge to assign unfairly high scores? This study is the first to reveal that\nstrategically embedded persuasive language can bias LLM judges when scoring\nmathematical reasoning tasks, where correctness should be independent of\nstylistic variation. Grounded in Aristotle's rhetorical principles, we\nformalize seven persuasion techniques (Majority, Consistency, Flattery,\nReciprocity, Pity, Authority, Identity) and embed them into otherwise identical\nresponses. Across six math benchmarks, we find that persuasive language leads\nLLM judges to assign inflated scores to incorrect solutions, by up to 8% on\naverage, with Consistency causing the most severe distortion. Notably,\nincreasing model size does not substantially mitigate this vulnerability.\nFurther analysis demonstrates that combining multiple persuasion techniques\namplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,\nthe persuasive effect persists under counter prompting strategies, highlighting\na critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need\nfor robust defenses against persuasion-based attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models take on growing roles as automated evaluators in\npractical settings, a critical question arises: Can individuals persuade an LLM\njudge to assign unfairly high scores? This study is the first to reveal that\nstrategically embedded persuasive language can bias LLM judges when scoring\nmathematical reasoning tasks, where correctness should be independent of\nstylistic variation. Grounded in Aristotle's rhetorical principles, we\nformalize seven persuasion techniques (Majority, Consistency, Flattery,\nReciprocity, Pity, Authority, Identity) and embed them into otherwise identical\nresponses. Across six math benchmarks, we find that persuasive language leads\nLLM judges to assign inflated scores to incorrect solutions, by up to 8% on\naverage, with Consistency causing the most severe distortion. Notably,\nincreasing model size does not substantially mitigate this vulnerability.\nFurther analysis demonstrates that combining multiple persuasion techniques\namplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,\nthe persuasive effect persists under counter prompting strategies, highlighting\na critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need\nfor robust defenses against persuasion-based attacks."
                },
                "authors": [
                    {
                        "name": "Yerin Hwang"
                    },
                    {
                        "name": "Dongryeol Lee"
                    },
                    {
                        "name": "Taegwan Kang"
                    },
                    {
                        "name": "Yongil Kim"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03785v2",
                "updated": "2025-08-11T09:40:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    40,
                    16,
                    0,
                    223,
                    0
                ],
                "published": "2025-04-30T16:25:51Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    25,
                    51,
                    2,
                    120,
                    0
                ],
                "title": "mAIstro: an open-source multi-agentic system for automated end-to-end\n  development of radiomics and deep learning models for medical imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mAIstro: an open-source multi-agentic system for automated end-to-end\n  development of radiomics and deep learning models for medical imaging"
                },
                "summary": "Agentic systems built on large language models (LLMs) offer promising\ncapabilities for automating complex workflows in healthcare AI. We introduce\nmAIstro, an open-source, autonomous multi-agentic framework for end-to-end\ndevelopment and deployment of medical AI models. The system orchestrates\nexploratory data analysis, radiomic feature extraction, image segmentation,\nclassification, and regression through a natural language interface, requiring\nno coding from the user. Built on a modular architecture, mAIstro supports both\nopen- and closed-source LLMs, and was evaluated using a large and diverse set\nof prompts across 16 open-source datasets, covering a wide range of imaging\nmodalities, anatomical regions, and data types. The agents successfully\nexecuted all tasks, producing interpretable outputs and validated models. This\nwork presents the first agentic framework capable of unifying data analysis, AI\nmodel development, and inference across varied healthcare applications,\noffering a reproducible and extensible foundation for clinical and research AI\nintegration. The code is available at: https://github.com/eltzanis/mAIstro",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic systems built on large language models (LLMs) offer promising\ncapabilities for automating complex workflows in healthcare AI. We introduce\nmAIstro, an open-source, autonomous multi-agentic framework for end-to-end\ndevelopment and deployment of medical AI models. The system orchestrates\nexploratory data analysis, radiomic feature extraction, image segmentation,\nclassification, and regression through a natural language interface, requiring\nno coding from the user. Built on a modular architecture, mAIstro supports both\nopen- and closed-source LLMs, and was evaluated using a large and diverse set\nof prompts across 16 open-source datasets, covering a wide range of imaging\nmodalities, anatomical regions, and data types. The agents successfully\nexecuted all tasks, producing interpretable outputs and validated models. This\nwork presents the first agentic framework capable of unifying data analysis, AI\nmodel development, and inference across varied healthcare applications,\noffering a reproducible and extensible foundation for clinical and research AI\nintegration. The code is available at: https://github.com/eltzanis/mAIstro"
                },
                "authors": [
                    {
                        "name": "Eleftherios Tzanis"
                    },
                    {
                        "name": "Michail E. Klontzas"
                    }
                ],
                "author_detail": {
                    "name": "Michail E. Klontzas"
                },
                "author": "Michail E. Klontzas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07785v1",
                "updated": "2025-08-11T09:15:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    15,
                    36,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T09:15:36Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    15,
                    36,
                    0,
                    223,
                    0
                ],
                "title": "Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts"
                },
                "summary": "The Mixture of Experts (MoE) architecture is a cornerstone of modern\nstate-of-the-art (SOTA) large language models (LLMs). MoE models facilitate\nscalability by enabling sparse parameter activation. However, traditional MoE\narchitecture uses homogeneous experts of a uniform size, activating a fixed\nnumber of parameters irrespective of input complexity and thus limiting\ncomputational efficiency. To overcome this limitation, we introduce Grove MoE,\na novel architecture incorporating experts of varying sizes, inspired by the\nheterogeneous big.LITTLE CPU architecture. This architecture features novel\nadjugate experts with a dynamic activation mechanism, enabling model capacity\nexpansion while maintaining manageable computational overhead. Building on this\narchitecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs\ndeveloped by applying an upcycling strategy to the Qwen3-30B-A3B-Base model\nduring mid-training and post-training. GroveMoE models dynamically activate\n3.14-3.28B parameters based on token complexity and achieve performance\ncomparable to SOTA open-source models of similar or even larger size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture is a cornerstone of modern\nstate-of-the-art (SOTA) large language models (LLMs). MoE models facilitate\nscalability by enabling sparse parameter activation. However, traditional MoE\narchitecture uses homogeneous experts of a uniform size, activating a fixed\nnumber of parameters irrespective of input complexity and thus limiting\ncomputational efficiency. To overcome this limitation, we introduce Grove MoE,\na novel architecture incorporating experts of varying sizes, inspired by the\nheterogeneous big.LITTLE CPU architecture. This architecture features novel\nadjugate experts with a dynamic activation mechanism, enabling model capacity\nexpansion while maintaining manageable computational overhead. Building on this\narchitecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs\ndeveloped by applying an upcycling strategy to the Qwen3-30B-A3B-Base model\nduring mid-training and post-training. GroveMoE models dynamically activate\n3.14-3.28B parameters based on token complexity and achieve performance\ncomparable to SOTA open-source models of similar or even larger size."
                },
                "authors": [
                    {
                        "name": "Haoyuan Wu"
                    },
                    {
                        "name": "Haoxing Chen"
                    },
                    {
                        "name": "Xiaodong Chen"
                    },
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tieyuan Chen"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00098v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00098v2",
                "updated": "2025-08-11T09:13:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    13,
                    37,
                    0,
                    223,
                    0
                ],
                "published": "2025-05-30T12:19:32Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    12,
                    19,
                    32,
                    4,
                    150,
                    0
                ],
                "title": "Interactive Imitation Learning for Dexterous Robotic Manipulation:\n  Challenges and Perspectives -- A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Imitation Learning for Dexterous Robotic Manipulation:\n  Challenges and Perspectives -- A Survey"
                },
                "summary": "Dexterous manipulation is a crucial yet highly complex challenge in humanoid\nrobotics, demanding precise, adaptable, and sample-efficient learning methods.\nAs humanoid robots are usually designed to operate in human-centric\nenvironments and interact with everyday objects, mastering dexterous\nmanipulation is critical for real-world deployment. Traditional approaches,\nsuch as reinforcement learning and imitation learning, have made significant\nstrides, but they often struggle due to the unique challenges of real-world\ndexterous manipulation, including high-dimensional control, limited training\ndata, and covariate shift. This survey provides a comprehensive overview of\nthese challenges and reviews existing learning-based methods for real-world\ndexterous manipulation, spanning imitation learning, reinforcement learning,\nand hybrid approaches. A promising yet underexplored direction is interactive\nimitation learning, where human feedback actively refines a robots behavior\nduring training. While interactive imitation learning has shown success in\nvarious robotic tasks, its application to dexterous manipulation remains\nlimited. To address this gap, we examine current interactive imitation learning\ntechniques applied to other robotic tasks and discuss how these methods can be\nadapted to enhance dexterous manipulation. By synthesizing state-of-the-art\nresearch, this paper highlights key challenges, identifies gaps in current\nmethodologies, and outlines potential directions for leveraging interactive\nimitation learning to improve dexterous robotic skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dexterous manipulation is a crucial yet highly complex challenge in humanoid\nrobotics, demanding precise, adaptable, and sample-efficient learning methods.\nAs humanoid robots are usually designed to operate in human-centric\nenvironments and interact with everyday objects, mastering dexterous\nmanipulation is critical for real-world deployment. Traditional approaches,\nsuch as reinforcement learning and imitation learning, have made significant\nstrides, but they often struggle due to the unique challenges of real-world\ndexterous manipulation, including high-dimensional control, limited training\ndata, and covariate shift. This survey provides a comprehensive overview of\nthese challenges and reviews existing learning-based methods for real-world\ndexterous manipulation, spanning imitation learning, reinforcement learning,\nand hybrid approaches. A promising yet underexplored direction is interactive\nimitation learning, where human feedback actively refines a robots behavior\nduring training. While interactive imitation learning has shown success in\nvarious robotic tasks, its application to dexterous manipulation remains\nlimited. To address this gap, we examine current interactive imitation learning\ntechniques applied to other robotic tasks and discuss how these methods can be\nadapted to enhance dexterous manipulation. By synthesizing state-of-the-art\nresearch, this paper highlights key challenges, identifies gaps in current\nmethodologies, and outlines potential directions for leveraging interactive\nimitation learning to improve dexterous robotic skills."
                },
                "authors": [
                    {
                        "name": "Edgar Welte"
                    },
                    {
                        "name": "Rania Rayyes"
                    }
                ],
                "author_detail": {
                    "name": "Rania Rayyes"
                },
                "author": "Rania Rayyes",
                "arxiv_comment": "27 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00098v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00098v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07781v1",
                "updated": "2025-08-11T09:13:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    13,
                    35,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T09:13:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    13,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech\n  Translation"
                },
                "summary": "This work proposes a grammar-based chunking strategy that segments input\nstreams into semantically complete units by parsing dependency relations (e.g.,\nnoun phrase boundaries, verb-object structures) and punctuation features. The\nmethod ensures chunk coherence and minimizes semantic fragmentation. Building\non this mechanism, we present SASST (Syntax-Aware Simultaneous Speech\nTranslation), an end-to-end framework integrating frozen Whisper encoder and\ndecoder-only LLM. The unified architecture dynamically outputs translation\ntokens or <WAIT> symbols to jointly optimize translation timing and content,\nwith target-side reordering addressing word-order divergence. Experiments on\nCoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation\nquality improvements across languages and validate the effectiveness of\nsyntactic structures in LLM-driven SimulST systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a grammar-based chunking strategy that segments input\nstreams into semantically complete units by parsing dependency relations (e.g.,\nnoun phrase boundaries, verb-object structures) and punctuation features. The\nmethod ensures chunk coherence and minimizes semantic fragmentation. Building\non this mechanism, we present SASST (Syntax-Aware Simultaneous Speech\nTranslation), an end-to-end framework integrating frozen Whisper encoder and\ndecoder-only LLM. The unified architecture dynamically outputs translation\ntokens or <WAIT> symbols to jointly optimize translation timing and content,\nwith target-side reordering addressing word-order divergence. Experiments on\nCoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation\nquality improvements across languages and validate the effectiveness of\nsyntactic structures in LLM-driven SimulST systems."
                },
                "authors": [
                    {
                        "name": "Zeyu Yang"
                    },
                    {
                        "name": "Lai Wei"
                    },
                    {
                        "name": "Roman Koshkin"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Satoshi Nakamura"
                    }
                ],
                "author_detail": {
                    "name": "Satoshi Nakamura"
                },
                "author": "Satoshi Nakamura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04773v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04773v3",
                "updated": "2025-08-11T09:00:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    0,
                    40,
                    0,
                    223,
                    0
                ],
                "published": "2025-02-17T09:52:17Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    9,
                    52,
                    17,
                    0,
                    48,
                    0
                ],
                "title": "Invisible Walls in Cities: Leveraging Large Language Models to Predict\n  Urban Segregation Experience with Social Media Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invisible Walls in Cities: Leveraging Large Language Models to Predict\n  Urban Segregation Experience with Social Media Content"
                },
                "summary": "Understanding experienced segregation in urban daily life is crucial for\naddressing societal inequalities and fostering inclusivity. The abundance of\nuser-generated reviews on social media encapsulates nuanced perceptions and\nfeelings associated with different places, offering rich insights into\nsegregation. However, leveraging this data poses significant challenges due to\nits vast volume, ambiguity, and confluence of diverse perspectives. To tackle\nthese challenges, we propose using Large Language Models (LLMs) to automate\nonline review mining for segregation prediction. We design a Reflective LLM\nCoder to digest social media content into insights consistent with real-world\nfeedback, and eventually produce a codebook capturing key dimensions that\nsignal segregation experience, such as cultural resonance and appeal,\naccessibility and convenience, and community engagement and local involvement.\nGuided by the codebook, LLMs can generate both informative review summaries and\nratings for segregation prediction. Moreover, we design a\nREasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and\nembedding capabilities of language models to integrate multi-channel features\nfor segregation prediction. Experiments on real-world data demonstrate that our\nframework greatly improves prediction accuracy, with a 22.79% elevation in R2\nand a 9.33% reduction in MSE. The derived codebook is generalizable across\nthree different cities, consistently improving prediction accuracy. Moreover,\nour user study confirms that the codebook-guided summaries provide cognitive\ngains for human participants in perceiving POIs' social inclusiveness. Our\nstudy marks an important step toward understanding implicit social barriers and\ninequalities, demonstrating the great potential of promoting social\ninclusiveness with AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding experienced segregation in urban daily life is crucial for\naddressing societal inequalities and fostering inclusivity. The abundance of\nuser-generated reviews on social media encapsulates nuanced perceptions and\nfeelings associated with different places, offering rich insights into\nsegregation. However, leveraging this data poses significant challenges due to\nits vast volume, ambiguity, and confluence of diverse perspectives. To tackle\nthese challenges, we propose using Large Language Models (LLMs) to automate\nonline review mining for segregation prediction. We design a Reflective LLM\nCoder to digest social media content into insights consistent with real-world\nfeedback, and eventually produce a codebook capturing key dimensions that\nsignal segregation experience, such as cultural resonance and appeal,\naccessibility and convenience, and community engagement and local involvement.\nGuided by the codebook, LLMs can generate both informative review summaries and\nratings for segregation prediction. Moreover, we design a\nREasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and\nembedding capabilities of language models to integrate multi-channel features\nfor segregation prediction. Experiments on real-world data demonstrate that our\nframework greatly improves prediction accuracy, with a 22.79% elevation in R2\nand a 9.33% reduction in MSE. The derived codebook is generalizable across\nthree different cities, consistently improving prediction accuracy. Moreover,\nour user study confirms that the codebook-guided summaries provide cognitive\ngains for human participants in perceiving POIs' social inclusiveness. Our\nstudy marks an important step toward understanding implicit social barriers and\ninequalities, demonstrating the great potential of promoting social\ninclusiveness with AI."
                },
                "authors": [
                    {
                        "name": "Bingbing Fan"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Songwei Li"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Pan Hui"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04773v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04773v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07770v1",
                "updated": "2025-08-11T08:56:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    56,
                    19,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T08:56:19Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    56,
                    19,
                    0,
                    223,
                    0
                ],
                "title": "AgentWorld: An Interactive Simulation Platform for Scene Construction\n  and Mobile Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentWorld: An Interactive Simulation Platform for Scene Construction\n  and Mobile Robotic Manipulation"
                },
                "summary": "We introduce AgentWorld, an interactive simulation platform for developing\nhousehold mobile manipulation capabilities. Our platform combines automated\nscene construction that encompasses layout generation, semantic asset\nplacement, visual material configuration, and physics simulation, with a\ndual-mode teleoperation system supporting both wheeled bases and humanoid\nlocomotion policies for data collection. The resulting AgentWorld Dataset\ncaptures diverse tasks ranging from primitive actions (pick-and-place,\npush-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)\nacross living rooms, bedrooms, and kitchens. Through extensive benchmarking of\nimitation learning methods including behavior cloning, action chunking\ntransformers, diffusion policies, and vision-language-action models, we\ndemonstrate the dataset's effectiveness for sim-to-real transfer. The\nintegrated system provides a comprehensive solution for scalable robotic skill\nacquisition in complex home environments, bridging the gap between\nsimulation-based training and real-world deployment. The code, datasets will be\navailable at https://yizhengzhang1.github.io/agent_world/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AgentWorld, an interactive simulation platform for developing\nhousehold mobile manipulation capabilities. Our platform combines automated\nscene construction that encompasses layout generation, semantic asset\nplacement, visual material configuration, and physics simulation, with a\ndual-mode teleoperation system supporting both wheeled bases and humanoid\nlocomotion policies for data collection. The resulting AgentWorld Dataset\ncaptures diverse tasks ranging from primitive actions (pick-and-place,\npush-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)\nacross living rooms, bedrooms, and kitchens. Through extensive benchmarking of\nimitation learning methods including behavior cloning, action chunking\ntransformers, diffusion policies, and vision-language-action models, we\ndemonstrate the dataset's effectiveness for sim-to-real transfer. The\nintegrated system provides a comprehensive solution for scalable robotic skill\nacquisition in complex home environments, bridging the gap between\nsimulation-based training and real-world deployment. The code, datasets will be\navailable at https://yizhengzhang1.github.io/agent_world/"
                },
                "authors": [
                    {
                        "name": "Yizheng Zhang"
                    },
                    {
                        "name": "Zhenjun Yu"
                    },
                    {
                        "name": "Jiaxin Lai"
                    },
                    {
                        "name": "Cewu Lu"
                    },
                    {
                        "name": "Lei Han"
                    }
                ],
                "author_detail": {
                    "name": "Lei Han"
                },
                "author": "Lei Han",
                "arxiv_comment": "Accepted by Conference on Robot Learning 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07768v1",
                "updated": "2025-08-11T08:54:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    54,
                    14,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T08:54:14Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    54,
                    14,
                    0,
                    223,
                    0
                ],
                "title": "Pareto Multi-Objective Alignment for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pareto Multi-Objective Alignment for Language Models"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in real-world\napplications that require careful balancing of multiple, often conflicting,\nobjectives, such as informativeness versus conciseness, or helpfulness versus\ncreativity. However, current alignment methods, primarily based on RLHF,\noptimize LLMs toward a single reward function, resulting in rigid behavior that\nfails to capture the complexity and diversity of human preferences. This\nlimitation hinders the adaptability of LLMs to practical scenarios, making\nmulti-objective alignment (MOA) a critical yet underexplored area. To bridge\nthis gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and\ncomputationally efficient algorithm designed explicitly for MOA in LLMs. In\ncontrast to computationally prohibitive multi-objective optimization (MOO)\nmethods, PAMA transforms multi-objective RLHF into a convex optimization with a\nclosed-form solution, significantly enhancing scalability. Traditional MOO\napproaches suffer from prohibitive O(n^2*d) complexity, where d represents the\nnumber of model parameters, typically in the billions for LLMs, rendering\ndirect optimization infeasible. PAMA reduces this complexity to O(n) where n is\nthe number of objectives, enabling optimization to be completed within\nmilliseconds. We provide theoretical guarantees that PAMA converges to a Pareto\nstationary point, where no objective can be improved without degrading at least\none other. Extensive experiments across language models ranging from 125M to 7B\nparameters demonstrate PAMA's robust and effective MOA capabilities, aligning\nwith its theoretical advantages. PAMA provides a highly efficient solution to\nthe MOA problem that was previously considered intractable, offering a\npractical and theoretically grounded approach to aligning LLMs with diverse\nhuman values, paving the way for versatile and adaptable real-world AI\ndeployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in real-world\napplications that require careful balancing of multiple, often conflicting,\nobjectives, such as informativeness versus conciseness, or helpfulness versus\ncreativity. However, current alignment methods, primarily based on RLHF,\noptimize LLMs toward a single reward function, resulting in rigid behavior that\nfails to capture the complexity and diversity of human preferences. This\nlimitation hinders the adaptability of LLMs to practical scenarios, making\nmulti-objective alignment (MOA) a critical yet underexplored area. To bridge\nthis gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and\ncomputationally efficient algorithm designed explicitly for MOA in LLMs. In\ncontrast to computationally prohibitive multi-objective optimization (MOO)\nmethods, PAMA transforms multi-objective RLHF into a convex optimization with a\nclosed-form solution, significantly enhancing scalability. Traditional MOO\napproaches suffer from prohibitive O(n^2*d) complexity, where d represents the\nnumber of model parameters, typically in the billions for LLMs, rendering\ndirect optimization infeasible. PAMA reduces this complexity to O(n) where n is\nthe number of objectives, enabling optimization to be completed within\nmilliseconds. We provide theoretical guarantees that PAMA converges to a Pareto\nstationary point, where no objective can be improved without degrading at least\none other. Extensive experiments across language models ranging from 125M to 7B\nparameters demonstrate PAMA's robust and effective MOA capabilities, aligning\nwith its theoretical advantages. PAMA provides a highly efficient solution to\nthe MOA problem that was previously considered intractable, offering a\npractical and theoretically grounded approach to aligning LLMs with diverse\nhuman values, paving the way for versatile and adaptable real-world AI\ndeployments."
                },
                "authors": [
                    {
                        "name": "Qiang He"
                    },
                    {
                        "name": "Setareh Maghsudi"
                    }
                ],
                "author_detail": {
                    "name": "Setareh Maghsudi"
                },
                "author": "Setareh Maghsudi",
                "arxiv_comment": "Accepted at ECML/PKDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17702v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17702v3",
                "updated": "2025-08-11T08:47:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    47,
                    10,
                    0,
                    223,
                    0
                ],
                "published": "2025-07-23T17:10:23Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    10,
                    23,
                    2,
                    204,
                    0
                ],
                "title": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts\n  Language Models"
                },
                "summary": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels."
                },
                "authors": [
                    {
                        "name": "Changxin Tian"
                    },
                    {
                        "name": "Kunlong Chen"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17702v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17702v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17634v2",
                "updated": "2025-08-11T08:36:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    36,
                    31,
                    0,
                    223,
                    0
                ],
                "published": "2025-07-23T16:02:06Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    2,
                    6,
                    2,
                    204,
                    0
                ],
                "title": "WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM\n  Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM\n  Pre-training"
                },
                "summary": "Recent advances in learning rate (LR) scheduling have demonstrated the\neffectiveness of decay-free approaches that eliminate the traditional decay\nphase while maintaining competitive performance. Model merging techniques have\nemerged as particularly promising solutions in this domain. We present\nWarmup-Stable and Merge (WSM), a general framework that establishes a formal\nconnection between learning rate decay and model merging. WSM provides a\nunified theoretical foundation for emulating various decay strategies-including\ncosine decay, linear decay and inverse square root decay-as principled model\naveraging schemes, while remaining fully compatible with diverse optimization\nmethods. Through extensive experiments, we identify merge duration-the training\nwindow for checkpoint aggregation-as the most critical factor influencing model\nperformance, surpassing the importance of both checkpoint interval and merge\nquantity. Our framework consistently outperforms the widely-adopted\nWarmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving\nsignificant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on\nMMLU-Pro. The performance advantages extend to supervised fine-tuning\nscenarios, highlighting WSM's potential for long-term model refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in learning rate (LR) scheduling have demonstrated the\neffectiveness of decay-free approaches that eliminate the traditional decay\nphase while maintaining competitive performance. Model merging techniques have\nemerged as particularly promising solutions in this domain. We present\nWarmup-Stable and Merge (WSM), a general framework that establishes a formal\nconnection between learning rate decay and model merging. WSM provides a\nunified theoretical foundation for emulating various decay strategies-including\ncosine decay, linear decay and inverse square root decay-as principled model\naveraging schemes, while remaining fully compatible with diverse optimization\nmethods. Through extensive experiments, we identify merge duration-the training\nwindow for checkpoint aggregation-as the most critical factor influencing model\nperformance, surpassing the importance of both checkpoint interval and merge\nquantity. Our framework consistently outperforms the widely-adopted\nWarmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving\nsignificant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on\nMMLU-Pro. The performance advantages extend to supervised fine-tuning\nscenarios, highlighting WSM's potential for long-term model refinement."
                },
                "authors": [
                    {
                        "name": "Changxin Tian"
                    },
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Kunlong Chen"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Jiaxin Mao"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07753v1",
                "updated": "2025-08-11T08:34:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    34,
                    28,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T08:34:28Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    34,
                    28,
                    0,
                    223,
                    0
                ],
                "title": "Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in various\ntasks, yet they remain vulnerable to faithfulness hallucinations, where the\noutput does not align with the input. In this study, we investigate whether\nsocial bias contributes to these hallucinations, a causal relationship that has\nnot been explored. A key challenge is controlling confounders within the\ncontext, which complicates the isolation of causality between bias states and\nhallucinations. To address this, we utilize the Structural Causal Model (SCM)\nto establish and validate the causality and design bias interventions to\ncontrol confounders. In addition, we develop the Bias Intervention Dataset\n(BID), which includes various social biases, enabling precise measurement of\ncausal effects. Experiments on mainstream LLMs reveal that biases are\nsignificant causes of faithfulness hallucinations, and the effect of each bias\nstate differs in direction. We further analyze the scope of these causal\neffects across various models, specifically focusing on unfairness\nhallucinations, which are primarily targeted by social bias, revealing the\nsubtle yet significant causal effect of bias on hallucination generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in various\ntasks, yet they remain vulnerable to faithfulness hallucinations, where the\noutput does not align with the input. In this study, we investigate whether\nsocial bias contributes to these hallucinations, a causal relationship that has\nnot been explored. A key challenge is controlling confounders within the\ncontext, which complicates the isolation of causality between bias states and\nhallucinations. To address this, we utilize the Structural Causal Model (SCM)\nto establish and validate the causality and design bias interventions to\ncontrol confounders. In addition, we develop the Bias Intervention Dataset\n(BID), which includes various social biases, enabling precise measurement of\ncausal effects. Experiments on mainstream LLMs reveal that biases are\nsignificant causes of faithfulness hallucinations, and the effect of each bias\nstate differs in direction. We further analyze the scope of these causal\neffects across various models, specifically focusing on unfairness\nhallucinations, which are primarily targeted by social bias, revealing the\nsubtle yet significant causal effect of bias on hallucination generation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Zhang"
                    },
                    {
                        "name": "Junzhe Zhang"
                    },
                    {
                        "name": "Xinyu Hu"
                    },
                    {
                        "name": "HuiXuan Zhang"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "arxiv_comment": "Accepted by CIKM 2025 (Full Paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]